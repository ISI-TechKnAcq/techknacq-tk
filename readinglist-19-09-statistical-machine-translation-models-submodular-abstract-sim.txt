Before Submodular: 
56
acl-P08-2040 - {'title': 'Exploiting N-best Hypotheses for SMT Self-Enhancement', 'abstract': ['Boxing Chen, Min Zhang, Aiti Aw and Haizhou Li', 'Institute for Infocomm Research 21 Heng Mui Keng Terrace, 119613, Singapore {bxchen, mzhang, aaiti, hli}@i2r.a-star.edu.sg', 'Word and n-gram posterior probabilities estimated on N-best hypotheses have been used to improve the performance of statistical machine translation (SMT) in a rescoring framework.', 'In this paper, we extend the idea to estimate the posterior probabilities on N-best hypotheses for translation phrase-pairs, target language n-grams, and source word re-orderings.', 'The SMT system is self-enhanced with the posterior knowledge learned from N-best hypotheses in a re-decoding framework.', 'Experiments on NIST Chinese-to-English task show performance improvements for all the strategies.', 'Moreover, the combination of the three strategies achieves further improvements and outperforms the baseline by 0.67 BLEU score on NIST-2003 set, and 0.64 on NIST-2005 set, respectively.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P08-2040', 'authors': ['Boxing Chen', 'Min Zhang', 'Aiti Aw', 'Haizhou Li'], 'year': 2008, 'book': 'Annual Meeting of the Association for Computational Linguistics', 'id': 'acl-P08-2040', 'score': 0.5261784134372253}
acl-P00-1056 - {'title': 'Improved Statistical Alignment Models', 'abstract': ['In this paper, we present and compare various single-word based alignment models for statistical machine translation.', 'We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications.', 'We present different methods to combine alignments.', 'As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.', 'We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P00-1056', 'authors': ['Franz Josef Och', 'Hermann Ney'], 'year': 2000, 'book': 'Annual Meeting of the Association for Computational Linguistics', 'id': 'acl-P00-1056', 'score': 0.767028756136451}
acl-P14-2095 - {'title': 'Cross-lingual Model Transfer Using Feature Representation Projection', 'abstract': ['Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579?585, Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics Cross-lingual Model Transfer Using Feature Representation Projection Mikhail Kozhevnikov MMCI, University of Saarland Saarbr?ucken, Germany mkozhevn@mmci.uni-saarland.de Ivan Titov ILLC, University of Amsterdam Amsterdam, Netherlands titov@uva.nl', 'Abstract', 'We propose a novel approach to cross-lingual model transfer based on feature representation projection.', 'First, a compact feature representation relevant for the task in question is constructed for either language independently and then the mapping between the two representations is determined using parallel data.', 'The target instance can then be mapped into the source-side feature representation using the derived mapping and handled directly by the source-side model.', 'This approach displays competitive performance on model transfer for semantic role labeling when compared to direct model transfer and annotation projection and suggests interesting directions for further research.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P14-2095', 'authors': ['Mikhail Kozhevnikov', 'Ivan Titov'], 'year': 2014, 'book': 'ACL', 'id': 'acl-P14-2095', 'score': 0.44787132327976686}
acl-W09-2413 - {'title': 'SemEval-2010 Task 3: Cross-lingual Word Sense Disambiguation', 'abstract': ['Els Lefever1,2 and Veronique Hoste', 'LT3, Language and Translation Technology Team, University College Ghent', 'Groot-Brittannielaan 45, 9000 Gent, Belgium Department of Applied Mathematics and Computer Science, Ghent University Krijgslaan 281 (S9), 9000 Gent, Belgium', '(Els.Lefever, Veronique.Hoste}@hogent.be', 'We propose a multilingual unsupervised Word Sense Disambiguation (WSD) task for a sample of English nouns.', 'Instead of providing manually sense-tagged examples for each sense of a polysemous noun, our sense inventory is built up on the basis of the Europarl parallel corpus.', 'The multilingual setup involves the translations of a given English polyse-mous noun in five supported languages, viz. Dutch, French, German, Spanish and Italian.', 'The task targets the following goals: (a) the manual creation of a multilingual sense inventory for a lexical sample of English nouns and (b) the evaluation of systems on their ability to disambiguate new occurrences of the selected polysemous nouns.', 'For the creation of the hand-tagged gold standard, all translations of a given polysemous English noun are retrieved in the five languages and clustered by meaning.', 'Systems can participate in 5 bilingual evaluation subtasks (English - Dutch, English - German, etc.)'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W09-2413', 'authors': ['Els Lefever', 'Véronique Hoste'], 'year': 2009, 'book': 'Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009)', 'id': 'acl-W09-2413', 'score': 0.4206817456456919}
acl-P08-2038 - {'title': 'A Linguistically Annotated Reordering Model for BTG-based Statistical Machine Translation', 'abstract': ['In this paper, we propose a linguistically annotated reordering model for BTG-based statistical machine translation.', 'The model incorporates linguistic knowledge to predict orders for both syntactic and non-syntactic phrases.', 'The linguistic knowledge is automatically learned from source-side parse trees through an annotation algorithm.', 'We empirically demonstrate that the proposed model leads to a significant improvement of 1.55% in the BLEU score over the baseline reordering model on the NIST MT-05 Chinese-to-English translation task.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P08-2038', 'authors': ['Deyi Xiong', 'Min Zhang', 'Aiti Aw', 'Haizhou Li'], 'year': 2008, 'book': 'Annual Meeting of the Association for Computational Linguistics', 'id': 'acl-P08-2038', 'score': 0.6368380145476898}
acl-J10-3009 - {'title': 'Linguistically Annotated Reordering: Evaluation and Analysis', 'abstract': ['Linguistic knowledge plays an important role in phrase movement in statistical machine translation.', 'To efficiently incorporate linguistic knowledge into phrase reordering, we propose a new approach: Linguistically Annotated Reordering (LAR).', 'In LAR, we build hard hierarchical skeletons and inject soft linguistic knowledge from source parse trees to nodes ofhard skeletons during translation.', 'The experimental results on large-scale training data show that LAR is comparable to boundary word-based reordering (BWR) (Xiong, Liu, and Lin 2006), which is a very competitive lexicalized reordering approach.', 'When combined with BWR, LAR provides complementary information for phrase reordering, which collectively improves the BLEU score significantly.', 'To further understand the contribution oflinguistic knowledge in LAR to phrase reordering, we introduce a syntax-based analysis method to automatically detect constituent movement in both reference and system translations, and summarize syntactic reordering patterns that are captured by reordering models.', 'With the proposed analysis method, we conduct a comparative analysis that not only provides the insight into how linguistic knowledge affects phrase movement but also reveals new challenges in phrase reordering.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/J10-3009', 'authors': ['Deyi Xiong', 'Min Zhang', 'Aiti Aw', 'Haizhou Li'], 'year': 2010, 'book': 'Computational Linguistics', 'id': 'acl-J10-3009', 'score': 0.5665052485545983}
acl-C94-2175 - {'title': 'Bilingual Text, Matching Using Bilingual Dictionary and Statistics', 'abstract': ['This paper describes a unified framework for bilingual text matching by combining existing handwritten bilingual dictionaries and statistical techniques.', 'The process of bilingual text matching consists of two major steps: sentence alignment and structural matching of bilingual sentences.', 'Statistical techniques are applied to estimate word correspondences not included in bilingual dictionaries.', 'Estimated word correspondences are useful for improving both sentence alignment and structural matching.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/C94-2175', 'authors': ['Takehito Utsuro', 'Hiroshi Ikeda', 'Masaya Yamane', 'Yuji Matsumoto', 'Makoto Nagao'], 'year': 1994, 'book': 'International Conference on Computational Linguistics', 'id': 'acl-C94-2175', 'score': 0.5927511131437411}
acl-W04-1121 - {'title': 'Aligning Bilingual Corpora Using Sentences Location Information', 'abstract': ['Large amounts of bilingual resource on the Internet provide us with the probability of building a large scale of bilingual corpus.', 'The irregular characteristics of the real texts, especially without the strictly aligned paragraph boundaries, bring a challenge to alignment technology.', 'The traditional alignment methods have some difficulties in competency for doing this.', 'This paper describes a new method for aligning real bilingual texts using sentence pair location information.', 'The model was motivated by the observation that the location of a sentence pair with certain length is distributed in the whole text similarly.', 'It uses (1:1) sentence beads instead of high frequency words as the candidate anchors.', 'The method was developed and evaluated through many different test data.', 'The results show that it can achieve good aligned performance and be robust and language independent.', 'It can resolve the alignment problem on real bilingual text.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W04-1121', 'authors': ['Weigang Li', 'Ting Liu', 'Zhen Wang', 'Sheng Li'], 'year': 2004, 'book': 'Workshop on Automatic Alignment and Extraction of Bilingual Domain Ontology for Medical Domain Web Search', 'id': 'acl-W04-1121', 'score': 0.5317585015077735}
acl-J13-3001 - {'title': 'Squibs: What is a Paraphrase?', 'abstract': ['Paraphrases are sentences or phrases that convey the same meaning using different wording.', 'Although the logical definition of paraphrases requires strict semantic equivalence, linguistics accepts a broader, approximate, equivalence?thereby allowing far more examples of ?quasi-paraphrase.?', 'But approximate equivalence is hard to define.', 'Thus, the phenomenon of paraphrases, as understood in linguistics, is difficult to characterize.', 'In this article, we list a set of 25 operations that generate quasi-paraphrases.', 'We then empirically validate the scope and accuracy of this list by manually analyzing random samples of two publicly available paraphrase corpora.', 'We provide the distribution of naturally occurring quasi-paraphrases in English text.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/J13-3001', 'authors': ['Rahul Bhagat', 'Eduard Hovy'], 'year': 2013, 'book': 'Computational Linguistics', 'id': 'acl-J13-3001', 'score': 0.6501073523435825}
acl-C10-4001 - {'title': 'Paraphrases and Applications', 'abstract': ['Shiqi Zhao Haifeng Wang', 'Baidu, Inc. Baidu, Inc.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/C10-4001', 'authors': ['Shiqi Zhao', 'Haifeng Wang'], 'year': 2010, 'book': 'COLING – Tutorial Notes', 'id': 'acl-C10-4001', 'score': 0.5877299308358976}
acl-W11-2103 - {'title': 'Findings of the 2011 Workshop on Statistical Machine Translation', 'abstract': ['Chris Callison-Burch Philipp Koehn', 'Center for Language and Speech Processing School of Informatics', 'Johns Hopkins University University of Edinburgh', 'Informatics Institute University of Amsterdam', 'Center for Language and Speech Processing Johns Hopkins University', 'This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics.', 'We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries.', 'We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics.', 'This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake.', "We also conducted a pilot 'tunable metrics' task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality."], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W11-2103', 'authors': ['Chris Callison-Burch', 'Philipp Koehn', 'Christof Monz', 'Omar F. Zaidan'], 'year': 2011, 'book': 'Proceedings of the Sixth Workshop on Statistical Machine Translation', 'id': 'acl-W11-2103', 'score': 0.7275698227756415}
acl-W10-1703 - {'title': 'Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation', 'abstract': ['Chris Callison-Burch Philipp Koehn Christof Monz', 'Johns Hopkins University University of Edinburgh University of Amsterdam ccb@cs.jhu.edu pkoehn@inf.ed.ac.uk c.monz@uva.nl', 'Kay Peterson and Mark Przybocki Omar F. Zaidan', 'This paper presents the results of the WMT10 and MetricsMATR10 shared tasks, which included a translation task, a system combination task, and an evaluation task.', 'We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries.', 'We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics.', "This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon's Mechanical Turk."], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W10-1703', 'authors': ['Chris Callison-Burch', 'Philipp Koehn', 'Christof Monz', 'Kay Peterson', 'Mark A. Przybocki', 'Omar F. Zaidan'], 'year': 2010, 'book': 'Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR', 'id': 'acl-W10-1703', 'score': 0.6678447745451398}
acl-P12-2007 - {'title': 'Head-Driven Hierarchical Phrase-based Translation', 'abstract': ["This paper presents an extension of Chiang's hierarchical phrase-based (HPB) model, called Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space.", "Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang's model with average gains of 1.91 points absolute in BLEU."], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P12-2007', 'authors': ['Junhui Li', 'Zhaopeng Tu', 'Guodong Zhou', 'Josef van Genabith'], 'year': 2012, 'book': 'ACL', 'id': 'acl-P12-2007', 'score': 0.7662520728002502}
acl-P09-1063 - {'title': 'Improving Tree-to-Tree Translation with Packed Forests', 'abstract': ['Yang Liu and Yajuan Lii and Qun Liu', 'Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences P.O.', 'Box 2704, Beijing 100190, China (yliu,lvyajuan,liuqun}@ict.ac.cn', 'Current tree-to-tree models suffer from parsing errors as they usually use only 1-best parses for rule extraction and decoding.', 'We instead propose a forest-based tree-to-tree model that uses packed forests.', 'The model is based on a probabilistic synchronous tree substitution grammar (STSG), which can be learned from aligned forest pairs automatically.', 'The decoder finds ways of decomposing trees in the source forest into elementary trees using the source projection of STSG while building target forest in parallel.', 'Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P09-1063', 'authors': ['Yang Liu', 'Yajuan Lü', 'Qun Liu'], 'year': 2009, 'book': 'ACL-IJCNLP', 'id': 'acl-P09-1063', 'score': 0.7290738296043581}
acl-C04-1032 - {'title': 'Symmetric Word Alignments for Statistical Machine Translation', 'abstract': ['In this paper, we address the word alignment problem for statistical machine translation.', 'We aim at creating a symmetric word alignment allowing for reliable one-to-many and many-to-one word relationships.', 'We perform the iterative alignment training in the source-to-target and the target-to-source direction with the well-known IBM and HMM alignment models.', 'Using these models, we robustly estimate the local costs of aligning a source word and a target word in each sentence pair.', 'Then, we use efficient graph algorithms to determine the symmetric alignment with minimal total costs (i. e. maximal alignment probability).', 'We evaluate the automatic alignments created in this way on the German–English Verb-mobil task and the French–English Canadian Hansards task.', 'We show statistically significant improvements of the alignment quality compared to the best results reported so far.', 'On the Verbmobil task, we achieve an improvement of more than 1% absolute over the baseline error rate of 4.7%.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/C04-1032', 'authors': ['Evgeny Matusov', 'Richard Zens', 'Hermann Ney'], 'year': 2004, 'book': 'International Conference on Computational Linguistics', 'id': 'acl-C04-1032', 'score': 0.720495037846413}
acl-C04-1045 - {'title': 'Improving Word Alignment Quality Using Morpho-Syntactic Information', 'abstract': ['In this paper, we present an approach to include morpho-syntactic dependencies into the training of the statistical alignment models.', 'Existing statistical translation systems usually treat different derivations of the same base form as they were independent of each other.', 'We propose a method which explicitly takes into account such interdependencies during the EM training of the statistical alignment models.', 'The evaluation is done by comparing the obtained Viterbi alignments with a manually annotated reference alignment.', 'The improvements of the alignment quality compared to the, to our knowledge, best system are reported on the German-English Verbmobil corpus.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/C04-1045', 'authors': ['Hermann Ney', 'Maja Popović'], 'year': 2004, 'book': 'International Conference on Computational Linguistics', 'id': 'acl-C04-1045', 'score': 0.6975033529905635}
acl-W12-3147 - {'title': 'LIUM’s SMT Machine Translation Systems for WMT 2012', 'abstract': ['This paper describes the development of French?English and English?French statistical machine translation systems for the 2012 WMT shared task evaluation.', 'We developed phrase-based systems based on the Moses decoder, trained on the provided data only.', 'Additionally, new features this year included improved language and translation model adaptation using the cross-entropy score for the corpus selection.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W12-3147', 'authors': ['Christophe Servan', 'Patrik Lambert', 'Anthony Rousseau', 'Holger Schwenk', 'Loïc Barrault'], 'year': 2012, 'book': 'Proceedings of the Seventh Workshop on Statistical Machine Translation', 'id': 'acl-W12-3147', 'score': 0.62039405477476}
acl-I08-1042 - {'title': 'Heterogeneous Automatic MT Evaluation Through Non-Parametric Metric Combinations', 'abstract': ['Jesus Gimenez and Lluis Märquez', 'Combining different metrics into a single measure of quality seems the most direct and natural way to improve over the quality of individual metrics.', 'Recently, several approaches have been suggested (Kulesza and Shieber, 2004; Liu and Gildea, 2007; Albrecht and Hwa, 2007a).', 'Although based on different assumptions, these approaches share the common characteristic of being parametric.', 'Their models involve a number of parameters whose weight must be adjusted.', 'As an alternative, in this work, we study the behaviour of non-parametric schemes, in which metrics are combined without having to adjust their relative importance.', 'Besides, rather than limiting to the lexical dimension, we work on a wide set of metrics operating at different linguistic levels (e.g., lexical, syntactic and semantic).', 'Experimental results show that non-parametric methods are a valid means of putting different quality dimensions together, thus tracing a possible path towards heterogeneous automatic MT evaluation.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/I08-1042', 'authors': ['Jesús Giménez', 'Lluís Màrquez'], 'year': 2008, 'book': 'Proceedings of the Third International Joint Conference on Natural Language Processing', 'id': 'acl-I08-1042', 'score': 0.7490549476435829}
acl-W12-3107 - {'title': 'SPEDE: Probabilistic Edit Distance Metrics for MT Evaluation', 'abstract': ["This paper describes Stanford University's submission to the Shared Evaluation Task of WMT 2012.", 'Our proposed metric (SPEDE) computes probabilistic edit distance as predictions of translation quality.', 'We learn weighted edit distance in a probabilistic finite state machine (pFSM) model, where state transitions correspond to edit operations.', 'While standard edit distance models cannot capture long-distance word swapping or cross alignments, we rectify these shortcomings using a novel pushdown automaton extension of the pFSM model.', 'Our models are trained in a regression framework, and can easily incorporate a rich set of linguistic features.', 'Evaluated on two different prediction tasks across a diverse set of datasets, our methods achieve state-of-the-art correlation with human judgments.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W12-3107', 'authors': ['Mengqiu Wang', 'Christopher Manning'], 'year': 2012, 'book': 'Proceedings of the Seventh Workshop on Statistical Machine Translation', 'id': 'acl-W12-3107', 'score': 0.7397981177652936}
acl-W11-2158 - {'title': 'LIUM’s SMT Machine Translation Systems for WMT 2011', 'abstract': ["LIUM's SMT Machine Translation Systems for WMT 2011", 'Holger Schwenk, Patrik Lambert, Loïc Barrault, Christophe Servan, Haithem Afli, Sadaf Abdul-Rauf and Kashif Shah', 'LIUM, University of Le Mans 72085 Le Mans cedex 9, FRANCE FirstName.LastName@lium.univ-lemans.fr', 'Abstract 2 Resources Used', 'This paper describes the development of French-English and English-French statistical machine translation systems for the 2011 WMT shared task evaluation.', 'Our main systems were standard phrase-based statistical systems based on the Moses decoder, trained on the provided data only, but we also performed initial experiments with hierarchical systems.', 'Additional, new features this year include improved translation model adaptation using monolingual data, a continuous space language model and the treatment of unknown words.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W11-2158', 'authors': ['Holger Schwenk', 'Patrik Lambert', 'Loïc Barrault', 'Christophe Servan', 'Sadaf Abdul-Rauf', 'Haithem Afli', 'Kashif Shah'], 'year': 2011, 'book': 'Proceedings of the Sixth Workshop on Statistical Machine Translation', 'id': 'acl-W11-2158', 'score': 0.6155744999697585}
acl-W10-1716 - {'title': 'LIUM SMT Machine Translation System for WMT 2010', 'abstract': ['Patrik Lambert, Sadaf Abdul-Rauf and Holger Schwenk', 'LIUM, University of Le Mans 72085 Le Mans cedex 9, FRANCE FirstName.LastName@lium.univ-lemans.fr', 'This paper describes the development of French-English and English-French machine translation systems for the 2010 WMT shared task evaluation.', 'These systems were standard phrase-based statistical systems based on the Moses decoder, trained on the provided data only.', 'Most of our efforts were devoted to the choice and extraction of bilingual data used for training.', 'We filtered out some bilingual corpora and pruned the phrase table.', 'We also investigated the impact of adding two types of additional bilingual texts, extracted automatically from the available monolingual data.', 'We first collected bilingual data by performing automatic translations of monolingual texts.', 'The second type of bilingual text was harvested from comparable corpora with Information Retrieval techniques.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W10-1716', 'authors': ['Patrik Lambert', 'Sadaf Abdul-Rauf', 'Holger Schwenk'], 'year': 2010, 'book': 'Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR', 'id': 'acl-W10-1716', 'score': 0.6131327846717648}
acl-H93-1001 - {'title': 'Overview of the ARPA Human Language Technology Workshop', 'abstract': ['For five years, 1988-1992, the Defense Advanced Projects Agency sponsored a series of meetings called the DARPA Speech and Natural Language Workshops.', 'These workshops provided a forum where researchers in speech and natural language, particularly as relating to the DARPA programs in spoken and written language understanding, could exchange information about recent research and technical progress.', 'Participants included researchers funded under the DARPA programs, other researchers who voluntarily participated in these programs or in related evaluations, government researchers and consumers of these research results, and invited attendees from inside and outside the US.', 'Proceedings of these workshops were published by Morgan Kaufmann.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/H93-1001', 'authors': ['Madeleine Bates'], 'year': 1993, 'book': 'Human Language Technology Conference', 'id': 'acl-H93-1001', 'score': 0.6896844984773962}
acl-W00-1429 - {'title': 'Knowledge Acquisition for Natural Language Generation', 'abstract': ['We describe the knowledge acquisition (KA) techniques used to build the STOP system, especially sorting and think-aloud protocols.', 'That is, we describe the ways in which we interacted with domain experts to determine appropriate user categories, schemas, detailed content rules, and so forth for STOP.', 'Informal evaluations of these techniques suggest that they had some benefit, but perhaps were most successful as a source of insight and hypotheses, and should ideally have been supplemented by other techniques when deciding on the specific rules and knowledge incorporated into STOP.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W00-1429', 'authors': ['Ehud Reiter', 'Roma Robertson', 'Liesl Osman'], 'year': 2000, 'book': 'International Conference on Natural Language Generation', 'id': 'acl-W00-1429', 'score': 0.5257659543297751}
acl-P01-1057 - {'title': 'Using a Randomised Controlled Clinical Trial to Evaluate an NLG System', 'abstract': ['The STOP system, which generates personalised smoking-cessation letters, was evaluated by a randomised controlled clinical trial.', 'We believe this is the largest and perhaps most rigorous task effectiveness evaluation ever performed on an NLG system.', 'The detailed results of the clinical trial have been presented elsewhere, in the medical literature.', 'In this paper we discuss the clinical trial itself: its structure and cost, what we did and did not learn from it (especially considering that the trial showed that STOP was not effective), and how it compares to other NLG evaluation techniques.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P01-1057', 'authors': ['Ehud Reiter', 'Roma Robertson', 'A. Scott Lennox', 'Liesl Osman'], 'year': 2001, 'book': 'Annual Meeting of the Association for Computational Linguistics', 'id': 'acl-P01-1057', 'score': 0.519462531022038}
acl-C08-2019 - {'title': 'Using Very Simple Statistics for Review Search: An Exploration', 'abstract': ['We report on work in progress on using very simple statistics in an unsupervised fashion to re-rank search engine results when review-oriented queries are issued; the goal is to bring opinionated or subjective results to the top of the results list.', 'We find that our proposed technique performs comparably to methods that rely on sophisticated pre-encoded linguistic knowledge, and that both substantially improve the initial results produced by the Yahoo!', 'search engine.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/C08-2019', 'authors': ['Bo Pang', 'Lillian Lee'], 'year': 2008, 'book': 'COLING – Posters', 'id': 'acl-C08-2019', 'score': 0.1716554896314715}
acl-P12-2018 - {'title': 'Baselines and Bigrams: Simple, Good Sentiment and Topic Classification', 'abstract': ['Variants of Naive Bayes (NB) and Support Vector Machines (SVM) are often used as baseline methods for text classification, but their performance varies greatly depending on the model variant, features used and task/ dataset.', 'We show that: (i) the inclusion of word bigram features gives consistent gains on sentiment analysis tasks; (ii) for short snippet sentiment tasks, NB actually does better than SVMs (while for longer documents the opposite result holds); (iii) a simple but novel SVM variant using NB log-count ratios as feature values consistently performs well across tasks and datasets.', 'Based on these observations, we identify simple NB and SVM variants which outperform most published results on sentiment analysis datasets, sometimes providing a new state-of-the-art performance level.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P12-2018', 'authors': ['Sida Wang', 'Christopher Manning'], 'year': 2012, 'book': 'ACL', 'id': 'acl-P12-2018', 'score': 0.1673894466436222}
acl-W12-3212 - {'title': 'Combining OCR Outputs for Logical Document Structure Markup. Technical Background to the ACL 2012 Contributed Task', 'abstract': ['We describe how paperXML, a logical document structure markup for scholarly articles, is generated on the basis of OCR tool outputs.', 'PaperXML has been initially developed for the ACL Anthology Searchbench.', 'The main purpose was to robustly provide uniform access to sentences in ACL Anthology papers from the past 46 years, ranging from scanned, typewriter-written conference and workshop proceedings papers, up to recent high-quality typeset, born-digital journal articles, with varying layouts.', 'PaperXML markup includes information on page and paragraph breaks, section headings, footnotes, tables, captions, boldface and italics character styles as well as bibliographic and publication meta-data.', 'The role of paperXML in the ACL Contributed Task Rediscovering 50 Years of Discoveries is to serve as fall-back source (1) for older, scanned papers (mostly published before the year 2000), for which born-digital PDF sources are not available, (2) for born-digital PDF papers on which the PDFExtract method failed, (3) for document parts where PDFExtract does not output useful markup such as currently for tables.', "We sketch transformation of paperXML into the ACL Contributed Task's TEI P5 XML."], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W12-3212', 'authors': ['Ulrich Schäfer', 'Benjamin Weitz'], 'year': 2012, 'book': 'Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries', 'id': 'acl-W12-3212', 'score': 0.6676956381545492}
acl-W06-1613 - {'title': 'Automatic Classification of Citation Function', 'abstract': ['Citation function is defined as the author’s reason for citing a given paper (e.g. acknowledgement of the use of the cited method).', 'The automatic recognition of the rhetorical function of citations in scientific text has many applications, from improvement of impact factor calculations to text summarisation and more informative citation indexers.', 'We show that our annotation scheme for citation function is reliable, and present a supervised machine learning framework to automatically classify citation function, using both shallow and linguistically-inspired features.', 'We find, amongst other things, a strong relationship between citation function and sentiment classification.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W06-1613', 'authors': ['Simone Teufel', 'Advaith Siddharthan', 'Dan Tidhar'], 'year': 2006, 'book': 'Conference on Empirical Methods in Natural Language Processing', 'id': 'acl-W06-1613', 'score': 0.6095497865399048}
acl-H93-1047 - {'title': 'Automatic Grammar Induction and Parsing Free Text: A Transformation-Based Approach', 'abstract': ['In this paper we describe a new technique for parsing free text: a transformational grammar/ is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled.', 'The algorithm works by beginning in a very naive state of knowledge about phrase structure.', 'By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error.', 'After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/H93-1047', 'authors': ['Eric Brill'], 'year': 1993, 'book': 'Human Language Technology Conference', 'id': 'acl-H93-1047', 'score': 0.5671820190796741}
acl-C00-1074 - {'title': 'Hybrid Neuro and Rule-Based Part of Speech Taggers', 'abstract': ['A hybrid system for tagging part of speech is described that consists of a neuro tagger and a rule-based corrector.', 'The neuro tagger is an initial-state annotator that uses different lengths of contexts based on longest context priority.', 'its inputs are weighted by information gains that are obtained by information maximization.', 'The rule-based corrector is constructed by a set of transformation rules to make up for the shortcomings of the neuro tagger.', 'Computer experiments show that almost 20% of the errors made by the neuro tagger are corrected by these transformation rules, so that the hybrid system can reach an accuracy of 95.5% counting only flue ambiguous words and 99.1% counting all words when a small Thai corpus with 22,311 ambiguous words is used for training.', 'This accuracy is far higher than that using an 11MM and is also higher than that using a rule-based model.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/C00-1074', 'authors': ['Qing Ma', 'Masaki Murata', 'Kiyotaka Uchimoto', 'Hitoshi Isahara'], 'year': 2000, 'book': 'International Conference on Computational Linguistics', 'id': 'acl-C00-1074', 'score': 0.5018102050885659}
acl-P14-2010 - {'title': 'Sprinkling Topics for Weakly Supervised Text Classification', 'abstract': ['aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa References'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P14-2010', 'authors': ['Swapnil Hingmire', 'Sutanu Chakraborti'], 'year': 2014, 'book': 'ACL', 'id': 'acl-P14-2010', 'score': 0.9969348779647177}
acl-P14-2015 - {'title': "Entities' Sentiment Relevance", 'abstract': ['aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa References'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P14-2015', 'authors': ['Zvi Ben-Ami', 'Ronen Feldman', 'Binyamin Rosenfeld'], 'year': 2014, 'book': 'ACL', 'id': 'acl-P14-2015', 'score': 0.992911134267781}
acl-C10-3013 - {'title': 'Cloud Computing for Linguists', 'abstract': ['Dorothée Beermann Pavel Mihaylov', 'Norwegian University of Science and Technology Ontotext', 'The system presented is a web application designed to aid linguistic research with data collection and online publishing.', 'It is a service mainly for linguists and language experts working with language description of less-documented and less-resourced languages.', 'When the central concern is in-depth linguistic analysis, maintaining and administering software can be a burden.', 'Cloud computing offers an alternative.', 'At present mainly used for archiving, we extend linguistic web applications to allow creation, search and storage of interlinear annotated texts.', 'By combining a conceptually appealing online glosser with an SQL database and a wiki, we make the online publication of linguistic data an easy task also for non-computationally oriented researchers.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/C10-3013', 'authors': ['Dorothee Beermann', 'Pavel Mihaylov'], 'year': 2010, 'book': 'COLING – Demos', 'id': 'acl-C10-3013', 'score': 0.5370970028311163}
acl-P02-1022 - {'title': 'GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications', 'abstract': ['In this paper we present GATE, a framework and graphical development environment which enables users to develop and deploy language engineering components and resources in a robust fashion.', 'The GATE architecture has enabled us not only to develop a number of successful applications for various language processing tasks (such as Information Extraction), but also to build and annotate corpora and carry out evaluations on the applications generated.', 'The framework can be used to develop applications and resources in multiple languages, based on its thorough Unicode support.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P02-1022', 'authors': ['Hamish Cunningham', 'Diana Maynard', 'Kalina Bontcheva', 'Valentin Tablan'], 'year': 2002, 'book': 'Annual Meeting of the Association for Computational Linguistics', 'id': 'acl-P02-1022', 'score': 0.5053581577856099}
acl-W02-0112 - {'title': 'Teaching Computational Linguistics at the University of Tartu: Experience, Perspectives and Challenges', 'abstract': ['The paper gives a review of teaching Computational Linguistics (CL) at the University of Tartu.', 'The current curriculum foresees the possibility of studying CL as an independent 4-year subject in the Faculty of Philosophy on the bachelor stage.', 'In connection with the higher education reform in Estonia, new curricula will be introduced from the next study year where the 3-year bachelor stage will be followed by a 2 year master’s stage.', 'It will then be possible to study CL proceeding from two paths: in the Faculty of Philosophy, and additionally also in the Faculty of Mathematics and Computer Science.', 'This way two types of specialists will be trained who will hopefully be able to complement each other in teamwork.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W02-0112', 'authors': ['Mare Koit', 'Tiit Roosmaa', 'Haldur Õim'], 'year': 2002, 'book': 'Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics', 'id': 'acl-W02-0112', 'score': 0.6840342841769208}
acl-W02-0106 - {'title': 'Design and Evolution of a Language Technologies Curriculum', 'abstract': ['The Language Technologies Institute (LTI) of the School of Computer Science at Carnegie Mellon University is one of the largest programs of its kind.', 'We present here the initial design and subsequent evolution of our MS and PhD programs in Language Technologies.', 'The motivations for the design and evolution are also presented.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W02-0106', 'authors': ['Robert E. Frederking', 'Eric H. Nyberg', 'Teruko Mitamura', 'Jaime G. Carbonnell'], 'year': 2002, 'book': 'Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics', 'id': 'acl-W02-0106', 'score': 0.6798698036644222}
acl-A97-1056 - {'title': 'Sequential Model Selection for Word Sense Disambiguation', 'abstract': ['Statistical models of word–sense disambiguation are often based on a small number of contextual features or on a model that is assumed to characterize the interactions among a set of features.', 'Model selection is presented as an alternative to these approaches, where a sequential search of possible models is conducted in order to find the model that best characterizes the interactions among features.', 'This paper expands existing model selection methodology and presents the first comparative study of model selection search strategies and evaluation criteria when applied to the problem of building probabilistic classifiers for word–sense disambiguation.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/A97-1056', 'authors': ['Ted Pedersen', 'Rebecca F. Bruce', 'Janyce Wiebe'], 'year': 1997, 'book': 'Applied Natural Language Processing Conference', 'id': 'acl-A97-1056', 'score': 0.3537152848989457}
acl-J14-3004 - {'title': 'Feature-Frequency–Adaptive On-line Training for Fast and Accurate Natural Language Processing', 'abstract': ['Feature-Frequency?Adaptive On-line Training for Fast and Accurate Natural Language Processing Xu Sun?', 'Peking University Wenjie Li??', 'Hong Kong Polytechnic University Houfeng Wang?', 'Peking University Qin Lu?', 'Hong Kong Polytechnic University Training speed and accuracy are two major concerns of large-scale natural language processing systems.', 'Typically, we need to make a tradeoff between speed and accuracy.', 'It is trivial to improve the training speed via sacrificing accuracy or to improve the accuracy via sacrificing speed.', 'Nevertheless, it is nontrivial to improve the training speed and the accuracy at the same time, which is the target of this work.', 'To reach this target, we present a new training method, feature-frequency?adaptive on-line training, for fast and accurate training of natural language processing systems.', 'It is based on the core idea that higher frequency features should have a learning rate that decays faster.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/J14-3004', 'authors': ['Xu Sun', 'Wenjie Li', 'Houfeng Wang', 'Qin Lu'], 'year': 2014, 'book': 'CL', 'id': 'acl-J14-3004', 'score': 0.6796828956026063}
acl-P09-1054 - {'title': 'Stochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty', 'abstract': ['Stochastic Gradient Descent Training for Ll-regularized Log-linear Models with Cumulative Penalty', 'Yoshimasa Tsuruoka"^ Jun\'ichi Tsujii^* Sophia Ananiadou"^', 't School of Computer Science, University of Manchester, UK', 'Stochastic gradient descent (SGD) uses approximate gradients estimated from subsets of the training data and updates the parameters in an online fashion.', 'This learning framework is attractive because it often requires much less training time in practice than batch training algorithms.', 'However, L1-regularization, which is becoming popular in natural language processing because of its ability to produce compact models, cannot be efficiently applied in SGD training, due to the large dimensions of feature vectors and the fluctuations of approximate gradients.', 'We present a simple method to solve these problems by penalizing the weights according to cumulative values for L1 penalty.', 'We evaluate the effectiveness of our method in three applications: text chunking, named entity recognition, and part-of-speech tagging.', 'Experimental results demonstrate that our method can produce compact and accurate models much more quickly than a state-of-the-art quasiNewton method for L1-regularized loglinear models.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P09-1054', 'authors': ['Yoshimasa Tsuruoka', "Jun'ichi Tsujii", 'Sophia Ananiadou'], 'year': 2009, 'book': 'ACL-IJCNLP', 'id': 'acl-P09-1054', 'score': 0.5636729215407645}
acl-W96-0210 - {'title': 'The Measure of a Model', 'abstract': ['This paper describes measures for evaluating the three determinants of how well a probabilistic classifier performs on a given test set.', 'These determinants are the appropriateness, for the test set, of the results of (1) feature selection, (2) formulation of the parametric form of the model, and (3) parameter estimation.', 'These are part of any model formulation procedure, even if not broken out as separate steps, so the tradeoffs explored in this paper are relevant to a wide variety of methods.', 'The measures are demonstrated in a large experiment, in which they are used to analyze the results of roughly 300 classifiers that perform word-sense disambiguation.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W96-0210', 'authors': ['Rebecca F. Bruce', 'Janyce Wiebe', 'Ted Pedersen'], 'year': 1996, 'book': 'Conference on Empirical Methods in Natural Language Processing', 'id': 'acl-W96-0210', 'score': 0.3205780000583197}
acl-J99-2002 - {'title': 'Decomposable Modeling in Natural Language Processing', 'abstract': ['at Asheville In this paper, we describe a framework for developing probabilistic classifiers in natural language processing.', 'Our focus is on formulating models that capture the most important interdependencies among features, to avoid overfitting the data while also characterizing the data well.', 'The class of probability models and the associated inference techniques described here were developed in mathematical statistics, and are widely used in artificial intelligence and applied statistics.', 'Our goal is to make this model selection framework accessible to researchers in NLP, and provide pointers to available software and important references.', 'In addition, we describe how the quality of the three determinants of classifier performance (the features, the form of the model, and the parameter estimates) can be separately evaluated.', 'We also demonstrate the classification performance of these models in a large-scale experiment involving the disambiguation of 34 words taken from the HECTOR word sense corpus (Hanks 1996).', 'In 10-fold cross-validations, the model search procedure performs significantly better than naive Bayes on 6 of the words without being significantly worse on any of them.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/J99-2002', 'authors': ['Rebecca F. Bruce', 'Janyce Wiebe'], 'year': 1999, 'book': 'Computational Linguistics', 'id': 'acl-J99-2002', 'score': 0.26524946213472506}
acl-P09-2060 - {'title': 'Toward Smaller, Faster, and Better Hierarchical Phrase-based SMT', 'abstract': ["We investigate the use of Fisher's exact significance test for pruning the translation table of a hierarchical phrase-based statistical machine translation system.", "In addition to the significance values computed by Fisher's exact test, we introduce compositional properties to classify phrase pairs of same significance values.", 'We also examine the impact of using significance values as a feature in translation models.', 'Experimental results show that 1% to 2% BLEU improvements can be achieved along with substantial model size reduction in an Iraqi/English two-way translation task.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P09-2060', 'authors': ['Mei Yang', 'Jing Zheng'], 'year': 2009, 'book': 'ACL-IJCNLP: Short Papers', 'id': 'acl-P09-2060', 'score': 0.37934743680956795}
acl-C10-1056 - {'title': 'Feature-Rich Discriminative Phrase Rescoring for SMT', 'abstract': ['Fei Huang and Bing Xiang', 'IBM T. J. Watson Research Center', 'This paper proposes a new approach to phrase rescoring for statistical machine translation (SMT).', 'A set of novel features capturing the translingual equivalence between a source and a target phrase pair are introduced.', 'These features are combined with linear regression model and neural network to predict the quality score of the phrase translation pair.', "These phrase scores are used to dis-criminatively rescore the baseline MT system's phrase library: boost good phrase translations while prune bad ones.", 'This approach not only significantly improves machine translation quality, but also reduces the model size by a considerable margin.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/C10-1056', 'authors': ['Fei Huang', 'Bing Xiang'], 'year': 2010, 'book': 'COLING', 'id': 'acl-C10-1056', 'score': 0.3184529170048533}
acl-D12-1088 - {'title': 'Entropy-based Pruning for Phrase-based Machine Translation', 'abstract': ['Phrase-based machine translation models have shown to yield better translations than Word-based models, since phrase pairs encode the contextual information that is needed for a more accurate translation.', 'However, many phrase pairs do not encode any relevant context, which means that the translation event encoded in that phrase pair is led by smaller translation events that are independent from each other, and can be found on smaller phrase pairs, with little or no loss in translation accuracy.', 'In this work, we propose a relative entropy model for translation models, that measures how likely a phrase pair encodes a translation event that is derivable using smaller translation events with similar probabilities.', 'This model is then applied to phrase table pruning.', 'Tests show that considerable amounts of phrase pairs can be excluded, without much impact on the translation quality.', 'In fact, we show that better translations can be obtained using our pruned models, due to the compression of the search space during decoding.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/D12-1088', 'authors': ['Wang Ling', 'João Graça', 'Isabel Trancoso', 'Alan Black'], 'year': 2012, 'book': 'EMNLP', 'id': 'acl-D12-1088', 'score': 0.3114890936388646}
acl-I05-1051 - {'title': 'Phrase-Based Statistical Machine Translation: A Level of Detail Approach', 'abstract': ["Hendra Setiawan', Haizhou Li,Min Zhang, and Beng Chin Ooi", 'Institute for Infocomm Research, 21 Heng Mui Keng Terrace, Singapore 119613 {stuhs, hli, mzhang}@i2r.a-star.edu.sg School of Computing, National University of Singapore, Singapore 117543 {hendrase, ooibc}@comp.nus.edu.sg', 'Abstract.', 'The merit of phrase-based statistical machine translation is often reduced by the complexity to construct it.', 'In this paper, we address some issues in phrase-based statistical machine translation, namely: the size of the phrase translation table, the use of underlying translation model probability and the length of the phrase unit.', 'We present Level-Of-Detail (LOD) approach, an agglomerative approach for learning phrase-level alignment.', 'Our experiments show that LOD approach significantly improves the performance of the word-based approach.', 'LOD demonstrates a clear advantage that the phrase translation table grows only sub-linearly over the maximum phrase length, while having a performance comparable to those of other phrase-based approaches.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/I05-1051', 'authors': ['Hendra Setiawan', 'Haizhou Li', 'Min Zhang', 'Beng Chin Ooi'], 'year': 2005, 'book': 'Second International Joint Conference on Natural Language Processing: Full Papers', 'id': 'acl-I05-1051', 'score': 0.3088836790750615}
acl-W02-0715 - {'title': 'Quality-Sensitive Test Set Selection for a Speech Translation System', 'abstract': ['by-one.', 'The actual plotted broken line is averaged over 10 random trials.', 'Figure 9 shows the relationship between iteration and the system’s TOEIC score.', 'In this figure, the horizontal axis represents the iteration, and the vertical axis TOEIC score.', 'The broken line and the solid line are plotted using the same denotation as that in Figure 8.', 'In Figure 8, the solid line always lies on a lower position than the broken line.', 'In Figure 9, from iteration 1 to around iteration 200, the broken line does not deviate from the actual system’s TOEIC score, which is 548.', 'Considering these results, the test set optimized for TDMT is shown to be applicable for evaluating ATR-MATRIX.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W02-0715', 'authors': ['Fumiaki Sugaya', 'Keiji Yasuda', 'Toshiyuki Takezawa', 'Seiichi Yamamoto'], 'year': 2002, 'book': 'Workshop on Speech-To-Speech Translation: Algorithms and Systems', 'id': 'acl-W02-0715', 'score': 0.28448395218006456}
acl-A92-1028 - {'title': 'Zero Pronoun Resolution in a Machine Translation System by Using Japanese to English Verbal Semantic Attributes', 'abstract': ['A method of anaphoral resolution of zero pronouns in Japanese language texts using the verbal semantic attributes is suggested.', 'This method focuses attention on the semantic attributes of verbs and examines the context from the relationship between the semantic attributes of verbs governing zero pronouns and the semantic attributes of verbs governing their referents.', 'The semantic attributes of verbs are created using 2 different viewpoints: dynamic characteristics of verbs and the relationship of verbs to cases.', 'By using this method, it is shown that, in the case of translating newspaper articles, the major portion (93%) of anaphoral resolution of zero pronouns necessary for machine translation can be achieved by using only linguistic knowledge.', 'Factors to be given special attention when incorporating this method into a machine translation system are examined, together with suggested conditions for the detection of zero pronouns and methods for their conversion.', 'This study considers four factors that are important when implementing this method in a Japanese to English machine translation system: the difference in conception between Japanese and English expressions, the difference in case frame patterns between Japanese and English, restrictions by voice and restriction by translation structure.', 'Implementation of the proposed method with due consideration of these points leads to a viable method for anaphoral resolution of zero pronouns in a practical machine translation system.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/A92-1028', 'authors': ['Hiromi Nakaiwa', 'Satoru Ikehara'], 'year': 1992, 'book': 'Applied Natural Language Processing Conference', 'id': 'acl-A92-1028', 'score': 0.7238661008796474}
acl-C96-2146 - {'title': 'Analyzing Japanese Double-Subject Construction Having an Adjective Predicate', 'abstract': ['This paper describes a method for analyzing Japanese double-subject construction having an adjective predicate based on the valency structure.', 'A simple sentence usually has only one subjective case in most languages.', 'However, many Japanese adjectives (and some verbs) can dominate two surface subjective cases within a simple sentence.', 'Such sentence structure is called the double-subject construction.', 'This paper classifies the Japanese double-subject construction into four types and describes problems arising when analyzing these types using ordinary Japanese construction approaches.', 'This paper proposes a method for analyzing a Japanese double-subject construction having an adjective predicate in order to overcome the problems described.', 'By applying this method to Japanese sentence analysis in Japanese-to-English machine translation systems, translation accuracy can be improved because this method can analyze correctly the double-subject construction.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/C96-2146', 'authors': ['Masahiro Oku'], 'year': 1996, 'book': 'International Conference on Computational Linguistics', 'id': 'acl-C96-2146', 'score': 0.6614859634739717}
acl-C04-1014 - {'title': 'Modeling of Long Distance Context Dependency', 'abstract': ['Ngram models are simple in language modeling and have been successfully used in speech recognition and other tasks.', 'However, they can only capture the short distance context dependency within an n-words window where currently the largest practical n for a natural language is three while much of the context dependency in a natural language occurs beyond a three words window.', 'In order to incorporate this kind of long distance context dependency in the ngram model of our Mandarin speech recognition system, this paper proposes a novel MI-Ngram modeling approach.', 'This new MI-Ngram model consists of two components: a normal ngram model and a novel MI model.', 'The ngram model captures the short distance context dependency within an n-words window while the MI model captures the context dependency between the word pairs over a long distance by using the concept of mutual information.', 'That is, the MI-Ngram model incorporates the word occurrences beyond the scope of the normal ngram model.', 'It is found that MI-Ngram modeling has much better performance than the normal word ngram modeling.', 'Experimentation shows that about 20% of errors can be corrected by using a MI-Trigram model compared with the pure word trigram model.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/C04-1014', 'authors': ['Guodong Zhou'], 'year': 2004, 'book': 'International Conference on Computational Linguistics', 'id': 'acl-C04-1014', 'score': 0.6569717581749753}
acl-D09-1078 - {'title': 'Less is More: Significance-Based N-gram Selection for Smaller, Better Language Models', 'abstract': ['Robert C. Moore Chris Quirk', 'The recent availability of large corpora for training N-gram language models has shown the utility of models of higher order than just trigrams.', 'In this paper, we investigate methods to control the increase in model size resulting from applying standard methods at higher orders.', 'We introduce significance-based N-gram selection, which not only reduces model size, but also improves perplexity for several smoothing methods, including Katz backoff and absolute discounting.', 'We also show that, when combined with a new smoothing method and a novel variant of weighted-difference pruning, our selection method performs better in the trade-off between model size and perplexity than the best pruning method we found for modified Kneser-Ney smoothing.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/D09-1078', 'authors': ['Robert C. Moore', 'Chris Quirk'], 'year': 2009, 'book': 'EMNLP', 'id': 'acl-D09-1078', 'score': 0.6362706169150789}
acl-P09-2008 - {'title': 'A Novel Word Segmentation Approach for Written Languages with Word Boundary Markers', 'abstract': ["Han-Cheol Cho} Do-Gil Lee,§ Jung-Tae Lee,§ Pontus Stenetorp} Jun'ichi TsujiiWid Hae-Chang Rim§", 't Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan §Dept.', 'of Computer & Radio Communications Engineering, Korea University, Seoul, Korea {hccho,pontus,tsujii}@is.s.u-tokyo.ac.jp, {dglee,jtlee,rim}@nlp.korea.ac.kr', 'Most NLP applications work under the assumption that a user input is error-free; thus, word segmentation (WS) for written languages that use word boundary markers (WBMs), such as spaces, has been regarded as a trivial issue.', 'However, noisy real-world texts, such as blogs, e-mails, and SMS, may contain spacing errors that require correction before further processing may take place.', 'For the Korean language, many researchers have adopted a traditional WS approach, which eliminates all spaces in the user input and reinserts proper word boundaries.', 'Unfortunately, such an approach often exacerbates the word spacing quality for user input, which has few or no spacing errors; such is the case, because a perfect WS model does not exist.', 'In this paper, we propose a novel WS method that takes into consideration the initial word spacing information of the user input.', 'Our method generates a better output than the original user input, even if the user input has few spacing errors.', 'Moreover, the proposed method significantly outperforms a state-of-the-art Korean WS model when the user input initially contains less than 10% spacing errors, and performs comparably for cases containing more spacing errors.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P09-2008', 'authors': ['Han-Cheol Cho', 'Do-Gil Lee', 'Jung-Tae Lee', 'Pontus Stenetorp', "Jun'ichi Tsujii", 'Hae-Chang Rim'], 'year': 2009, 'book': 'ACL-IJCNLP: Short Papers', 'id': 'acl-P09-2008', 'score': 0.2753588349223253}
acl-P07-2020 - {'title': 'Ensemble document clustering using weighted hypergraph generated by NMF', 'abstract': ['Ensemble Document Clustering Using Weighted Hypergraph Generated by NMF', 'Hiroyuki Shinnou, Minoru Sasaki', 'In this paper, we propose a new ensemble document clustering method.', 'The novelty of our method is the use of Non-negative Matrix Factorization (NMF) in the generation phase and a weighted hypergraph in the integration phase.', 'In our experiment, we compared our method with some clustering methods.', 'Our method achieved the best results.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P07-2020', 'authors': ['Hiroyuki Shinnou', 'Minoru Sasaki'], 'year': 2007, 'book': '45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions', 'id': 'acl-P07-2020', 'score': 0.23119178840532434}
acl-P09-1018 - {'title': 'Revisiting Pivot Language Approach for Machine Translation', 'abstract': ['Hua Wu and Haifeng Wang', 'Toshiba (China) Research and Development Center 5/F., Tower W2, Oriental Plaza, Beijing, 100738, China {wuhua, wanghaifeng}@rdc.toshiba.com.cn', 'This paper revisits the pivot language approach for machine translation.', 'First, we investigate three different methods for pivot translation.', 'Then we employ a hybrid method combining RBMT and SMT systems to fill up the data gap for pivot translation, where the source-pivot and pivot-target corpora are independent.', 'Experimental results on spoken language translation show that this hybrid method significantly improves the translation quality, which outperforms the method using a source-target corpus of the same size.', 'In addition, we propose a system combination approach to select better translations from those produced by various pivot translation methods.', 'This method regards system combination as a translation evaluation problem and formalizes it with a regression learning model.', 'Experimental results indicate that our method achieves consistent and significant improvement over individual translation outputs.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/P09-1018', 'authors': ['Hua Wu', 'Haifeng Wang'], 'year': 2009, 'book': 'ACL-IJCNLP', 'id': 'acl-P09-1018', 'score': 0.5256681349989019}
acl-D14-1174 - {'title': 'Improving Pivot-Based Statistical Machine Translation by Pivoting the Co-occurrence Count of Phrase Pairs', 'abstract': ['Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1665?1675, October 25-29, 2014, Doha, Qatar.', 'Abstract To overcome the scarceness of bilingual corpora for some language pairs in machine translation, pivot-based SMT uses pivot language as a "bridge" to generate source-target translation from source-pivot and pivot-target translation.', 'One of the key issues is to estimate the probabilities for the generated phrase pairs.', 'In this paper, we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs.', 'Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/D14-1174', 'authors': ['Xiaoning Zhu', 'Zhongjun He', 'Hua Wu', 'Conghui Zhu', 'Haifeng Wang', 'Tiejun Zhao'], 'year': 2014, 'book': 'EMNLP', 'id': 'acl-D14-1174', 'score': 0.5239626365947524}
acl-W07-0724 - {'title': "NRC's PORTAGE System for WMT 2007", 'abstract': ["NRC's PORTAGE system for WMT 2007", 'Nicola Ueffing, Michel Simard, Samuel Larkin Howard Johnson', 'Interactive Language Technologies Group Interactive Information Group', 'National Research Council Canada', 'Gatineau, Quebec, Canada firstname.lastname@nrc.gc.ca', 'National Research Council Canada Ottawa, Ontario, Canada Howard.Johnson@nrc.gc.ca', 'We present the PORTAGE statistical machine translation system which participated in the shared task of the ACL 2007 Second Workshop on Statistical Machine Translation.', 'The focus of this description is on improvements which were incorporated into the system over the last year.', 'These include adapted language models, phrase table pruning, an IBMl-based decoder feature, and rescor-ing with posterior probabilities.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/W07-0724', 'authors': ['Nicola Ueffing', 'Michel Simard', 'Samuel Larkin', 'Howard Johnson'], 'year': 2007, 'book': 'Workshop on Statistical Machine Translation', 'id': 'acl-W07-0724', 'score': 0.5167615615075047}
acl-D07-1030 - {'title': 'Using RBMT Systems to Produce Bilingual Corpus for SMT', 'abstract': ['Xiaoguang Hu, Haifeng Wang, Hua Wu', 'Toshiba (China) Research and Development Center 5/F., Tower W2, Oriental Plaza No.1, East Chang An Ave., Dong Cheng District Beijing, 100738, China', '{huxiaoguang, wanghaifeng, wuhua}@rdc.toshiba.com.cn', 'This paper proposes a method using the existing Rule-based Machine Translation (RBMT) system as a black box to produce synthetic bilingual corpus, which will be used as training data for the Statistical Machine Translation (SMT) system.', 'We use the existing RBMT system to translate the monolingual corpus into synthetic bilingual corpus.', 'With the synthetic bilingual corpus, we can build an SMT system even if there is no real bilingual corpus.', 'In our experiments using BLEU as a metric, the system achieves a relative improvement of 11.7% over the best RBMT system that is used to produce the synthetic bilingual corpora.', 'We also interpolate the model trained on a real bilingual corpus and the models trained on the synthetic bilingual corpora.', 'The interpolated model achieves an absolute improvement of 0.0245 BLEU score (13.1% relative) as compared with the individual model trained on the real bilingual corpus.'], 'type': 'unknown', 'url': 'https://aclweb.org/anthology/D07-1030', 'authors': ['Xiaoguang Hu', 'Haifeng Wang', 'Hua Wu'], 'year': 2007, 'book': '2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)', 'id': 'acl-D07-1030', 'score': 0.5141486557633063}
Lambda = 0 -> No penalty
bound: 0.0
find mmr max: 25.968315672901436
len: 1 55
bound: 25.968315672901436
find mmr max: 48.6757799824111
len: 2 54
bound: 48.6757799824111
find mmr max: 70.00399022960438
len: 3 53
bound: 70.00399022960438
find mmr max: 90.10210906970843
len: 4 52
bound: 90.10210906970843
find mmr max: 108.60453373323487
len: 5 51
bound: 108.60453373323487
find mmr max: 125.94259510688232
len: 6 50
bound: 125.94259510688232
find mmr max: 142.2414879680298
len: 7 49
bound: 142.2414879680298
find mmr max: 157.35281885450914
len: 8 48
bound: 157.35281885450914
find mmr max: 171.384303777766
len: 9 47
bound: 171.384303777766
find mmr max: 184.47161509750975
len: 10 46
bound: 184.47161509750975
find mmr max: 196.43601432060547
len: 11 45
bound: 196.43601432060547
find mmr max: 207.27172904921827
len: 12 44
bound: 207.27172904921827
find mmr max: 217.2017145409948
len: 13 43
bound: 217.2017145409948
find mmr max: 226.13205102343963
len: 14 42
bound: 226.13205102343963
find mmr max: 233.97579610317078
len: 15 41
bound: 233.97579610317078
find mmr max: 241.01705714626686
len: 16 40
bound: 241.01705714626686
find mmr max: 247.30120214463437
len: 17 39
bound: 247.30120214463437
find mmr max: 252.61620449677787
len: 18 38
bound: 252.61620449677787
find mmr max: 257.2637923253311
len: 19 37
bound: 257.2637923253311
find mmr max: 261.043556095172
len: 20 36
bound: 261.043556095172
find mmr max: 263.99960421102014
len: 21 35
bound: 263.99960421102014
find mmr max: 266.26110358710474
len: 22 34
bound: 266.26110358710474
find mmr max: 267.83591275777763
len: 23 33
bound: 267.83591275777763
find mmr max: 268.9437498712868
len: 24 32
bound: 268.9437498712868
find mmr max: 269.67911102241925
len: 25 31
bound: 269.67911102241925
find mmr max: 269.7786292800426
len: 26 30
bound: 269.7786292800426
find mmr max: 269.7751839291424
len: 26 29
bound: 269.7786292800426
find mmr max: 269.40155694048985
len: 26 28
bound: 269.7786292800426
find mmr max: 269.3088981383785
len: 26 27
bound: 269.7786292800426
find mmr max: 269.3003807333152
len: 26 26
bound: 269.7786292800426
find mmr max: 269.2237695336678
len: 26 25
bound: 269.7786292800426
find mmr max: 269.2118904033271
len: 26 24
bound: 269.7786292800426
find mmr max: 269.20511200745125
len: 26 23
bound: 269.7786292800426
find mmr max: 269.13541957661215
len: 26 22
bound: 269.7786292800426
find mmr max: 269.1120649654516
len: 26 21
bound: 269.7786292800426
find mmr max: 269.1089842701608
len: 26 20
bound: 269.7786292800426
find mmr max: 269.082925223862
len: 26 19
bound: 269.7786292800426
find mmr max: 269.048215006598
len: 26 18
bound: 269.7786292800426
find mmr max: 269.01325813491644
len: 26 17
bound: 269.7786292800426
find mmr max: 268.9784563179709
len: 26 16
bound: 269.7786292800426
find mmr max: 268.97728344263857
len: 26 15
bound: 269.7786292800426
find mmr max: 268.9671739835354
len: 26 14
bound: 269.7786292800426
find mmr max: 268.9160838973369
len: 26 13
bound: 269.7786292800426
find mmr max: 268.9122740035709
len: 26 12
bound: 269.7786292800426
find mmr max: 268.8991198296466
len: 26 11
bound: 269.7786292800426
find mmr max: 268.8936376443253
len: 26 10
bound: 269.7786292800426
find mmr max: 268.87111544183523
len: 26 9
bound: 269.7786292800426
find mmr max: 268.8573641160358
len: 26 8
bound: 269.7786292800426
find mmr max: 268.83632125778814
len: 26 7
bound: 269.7786292800426
find mmr max: 268.8240375047036
len: 26 6
bound: 269.7786292800426
find mmr max: 268.79621448160634
len: 26 5
bound: 269.7786292800426
find mmr max: 268.7946692353461
len: 26 4
bound: 269.7786292800426
find mmr max: 268.78461403386655
len: 26 3
bound: 269.7786292800426
find mmr max: 268.62786419079424
len: 26 2
bound: 269.7786292800426
find mmr max: 268.62051160616454
len: 26 1
bound: 269.7786292800426
find mmr max: 268.45085013790214
len: 26 0
26
acl-J99-2002
acl-A92-1028
acl-C04-1032
acl-W96-0210
acl-W04-1121
acl-D14-1174
acl-W09-2413
acl-A97-1056
acl-W11-2103
acl-C04-1045
acl-I05-1051
acl-P09-1054
acl-P08-2040
acl-H93-1047
acl-C04-1014
acl-P09-1018
acl-W10-1716
acl-W00-1429
acl-P08-2038
acl-C10-3013
acl-C00-1074
acl-P02-1022
acl-W12-3147
acl-P14-2010
acl-D09-1078
acl-P12-2007
Lambda = 0.5 -> less penalty
bound: 0.0
find mmr max: 25.968315672901436
len: 1 55
bound: 25.968315672901436
find mmr max: 48.077914683459184
len: 2 54
bound: 48.077914683459184
find mmr max: 68.2477628361111
len: 3 53
bound: 68.2477628361111
find mmr max: 86.56986577311302
len: 4 52
bound: 86.56986577311302
find mmr max: 102.79807167518253
len: 5 51
bound: 102.79807167518253
find mmr max: 117.50296519502487
len: 6 50
bound: 117.50296519502487
find mmr max: 130.72483668058558
len: 7 49
bound: 130.72483668058558
find mmr max: 142.3230145436083
len: 8 48
bound: 142.3230145436083
find mmr max: 152.26165017211756
len: 9 47
bound: 152.26165017211756
find mmr max: 160.61697697223877
len: 10 46
bound: 160.61697697223877
find mmr max: 167.53526576910858
len: 11 45
bound: 167.53526576910858
find mmr max: 173.16240655687642
len: 12 44
bound: 173.16240655687642
find mmr max: 177.4046701175233
len: 13 43
bound: 177.4046701175233
find mmr max: 180.63640167518867
len: 14 42
bound: 180.63640167518867
find mmr max: 182.68168764673794
len: 15 41
bound: 182.68168764673794
find mmr max: 183.91916113330905
len: 16 40
bound: 183.91916113330905
find mmr max: 184.86165364795232
len: 17 39
bound: 184.86165364795232
find mmr max: 184.8924526635146
len: 18 38
bound: 184.8924526635146
find mmr max: 184.7258289293271
len: 18 37
bound: 184.8924526635146
find mmr max: 184.71703341220285
len: 18 36
bound: 184.8924526635146
find mmr max: 184.67564451346882
len: 18 35
bound: 184.8924526635146
find mmr max: 184.65835746938228
len: 18 34
bound: 184.8924526635146
find mmr max: 184.63798869091798
len: 18 33
bound: 184.8924526635146
find mmr max: 184.58147135930386
len: 18 32
bound: 184.8924526635146
find mmr max: 184.5494566420762
len: 18 31
bound: 184.8924526635146
find mmr max: 184.5052754147958
len: 18 30
bound: 184.8924526635146
find mmr max: 184.48918926806044
len: 18 29
bound: 184.8924526635146
find mmr max: 184.47871766683267
len: 18 28
bound: 184.8924526635146
find mmr max: 184.47439589014695
len: 18 27
bound: 184.8924526635146
find mmr max: 184.44307925661286
len: 18 26
bound: 184.8924526635146
find mmr max: 184.4001716095149
len: 18 25
bound: 184.8924526635146
find mmr max: 184.39947870501456
len: 18 24
bound: 184.8924526635146
find mmr max: 184.3734955791938
len: 18 23
bound: 184.8924526635146
find mmr max: 184.33356117601397
len: 18 22
bound: 184.8924526635146
find mmr max: 184.31399733352015
len: 18 21
bound: 184.8924526635146
find mmr max: 184.28590722424323
len: 18 20
bound: 184.8924526635146
find mmr max: 184.27419769961384
len: 18 19
bound: 184.8924526635146
find mmr max: 184.27030881967903
len: 18 18
bound: 184.8924526635146
find mmr max: 184.23529204547134
len: 18 17
bound: 184.8924526635146
find mmr max: 184.23442788406226
len: 18 16
bound: 184.8924526635146
find mmr max: 184.2332909238602
len: 18 15
bound: 184.8924526635146
find mmr max: 184.21422196755842
len: 18 14
bound: 184.8924526635146
find mmr max: 184.17350111725435
len: 18 13
bound: 184.8924526635146
find mmr max: 184.1726537010455
len: 18 12
bound: 184.8924526635146
find mmr max: 184.11226904802885
len: 18 11
bound: 184.8924526635146
find mmr max: 184.09796870770347
len: 18 10
bound: 184.8924526635146
find mmr max: 184.0901635213663
len: 18 9
bound: 184.8924526635146
find mmr max: 184.06107595696733
len: 18 8
bound: 184.8924526635146
find mmr max: 183.9745172406863
len: 18 7
bound: 184.8924526635146
find mmr max: 183.8785843560167
len: 18 6
bound: 184.8924526635146
find mmr max: 183.87458848600397
len: 18 5
bound: 184.8924526635146
find mmr max: 183.79640617734978
len: 18 4
bound: 184.8924526635146
find mmr max: 183.7175055310112
len: 18 3
bound: 184.8924526635146
find mmr max: 183.67482497681803
len: 18 2
bound: 184.8924526635146
find mmr max: 183.67258438035228
len: 18 1
bound: 184.8924526635146
find mmr max: 182.88684276224302
len: 18 0
18
acl-J99-2002
acl-A92-1028
acl-C04-1032
acl-W96-0210
acl-W04-1121
acl-D14-1174
acl-W11-2103
acl-A97-1056
acl-I05-1051
acl-W09-2413
acl-H93-1047
acl-P08-2040
acl-W00-1429
acl-D09-1078
acl-W11-2158
acl-P14-2010
acl-J10-3009
acl-C10-4001
Lambda = 1.0 -> same degree penalty
bound: 0.0
find mmr max: 25.968315672901436
len: 1 55
bound: 25.968315672901436
find mmr max: 47.48004938450726
len: 2 54
bound: 47.48004938450726
find mmr max: 66.4915354426178
len: 3 53
bound: 66.4915354426178
find mmr max: 83.03762247651761
len: 4 52
bound: 83.03762247651761
find mmr max: 97.04578275575604
len: 5 51
bound: 97.04578275575604
find mmr max: 109.07671813142676
len: 6 50
bound: 109.07671813142676
find mmr max: 119.40693144769622
len: 7 49
bound: 119.40693144769622
find mmr max: 127.39330901185718
len: 8 48
bound: 127.39330901185718
find mmr max: 133.24517021169282
len: 9 47
bound: 133.24517021169282
find mmr max: 137.58649662454457
len: 10 46
bound: 137.58649662454457
find mmr max: 140.27896510684474
len: 11 45
bound: 140.27896510684474
find mmr max: 141.5522238245619
len: 12 44
bound: 141.5522238245619
find mmr max: 142.4740342861267
len: 13 43
bound: 142.4740342861267
find mmr max: 142.50483330168896
len: 14 42
bound: 142.50483330168896
find mmr max: 142.11986576249396
len: 14 41
bound: 142.50483330168896
find mmr max: 142.1098709928828
len: 14 40
bound: 142.50483330168896
find mmr max: 141.90998976003982
len: 14 39
bound: 142.50483330168896
find mmr max: 141.89533688966975
len: 14 38
bound: 142.50483330168896
find mmr max: 141.83481602314168
len: 14 37
bound: 142.50483330168896
find mmr max: 141.78415170368206
len: 14 36
bound: 142.50483330168896
find mmr max: 141.75651252264063
len: 14 35
bound: 142.50483330168896
find mmr max: 141.73106324275625
len: 14 34
bound: 142.50483330168896
find mmr max: 141.7128634952877
len: 14 33
bound: 142.50483330168896
find mmr max: 141.68058865924866
len: 14 32
bound: 142.50483330168896
find mmr max: 141.65720480585043
len: 14 31
bound: 142.50483330168896
find mmr max: 141.64206066716787
len: 14 30
bound: 142.50483330168896
find mmr max: 141.63826108662494
len: 14 29
bound: 142.50483330168896
find mmr max: 141.6308508975032
len: 14 28
bound: 142.50483330168896
find mmr max: 141.60224810525256
len: 14 27
bound: 142.50483330168896
find mmr max: 141.59427304094407
len: 14 26
bound: 142.50483330168896
find mmr max: 141.5257193741419
len: 14 25
bound: 142.50483330168896
find mmr max: 141.50911195786873
len: 14 24
bound: 142.50483330168896
find mmr max: 141.48802564131034
len: 14 23
bound: 142.50483330168896
find mmr max: 141.4849885649203
len: 14 22
bound: 142.50483330168896
find mmr max: 141.4502457561416
len: 14 21
bound: 142.50483330168896
find mmr max: 141.41224765088134
len: 14 20
bound: 142.50483330168896
find mmr max: 141.36853795200165
len: 14 19
bound: 142.50483330168896
find mmr max: 141.31047564918467
len: 14 18
bound: 142.50483330168896
find mmr max: 141.30245149914697
len: 14 17
bound: 142.50483330168896
find mmr max: 141.22461067206766
len: 14 16
bound: 142.50483330168896
find mmr max: 141.1925122848666
len: 14 15
bound: 142.50483330168896
find mmr max: 141.15401384242764
len: 14 14
bound: 142.50483330168896
find mmr max: 141.15329854707784
len: 14 13
bound: 142.50483330168896
find mmr max: 141.15004287215885
len: 14 12
bound: 142.50483330168896
find mmr max: 141.1389693224491
len: 14 11
bound: 142.50483330168896
find mmr max: 141.1037355704848
len: 14 10
bound: 142.50483330168896
find mmr max: 141.0417165397307
len: 14 9
bound: 142.50483330168896
find mmr max: 140.9930961695896
len: 14 8
bound: 142.50483330168896
find mmr max: 140.97746074665346
len: 14 7
bound: 142.50483330168896
find mmr max: 140.94604307574946
len: 14 6
bound: 142.50483330168896
find mmr max: 140.86426093636874
len: 14 5
bound: 142.50483330168896
find mmr max: 140.80874839851464
len: 14 4
bound: 142.50483330168896
find mmr max: 140.7203778251212
len: 14 3
bound: 142.50483330168896
find mmr max: 140.6991886818924
len: 14 2
bound: 142.50483330168896
find mmr max: 140.59180616422483
len: 14 1
bound: 142.50483330168896
find mmr max: 139.4266437632537
len: 14 0
14
acl-J99-2002
acl-A92-1028
acl-C04-1032
acl-W96-0210
acl-D14-1174
acl-W11-2103
acl-W04-1121
acl-A97-1056
acl-I05-1051
acl-C10-3013
acl-W00-1429
acl-P08-2038
acl-P14-2010
acl-C10-4001
Lambda = 2.0 -> double degree penalty
bound: 0.0
find mmr max: 25.968315672901436
len: 1 55
bound: 25.968315672901436
find mmr max: 46.28431878660343
len: 2 54
bound: 46.28431878660343
find mmr max: 62.97908065563122
len: 3 53
bound: 62.97908065563122
find mmr max: 75.97313588332679
len: 4 52
bound: 75.97313588332679
find mmr max: 85.89067848782753
len: 5 51
bound: 85.89067848782753
find mmr max: 92.80123210073336
len: 6 50
bound: 92.80123210073336
find mmr max: 97.39238539452747
len: 7 49
bound: 97.39238539452747
find mmr max: 99.68018456854134
len: 8 48
bound: 99.68018456854134
find mmr max: 100.64633378604668
len: 9 47
bound: 100.64633378604668
find mmr max: 100.67713280160896
len: 10 46
bound: 100.67713280160896
find mmr max: 100.59965480729886
len: 10 45
bound: 100.67713280160896
find mmr max: 100.35600106142682
len: 10 44
bound: 100.67713280160896
find mmr max: 100.29629287069572
len: 10 43
bound: 100.67713280160896
find mmr max: 100.29159743472893
len: 10 42
bound: 100.67713280160896
find mmr max: 100.26848132346633
len: 10 41
bound: 100.67713280160896
find mmr max: 100.26814561882915
len: 10 40
bound: 100.67713280160896
find mmr max: 100.26645334245403
len: 10 39
bound: 100.67713280160896
find mmr max: 100.17615835818101
len: 10 38
bound: 100.67713280160896
find mmr max: 100.03701250952517
len: 10 37
bound: 100.67713280160896
find mmr max: 100.02150901625266
len: 10 36
bound: 100.67713280160896
find mmr max: 99.92133075627969
len: 10 35
bound: 100.67713280160896
find mmr max: 99.81381668909493
len: 10 34
bound: 100.67713280160896
find mmr max: 99.7944128128356
len: 10 33
bound: 100.67713280160896
find mmr max: 99.77793928923948
len: 10 32
bound: 100.67713280160896
find mmr max: 99.76081035937946
len: 10 31
bound: 100.67713280160896
find mmr max: 99.75560603376145
len: 10 30
bound: 100.67713280160896
find mmr max: 99.66166755957481
len: 10 29
bound: 100.67713280160896
find mmr max: 99.5831709483996
len: 10 28
bound: 100.67713280160896
find mmr max: 99.5777958752126
len: 10 27
bound: 100.67713280160896
find mmr max: 99.5567041897317
len: 10 26
bound: 100.67713280160896
find mmr max: 99.5215177745601
len: 10 25
bound: 100.67713280160896
find mmr max: 99.48692660980915
len: 10 24
bound: 100.67713280160896
find mmr max: 99.48037433261543
len: 10 23
bound: 100.67713280160896
find mmr max: 99.46142225149062
len: 10 22
bound: 100.67713280160896
find mmr max: 99.4501271145765
len: 10 21
bound: 100.67713280160896
find mmr max: 99.3422255831475
len: 10 20
bound: 100.67713280160896
find mmr max: 99.33917099765524
len: 10 19
bound: 100.67713280160896
find mmr max: 99.29701389755552
len: 10 18
bound: 100.67713280160896
find mmr max: 99.20752884253343
len: 10 17
bound: 100.67713280160896
find mmr max: 99.1751181632073
len: 10 16
bound: 100.67713280160896
find mmr max: 99.17245046148422
len: 10 15
bound: 100.67713280160896
find mmr max: 99.11473786364463
len: 10 14
bound: 100.67713280160896
find mmr max: 99.08114613424496
len: 10 13
bound: 100.67713280160896
find mmr max: 98.88398079628551
len: 10 12
bound: 100.67713280160896
find mmr max: 98.87718208118584
len: 10 11
bound: 100.67713280160896
find mmr max: 98.83398541299344
len: 10 10
bound: 100.67713280160896
find mmr max: 98.79213385438803
len: 10 9
bound: 100.67713280160896
find mmr max: 98.77582865146748
len: 10 8
bound: 100.67713280160896
find mmr max: 98.72280385600507
len: 10 7
bound: 100.67713280160896
find mmr max: 98.70678456109206
len: 10 6
bound: 100.67713280160896
find mmr max: 98.61400651696786
len: 10 5
bound: 100.67713280160896
find mmr max: 98.59085295332825
len: 10 4
bound: 100.67713280160896
find mmr max: 98.55669502773144
len: 10 3
bound: 100.67713280160896
find mmr max: 98.51760485829512
len: 10 2
bound: 100.67713280160896
find mmr max: 98.46581460534573
len: 10 1
bound: 100.67713280160896
find mmr max: 95.64328201911438
len: 10 0
10
acl-J99-2002
acl-A92-1028
acl-C04-1032
acl-W96-0210
acl-W11-2103
acl-D14-1174
acl-C00-1074
acl-J10-3009
acl-P14-2010
acl-C10-4001
#Lambda = 4.0 -> as the paper chosen
bound: 0.0
find mmr max: 25.968315672901436
len: 1 55
bound: 25.968315672901436
find mmr max: 43.89285759079576
len: 2 54
bound: 43.89285759079576
find mmr max: 55.95417108165805
len: 3 53
bound: 55.95417108165805
find mmr max: 62.925603527462634
len: 4 52
bound: 62.925603527462634
find mmr max: 65.99950764961625
len: 5 51
bound: 65.99950764961625
find mmr max: 66.7995341130036
len: 6 50
bound: 66.7995341130036
find mmr max: 66.83033312856588
len: 7 49
bound: 66.83033312856588
find mmr max: 65.7787781228381
len: 7 48
bound: 66.83033312856588
find mmr max: 65.26645954679817
len: 7 47
bound: 66.83033312856588
find mmr max: 65.2289175673978
len: 7 46
bound: 66.83033312856588
find mmr max: 65.1336415543345
len: 7 45
bound: 66.83033312856588
find mmr max: 64.88297694138762
len: 7 44
bound: 66.83033312856588
find mmr max: 64.87475440922901
len: 7 43
bound: 66.83033312856588
find mmr max: 64.69637685965523
len: 7 42
bound: 66.83033312856588
find mmr max: 64.67079384863595
len: 7 41
bound: 66.83033312856588
find mmr max: 64.65356995509632
len: 7 40
bound: 66.83033312856588
find mmr max: 64.56460121214971
len: 7 39
bound: 66.83033312856588
find mmr max: 64.49463117954525
len: 7 38
bound: 66.83033312856588
find mmr max: 64.49315281162464
len: 7 37
bound: 66.83033312856588
find mmr max: 64.42337759598504
len: 7 36
bound: 66.83033312856588
find mmr max: 64.36666017142392
len: 7 35
bound: 66.83033312856588
find mmr max: 64.35553838858863
len: 7 34
bound: 66.83033312856588
find mmr max: 64.26399428739262
len: 7 33
bound: 66.83033312856588
find mmr max: 64.25970350428165
len: 7 32
bound: 66.83033312856588
find mmr max: 64.13433479918268
len: 7 31
bound: 66.83033312856588
find mmr max: 64.10463037355206
len: 7 30
bound: 66.83033312856588
find mmr max: 64.07651466321322
len: 7 29
bound: 66.83033312856588
find mmr max: 64.07502023592153
len: 7 28
bound: 66.83033312856588
find mmr max: 64.01666058467188
len: 7 27
bound: 66.83033312856588
find mmr max: 64.0035350604814
len: 7 26
bound: 66.83033312856588
find mmr max: 63.98517210088098
len: 7 25
bound: 66.83033312856588
find mmr max: 63.94896350330457
len: 7 24
bound: 66.83033312856588
find mmr max: 63.84909856209561
len: 7 23
bound: 66.83033312856588
find mmr max: 63.7038801441455
len: 7 22
bound: 66.83033312856588
find mmr max: 63.61585208863045
len: 7 21
bound: 66.83033312856588
find mmr max: 63.59937252467588
len: 7 20
bound: 66.83033312856588
find mmr max: 63.50779073344966
len: 7 19
bound: 66.83033312856588
find mmr max: 63.39622951478987
len: 7 18
bound: 66.83033312856588
find mmr max: 63.34762062469334
len: 7 17
bound: 66.83033312856588
find mmr max: 63.16834845831369
len: 7 16
bound: 66.83033312856588
find mmr max: 63.0969368878163
len: 7 15
bound: 66.83033312856588
find mmr max: 63.04452597198689
len: 7 14
bound: 66.83033312856588
find mmr max: 63.016358634275356
len: 7 13
bound: 66.83033312856588
find mmr max: 62.98169010081288
len: 7 12
bound: 66.83033312856588
find mmr max: 62.9736278736056
len: 7 11
bound: 66.83033312856588
find mmr max: 62.88574879337981
len: 7 10
bound: 66.83033312856588
find mmr max: 62.81169417495466
len: 7 9
bound: 66.83033312856588
find mmr max: 62.55984738558327
len: 7 8
bound: 66.83033312856588
find mmr max: 62.412124112214435
len: 7 7
bound: 66.83033312856588
find mmr max: 62.34037198930488
len: 7 6
bound: 66.83033312856588
find mmr max: 62.31737169396759
len: 7 5
bound: 66.83033312856588
find mmr max: 62.02790071929037
len: 7 4
bound: 66.83033312856588
find mmr max: 61.90406932192934
len: 7 3
bound: 66.83033312856588
find mmr max: 61.62938913140947
len: 7 2
bound: 66.83033312856588
find mmr max: 60.72345625707822
len: 7 1
bound: 66.83033312856588
find mmr max: 57.63035959195324
len: 7 0
7
acl-J99-2002
acl-A92-1028
acl-C04-1032
acl-W11-2103
acl-C00-1074
acl-P14-2010
acl-C10-4001
