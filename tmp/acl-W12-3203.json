{
  "info": {
    "authors": [
      "Yanchuan Sim",
      "Noah A. Smith",
      "David A. Smith"
    ],
    "book": "Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries",
    "id": "acl-W12-3203",
    "title": "Discovering Factions in the Computational Linguistics Community",
    "url": "https://aclweb.org/anthology/W12-3203",
    "year": 2012
  },
  "references": [
    "acl-A00-2018",
    "acl-D08-1038",
    "acl-D11-1055",
    "acl-L08-1093",
    "acl-P11-1135",
    "acl-W02-1018",
    "acl-W09-1117",
    "acl-W09-3607",
    "acl-W11-1516"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a joint probabilistic model of who cites whom in computational linguistics, and also of the words they use to do the citing.",
        "The model reveals latent factions, or groups of individuals whom we expect to collaborate more closely within their faction, cite within the faction using language distinct from citation outside the faction, and be largely understandable through the language used when cited from without.",
        "We conduct an exploratory data analysis on the ACL Anthology.",
        "We extend the model to reveal changes in some authors?",
        "faction memberships over time."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The ACL Anthology presents an excellent dataset for studying both the language and the social connections in our evolving research field.",
        "Extensive studies using techniques from the field of biblio-metrics have been applied to this dataset (Radev et al., 2009a), quantifying the importance and impact factor of both authors and articles in the community.",
        "Moreover, recent work has leveraged the availability of digitized publications to study trends and influences within the ACL community (Hall et al., 2008; Gerrish and Blei, 2010; Yogatama et al., 2011) and to analyze academic collaborations (Johri et al., 2011).",
        "To the best of our knowledge, however, existing work has mainly pursued ?macroscopic?",
        "investigations of the interaction of authors in collaboration, citation networks, or the textual content of whole papers.",
        "We seek to complement these results with a ?microscopic?",
        "investigation of authors?",
        "interactions by considering the individual sentences authors use to cite each other.",
        "In this paper, we present a joint model of who cites whom in computational linguistics, and also of how they do the citing.",
        "Central to this model is the idea of factions, or groups of individuals whom we expect to (i) collaborate more closely within their faction, (ii) cite within the faction using language distinct from citation outside the faction, (iii) be largely understandable through the language used when cited from without, and (iv) evolve over time.",
        "Factions can be thought of as ?communities,?",
        "which are loosely defined in the literature on networks as subgraphs where internal connections are denser than external ones (Radicchi et al., 2004).",
        "The distinction here is that the strength of connections depends on a latent language model estimated from citation contexts.",
        "This paper is an exploratory data analysis using a Bayesian generative model.",
        "We aim both to discover meaningful factions in the ACL community and also to illustrate the use of a probabilistic model for such discovery.",
        "As such, we do not present any objective evaluation of the model or make any claims that the factions optimally explain the research community.",
        "Indeed, we suspect that reaching a broad consensus among community members about factions (i.e., a ?gold standard?)",
        "would be quite difficult, as any social community's factions are likely perceived very",
        "Our factions are computational abstractions?clusters of authors?discovered entirely from the corpus.",
        "We do not claim that factions are especially contentious, any more than ?sub-communities?",
        "in social networks are especially collegial.",
        "subjectively.",
        "It is for this reason that a probabilistic generative model, in which all assumptions are made plain, is appropriate for the task.",
        "We hope this analysis will prove useful in future empirical research on social communities (including scientific ones) and their use of language."
      ]
    },
    {
      "heading": "2 Model",
      "text": [
        "In this paper, our approach is a probabilistic model over (i) coauthorship relations and (ii) the words in sentences containing citations.",
        "The words are assumed to be generated by a distribution that depends on the (latent) faction memberships of the citing authors, the cited authors, and whether the authors have coauthored before.",
        "To model these different effects on language, we use a sparse additive generative (SAGE) model (Eisenstein et al., 2011).",
        "In contrast to the popular Dirichlet-multinomial for topic modeling, which directly models lexical probabilities associated with each (latent) topic, SAGE models the deviation in log frequencies from a background lexical distribution.",
        "Imposing a sparsity-inducing prior on the deviation vectors limits the number of terms whose probabilities diverge from the background lexical frequencies, thereby increasing robustness to limited training data.",
        "SAGE can be used with or without latent topics; our model does not include topics.",
        "Figure 1 shows the plate diagram for our model.",
        "We describe the generative process: ?",
        "Generate the multinomial distribution over faction memberships from a Dirichlet distribution: ?",
        "?",
        "Dir(?).",
        "?",
        "Generate the binomial distribution for whether two authors coauthor, given that they are in the same faction, from a Beta distribution: ?",
        "G are the fixed numbers of authors and factions, respectively.",
        "m is the background word distribution, ?, ?",
        ", ?",
        "are hyperparameters, a are latent author factions, z and w are the observed coauthorship relations and observed words in citation sentences between authors, respectively.",
        "Each of the a",
        "(i) , denoting author i's faction alignment, are sampled once every iteration conditioned on all the other a (j) .",
        "If i and j are coauthors or i cited j in some publication, a (i) and a (j) will not be conditionally independent due to the v-structure.",
        "?",
        "same and ?",
        "diff are binomial distributions over whether two authors have collaborated together before, given that they are assigned to the same/different factions.",
        "Dashed variables are collapsed",
        "out in the Gibbs sampler, while double bordered variables are optimized in the M-step.",
        "deviations in word log-frequencies when faction g is citing faction h. ?",
        "For each word v in the vocabulary, let the uni-gram probability that an author in faction g uses to cite an author in faction h be",
        "?",
        "For each ordered pair of authors (i, j), ?",
        "For each word that i uses to cite j, draw",
        "?",
        "Binomial(?diff).",
        "Thus, our goal is to maximize the conditional likelihood of the observed data",
        "with respect to ?",
        "and ?.",
        "We fix ?",
        "and ?, which are hyperparameters that encode our prior beliefs, and m, which we assume to be a fixed background word distribution.",
        "Exact inference in this model is intractable, so we resort to an approximate inference technique based on Markov Chain Monte Carlo simulation.",
        "We perform Bayesian inference over the latent author factions while using maximum a posteriori estimates of ?",
        "because Bayesian inference of ?",
        "is problematic due to the logistic transformation.",
        "We refer the interested reader to Eisenstein et al. (2011).",
        "We take an empirical Bayes approach to setting the hyper-parameter ?.",
        "Our overall learning procedure is a Monte Carlo Expectation Maximization algorithm (Wei and Tanner, 1990)."
      ]
    },
    {
      "heading": "3 Learning and Inference",
      "text": [
        "Our learning algorithm is a two-step iterative procedure.",
        "During the E-step, we perform collapsed Gibbs sampling to obtain distributions over factions for each author, given the current setting of the hyperparameters.",
        "In the M-step, we obtain point estimates for the hyperparameters ?",
        "and ?",
        "given the current posterior distributions for the author factions."
      ]
    },
    {
      "heading": "3.1 E-step",
      "text": [
        "As the Dirichlet and Beta distributions are conjugate priors to the multinomial and binomial respectively, we can integrate out the latent variables ?, ?",
        "(same) and ?",
        "(diff) .",
        "For an author i, we sample his faction alignment a (i) conditioned on faction assignments to all other authors and citation words between i and other authors (in both directions).",
        "Denoting a ?i as the current faction assignments for all the authors except i,",
        "where Ng is the number of authors (except i) who are assigned to faction g, ij = ?same?",
        "if g = a(j) and ij = ?diff?",
        "otherwise, and N 1, N",
        "the number of author pairs that have/have not coau-thored before respectively, given the status of their factions .",
        "We elide the subscripts of and superscript of z for notational simplicity and abuse notation to let w (i) refer to all author i's citation words, both incoming and outgoing.",
        "Using SAGE, the factor for an author's words is",
        "v is the observed count of the number of times word v has been used when author i cites j; j ranges over the A authors.",
        "We sample each author's faction in turn and do so several times during the E-step, collecting samples to estimate our posterior distribution over a."
      ]
    },
    {
      "heading": "3.2 M-step",
      "text": [
        "In the M-step, we optimize all ?",
        "(g,h) and ?",
        "given the posterior distribution over author factions.",
        "Optimizing ?.",
        "Eisenstein et al. (2011) postulated that the components of ?",
        "are drawn from a compound model ?",
        "N (?",
        ";?, ?)E(?",
        "; ?",
        ")d?, where E(?",
        "; ?)",
        "indicates the Exponential distribution.",
        "They fit a variational distribution Q(?)",
        "and optimized the log-likelihood of the data by iteratively fitting the parameters ?",
        "using a Newton optimization step and maximizing the variational bound.",
        "The compound model described is equivalent to the Laplace distribution L(?",
        ";?, ?)",
        "(Lange and Sin-sheimer, 1993; Figueiredo, 2003).",
        "Moreover, a zero mean Laplace prior has the same effect as placing an L1 regularizer on ?.",
        "Therefore, we can equivalently",
        "v ?",
        "and ?",
        "is the regularization constant.",
        "The regularization constant and Laplace variance are related by ?",
        "= ?",
        "?1 (Tibshirani, 1996).",
        "We use the gradient-based optimization routine OWL-QN (Andrew and Gao, 2007) to maximize the above objective function with respect to ?",
        "(g,h) for each pair of factions g and h. Optimizing ?.",
        "As in the empirical Bayes approach, we learn the hyperparameter setting of ?",
        "from the data by maximizing the log likelihood with respect to ?.",
        "By treating ?",
        "as the parameter of a Dirichlet-multinomial compound distribution, we can directly use the samples of author factions produced by our Gibbs sampler to estimate ?.",
        "Minka (2009) describes in detail several iterative approaches to estimate ?",
        "; we use the linear-time Newton-Raphson iterative update to estimate the components of ?."
      ]
    },
    {
      "heading": "4 Data Analysis",
      "text": []
    },
    {
      "heading": "4.1 Dataset",
      "text": [
        "We used the ACL Anthology Network Corpus (Radev et al., 2009b), which currently contains 18,041 papers written by 12,777 authors.",
        "These papers are published in the field of computational linguistics between 1965 and 2011.",
        "Furthermore, the corpus provides bibliographic data such as authors of the papers and bibliographic references between each paper in the corpus.",
        "We extracted sentences containing citations using regular expressions and linked them between authors with the help of meta-data provided in the corpus.",
        "We tokenized the extracted sentences and down-cased them.",
        "Words that are numeric, appear less",
        "For a list of the journals, conferences and workshops archived by the ACL anthology, please visit http:// aclweb.org/anthology-new.",
        "than 20 times, or are in a stop word list are discarded.",
        "For papers with multiple authors, we divided the word counts by the number of pairings between authors in both papers, assigning each word to each author-pair (i.e., a count of",
        "nn?",
        "if a paper with n authors cites a paper with n ?",
        "authors).",
        "Due to the large number of authors, we only used the 500 most cited authors (within the corpus) who have published at least 5 papers.",
        "Papers with no authors left are removed from the dataset.",
        "As a result, we have 8,144 papers containing 80,776 citation sentences (31,659 citation pairs).",
        "After text processing, there are 391,711 tokens and 3,037 word types.",
        "In each iteration of the EM algorithm, we run the E-step Gibbs sampler for 300 iterations, discarding the first 100 samples for burn-in and collecting samples at every 3rd iteration to avoid autocorrelation.",
        "At the M-step, we update our ?",
        "and ?",
        "using the samples collected.",
        "We run the model for 100 EM iterations.",
        "We fixed ?",
        "= 5, ?same = (0.5, 1) and ?diff = (1, 0.5).",
        "Our setting of ?",
        "reflects our prior beliefs that coauthors tend to be from the same faction."
      ]
    },
    {
      "heading": "4.2 Factions in ACL (1965?2011)",
      "text": [
        "We ran the model withG = 30 factions and selected the most probable faction for each author from the posterior distribution of the author-faction alignment obtained in the final E step.",
        "Only 26 factions were selected as most probable for some author.",
        "Table 1 presents members of selected factions, along with citation words that have the largest positive log frequency deviation from the background distribution.",
        "Table 2 shows a list of the top three authors associated with factions not shown in Table 1.",
        "Incoming (outgoing) citation words are found by summing the log deviation vectors ?",
        "across citing (cited) factions.",
        "The author factions are manually labeled.",
        "We see from Table 1, the model has selected keywords that are arguably significant in certain sub-fields in computational linguistics.",
        "Incoming citations are generally indicative of the subject areas in",
        "In future work, nonparametric priors might be employed to automate the selection of G.",
        "We found it quite difficult to make sense of terms with negative log frequency deviations.",
        "This suggests exploring a model allowing only positive deviations; we leave that for future work.",
        "Formalisms (31) Fernando Pereira, Jason M. Eisner, Stuart M. Shieber, Walter Daelemans, Hitoshi Isa-hara Self cites: parsing In cites: parsing, semiring, grammars, tags, grammar, tag, lexicalized, dependency Out cites: tagger, regular, dependency, transformationbased, tagging, stochastic, grammars, sense Evaluation (17) Salim Roukos, Eduard Hovy, Marti A. Hearst, Chin-Yew Lin, Dekang Lin Self cites: automatic, bleu, linguistics, evaluation, computational, text, proceedings In cites: automatic, bleu, segmentation, method, proceedings, dependency, parses, text Out cites: paraphrases, cohesion, agreement, hierarchical, entropy, phrasebased, evaluation, treebank Semantics (26) Martha Palmer, Daniel Jurafsky, Mihai Surdeanu, David Weir, German Rigau Self cites: sense, semantic, wordnet In cites: framenet, sense, semantic, task, wordnet, word, project, question Out cites: sense, wordnet, moses, preferences, distributional, semantic, focus, supersense"
      ]
    },
    {
      "heading": "Machine Translation",
      "text": [
        "Kevin Knight, Michel Galley, Jonathan Graehl, Wei Wang, Sanjeev P. Khudanpur Self cites: inference, scalable, model In cites: scalable, inference, machine, training, generation, translation, model, syntaxbased Out cites: phrasebased, hierarchical, inversion, forest, transduction, translation, ibm, discourse",
        "In cites: sense, preferences, wordnet, acquired, semcor, word, semantic, calle Out cites: sense, subcategorization, acquisition, automatic, corpora, lexical, processing, wordnet Parsing (20) Michael John Collins, Eugene Charniak, Mark Johnson, Stephen Clark, Massimiliano Ciaramita Self cites: parser, parsing, model, perceptron, parsers, dependency In cites: parser, perceptron, supersense, parsing, dependency, results, hmm, models Out cites: parsing, forest, treebank, model, coreference, stochastic, grammar, task Discourse (29) Daniel Marcu, Aravind K. Joshi, Barbara J. Grosz, Marilyn A. Walker, Bonnie Lynn Webber Self cites: discourse, structure, centering In cites: discourse, phrasebased, centering, tag, focus, rhetorical, tags, lexicalized Out cites: discourse, rhetorical, framenet, realizer, tags, resolution, grammars, synonyms"
      ]
    },
    {
      "heading": "Machine Translation",
      "text": [
        "Franz Josef Och, Hermann Ney, Mitchell P. Marcus, David Chiang, Dekai Wu Self cites: training, error In cites: error, giza, rate, alignment, training, minimum, translation, phrasebased Out cites: forest, subcategorization, arabic, model, translation, machine, models, heuristic",
        "highest expected incoming citations (i.e p(faction |author) ?",
        "citations).",
        "Factions are labeled manually, referring to key sub-fields in computational linguistics.",
        "Faction sizes are in parenthesis following the labels.",
        "The citation words with the strongest positive weights in the deviation vectors are shown.",
        "which the faction holds recognized expertise.",
        "For instance, the faction labeled ?semantics?",
        "has citation terms commonly associated with propositional semantics: sense, framenet, wordnet.",
        "On the other hand, outgoing citations hint at the related work that a faction builds on; discourse might require building on components involving framenet, grammars, synonyms, while word sense disambiguation involves solving problems like acquisition and modeling subcategorization."
      ]
    },
    {
      "heading": "4.3 Sensitivity",
      "text": [
        "Given the same initial parameters, we found our model to be fairly stable across iterations of Monte",
        "displayed in Table 1.",
        "Carlo EM.",
        "We found that when G was too small (e.g., 10), groups were more mixed and the ?",
        "vectors could not capture variation among them well.",
        "When G was larger, the factions were subjectively cleaner, but fields like translation split into many factions (as is visible in the G = 30 case illustrated in Tables 1 and 2.",
        "Strengthening the L1 penalty made ?",
        "more sparse, of course, but gave less freedom in fitting the data and therefore more grouping of authors into a fewer effective factions."
      ]
    },
    {
      "heading": "4.4 Inter-Faction Relationships",
      "text": [
        "By using the most probable a posteriori faction for each author, we can compute the number of citations between factions.",
        "We define the average inter-faction citations by:",
        "where ?",
        "(g ?",
        "h) is the total number of papers written by authors in g that cite papers written by authors in h. Figure 2 presents a graph of selected factions and how these factions talk about each other.",
        "As we would expect, the machine translation faction is quite strongly connected to formalisms and parsing factions, reflecting the heavy use of grammars and",
        "factions.",
        "Factions on the horizontal axis are being cited; factions on the vertical axis are citing.",
        "Darker shades denote higher average",
        "parsing algorithms in translation.",
        "Moreover, we can observe that ?deeper?",
        "linguistics research, such as semantics and discourse, are less likely to be cited by the other factions.",
        "This is reflected in Figure 3, where the statistical MT and parsing factions in the bottom left exhibit higher citation activity amongst each other.",
        "In addition, we note that factions tend to self-cite more often than out of their own factions; this is unsurprising given the prior we selected.",
        "The IFC between discourse and MT2 (as shown by the edge thickness in figure 2) is higher than expected, given our prior knowledge of the computational linguistics community.",
        "Further investigation revealed that, Daniel Marcu, posited by our model to be a member of the discourse faction, has coau-thored numerous highly cited papers in MT in recent years (Marcu and Wong, 2002).",
        "However, the model split the translation field, which fragmented the counts of MT related citation words.",
        "Thus, assigning Daniel Marcu to the discourse faction, which also has a less diverse citation vocabulary, is more probable than assigning him to one of the MT factions.",
        "In ?4.6, we consider a model of factions over time to mitigate this problem."
      ]
    },
    {
      "heading": "4.5 Comparison to Graph Clustering",
      "text": [
        "Work in the field of bibliometrics has largely focused on using the link structure of citation networks to study higher level structures.",
        "See Osareh (1996) for a review.",
        "Popular methods include bibliographic coupling (Kessler, 1963), and co-citation",
        "giza, translation, model ?memory, judges, voice, allow, sequences",
        "to the average number of inter-faction citations (equation 1).",
        "The words on the edges are the highest weighted words from the deviation vectors ?, with the arrow denoting the direction of the citation.",
        "Edges with below average IFC scores are represented as dashed lines, and their citations words are not shown to preserve readability.",
        "analysis (Small, 1973).",
        "By using authors as an unit of analysis in co-citation pairs, author co-citations have been presented as a technique to analyze their subject specialties (White and Griffith, 1981).",
        "Using standard graph clustering algorithms on these author co-citation networks, one can obtain a semblance of author factions.",
        "Hence, we performed graph clustering on both collaboration and citation graphs",
        "of authors in our dataset using Graclus",
        ", a graph clustering implementation based on normalized cuts and ratio associations (Dhillon et al., 2004).",
        "In Table 3, we compare, for selected authors, how their faction-mates obtained by our model and graph clustering differ.",
        "When clustering on the author collaboration network, we obtained some clusters easily identified with research labs (e.g., Daniel Marcu at the Information Sciences Institute).",
        "The co-citation graph leads to groupings dominated by",
        "We converted the directed citation graph into a symmetric graph by performing bibliometric symmetrization described in Satuluri and Parthasarathy (2011, section 3.3)."
      ]
    },
    {
      "heading": "Software/graclus.html",
      "text": [
        "heavily co-cited papers in major research areas.",
        "While we do not have an objective measurement of quality or usefulness, we believe that the factions identified by our model align somewhat better with familiar technical themes around which sub-communities naturally form than major research problems or institutions."
      ]
    },
    {
      "heading": "4.6 Factions over Time",
      "text": [
        "Faction alignments may be dynamic; we expect that, over time, individual researchers may move from one faction to another as their interests evolve.",
        "We consider a slightly modified model whereby authors are split into different copies of themselves during a non-overlapping set of discrete time periods.",
        "Given a set of disjoint time periods T , we denote each author-faction node by {a (i,t) |(i, t) ?",
        "A?",
        "T}.",
        "As we treat each ?incarnation?",
        "of an author as a distinct individual, we can simply use the same inference algorithm described in ?2.",
        "(In future work we might impose an expectation of gradual changes along a more continuous representation of time.)",
        "incoming citations are shown.",
        "For our model, we show the largest weighted words in the SAGE vector of incoming citations for the faction, while for graph clustering, we show words with the highest tf-idf weight.",
        "We split the same data as the earlier sections into four disjoint time periods, 1965?1989, 1990?1999, 2000?2005 and 2006?2011.",
        "The split across time is unequal due to the number of papers published in each period: these four periods include 1,917, 3,874, 3,786, and 8,105 papers, respectively.",
        "Here we used G = 20 factions for faster runtime, leading to diminished interpretability, though the sparsity of the deviation vectors mitigates this problem somewhat.",
        "Figure 4 shows graphical plots of selected authors and their faction membership posteriors over time (drawn from the final E-step).",
        "With a simple extension of the original model, we can learn shifts in the subject area the author is publishing about.",
        "Consider Eugene Charniak: the model observed a major change in faction alignment around 2000, when one of the popular Charniak parsers (Charniak, 2000) was released; this is somewhat later than Charniak's interests shifted, and the earlier faction's words are not clearly an accurate description of his work at that time.",
        "More fine-grained modeling of time and also accounting for the death and birth of factions might ameliorate these inconsistencies with our background knowledge about Charniak.",
        "The model finds that Aravind Joshi was associated with the tagging/parsing faction in the 1990s and in recent years moved back towards discourse (Prasad et al., 2008).",
        "David Yarowsky, known for his early work on word sense disambiguation, has since focused on applying word sense disambiguation techniques in a multilingual context (Garera et al., 2009; Bergsma et al., 2011).",
        "As mentioned in the previous section, we observe that the extended model is able to capture Daniel Marcu's shift from discourse-related work to MT with his work in phrase-based statistical MT (Marcu and Wong, 2002)."
      ]
    },
    {
      "heading": "5 Related Work",
      "text": [
        "A number of algorithms use topic modeling to analyze the text in the articles.",
        "Topic models such as latent Dirichlet alocation (Blei et al., 2003) and its variations have been increasingly used to study trends in scientific literature (McCallum et al., 2006; Dietz et al., 2007; Hall et al., 2008; Gerrish and Blei, 2010), predict citation information (McNee et al.,",
        "parser, parsing, stylistic, treebank, reduction sense, npcomplete, inducing, wsd, unsupervised building, annotated, discourse, treebank, kappa cotraining, scalable, moses, open, implemen framenet, roles, variation, semantic, propbank moses, meteor, open, bbn, discovery bleu, automatic, method, rouge, eval pcfg, temporal, logic, linguistic, noun bengston, shallow, conll, learning, kernel multitext, linking, alignment, competitive, bilingu phrasebased, forest, joint, hierarchical, kbest whats, moses, open, rule, source, syntaxbased human, metric, spade, evaluation, metrics distributional, rasp, similarity, clustering, deep tagger, pos, entropy, partofspeech, mathematics propbank, labelled, dependency, lfg, correlation vari, perceptron, ccg, counts, connectives dependency, parser, proc, parse, parsing contrastive, minimize, synchron, anneal, logist giza, lins, minipar, error, alignment",
        "records in at least three periods.",
        "The key for each entry contains the five highest weighted words in the deviation vectors for the faction's incoming citations.",
        "For each author, we show factions with which he or she is associated with probability > 0.1 in at least one time period.",
        "2002; Ib?a?nez et al., 2009; Nallapati et al., 2008) and analyze authorship (Rosen-Zvi et al., 2004; Johri et al., 2011).",
        "Assigning author factions can be seen as network classification problem, where the goal is to label nodes in a network such that there is (i) a correlation between a node's label and its observed attributes and (ii) a correlation between labels of interconnected nodes (Sen et al., 2008).",
        "Such collective network-based approaches have been used on scientific literature to classify papers/web pages into its subject categories (Kubica et al., 2002; Getoor, 2005; Angelova and Weikum, 2006).",
        "If we knew the word distributions between factions beforehand, learning the author factions in our model would be equivalent to the network classification task, where our edge weights are proportional to the probability of coauthorship multiplied by the probability of observing the citation words given the author's faction labels."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "In this work, we have defined factions in terms of how authors talk about each other's work, going beyond co-authorship and citation graph representations of a research community.",
        "We take a first step toward computationally modeling faction formation by using a latent author faction model and applied it to the ACL community, revealing both factions and how they cite each other.",
        "We also extended the model to capture authors?",
        "faction changes over time."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors thank members of the ARK group and the anonymous reviewers for helpful feedback.",
        "We gratefully acknowledge technical assistance from Matthew Fiorillo.",
        "This research was supported in part by an A ?",
        "STAR fellowship to Y. Sim, NSF grant IIS-0915187 to N. Smith, and the Center for Intelligent Information Retrieval and NSF grant IIS-0910884 for D. Smith."
      ]
    }
  ]
}
