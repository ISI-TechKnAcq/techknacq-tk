{
  "info": {
    "authors": [
      "Yi Zhang",
      "Stephan Oepen",
      "John Carroll"
    ],
    "book": "Tenth International Conference on Parsing Technologies",
    "id": "acl-W07-2207",
    "title": "Efficiency in Unification-Based N-Best Parsing",
    "url": "https://aclweb.org/anthology/W07-2207",
    "year": 2007
  },
  "references": [
    "acl-A00-2023",
    "acl-J97-4005",
    "acl-J98-2004",
    "acl-N03-1016",
    "acl-P02-1035",
    "acl-P02-1036",
    "acl-P04-1014",
    "acl-P89-1018",
    "acl-P99-1069",
    "acl-W02-2018",
    "acl-W05-1506"
  ],
  "sections": [
    {
      "text": [
        "We extend a recently proposed algorithm for n-best unpacking of parse forests to deal efficiently with (a) Maximum Entropy (ME) parse selection models containing important classes of non-local features, and (b) forests produced by unification grammars containing significant proportions of globally inconsistent analyses.",
        "The new algorithm empirically exhibits a linear relationship between processing time and the number of analyses unpacked at all degrees of ME feature non-locality; in addition, compared with agenda-driven best-first parsing and exhaustive parsing with post-hoc parse selection it leads to improved parsing speed, coverage, and accuracy."
      ]
    },
    {
      "heading": "1. Background – Motivation",
      "text": [
        "Technology for natural language analysis using linguistically precise grammars has matured to a level of coverage and efficiency that enables parsing of large amounts of running text.",
        "Research groups working within grammatical frameworks like CCG (Clark & Curran, 2004), LFG (Riezler et al., 2002),",
        "Flickinger, Toutanova, & Manning, 2004; Miyao, Ninomiya, & Tsujii, 2005) have successfully integrated broad-coverage computational grammars with sophisticated statistical parse selection models.",
        "The former delineate the space of possible analyses, while the latter provide a probability distribu-",
        "^The first author warmly acknowledges the guidance of his PhD advisors, Valia Kordoni and Hans Uszkoreit.",
        "We are grateful to Ulrich Callmeier, Berthold Crysmann, Dan Flickinger, and Erik Velldal for many discussions and their support.",
        "We thank Ron Kaplan, Martin Kay, and Bob Moore for providing insightful information about related approaches, notably the XLE and CLE parsers.",
        "tion over competing hypotheses.",
        "Parse selection approaches for these frameworks often use discriminative Maximum Entropy (ME) models, where the probability of each parse tree, given an input string, is estimated on the basis of select properties (called features) of the tree (Abney, 1997; Johnson, Ge-man, Canon, Chi, & Riezler, 1999).",
        "Such features, in principle, are not restricted in their domain of locality, and enable the parse selection process to take into account properties that extend beyond local contexts (i.e. sub-trees of depth one).",
        "There is a trade-off in this set-up between the accuracy of the parse selection model, on the one hand, and the efficiency of the search for the best solu-tion(s), on the other hand.",
        "Extending the context size of ME features, within the bounds of available training data, enables increased parse selection accuracy.",
        "However, the interplay of the core parsing algorithm and the probabilistic ranking of alternate (sub-)hypotheses becomes considerably more complex and costly when the feature size exceeds the domain of locality (of depth-one trees) that is characteristic of phrase structure grammar-based formalisms.",
        "One current line of research focuses on finding the best balance between parsing efficiency and parse selection techniques of increasing complexity, aiming to identify the most probable solution(s) with minimal effort.",
        "This paper explores a range of techniques, combining a broad-coverage, high-efficiency HPSG parser with a series of parse selection models with varying context size of features.",
        "We sketch three general scenarios for the integration: (a) a baseline sequential configuration, where all results are enumerated first, and subsequently ranked; (b) an interleaved but approximative solution, performing a greedy search for an n-best list of results; and (c) a two-phase approach, where a complete packed for-",
        "Proceedings of the 10th Conference on Parsing Technologies, pages 48-59, Prague, Czech Republic, June 2007.",
        "(c 2007 Association for Computational Linguistics",
        "est is created and combined with a specialized graph search procedure to selectively enumerate results in (globally) correct rank order.",
        "Although conceptually simple, the second technique has not previously been evaluated for HPSG parsing (to the best of our knowledge).",
        "The last of these techniques, which we call selective unpacking, was first proposed by Carroll & Oepen (2005) in the context of chart-based generation.",
        "However, they only provide an account of the algorithm for local ME properties and assert that the technique should generalize to larger contexts straightforwardly.",
        "This paper describes this generalization of selective unpacking, in its application to parsing, and demonstrates that the move from features that resemble a context-free domain of locality to features of, in principle, arbitrary context size can indeed be based on the same algorithm, but the required extensions are non-trivial.",
        "The structure of the paper is as follows.",
        "Section 2 summarizes our formalism, grammars used, parse selection approach, and training and test data.",
        "Section 3 discusses the range of possibilities for structuring the process of statistical, grammar-based parsing, and Sections 4 to 6 describe our approach to efficient n-best parsing.",
        "We present experimental results in Section 7, compare our approach to previous ones (Section 8), and finally conclude."
      ]
    },
    {
      "heading": "2. Overall Set-up",
      "text": [
        "While couched in the HPSG framework, the techniques explored here are applicable to the larger class of unification-based grammar formalisms.",
        "We make use of the DELPH-IN reference formalism, as implemented by a variety of systems, including the LKB (Copestake, 2002) and PET (Callmeier, 2002).",
        "For the experiments discussed here, we adapted the open-source PET parsing engine in conjunction with two publicly available grammars, the English Resource Grammar (ERG; Flickinger, 2000) and the DFKI German Grammar (GG; Muller & Kasper, 2000, Crysmann, 2005).",
        "Our parse selection models were trained and evaluated on HPSG treebanks that are distributed with these grammars.",
        "The following paragraphs summarize relevant properties of the structures manipulated by the parser,",
        "Figure 1: Sample HPSG derivation tree for the sentence the dog barks.",
        "Phrasal nodes are labeled with identifiers of grammar rules, and (pre-terminal) lexical nodes with class names for types of lexical entries.",
        "followed by relevant background on parse selection.",
        "Figure 1 shows an example ERG derivation tree.",
        "Internal tree nodes are labeled with identifiers of grammar rules, and leaves with lexical entries.",
        "The derivation tree provides complete information about the actual HPSG analysis, in the sense that it can be viewed as a recipe for computing it.",
        "Lexical entries and grammar rules alike are ultimately just feature structures, complex and highly-structured linguistic categories.",
        "When unified together in the configuration depicted by the derivation tree, the resulting feature structure yields an HPSG sign, a detailed representation of the syntactic and semantic properties of the input string.",
        "Just as the full derivation denotes a feature structure, so do its sub-trees, and for grammars like the ERG and GG each such structure will contain hundreds of feature-value pairs.",
        "Because of the lexicalized nature of HPSG (and similar frameworks) our parsers search for well-formed derivations in a pure bottom-up fashion.",
        "Other than that, there are no hard-wired assumptions about the order of computation, i.e. the specific parsing strategy.",
        "Our basic set-up closely mimics that of Oepen & Carroll (2002), where edges indexed by sub-string positions in a chart represent the nodes of the tree, recording both a feature structure (as its category label) and the identity of the underlying lexical entry or rule in the grammar.",
        "Multiple edges derived for identical sub-strings can be 'packed' into a single chart entry in case their feature structures are compatible, i.e. stand in an equivalence or subsumption relation.",
        "By virtue of having each edge keep back-pointers to its daughter edges – the immediate sub-nodes in the tree whose combination resulted in the mother edge – the parse forest provides a complete and explicitencoding ofall possible results in a maximally compact form.",
        "A simple unpacking procedure is obtained from the cross-multiplication of all local combinatorics, which is directly amenable to dynamic programming.",
        "Figure 2 shows a hypothetical forest (on the left), where sets of edges exhibiting local ambiguity have been packed into a single 'representative' edge, viz. the one in each set with one or more incoming dominance arcs.",
        "Confirming the findings of Oepen & Carroll (2002), in our experiments packing under feature structure subsumption is much more effective than packing under mere equivalence, i.e. for each pair of edges (over identical sub-strings) that stand in a subsumption relation, a technique that Oepen & Carroll (2002) termed retroactive packing ensures that the more general of the two edges remains in the chart.",
        "When packing under subsump-tion, however, some of the crossproduct of local ambiguities in the forest may not be globally consistent.",
        "Assume for example that, in Figure 2, edges \\6\\ and \\8\\ subsume [7J and \\9\\, respectively; combining [7J and \\9\\ into the same tree during unpacking can in principle fail.",
        "Thus, unpacking effectively needs to deterministically replay unifications, but this extra expense in our experience is negligible when compared to the decreased cost of constructing the forest under subsumption.",
        "In Section 3 we argue that this very property, in addition to increasing parsing efficiency, interacts beneficially with parse selection and on-demand enumeration of results in rank order",
        "Following (Johnson et al., 1999), a conditional ME model of the probabilities of trees (t1... tn} for a string s, and assuming a set of feature functions (f1... fm} with corresponding weights (Ai... Am}, is defined as:",
        "Type Sample Features",
        "Table 1: Examples of structural features extracted from the derivation tree in Figure 1.",
        "The Type column indicates the template corresponding to each sample feature; the integer that starts each feature indicates the degree ofgrandparenting (in the case of type 1 and 2 features) or n-gram size (type 3 features).",
        "The symbols A and < denote the root of the tree and left periphery ofthe yield, respectively.",
        "Feature functions fj can test for arbitrary structural properties of analyses U, and their value typically is the number of times a specific property is present in tj.",
        "Toutanova, Manning, Flickinger, & Oepen (2005) propose an inventory of features that perform well in HPSG parse selection; currently we restrict ourselves to the best-performing of these, of the form illustrated in Table 1, comprising depth-one sub-trees (or portions of these) with grammar-internal identifiers as node labels, plus optionally a chain of one or more dominating nodes (i.e. levels of grandparents).",
        "If a grandparents chain is present then the feature is non-local.",
        "For expository purposes, Table 1 includes another feature type, n-grams over leaf nodes of the derivation; in Section 5 below we speculate about the incorporation of these (and similar) features in our algorithm."
      ]
    },
    {
      "heading": "3. Interleaving Parsing and Ranking",
      "text": [
        "At an abstract level, given a grammar and an associated ME parse selection model, there are three basic ways of combining them in order to find the single 'best' or small set of n-best results.",
        "The first way is a naive sequential set-up, in which the parser first enumerates the full set of analyses, computes a score for each using the model, and returns the highest-ranking n results.",
        "For carefully",
        "(0 subjh hspec third_sg_fin_verb) (1 A subjh hspec third_sg_fin_verb) (0 hspec deLtheJe sing_noun) (1 subjh hspec deLtheJe sing_noun) (2 A subjh hspec deLtheJe sing_noun) (0 subjh third_sg_fin_verb) (0 subjh hspce) (1 subjh hspec deLtheJe) (1 subjh hspec sing_noun) (1 nJntrJe dog) (2 deLtheJe nJntrJe dog) (3 < deLtheJe nJntrJe dog)",
        "Figure 2: Sample forest and sub-node decompositions: ovals in the forest (on the left) indicate packing of edges under subsump-tion, i.e. edges [7J, \\9\\, and |111 are not in the chart proper.",
        "During unpacking, there will be multiple ways of instantiating a chart edge, each obtained from cross-multiplying alternate daughter sequences locally.",
        "The elements of this crossproduct we call decomposition, and they are pivotal points both for stochastic scoring and dynamic programming in selective unpacking.",
        "The table on the right shows all non-leaf decompositions for our example packed forest: given two ways of decomposing \\6\\, there will be three candidate ways of instantiating \\2\\ and six for [4], respectively, for a total of nine full trees.",
        "crafted grammars and inputs of average complexity the approach can perform reasonably well.",
        "Another mode of operation is to organize the parser's search according to an agenda (i.e. priority queue) that assigns numeric scores to parsing moves (Erbach, 1991).",
        "Each such move is an application of the fundamental rule of chart parsing, combining an active and a passive edge, and the scores represent the expected 'figure of merit' (Caraballo & Char-niak, 1998) of the resulting structure.",
        "Assuming a parse selection model of the type sketched in Section 2, we can determine the agenda priority for a parsing move according to the (unnormalized) ME score of the derivation (sub-)tree that would result from its successful execution.",
        "Note that, unlike in probabilistic context-free grammars (PCFGs), ME scores of partial trees do not necessarily decrease as the tree size increases; instead, the distribution of feature weights is in the range ( – to, +oo), centered around 0, where negative weights intuitively correspond to dis-preferred properties.",
        "This lack of monotonicity in the scores associated with sub-trees, on the one hand, is beneficial, in that performing a greedy best-first search becomes practical: in contrast, with PCFGs and their monotonically decreasing probabilities on larger sub-trees, once the parser finds the first full tree the chart necessarily has been instantiated almost completely.",
        "On the other hand, the same property prohibits the application of exact best-first techniques like A* search, because there is no reliable future cost estimate; in this respect, our set-up differs fundamentally from that of Klein & Manning (2003) and related PCFG parsing work.",
        "Using the unnormalized sum of ME weights on a partial solution as its agenda score, effectively, means that sub-trees with low scores 'sink' to the bottom of the agenda; highly-ranked partial constituents, in turn, instigate the immediate creation of larger structures, and ideally the bottom-up agenda-driven search will greedily steer the parser towards full analyses with high scores.",
        "Given its heuristic nature, this procedure cannot guarantee that its n-best list of results corresponds to the globally correct rank order, but it may in practice come reasonably close to it.",
        "While conceptually simple, greedy best-first search does not combine easily with ambiguity packing in the chart: (a) at least when packing under subsumption, it is not obvious how to accurately compute the agenda score of packed nodes, and (b) to the extent that the greedy search avoids exploration of dis-preferred local ambiguity, the need for packing should be greatly reduced.",
        "Unfortunately, in scoring bottom-up parsing moves, ME features involving grandparenting are not applicable, leading to a second potential source of reduced parse selection accuracy.",
        "In Section 7 below, we provide an empirical evaluation of both the naive sequential and greedy best-first approaches."
      ]
    },
    {
      "heading": "4. Selective Unpacking",
      "text": [
        "Carroll & Oepen (2005) observe that, at least for grammars like the ERG, the construction of the parse forest can be very efficient (with observed polynomial complexity), especially when packing edges under subsumption.",
        "Their selective unpacking procedure, originally proposed for the forest created by a chart generator, aims to unpack the n-best set"
      ]
    },
    {
      "heading": "1. procedure selectively-unpack-edge(edge, n) =",
      "text": [
        "4 hypothesis – hypothesize-edge(edge, i); i – i + 1;"
      ]
    },
    {
      "heading": "5. if (new – instantiate-hypothesis(hypothesis)) then",
      "text": []
    },
    {
      "heading": "7. while (hypothesis and n > 1)",
      "text": []
    },
    {
      "heading": "8. return results;",
      "text": []
    },
    {
      "heading": "9. procedure hypothesize-edge(edge, i) =",
      "text": []
    },
    {
      "heading": "10. if (edge.hypotheses[i]) return edge.hypotheses[i];",
      "text": []
    },
    {
      "heading": "12. for each (decomposition in decompose-edge(edge)) do",
      "text": []
    },
    {
      "heading": "14. for each (edge in decomposition.rhs) do",
      "text": []
    },
    {
      "heading": "15. daughters – daughters e (hypothesize-edge(edge, 0));",
      "text": []
    },
    {
      "heading": "17. new-hypothesis(edge, decomposition, daughters, indices);",
      "text": []
    },
    {
      "heading": "18. if (hypothesis – edge.agenda.pop()) then",
      "text": [
        "19 for each (indices in advance-indices(hypothesis.indices)) do 20 if (indices e hypothesis.decomposition.indices) then continue 22 for each (edge in hypothesis.decomposition.rhs) each (i in indices) do"
      ]
    },
    {
      "heading": "23. daughter – hypothesize-edge(edge, i);",
      "text": []
    },
    {
      "heading": "24. if (not daughter) then daughters – ( ) ; break",
      "text": []
    },
    {
      "heading": "25. daughters – daughters e (daughter);",
      "text": [
        "26 if (daughters) then new-hypothesis(edge, hypothesis.decomposition, daughters, indices)"
      ]
    },
    {
      "heading": "27. edge.hypotheses[i] – hypothesis;",
      "text": []
    },
    {
      "heading": "28. return hypothesis;",
      "text": [
        "29 procedure new-hypothesis(edge, decomposition, daughters, indices) = 30 hypothesis – new hypothesis(decomposition, daughters, indices); 31 edge.agenda.insert(score-hypothesis(hypothesis), hypothesis);"
      ]
    },
    {
      "heading": "32. decomposition.indices – decomposition.indices u {indices};",
      "text": [
        "Figure 3: Selective unpacking procedure, enumerating the n best realizations for a top-level result edge from a packed forest.",
        "An auxiliary function decompose-edge() performs local cross-multiplication as shown in the examples in Figure 2.",
        "Another utility function not shown in pseudo-code is advance-indices(), a 'driver' routine searching for alternate instantiations of daughter edges, e.g. advance-indices((0 2 1)) – {(1 2 1) (0 3 1) (0 2 2)}.",
        "Finally, instantiate-hypothesis() is the function that actually builds result trees, replaying the unifications of constructions from the grammar (as identified by chart edges) with the feature structures of daughter constituents.",
        "of full trees from the forest, guaranteeing the globally correct rank order according to the probability distribution, with a minimal amount of search.",
        "The basic algorithm is a specialized graph search through the forest, with local contexts of optimization corresponding to packed nodes.",
        "Each such node represents local combinatorics, and two key notions in the selective unpacking procedure are the concepts of (a) decomposing an edge locally into candidate ways of instantiating it, and of (b) nested contexts of local search for ranked hypotheses (i.e. uninstantiated edges) about candidate subtrees.",
        "See Figure 2 for examples of the decomposition of edges.",
        "Given one decomposition – i.e. a vector of candidate daughters for a particular rule – there can be multiple ways of instantiating each daughter: a parallel index vector / = (i0 ... in) serves to keep track of 'vertical' search among daughter hypotheses, where each index ij denotes the i-th best instantiation (hypothesis) of the daughter at position j.",
        "If we restrict ME features to a depth of one (i.e. without grandparenting), then given the additive nature of ME scores on complete derivations, it can be guaranteed that hypothesized trees including an edge e as an immediate daughter must use the best instantiation of e in their own best instantiation.",
        "Assuming a binary rule, the corresponding hypothesis would use daughter indices of (0 0).",
        "The second-best instantiation, in turn, can be obtained from moving to the second-best hypothesis forone ofthe elements inthe (right-hand side of the) decomposition, e.g. indices (0 1) or (1 0) in the binary example.",
        "Hypotheses are associated with ME scores and ordered within each nested context by means of a local priority queue (stored in the original representative edge, for convenience).",
        "Therefore, nested local optimizations result in a top-down, breadth-first, exact n-best search through the packed forest, while avoiding exhaustive cross-multiplication of packed nodes.",
        "Figure 3 shows the unchanged pseudo-code of Carroll & Oepen (2005).",
        "The main function hypothesize-edge() controls both the 'horizontal' and 'vertical' search, initializing the set of decompositions and pushing initial hypotheses onto the local agenda when called on an edge for the first time (lines 11-17).",
        "For each call, the procedure retrieves the current next-best hypothesis from the agenda (line 18), generates new hypotheses by advancing daughter indices (while skipping over configurations seen earlier) and calling itself recursively for each new index (lines 19-26), and, finally, arranging for the resulting hypothesis to be cached for later invocations on the same edge and i values (line 27).",
        "Note that unification (in instantiate-hypothesis()) is only invoked on complete, top-level hypotheses, as our structural ME features can actually be evaluated prior to building each full feature structure.",
        "However, as Carroll & Oepen (2005) suggest, the procedure could be adapted to perform instantiation of sub-hypotheses within each local search, should additional features require it.",
        "For better efficiency, the instantiate-hypothesis() routine applies dynamic programming (i.e. memoization) to intermediate results."
      ]
    },
    {
      "heading": "5. Generalizing the Algorithm",
      "text": [
        "Carroll & Oepen (2005) offer no solution for selective unpacking with larger context ME features.",
        "Yet, both Toutanova et al.",
        "(2005) and our own experiments (described in Section 7 below) suggest that properties of larger contexts and especially grandparenting can greatly improve parse selection accuracy.",
        "The following paragraphs outline how to generalize the basic selective unpacking procedure, while retaining its key properties: exact n-best enumeration with minimal search.",
        "Our generalization of the algorithm distinguishes between 'upward' contexts, with grandparenting with dominating nodes as a representative feature type, and 'downward' extensions, which we discuss for the example of lexical n-gram features.",
        "A naive approach to selective unpacking with grandparenting might be extending the cross-multiplication of local ambiguity to trees of more than depth one.",
        "However, with multiple levels of grandparenting this approach would greatly increase the combinatorics to be explored, and it would pose the puzzle of overlapping local contexts of optimization.",
        "Choices made among the alternates for one packed node would interact with other ambiguity contexts in their internal nodes, rather than merely at the leaves of their decompositions.",
        "However, it is sufficient to keep the depth of decompositions to minimal sub-trees and rather contextualize each decomposition as a whole.",
        "Assuming our sample forest and set of decompositions from Figure 2, let ([111) :[6]-> (Qo]) denote the decomposition of node M in the context of H and U as its immediate parents.",
        "When descending through the forest, hypothesize-edge() can, without significant extra cost, maintain a vector P = (pn ... p0) of parents of the current node, for n-level grandparenting.",
        "For each packed node, the bookkeeping elements of the graph search procedure need to be contextualized on p, viz. (a) the edge-local priority queue, (b) the record of index vectors hypothesized already, and (c) the cache of previous instantiations.",
        "Assuming each is stored in an associative array, then all references to edge.agenda in the original procedure can be replaced by edge.agenda[P], and likewise for other slots.",
        "With these extensions in place, the original control structure of nested, on-demand creation of hypotheses and dynamic programming of partial results can be retained, and for each packed node with multiple parents (\\6\\ in our sample forest) there will be parallel, contextualized partitions of optimization.",
        "Thus, extra combinatorics introduced in this generalized procedure are confined to only such nodes, which (intuitively at least) appears to establish the lower bound of added search needed – while keeping the algorithm non-approximative.",
        "Section 7 provides empirical data on the degradation of the procedure in growing levels of grandparenting and the number of n-best results to be extracted from the forest.",
        "Finally, we turn to enlarged feature contexts that capture information from nodes below the elements of a local decomposition.",
        "Consider the example of feature type 3 in Table 1, n-grams (of various size) over properties of the yield of the parse tree.",
        "For now we only consider lexical bi-grams.",
        "For an edge e dominating a sub-string of n words (wi ... wi+n_ i) there will be n – 1 bi-grams internal to e, and two bi-grams that interact with wi_1and wi+n – which will be determined by the left-and right-adjacent edges to e in a complete tree.",
        "The internal bi-grams are unproblematic, and we can assume that ME weights corresponding to these features have been included in the sum of weights associated to e. Seeing that e may occur in multiple trees, with different sister edges, the selective unpacking procedure has to take this variation into account when evaluating local contexts of optimization.",
        "Let xey denote an edge e, with x and y as the lexical types of its leftmost and rightmost daughters, respectively.",
        "Returning to our sample forest, assume legalizations rtfiok and 7fTT17 (each spanning only one word), with f3 = 7.",
        "Obviously, when decomposing g] as (\\8\\ \\6\\), its ME score, in turn, will depend on the choice made in the expansion of \\6\\: the sequences (aHL and (aHL 7HU) will differ in (at least) the scores associated with the bi-grams (a/3) vs. (a 7).",
        "Accordingly, when evaluating candidate decompositions of H, the number of hypotheses that need to be considered is doubled; as an immediate consequence, there can be up to eight distinct lexicalized variants for the decomposition [Jj – > (0][3]) further up in the tree.",
        "It may look as if combinatorics will cross-multiply throughout the tree – in the worst case returning us to an exponential number of hypotheses – but this is fortunately not the case: regarding the external bi-grams of U, node \\6\\ no longer participates in its left-or rightmost periphery, so variation internal to \\6\\ is not a multiplicative factor at this level.",
        "This is essentially the observation of Langkilde (2000), and her bottom-up factoring of n-gram computation is easily incorporated into our top-down selective unpacking control structure.",
        "At the point where hypothesize-edge() invokes itself recursively (line 23 in Figure 3), its return value is now a set of lexicalized alternates, and hypothesis creation (in line 26) can take into account the local crossproduct of all such alternation.",
        "Including additional properties from non-local subtrees (for example higher-order n-grams and head lexicalization) is a straightforward extension of this scheme, replacing our per-edge left-and rightmost periphery symbols with a generalized vector of externally relevant, internal properties.",
        "In addition to traditional (head) lexicalization as we have just discussed it, such extended 'downward' properties on decompositions – percolated from daughters to mothers and cross-multiplied as appropriate – could include metrics of constituent weight too, for example to enable the ME model to prefer 'balanced' coordination structures.",
        "However, given that Toutanova et al.",
        "(2005) obtain only marginally improved parse selection accuracy from the inclusion of n-gram (and other lexical) ME features, we have left the implementation of lex-icalization and empirical evaluation for future work."
      ]
    },
    {
      "heading": "6. Failure Caching and Propagation",
      "text": [
        "As we pointed out at the end of Section 4, during the unpacking phase, unification is only replayed in instantiate-hypothesis() on the top-level hypotheses.",
        "It is only at this step that inconsistencies in the local combinatorics are discovered.",
        "However, such a discovery can be used to improve the unpacking routine by (a) avoiding further unification on hypotheses that have already failed to instantiate, (b) avoiding creating new hypotheses based on failed sub-hypotheses.",
        "This requires some changes to the routines instantiate-hypothesis() and hypothesize-edge(), as well as an extra boolean marker for each hypothesis.",
        "The extended instantiate-hypothesis() starts by checking whether the hypothesis is already marked as failed.",
        "If it is not so marked, the routine recursively instantiates all sub-hypotheses.",
        "Any failure will again lead to instant return.",
        "Otherwise, unification is used to create a new edge from the outcome of the sub-hypothesis instantiations.",
        "If this unification fails, the current hypothesis is marked.",
        "Moreover, all its ancestor hypotheses are also marked (by recursively following the pointers to the direct parent hypotheses) as they are also guaranteed to fail.",
        "Correspondingly, hypothesize-edge() needs to check the instantiation failure marker to avoid returning hypotheses that are guaranteed to fail.",
        "If a hypothesis coming out of the agenda is already marked as failed, it will be used to create new hypotheses (with advance-indices()), but dropped afterward.",
        "Subsequent hypotheses will be popped from the agenda until either a hypothesis that is not marked as failed is returned, or the agenda is empty.",
        "Moreover, hypothesize-edge() also needs to avoid creating new hypotheses based on failed sub-hypotheses.",
        "When a failed sub-hypothesis is found, the creation of the new hypothesis is skipped.",
        "But the index vector I may not be simply discarded.",
        "Otherwise hypotheses based on advance-indices(I) will not be reachable in the search.",
        "On the other hand, simply adding every advance-indices(I) on to the pending creation list is not efficient either in the case where multiple sub-hypotheses fail.",
        "To solve the problem, we compute a failure vector F = (f0 ... fn), where fj is 1 when the sub-hypothesis at position j is known as failed, and 0 otherwise.",
        "If a sub-hypothesis at position j is failed then all the index vectors having value ij at position j must also fail.",
        "By putting the result of + F on the pending creation list, we can safely skip the failed rows of sub-hypotheses, while not losing the reachability of the others.",
        "As an example, suppose we have a ternary index vector (3 12) for which a new hypothesis is to be created.",
        "By checking the instantiation failure marker of the sub-hypotheses, we find that the first and the third sub-hypotheses are already marked.",
        "The failure recording vector will then be (1 0 1).",
        "By putting (413) = (3 12) + (10 1) on to the pending hypothesis creation list, the failed sub-hypotheses are skipped.",
        "We evaluate the effects of instantiation failure caching and propagation below in Section 7."
      ]
    },
    {
      "heading": "7. Empirical Results",
      "text": [
        "To evaluate the performance ofthe selective unpacking algorithm, we carried out a series of empirical evaluations with the ERG and GG, in combination with a modified version of the PET parser.",
        "When running the ERG we used as our test set the JH4 section of the LOGON treebank , which contains 1603 items with an average sentence length of 14.6 words.",
        "The remaining LOGON treebank (of around",
        "Table 2: Coverage on the ERG for different configurations, with fixed resource consumption limits (of 100k passive edges or 300 seconds).",
        "In all cases, up to ten 'best' results were searched, and Coverage shows the percentage of inputs that succeed to parse within the available resource.",
        "Time shows the end-to-end processing time for each batch.",
        "String Length (Number of Input Tokens)",
        "Figure 4: Parsing times for different configurations using the ERG, in all three cases searching for up to ten results, without the use of grandparenting.",
        "8,000 items) was used in training the various ME parse disambiguation models.",
        "For the experiment with GG, we designated a 2825-item portion of the DFKI Verbmobil treebank for our tests, and trained ME models on the remaining 10,000 utterances.",
        "At only 7.4 words, the average sentence length is much shorter in the Verbmobil data.",
        "We ran seven different configurations of the parser with different search strategies and (un-)packing mechanisms:",
        "• Agenda driven greedy n-best parsing using the ME score without grandparenting features; no local ambiguity packing;",
        "• Local ambiguity packing with exhaustive unpacking, without grandparenting features;",
        "Configuration",
        "GP",
        "Coverage",
        "Time (s)",
        "greedy best-first",
        "0",
        "91.6%",
        "3889",
        "exhaustive unpacking",
        "0",
        "84.5%",
        "4673",
        "0",
        "94.3%",
        "2245",
        "1",
        "94.3%",
        "2529",
        "selective unpacking",
        "2",
        "94.3%",
        "3964",
        "3",
        "94.2%",
        "3199",
        "4",
        "94.2%",
        "3502",
        "Figure 5: Mean times for selective unpacking of all test items for n-best parsing with the ERG, for varying n and grandparenting (GP) levels",
        "• Local ambiguity packing and selective unpacking for n-best parsing, with 0 through 4 levels of grandparenting (GP) features.",
        "As a side-effect of differences in efficiency, some configurations could not complete parsing all sentences given reasonable memory constraints (which we set at a limit of 100k passive edges or 300 seconds processing time per item).",
        "The overall coverage and processing time of different configurations on JH4 are given in Table 2.",
        "The correlation between processing time and coverage is interesting.",
        "However, it makes the efficiency comparison difficult as parser behavior is not clearly defined when the memory limit is exceeded.",
        "To circumvent this problem, in the following experiments we average only over those 1362 utterances from JH4 that complete parsing within the resource limit in all seven configurations.",
        "Nevertheless, it must be noted that this restriction potentially reduces efficiency differences between configurations, as some of the more challenging inputs (which typically lead to the largest differences) are excluded.",
        "Figure 4 compares the processing time of different configurations.",
        "The difference is much more significant for longer sentences (i.e. with more than 15 words).",
        "If the parser unpacks exhaustively, the time for unpacking grows with sentence length at a quickly increasing rate.",
        "In such cases, the efficiency gain with ambiguity packing in the parsing phase is mostly lost in the unpacking phase.",
        "The graph shows that greedy best-first parsing without packing outperforms exhaustive unpacking for sentences of",
        "Table 3: Parse selection accuracy for various levels ofgrandpar-enting.",
        "The exact match column shows the percentage of cases in which the correct tree, according to the treebank, was ranked highest by the model; conversely, the top ten column indicates how often the correct tree was among the ten top-ranking results.",
        "less than 25 words.",
        "With sentences longer than 25 words, the packing mechanism helps the parser to overtake greedy best-first parsing, although the exhaustive unpacking time also grows fast.",
        "With the selective unpacking algorithm presented in the previous sections, unpacking time is reduced, and grows only slowly as sentence length increases.",
        "Unpacking up to ten results, when contrasted with the timings for forest creation (i.e. the first parsing phase) in Figure 4, adds a near-negligible extra cost to the total time required for both phases.",
        "Moreover, Figure 5 shows that with selective unpacking, as n is increased, unpacking time grows roughly linearly for all levels of grandparenting (albeit always with an initial delay in unpacking the first result).",
        "Table 4 summarizes a number of internal parser measurements using the ERG with different packing/unpacking settings.",
        "Besides the difference in processing time, we also see a significant difference in \"space\" between exhaustive and selective unpacking.",
        "Also, the difference in \"unifications \" and \"copies \" indicates that with our selective unpacking algorithm, these expensive operations on typed feature structures are significantly reduced.",
        "In return for increased processing time (and marginal loss in coverage) when using grandparenting features, Table 3 shows some large improvements in parse selection accuracy (although the picture is less clear-cut at higher-order levels of grand-parenting).",
        "A balance point between efficiency",
        "Configuration",
        "Exact Match",
        "Top Ten",
        "random choice",
        "11.34",
        "43.06",
        "no grandparenting",
        "52.52",
        "68.38",
        "greedy best-first",
        "51.79",
        "69.48",
        "grandparenting[ 1 ]",
        "56.83",
        "85.33",
        "grandparenting[2]",
        "56.55",
        "84.14",
        "grandparenting[3]",
        "56.37",
        "84.14",
        "grandparenting[4]",
        "56.28",
        "84.51",
        "Table 4: Contrasting the efficiency of various (un-)packing settings in use with ERG on short (top) and medium-length (bottom) inputs; in each configuration, up to ten trees are extracted.",
        "Unification and Copies is the count of top-level FS operations, where only successful unifications require a subsequent copy (when creating a new edge).",
        "Unpack and Total are unpacking and total parse time, respectively.",
        "and accuracy can be made according to application needs.",
        "Finally, we compare the processing time of the selective unpacking algorithm with and without instantiation failure caching and propagation (as described in Section 4 above).",
        "The empirical results for GG are summarized in Table 5, showing clearly that the technique reduced unnecessary hypotheses and instantiation failures.",
        "The design philosophy of the ERG and GG differ During the first, forest creation phase, GG suppresses a number of features (in the HPSG sense, not the ME sense) that can actually constrain the combinatorics of edges.",
        "This move makes the packed forest more compact, but it implies that uniication failures will be more frequent during unpacking.",
        "In a sense, GG thus moves part of the search for globally consistent derivations into the second phase, and it is possible for the forest to contain 'result' trees that ultimately turn out to be incoherent.",
        "Dynamic programming of instantiation failures makes this approach tractable, while retaining the general breadth-irst characteristic of the selective unpacking regime.",
        "further optimization of hyper-parameters for individual conig-urations would moderately improve model performance, especially for higher-order grandparenting levels with large numbers of features."
      ]
    },
    {
      "heading": "8. Discussion",
      "text": [
        "The approach to n-best parsing described in this paper takes as its point of departure recent work of Carroll & Oepen (2005), which describes an efficient algorithm for unpacking n-best trees from a forest produced by a chart-based sentence generator and containing local ME properties with associated weights.",
        "In an almost contemporaneous study, but in the context of parsing with treebank grammars, Huang & Chiang (2005) develop a series of increasingly efficient algorithms for unpacking n-best results from a weighted hypergraph representing a parse forest.",
        "The algorithm of Carroll & Oepen (2005) and the inal one of Huang & Chiang (2005) are essentially equivalent, and turn out to be reformulations of an approach originally described by Jimenez & Marzal (2000) (although expressed there only for grammars in Chomsky Normal Form).",
        "In this paper we have considered ME properties that extend beyond immediate dominance relations, extending up to 4 levels of grandparenting.",
        "Previous work has either assumed properties that are restricted to the minimal parse fragments (i.e. subtrees of depth one) that make up the packed representation (Geman & Johnson, 2002), or has taken a more relaxed approach by allowing non-local prop-",
        "Configuration",
        "GP",
        "Unifications",
        "(#)",
        "Copies",
        "(#)",
        "Space",
        "(kbyte)",
        "Unpack",
        "(s)",
        "Total",
        "(s)",
        "greedy best-first",
        "0",
        "1845",
        "527",
        "2328",
        "-",
        "0.12",
        "exhaustive unpacking",
        "0",
        "2287",
        "795",
        "8907",
        "0.01",
        "0.12",
        "< 15 words",
        "0",
        "1912",
        "589",
        "8109",
        "0.00",
        "0.12",
        "1",
        "1913",
        "589",
        "8109",
        "0.01",
        "0.12",
        "selective unpacking",
        "2",
        "1914",
        "589",
        "8109",
        "0.01",
        "0.12",
        "3",
        "1914",
        "589",
        "8110",
        "0.01",
        "0.12",
        "4",
        "1914",
        "589",
        "8110",
        "0.02",
        "0.13",
        "greedy best-first",
        "0",
        "25233",
        "5602",
        "24646",
        "-",
        "1.66",
        "exhaustive unpacking",
        "0",
        "39095",
        "15685",
        "80832",
        "0.85",
        "1.95",
        "> 15 words",
        "0",
        "17489",
        "4422",
        "33326",
        "0.03",
        "1.17",
        "1",
        "17493",
        "4421",
        "33318",
        "0.05",
        "1.21",
        "selective unpacking",
        "2",
        "17493",
        "4421",
        "33318",
        "0.09",
        "1.25",
        "3",
        "17495",
        "4422",
        "33321",
        "0.13",
        "1.27",
        "4",
        "17495",
        "4422",
        "33320",
        "0.21",
        "1.34",
        "Table 5: Efficiency effects of the instantiation failure caching and propagation with GG, without grandparenting.",
        "All statistics are averages over the 1941 items that complete within the resource bounds in all three configurations.",
        "Unification, Copies, Unpack, and Total have the same interpretation as in Table 4, and Hypotheses is the average count of hypothesized sub-trees.",
        "erties but without addressing the problem of how to eficiently extract the top-ranked trees from a packed forest (Miyao & Tsujii, 2002).",
        "Probably the work closest in spirit to our approach is that of Malouf & van Noord (2004), who use an HPSG grammar comparable to the ERG and GG, non-local ME features, and a two-phase parse forest creation and unpacking approach.",
        "However, their unpacking phase uses a beam search to ind a good (single) candidate for the best parse; in contrast – for ME models containing the types of non-local features that are most important for accurate parse selection – we avoid an approximative search and efficiently identify exactly the n-best parses.",
        "When parsing with context free grammars, a (single) parse can be retrieved from a parse forest in time linear in the length of the input string (Billot & Lang, 1989).",
        "However, as discussed in Section 2, when parsing with a uniication-based grammar and packing under feature structure subsump-tion, the crossproduct of some local ambiguities may not be globally consistent.",
        "This means that additional uniications are required at unpacking time.",
        "In principle, when parsing with a pathological grammar with a high rate of failure, extracting a single consistent parse from the forest could take exponential time (see Lang (1994) for a discussion of this issue with respect to Indexed Grammars).",
        "In the case of GG, a high rate of unification failure in unpacking is dramatically reduced by our instantiation failure caching and propagation mechanism."
      ]
    },
    {
      "heading": "9. Conclusions and Future Work",
      "text": [
        "We have described and evaluated an algorithm for eficiently computing the n-best analyses from a parse forest produced by a uniication grammar, with respect to a Maximum Entropy (ME) model containing two classes of non-local features.",
        "The algorithm is eficient in that it empirically exhibits a linear relationship between processing time and the number of analyses unpacked, at all degrees of ME feature non-locality.",
        "It improves over previous work in providing the only exact procedure for retrieving n-best analyses from a packed forest that can deal with features with extended domains of locality and with forests created under subsumption.",
        "Our algorithm applies dynamic programming to intermediate results and local failures in unpacking alike.",
        "The experiments compared the new algorithm with baseline systems representing other possible approaches to parsing with ME models: (a) a single phase of agenda-driven parsing with on-line pruning based on intermediate ME scores, and (b) two-phase parsing with exhaustive unpacking and post-hoc ranking of complete trees.",
        "The new approach showed better speed, coverage, and accuracy than the baselines.",
        "Although we have dealt with the non-local ME features that in previous work have been found to be the most important for parse selection (i.e. grandparenting and n-grams), this does not exhaust the full range of features that could possibly be useful.",
        "For example, it may be the case that accurately resolving some kinds of ambiguities can only be done with reference to particular parts – or combinations of parts – of the HPSG feature structures representing the analysis of a complete constituent.",
        "To deal with such cases we are currently designing an extension to the algorithms described here which would add a 'controlled' beam search, in which the size of the beam was limited by the interval of score adjustments for ME features that could only be evaluated once the full linguistic structure became available.",
        "This approach would involve a constrained amount of extra search, but would still produce the exact n-best trees.",
        "Configuration",
        "Unifications",
        "(#)",
        "Copies",
        "(#)",
        "Hypotheses",
        "(#)",
        "Space",
        "(kbyte)",
        "Unpack",
        "(ms)",
        "Total",
        "(ms)",
        "greedy best-first",
        "5980",
        "1447",
        "-",
        "9202",
        "-",
        "400",
        "selective, no caching",
        "5535",
        "1523",
        "1245",
        "27188",
        "70",
        "410",
        "selective, with cache",
        "4915",
        "1522",
        "382",
        "27176",
        "10",
        "350"
      ]
    }
  ]
}
