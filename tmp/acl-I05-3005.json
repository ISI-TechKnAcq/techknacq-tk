{
  "info": {
    "authors": [
      "Huihsin Tseng",
      "Daniel Jurafsky",
      "Christopher D. Manning"
    ],
    "book": "Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-I05-3005",
    "title": "Morphological features help POS tagging of unknown words across language varieties",
    "url": "https://aclweb.org/anthology/I05-3005",
    "year": 2005
  },
  "references": [
    "acl-A00-1031",
    "acl-C02-1145",
    "acl-J93-2004",
    "acl-N03-1033",
    "acl-P99-1023",
    "acl-W00-1201",
    "acl-W04-3236",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Part-of-speech tagging, like any supervised statistical NLP task, is more difficult when test sets are very different from training sets, for example when tagging across genres or language varieties.",
        "We examined the problem of POS tagging of different varieties of Mandarin Chinese (PRC-Mainland, PRC-Hong Kong, and Taiwan).",
        "An analytic study first showed that unknown words were a major source of difficulty in cross-variety tagging.",
        "Unknown words in English tend to be proper nouns.",
        "By contrast, we found that Mandarin unknown words were mostly common nouns and verbs.",
        "We showed these results are caused by the high frequency of morphological compounding in Mandarin; in this sense Mandarin is more like German than English.",
        "Based on this analysis, we propose a variety of new morphological unknown-word features for POS tagging, extending earlier work by others on unknown-word tagging in English and German.",
        "Our features were implemented in a maximum entropy Markov model.",
        "Our system achieves state-of-the-art performance in Mandarin tagging, including improving unknown-word tagging performance on unseen varieties in Chinese Treebank 5.0 from 61% to 80% correct."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Part-of-speech tagging is an important enabling task for natural language processing, and state-of-the-art taggers perform quite well, when training and test data are drawn from the same corpus.",
        "Part-of-speech tagging is more difficult, however, when a test set is drawn from a corpus that includes significantly different varieties of the language.",
        "One factor that may play a role in this cross-variety difficulty is the presence of test-set words that were unseen in cross-variety training sets.",
        "We chose Mandarin Chinese to study this question of cross-variety and unknown-word POS tagging.",
        "Mandarin is both a spoken and a written language; as a written language, it is the official written language of the PRC (Mainland and Hong Kong), and Taiwan.",
        "Thus regardless of which dialect people speak at home, they write in Mandarin.",
        "But the varieties of Mandarin written in the PRC (Mainland and Hong Kong) and Taiwan differ in orthography, lexicon, and even grammar about as much as the British, American, and Australian varieties of English (or more in some cases).",
        "The corpus we use, Chinese Treebank 5.0 (Palmer et al., 2005), contains data from the three language varieties as well as different genres within the varieties.",
        "It thus provides a good data set for the impact of language variation on tagging performance.",
        "Previous work on POS tagging of unknown words has proposed a number of features based on prefixes and suffixes and spelling cues like capitalization (Toutanova et al.",
        "2003, Brants 2000, Ratnaparkhi 1996).",
        "For example, these systems followed Samuelsson (1993) in using n-grams of letter sequences ending and starting each word as unknown word features.",
        "But these features have mainly been tested on inflectional languages like English and German, whose derivational and inflectional affixes tend to be a strong indicator of word classes; Brants (2000), for example, showed that an English word ending in the suffix able was very likely to be an adjective.",
        "Chinese, by contrast, has more than 4000 frequent affix characters.",
        "The amount of training data for each affix is thus quite sparse and (as we will show later) Chinese affixes are quite ambiguous in their part-of-speech identity.",
        "Furthermore, it is possible that n-gram features may not be suited to Chinese at all, since Chinese words are much shorter than English (averaging 2.4 characters per word compared with 7.7 for English, for unknown words in CTB 5.0 and Wall Street Journal (Marcus et el., 1993)).",
        "In order to deal with these difficulties, we first performed an analytic study with the goal of understanding the morphological properties of unknown words in Chinese.",
        "Based on this analysis, we then propose new morphological features for addressing the unknown word problem.",
        "We also showed how these features could make use of a non-CTB corpus that had been labeled with very different POS tags, by converting those tags into features.",
        "The remainder of the paper is organized as follows.",
        "The next section is concerned with a corpus analysis of cross language variety differences and introduces Chinese morphology.",
        "In Section 3, we evaluate a number of lexical, sequence, and linguistic features.",
        "Section 4 reviews related work and summarizes our contribution.",
        "source as the training data (XH), 4.63% of the words were unseen in training, compared to the much larger numbers of unknown words in the cross-variety datasets (14.3% and 16.7%).",
        "Some of this difference is probably due to genre as well, especially for the outlier-genre SM set."
      ]
    },
    {
      "heading": "2 Data",
      "text": [
        "Chinese Treebank 5.0 (CTB) contains 500K words of newspaper and magazine articles annotated with segmentation, part-of-speech, and syntactic constituency information.",
        "It includes data from three major media sources, XH1 from PRC, HKSAR2 from Hong Kong, and SM3 from Taiwan.",
        "In terms of genre, both XH and HKSAR focus on politics and economic issues, and SM more on topics such as culture, health, education and travel.",
        "All of the files in CTB are encoded using Guo Biao (GB) and use simplified characters.",
        "We did some cleanup of character encoding errors in CTB before running our experiments.",
        "Taiwan and Hong Kong still use the traditional forms of characters, while PRC-Mainland has adopted simplified forms of many characters, which also collapse some distinctions between characters.",
        "Additionally a different character set encoding is standardly used.",
        "The articles in HKSAR and SM originally used traditional characters and Big 5 encoding, but prior to inclusion in the CTB corpus they had been converted into simplified characters and GB.",
        "Some errors seem to have crept into this conversion process, accidentally leaving traditional characters such as ᕠ instead of simplified ৢ (after), ᮐ for Ѣ (for), ⫮ 咑 and Ҕ咑 and Ҕ М (what), all of which we fixed.",
        "We also normalized half width numbers, alphabets, and punctuation to full width.",
        "Finally we removed the - NONE traces left over from CTB parse trees."
      ]
    },
    {
      "heading": "3 Corpus analysis",
      "text": [
        "We begin with an analytic study of potential problems for POS tagging on cross language variety data."
      ]
    },
    {
      "heading": "3.1 More unknown words across varieties?",
      "text": [
        "We first test our hypothesis that a test set from a different language variety will contain more unknown words.",
        "Table 1 has the number of words in our devset that were unseen in the XH-only training set (we describe our training/dev/test split more fully in the next section).",
        "The devset contains equal amounts of data from all three varieties (XH, HKSAR, and SM).",
        "As table 1 shows, in data taken from the same"
      ]
    },
    {
      "heading": "3.2 What are the unknown words?",
      "text": [
        "In this section, we analyze the part-of-speech characteristics of the unknown words in our devset.",
        "Table 2 shows that the majority of Chinese unknown words are common nouns (NN) and verbs (VV).",
        "This holds both within and across different varieties.",
        "Beyond the content words, we find that 10.96% and 21.31% of unknown words are function words in HKSAR and SM data.",
        "Such unknown function words include the determiner gewei (“everybody”), the conjunction huoshi (“or”), the preposition liantong (“with”), the pronoun nali (“where”), and symbols used as quotes “ ǋ” and “ǌ” (punctuation).",
        "XH does contain words with similar function (huozhe",
        "“or”, yu “with”, dajia “everybody”, quotation marks “ ”and “ ”).",
        "Our result thus suggests that each Mandarin variety may have characteristic function words."
      ]
    },
    {
      "heading": "3.3 Cross language comparison",
      "text": [
        "A key goal of our work is to understand the way that unknown words differ across languages.",
        "We thus compare Chinese, German, and English.",
        "Following Brants (2000), we extracted 10% of the data from the Penn Treebank Wall Street Journal (WSJ 4 ) and NEGRA5 (Brants et al., 1999) as observation samples to compare to the rest of the corpora.",
        "In these observation samples, we found that Chinese words are more ambiguous in POS than English and German; 29.9% of tokens in CTB have more than one POS tag, while only 19.8% and 22.9% of tokens are ambiguous in English and German, respectively.",
        "Table 3 shows that 40.6% of unknown words are proper nouns6 in English, while both Chinese and German have less than 15% of unknown words as proper nouns.",
        "Unlike English, 60% of the unknown words in Chinese and German are verbs and common nouns.",
        "In the next section we investigate the cause of this similarity between Chinese and German unknown word distribution.",
        "Table 3 Comparison of unknown words in English, German and Mandarin.",
        "The English and German data are extracted from WSJ and NEGRA.",
        "Chinese data is our CTB devset."
      ]
    },
    {
      "heading": "4 Morphological analysis",
      "text": [
        "In order to understand the causes of the similarity of Chinese and German, and to help suggest possible features, we turn here to an introduction to Chinese morphology and its implications for part-of-speech tagging."
      ]
    },
    {
      "heading": "4.1 Chinese morphology",
      "text": [
        "Chinese words are typically formed by four morphological processes: affixation, compounding, idiomization, and reduplication, as shown in Table 4.",
        "In affixation, a bound morpheme is added to other morphemes, forming a larger unit.",
        "Chinese has a small number of prefixes and infixes7 and numerous suffixes (Chao 1968, Li and Thompson 1981).",
        "Chinese prefixes include items such as gui (“noble”) in guixing (“your name”), bu (“not”) in budaode (“immoral”), and lao (“senior”) in laohu (“tiger”) and laoshu (“mouse”).",
        "There are a number of Chinese suffixes, including zhe (“marks a person who is an agent of an action”) in zuozhe (“author”), shi (“master”) in laoshi (“teacher”), ran (-ly) in huran (“suddenly”), and xin (-ity or –ness) in kenengxin (“possibility”).",
        "Compound words are composed of multiple stem morphemes.",
        "Chao (1968) describes a few of the different compounding rules in Mandarin, such as coordinate compound, subject predicate compound, noun noun compound, adj noun compound and so on.",
        "Two examples of coordinate compounds are anpai ARRANGE-ARRANGE (“to arrange, arrangement”) and xuexi STUDY-STUDY (“to study”).",
        "Compounding is extremely common in both Chinese and German.",
        "The phrase “income tax” is treated as an NP in English, but it is a word in German, Ein-kommensteuer, and in Chinese, suodesui.",
        "We suggest that it is this rich use of compounding that causes the wide variety of unknown common nouns and verbs in Chinese and German.",
        "However, there are still differences in their compound rules.",
        "German compounds can compose with a large number of elements, but Chinese compounds normally consist of two bases.",
        "Most German compounds are nouns, but Chinese has both noun and verb compounds.",
        "Two final types of Chinese morphological processes that we will not focus on are idiomization (in which a whole phrase such as wanshiruyi (“everything is fine”) functions as a word, and reduplication, in which a morpheme or word is repeated to form a new word such as the formation of changchang (“taste a",
        "bit”), from chang “taste”.",
        "(Chao 1968, Li and Thompson 1981)."
      ]
    },
    {
      "heading": "4.2 Difficulty",
      "text": [
        "The morphological characteristics of Chinese create various problems for part-of-speech tagging.",
        "First, Chinese suffixes are short and sparse.",
        "Because of the prevalence of compounding and the fact that the morphemes are short (1 character long), there are more than 4000 affixes.",
        "This means that the identity of an affix is often a sparsely-seen feature for predicting POS.",
        "Second, Chinese affixes are poor cues to POS because they are ambiguous; for example 63% of Chinese suffix tokens in CTB have more than one possible tag, while only 31% of English suffix tokens in WSJ have more than one tag.",
        "Most English suffixes are derivational and inflectional suffixes like - able, s and -ed.",
        "Such functional suffixes are used to indicate word classes or syntactic function.",
        "Chinese, however, has no inflectional suffixes and only a few derivational suffixes and so suffixes may not be as good a cue for word classes.",
        "Finally, since Chinese has no derivational morpheme for nominalization, it is difficult to distinguish a nominalization and a verb.",
        "These points suggest that morpheme identity, which is the major feature used in previous research on unknown words in English and German, will be insufficient in Chinese.",
        "This suggests the need for more sophisticated features, which we will introduce below."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "We evaluate our tagger under several experimental conditions: after showing the effects of data cleanup we show basic results based on features found to be useful by previous research.",
        "Next, we introduce additional morphology-based unknown word features, and finally, we experiment with training data of variable sizes and different language varieties."
      ]
    },
    {
      "heading": "5.1 Data sets",
      "text": [
        "To study the significance of training on different varieties of data, we created three training sets: training set I contains data only from one variety, training set II contains data from 3 varieties, and is similar in total size to training set I.",
        "Training set III also contains data from 3 varieties and has twice much data as training set I.",
        "To facilitate comparison of performance both between and within Mandarin varieties, both the devset and the test set we created are composed of three varieties of data.",
        "The XH test data we selected was identical to the test set used in previous parsing research by Bikel and Chiang (2000).",
        "For the remaining data, we included HKSAR and SM data that is similar in size to the XH test set.",
        "Table 5 details characteristics of the data sets."
      ]
    },
    {
      "heading": "5.2 The model",
      "text": [
        "Our model builds on research into loglinear models by Ng and Low (2004), Toutanova et al., (2003) and Ratnaparkhi (1996).",
        "The first research uses independent maximum entropy classifiers, with a sequence model imposing categorical valid tag sequence constraints.",
        "The latter two use maximum entropy Markov models (MEMM) (McCallum et al., 2000), that use log-linear models to obtain the probabilities of a state transition given an observation and the previous state, as illustrated in Figure 1 (a).",
        "Using left-to-right transition probabilities, as in Figure 1 (a), the equation for the MEMM can be formally stated as the following, where by di represents the set of features the transition probabilities are conditioned on:",
        "Maximum entropy is used to calculate the probability P(ti |di) using the equation below.",
        "Here, fj(ti,di) represents a feature derived from the available contextual information (e.g. current word, previous tag, next word, etc.)",
        "uence fea-sult.",
        "2Rw+2Lw 87.11 - 47.03 +Cleanup 87.57 0.46 48.54 +PU 87.65 0.08 49.26"
      ]
    },
    {
      "heading": "5.4 Sequence features",
      "text": [
        "We examined several tag sequence features from both left and right side of the current word.",
        "We use the term lexical features to refer to features derived from the identity of a word, and tag sequence features refer to features derived from the tags of surrounding words.",
        "These features have been shown to be useful in previous research on English (Toutanova et al., 2003, Brants 2000, Thede and Harper 1999) The models9 in Table 7 list the different tag sequence features used; they also use the same lexical features from the model 2Rw+2Lw shown in Table 6.",
        "The table shows that Model Lt+LLt conditioning on the previous tag and the conjunction of the two previous",
        "Our affixation feature is motivated by similar features seen in inflectional language models.",
        "(Ng and Low 2004, Toutanova et al., 2003, Brants 2000, Ratnaparkhi 1996, Samuelsson 1993).",
        "Since Chinese also has affixation, it is reasonable to incorporate this feature into our model.",
        "For this feature, we use character n-gram prefixes and suffixes for n up to 4.10 An example is:"
      ]
    },
    {
      "heading": "5.5.2 CTBMorph (CTBM)",
      "text": [
        "While affix information can be very informative, we showed earlier that affixes in Chinese are sparse, short, and ambiguous.",
        "Thus as our first new feature we used a POS-vector of the set of tags a given affix could have.",
        "We used the training set to build a morpheme/POS dictionary with the possible tags for each",
        "morpheme.",
        "Thus for each prefix and suffix that occurs with each CTB tag in the training set I, we associate a set of binary features corresponding to each CTB tag.",
        "In the example below the prefix 䌘 occurred in both NN and VV words, but not AD or AS.",
        "This model smoothes affix identity and the quantity of active CTBMorph features for a given affix expresses the degree of ambiguity associated with that affix.",
        "for each single-character prefix or suffix k of W if t.affixList contain k f.appendPair(t, 1) else f.appendPair(t, 0)"
      ]
    },
    {
      "heading": "5.5.3 ASBC",
      "text": [
        "One way to deal with robustness is to add more varied training data.",
        "For example the Academic Sinica Balanced Corpus11 contains POS-tagged data from a different variety (Taiwanese Mandarin).",
        "But the tags in this corpus are not easily converted to the CTB tags.",
        "This problem of labeled data from very different tagsets can happen more generally.",
        "We introduce two alternative methods for making use of such a corpus."
      ]
    },
    {
      "heading": "5.5.3.1 ASBCMorph (ASBCM)",
      "text": [
        "The ASBCMorph feature set is generated in an identical manner to the CTBMorph feature set, except that rather than generating the morpheme table using CTB, another corpus is used.",
        "The morpheme table is generated from the Academic Sinica Balanced Corpus, ASBC (Huang and Chen 1995), a 5 M word balanced corpus written in Taiwanese Mandarin.",
        "As the CTB annotation guide 12 states, the mapping between the tag sets used in the two corpora is non-trivial.",
        "As such, the ASBC data can not be directly used to augment the training set.",
        "However, using our ASBCMorph feature, we are still able to derive some benefit out of such an alternative corpus."
      ]
    },
    {
      "heading": "5.5.3.2 ASBCWord (ASBCW)",
      "text": [
        "The ASBCWord feature is identical to the ASBCMorph feature, except that instead of using a table of tags that occur with each affix, we use a table of tags that a word occurs with in the ASBC data.",
        "Thus, a rare word in the CTB training/test set is augmented with features that correspond to all of the tags that the given word occurred with in the ASBC corpus, i.e. in this case, the pos tag of the identical word in ASBC,䌘᭭㹟."
      ]
    },
    {
      "heading": "5.5.4 Verb affix",
      "text": [
        "This feature set contains only two feature values, based on whether a list of verb affixes contains the prefix or suffix of an unknown word.",
        "We use the verb affix list created by the Chinese Knowledge Information Processing Group13 at Academia Sinica.",
        "It contains 735 frequent verb prefixes and 282 frequent verb suffixes.",
        "For example,"
      ]
    },
    {
      "heading": "5.5.5 Radicals",
      "text": [
        "Radicals are the basic building blocks of Chinese characters.",
        "There are over 214 radicals, and all Chinese characters contain one or more of them.",
        "Sometimes radicals reflect the meaning of a character.",
        "For example, the characters ⤈ (monkey), ⣾ (pig)⣿ (kitty cat) all contain the radical ⢁ that roughly means “something that is an animal”.",
        "For our radical based feature, we use the radical map from the Uni-han database.",
        "14 The radicals associated with the characters in the prefix and suffix of unknown words were incorporated into the model as features, for example:"
      ]
    },
    {
      "heading": "5.5.6 Named Entity Morpheme (NEM)",
      "text": [
        "There is a convention that the suffix of a named entity points out the essential meaning of the named entity.",
        "For example, the suffix bao (newspaper) appears in Chinese translation of “WSJ”, huaerjierebao.",
        "The suffix he (river) is used to identify rivers, for example in huanghe (yellow river).",
        "To take advantage of this fact, we made 3 tables of named entity characters from the Chinese English Named Entity Lists (CENEL) (Huang 2002).",
        "These lists consist of a table of Chinese first name characters, a table of Chinese last name characters, and a",
        "table of named entity suffixes such as organization, place, and company names in CENEL.",
        "Our named entity feature set contains 3 features, each corresponding to one of the three tables just described.",
        "To generate these features, first, we check if the prefix of an unknown is in the Chinese last name table.",
        "Second, we check if the suffix is in the Chinese first name table.",
        "Third, we check if the suffix of an unknown word is in the table of named entity suffixes.",
        "In Chinese last names are written in front of a first name, and the whole name is considered as a word, for example:",
        "FNEM={(last name, 0), (first name, 0), (NE suffix, 1)}"
      ]
    },
    {
      "heading": "5.5.7 Length of a word",
      "text": [
        "The length of a word can be a useful feature, because the majority of words in CTB have less than 3 characters.",
        "Words that have more than 3 characters are normally proper nouns, numbers, and idioms.",
        "Therefore, we incorporate this feature into the system.",
        "For example: Wi= , Flength={(length , 3)}"
      ]
    },
    {
      "heading": "5.5.8 Evaluation",
      "text": [
        "Table 8 shows our results using the standard maximum entropy forward feature selection algorithm; at each iteration we add the feature family that most significantly improves the log likelihood of the training data given the model.",
        "We seed the feature space search with the features in Model Lt+LLt.",
        "From this model, adding suffix information gives a 9.58% (absolute) gain on unknown word tagging.",
        "Subsequently adding in prefix makes unknown word accuracy go up to 63.66%.",
        "Our first result is that Chinese affixes are indeed informative for unknown words.",
        "On the right side of Table 8, we can see that this performance gain is derived from better tagging of common nouns, verbs, proper nouns, numbers and others.",
        "Because earlier work in many languages including Chinese uses these simple prefix and suffix features (Brants 2000, Ng and Low 2004) we consider this performance (63.66% on unknown words) as our baseline.",
        "Adding in the feature set CTBM gives another 12.47% (absolute) improvement on unknown words.",
        "With this feature, punctuation shows the largest tagging improvement.",
        "The CTBM feature helps to identify punctuation since all other characters have been seen in different morpheme table made from the training set.",
        "That is, for a given word the lack of CTBM features cues that the word is a punctuation mark.",
        "Also, while this feature set generally helps all tagsets, it hurts a bit on nouns.",
        "Adding in the ASBC feature sets yields another 1.23% and 1.48% (absolute) gains on unknown words.",
        "These two feature sets generally improve performance on all tagsets.",
        "Including the verb affix feature helps with common nouns and proper nouns, but hurts the performance on verbs.",
        "Overall, it yields 0.21% gain on unknown words.",
        "Finally, adding the radical feature helps the most on nouns, while subsequently adding in the name entity morphemes help to reduce the error on proper nouns by 2.50%.",
        "Finally, adding in feature length renders a 0.25% gain on unknown words.",
        "Commutatively, applying the feature sets results in an overall accuracy of 91.97% and an unknown word accuracy of 79.86%."
      ]
    },
    {
      "heading": "5.6 Experiments with the training sets of variable sizes and varieties",
      "text": [
        "In this section, we compare our best model with the baseline model using different corpora size and language varieties in the training set.",
        "All the evaluations are reported on the test set, which has roughly equal amounts of data from XH, HKSAR, and SM.",
        "The left column of Table 9 shows that when we train a model only on a single language variety and test on a mixed variety data, our unknown word accuracy is 79.50%, which is 18.48% (absolute) better than the baseline.",
        "The middle column shows when the training set is composed of different varieties and hence looks like the test set, performance of both the baseline and our best model improves.",
        "The right column shows the effect of doubling the training set size, using mixed varieties.",
        "As expected, using more data benefits both models.",
        "These results show that having training data from different varieties is better than having data from one source.",
        "But crucially, our morphological-based features improve the tagging performance on unknown words even when the training set includes some data that resembles the test set.",
        "How good are our best numbers, i.e. 93.7% on POS tagging in CTB 5.0?",
        "Unfortunately, there are no clean direct comparisons in the literature.",
        "The closest result in the literature is Xue et al.",
        "(2002), who retrain the Ratnaparkhi (1996) tagger and reach accuracies of 93% using CTB-I.",
        "However CTB-I contains only XH data and furthermore the data split is no longer known for this experiment (Xue p.c.)",
        "so a comparison is not informative.",
        "However, our performance on tagging when trained on Training I and tested on just the XH part of the test set is 94.44%, which might be a more relevant comparison to Xue et al.",
        "(2002)."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "Previous research in part-of-speech tagging has resulted in taggers that perform well when the training set and test set are both drawn from the same corpus.",
        "Unfortunately, for many potential real world applications, such an arrangement is just not possible.",
        "Our results show that using sophisticated morphological features can help solve this robustness problem.",
        "These features would presumably also be applicable to other languages and NLP tasks that could benefit from the use of morphological information Besides these tagging results, our research provides valuable analytic results on understanding the nature of unknown words cross-linguistically.",
        "Our results that unknown words in Chinese are not proper nouns like in English, but rather common nouns and verbs, suggest a similarity to German.",
        "We suggest this is because both German and Chinese, despite their huge differences in genetic, area, and other typological characteristics, tend to form unknown words through a similar word formation rule, compounding."
      ]
    },
    {
      "heading": "7 Acknowledgement",
      "text": [
        "Thanks to Kristina Toutanova and Galen Andrew for their generous help and to the anonymous reviewers.",
        "This work was partially funded by ARDA AQUAINT and by NSF award IIS-0325646."
      ]
    },
    {
      "heading": "8 References",
      "text": []
    }
  ]
}
