{
  "info": {
    "authors": [
      "Masato Hagiwara",
      "Satoshi Sekine"
    ],
    "book": "Proceedings of the 4th Named Entity Workshop (NEWS) 2012",
    "id": "acl-W12-4404",
    "title": "Latent Semantic Transliteration using Dirichlet Mixture",
    "url": "https://aclweb.org/anthology/W12-4404",
    "year": 2012
  },
  "references": [
    "acl-H05-1120",
    "acl-J98-4003",
    "acl-N09-1022",
    "acl-P04-1021",
    "acl-P07-1016",
    "acl-P11-2010",
    "acl-W09-3502",
    "acl-W10-2401",
    "acl-W99-0908"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Transliteration has been usually recognized by spelling-based supervised models.",
        "However, a single model cannot deal with mixture of words with different origins, such as ?get?",
        "in ?piaget?",
        "and ?target?.",
        "Li et al. (2007) propose a class transliteration method, which explicitly models the source language origins and switches them to address this issue.",
        "In contrast to their model which requires an explicitly tagged training corpus with language origins, Hagiwara and Sekine (2011) have proposed the latent class transliteration model, which models language origins as latent classes and train the transliteration table via the EM algorithm.",
        "However, this model, which can be formulated as uni-gram mixture, is prone to overfitting since it is based on maximum likelihood estimation.",
        "We propose a novel latent semantic transliteration model based on Dirichlet mixture, where a Dirichlet mixture prior is introduced to mitigate the overfitting problem.",
        "We have shown that the proposed method considerably outperform the conventional transliteration models."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Transliteration (e.g., ??????",
        "baraku obama ?Barak Obama?)",
        "is phonetic translation between languages with different writing systems, which is a major way of importing foreign words into different languages.",
        "Supervised, spelling-based grapheme-to-grapheme models such as (Brill and Moore, 2000; Li et al., 2004), which directly align characters in the training corpus without depending on phonetic information, and statistically computing their correspondence, have been a popular method to detect and/or generate transliterations, in contrast to phonetic-based methods such as (Knight and Jonathan, 1998).",
        "However, single, monolithic models fail to deal with sets of foreign words with multiple language origins mixed together.",
        "For example, the ?get?",
        "part of ?piaget / ????piaje?",
        "and ?target / ????",
        "?",
        "t?getto?",
        "differ in pronunciation and spelling correspondence depending on their source languages, which are French and English in this case.",
        "To address this issue, Li et al. (2007) have proposed class transliteration model, which explicitly models and classifies classes of languages (such as Chinese Hanzi, Japanese Katakana, and so on) and genders, and switches corresponding transliteration models based on the input.",
        "This model requires training sets of transliterated word pairs tagged with language origin, which is difficult to obtain.",
        "Hagiwara and Sekine proposed the latent class transliteration (LCT) model (Hagiwara and Sekine, 2011), which models source language origins as directly unobservable latent classes and applies appropriate transliteration models to given transliteration pairs.",
        "The model parameters are learned from corpora without language origins in an unsupervised manner.",
        "This enables us to correctly assign latent classes for English and French to ?piaget / ????piaje?",
        "and ?target / ???",
        "?",
        "?",
        "t?getto?",
        "and to identify their transliteration correspondence correctly.",
        "However, this model is based on maximum likelihood estimation on multinomials and thus sensitive to noise in the training data such as transliteration pairs with irregular pronunciation, and tends to overfit the data.",
        "Considering the atomic rewriting unit (transliteration unit, or TU, e.g., ?get / ??",
        "?",
        "getto?)",
        "as a word, and a transliteration pair as a document consisting of a word sequence, class-based transliteration can be modeled by the perfect analogy to document topic models proposed in tha past.",
        "In fact, the LCT model, where the transliteration probability is defined by a mixture of multinomials, can be regarded as a variant of a topic model, namely Unigram Mixuture (UM) (Nigam et al., 2000).",
        "There has been an extension of unigram mixture proposed (Sj?lander et al., 1996; Yamamoto and Sadamitsu, 2005) which introduces a Dirichlet mixture distribution as a prior and alleviates the overfitting problem.",
        "We can expect to improve the transliteration accuracy by formulating the transliteration problem using a similar framework to these topic models.",
        "In this paper, we formalize class-based transliteration based on language origins in the framework of topic models.",
        "We then propose the latent semantic transliteration model based on Dirichlet mixture (DM-LST).",
        "We show through experiments that it can significantly improve the transliteration performance by alleviating the overfitting issue.",
        "Note that we tackle the task of transliteration generation in this paper, in contrast to transliteration recognition.",
        "A transliteration generation task is, given an input word s (such as ?piaget?",
        "), the system is asked to generate from scratch the most probable transliterated word t (e.g., ?????piaje?).",
        "The transliteration recognition task, on the other hand, is to induce the most probable transliteration t?",
        "?",
        "T such that t?",
        "= arg maxt?T P (?s, t?)",
        "given the input word s and a pool of transliteration candidates T .",
        "We call P (?s, t?)",
        "transliteration model in this paper.",
        "This model can be regarded as the hybrid of an unsupervised alignment technique for transliteration and class-based transliteration.",
        "Related researches for the former include (Ahmad and Kondrak, 2005), who estimate character-based error probabilities from query logs via the EM algorithm.",
        "For the latter, Llitjos and Black (2001) showed that source language origins may improve the pronunciation of proper nouns in text-to-speech systems.",
        "The structure of this paper is as follows: we introduce the alpha-beta model(Brill and Moore, 2000) in Section 2, which is the most basic spelling-based transliteration model on which other models are based.",
        "In the following Section 3, we introduce and relate the joint source channel (JSC) model (Li et al., 2004) to the alpha-beta model.",
        "We describe the LCT model as an extension to the JSC model in Section 4.",
        "In Section 5, we propose the DM-LST model, and show the experimental results on transliteration generation in Section 6."
      ]
    },
    {
      "heading": "2 Alpha-Beta Model",
      "text": [
        "In this section, we describe the alpha-beta model, which is one of the simplest spelling-based transliteration models.",
        "Though simple, the model has been shown to achieve better performance in tasks such as spelling correction (Brill and Moore, 2000), transliteration (Brill et al., 2001), and query alteration (Hagiwara and Suzuki, 2009).",
        "The method directly models spelling-based rewriting probabilities of transliteration pairs.",
        "It is an extension to the normal edit distance, where the cost of operations (substitution, insertion, and deletion) is fixed to 1, and assigns a probability to a string edit operation of the form si ?",
        "ti (si and ti are any substrings of length 0 to w).",
        "We call the unit operation of string rewriting ui = ?si, ti?",
        "as transliteration unit (TU) as in (Li et al., 2004).",
        "The total transliteration probability of rewriting a word s to t is given",
        "where f is the number of TUs and u1...uf is any sequence of TUs (e.g., ?pi ??",
        "a ??",
        "get ?",
        "???)",
        "created by splitting up the input/output transliteration pair ?s, t?.",
        "The above equation can be interpreted as a problem of finding a TU sequence u1...uf which maximizes the probability defined by the product of individual probabilities of independent TUs.",
        "After taking the logarithm of the both sides, and regarding ?",
        "logP (ui) as the cost of string substitution si ?",
        "ti, the problem is equivalent to minimizing the sum of rewriting costs, and therefore can be efficiently solved by dynamic programming as done in the normal edit distance.",
        "TU probabilities P (ui) are calculated from a training set of transliteration pairs.",
        "However, training sets usually lack alignment information specifying which characters in s corresponding which characters in t. Brill and Moore (2000) resorted to heuristics to align same characters and to induce the alignment of string chunks.",
        "Hagiwara and Sekine (2011) converted Japanese Katakana sequences into Roman alphabets because their model also assumed that the strings si and ti are expressed in the same alphabet system.",
        "Our method on the contrary, does not pose such assumption so that strings in different writing systems (such as Japanese Katakana and English alphabets, and Chinese characters and English alphabets, etc.)",
        "can be aligned without being converted to phonetic representation.",
        "For this reason, we cannot adopt algorithms (such as the one described in (Brill and Moore, 2000)) which heuristically infer alignment based on the correspondence of the same characters.",
        "When applying this alpha-beta model, we computed TU probabilities by counting relative frequencies of all the alignment possibilities for a transliteration pair.",
        "For example, all the alignment possibilities for a pair of strings ?abc?",
        "and ?xy?",
        "are (a-x by c-?",
        "), (a-x b-?",
        "c-y), and (a-?",
        "b-x c-y).",
        "By considering merging up to two adjacent aligned characters in the first alignment, one obtains the following five aligned string pairs: a-x, b-y, c-?, ab-xy bc-y.",
        "Note that all the transliteration models described in this paper implicitly depend on the parameter w indicating the maximum length of character n-grams.",
        "We fixed"
      ]
    },
    {
      "heading": "3 Joint Source Channel Model",
      "text": [
        "The alpha-beta model described above has shortcomings that the character alignment is fixed based on heuristics, and it cannot capture the dependencies between TUs.",
        "One example of such dependencies is the phenomenon that the suffix ?-ed?",
        "in English verbs following a voiced consonant is pronounced /d/, whereas the one followed by an unvoiced consonant is /t/.",
        "This section describes the JSC model(Li et al., 2004), which was independently proposed from the alpha-beta model.",
        "The JSC model is essentially equivalent to the alpha-beta model except: 1) it can also incorporate higher order of n-grams of TUs and 2) the TU statistics is taken not by fixing the heuristic initial alignment but by iteratively updating via an EM-like algorithm.",
        "In the JSC model, the transliteration probability is defined by the n-gram probabilities of",
        "Again, f is the number of TUs.",
        "The TU n-gram probabilities P (ui|ui?n+1, ..., ui?1) can be calculated by the following iterative updates similar to the EM algorithm:",
        "1.",
        "Set the initial alignment randomly.",
        "2.",
        "E-step: Take the TU n-gram statistics fixing the current alignment, and update the transliteration model.",
        "3.",
        "M-step: Compute the alignment based on the current transliteration model.",
        "The alignment is inferred by dynamic programming similar to the alpha-beta model.",
        "4.",
        "Iterate the E-and M-step until convergence.",
        "Notice the alpha-beta model and the JSC model are both transliteration recognition models.",
        "In order to output a transliterated word t for a given input s, we generated transliteration candidates with high probability using a stack",
        "of ????",
        "sumisu?",
        "from the input ?smith?)",
        "decoder, whose overview is shown in Figure 1.",
        "One character in the input string s (which is ?smith?",
        "in the figure) is given at a time, which is appended at the end of the last TUs for each candidate.",
        "(the append operation in the figure).",
        "Next, the last TU of each candidate is either reduced or shifted.",
        "When it is reduced, top R TUs with highest probabilities are generated and fixed referring to the TU table (shown in the bottom-left of the figure).",
        "In Figure 1, two candidates, namely (?s?, ??",
        "su?)",
        "and (?s?, ??",
        "zu?)",
        "are generated after the character ?s?",
        "is given.",
        "When the last TU is shifted, it remains unchanged and unfixed for further updates.",
        "Every time a single character is given, the transliteration probability is computed using Eq.",
        "2 for each candidate, and all but the top-B candidates with highest probabilities are discarded.",
        "The reduce width R and the beam width B were determined using the determined using development sets, as mentioned in Section 6."
      ]
    },
    {
      "heading": "4 Latent Class Transliteration Model",
      "text": [
        "As mentioned in Section 1, the alpha-beta model and the JSC model build a single transliteration model which is simply the monolithic average of training set statistics, failing to capture the difference in the source language origins.",
        "Li et al. (2004) address this issue by defining classes c, i.e., the factors such as source language origins, gender, and first/last names, etc.",
        "which affect the transliteration probability.",
        "The authors then propose the class transliteration model which gives the probability of s ?",
        "t as follows:",
        "However, this model requires a training set explicitly tagged with the classes.",
        "Instead of assigning an explicit class c to each transliterated pair, Hagiwara and Sekine (2011) introduce a random variable z which indicates implicit classes and conditional TU probability P (ui|z).",
        "The latent class transliteration (LCT) model is then defined as1:",
        "where K is the number of the latent classes.",
        "The latent classes z correspond to classes such as the language origins and genders mentioned above, shared by sets of transliterated pairs with similar rewriting characteristics.",
        "The classes z are not directly observable from the training set, but can be induced by maximizing the training set likelihood via the EM algorithm as follows.",
        "n ?nk.",
        "Here, ?sn, tn?",
        "is the nth transliterated pair in the training set, and fn and fn(ui) indicate how many TUs there are in total in the nth transliterated pair, and how many times the TU ui appeared in it, respectively.",
        "As done in the JSC model, we update the alignment in the training set before the E-Step for each iteration.",
        "Thus fn takes different values",
        "from iteration to iteration in general.",
        "Furthermore, since the alignment is updated based on P (ui|z) for each z = k, M different alignment candidates are retained for each transliterated pairs, which makes the value of fn dependent on k, i.e., fkn .",
        "We initialize P (z = k) = 1/M to and P (ui|z) = PAB(u) + ?, that is, the TU probability induced by the alpha-beta algorithm plus some random noise ?.",
        "Considering a TU as a word, and a transliteration pair as a document consisting of a word sequence, this LCT model defines the transliteration probability as the mixture of multinomials defined over TUs.",
        "This can be formulated by unigram mixture (Nigam et al., 2000), which is a topic model over documents.",
        "This follows a generation story where documents (i.e., transliterated pairs) are generated firstly by choosing a class z by P (z) and then by generating a word (i.e., TU) by P (ui|z).",
        "Nevertheless, as mentioned in Section 1, since this model trains the parameters based on the maximum likelihood estimation over multinomials, it is vulnerable to noise in the training set, thus prone to overfit the data."
      ]
    },
    {
      "heading": "5 Latent Semantic Transliteration",
      "text": [
        "Model based on Dirichlet Mixture We propose the latent semantic transliteration model based on Dirichlet mixture (DM-LST), which is an extension to the LCT model based on unigram mixture.",
        "This model enables to prevent multinomials from being exceedingly biased towards the given data, still being able to model the transliteration generation by a mixture of multiple latent classes, by introducing Dirichlet mixture as a prior to TU multinomials.",
        "The compound distribution of multinomials when their parameters are given by Dirichlet mixtures is given by the Polya mixture distribu-tion(Yamamoto and Sadamitsu, 2005):",
        "where PMul(?",
        ";p) is multinomial with the parameter p. PDM is Dirichlet mixture, which is a mixture (with coefficients ?1, ..., ?K) of K Dirichlet distributions with parameters ?K1 = (?1,?2, ...,?K).",
        "The model parameters can be induced by the following EM algorithm.",
        "Notice that we adopted a fast induction algorithm which extends an induction method using leaving-one-out to mixture distributions(Yamamoto et al., 2003).",
        "Parameters: ?",
        "= (?1, ..., ?K),",
        "The prediction distribution when a single TU u is the input is given PDM (u) = ?K k=1 ?k?ku/?k.",
        "We therefore updated the alignment in the training corpus, as done in the JSC model updates, based on the probability proportional to ?ku/?k for each k before every M-Step.",
        "The parameters are initially set to ?k = 1/K, ?ku = PAB(u) + ?, as explained in the previous section.",
        "Since neither LCT nor DM-LST is a transliteration generation model, we firstly generated transliteration candidates T by using the JSC model and the stack decoder (Section 3) as a",
        "baseline, then re-ranked the candidates using the probabilities given by LCT (Eq.",
        "4 or DM-LST (Eq.",
        "11), generating the re-ranked list of transliterated outputs.",
        "Because the parameters trained by the EM algorithm differ depending on the initial values, we trained 10 models P 1DM , ..., P 10DM using the same training data and random initial values and computed the average 110",
        "DM (?s, t?)",
        "to be used as the final transliteration model.",
        "It is worth mentioning that another topic model, namely latent Dirichlet alocation (LDA) (Blei et al., 2003), assumes that words in a document can be generated from different topics from each other.",
        "This assumption corresponds to the notion that TUs in a single transliterated pairs can be generated from different source languages, which is presumably a wrong assumption for transliteration tasks, probably except for compound-like words with mixed origins such as ?naiveness?.",
        "In fact, we confirmed through a preliminary experiment that LDA does not improve the transliteration performance over the baseline."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": []
    },
    {
      "heading": "6.1 Evaluation",
      "text": [
        "In this section, we compare the following models: alpha-beta (AB), joint source channel (JSC), latent class transliteration (LCT), and latent semantic transliteration based on Dirichlet mixture (DM-LST).",
        "For the performance evaluation, we used three language pairs, namely, English-Japanese (En-Ja), English-Chinese (En-Ch), and English-Korean (En-Ko), from the transliteration shared task at NEWS 2009 (Li et al., 2009a; Li et al., 2009b).",
        "The size of each training/test set is shown in the first column of Table 1.",
        "In general, rn, a set of one or more reference transliterated words, is associated with the nth input sn in the training/test corpus.",
        "Let cn,i, cn,2, ... be the output of the transliteration system, i.e., the candidates with highest probabilities assigned by the transliteration model being evaluated.",
        "We used the following three performance measures: ?",
        "ACC (averaged Top-1 accuracy): For every ?sn, rn?, let an be an = 1 if the candidate with the highest probability cn,1 is contained in the reference set rn and an = 0 otherwise.",
        "ACC is then calculated as",
        "?",
        "MFS (mean F score): Let the reference transliterated word closest to the top-1 candidate cn, 1 be r?n = arg minrn,j?rn ED(cn,1, rn,j), where ED is the edit distance.",
        "The F-score of the top candidate cn,1 for the nth input sn is then given by:",
        "where |x |is the length of string x, and LCS(x, y) is the length of the longest common subsequence of x and y. Edit distance, lengths of strings, and LCS are measured in Unicode characters.",
        "Finally, MFS is de",
        "?",
        "MRR (mean reciprocal rank): Of the ranked candidates cn,1, cn,2, ..., let the highest ranked one which is also included in the reference set rn be cn,j .",
        "We then define reciprocal rank RRn = 1/j.",
        "If none of the candidates are in the reference, RRn = 0.",
        "MRR is then defined by",
        "We used Kneser-Nay smoothing to smooth the TU probabilities for LCT.",
        "The number of EM iterations is fixed to 15 for all the models, based on the result of preliminary experiments.",
        "The reduce width R and the beam width B for the stack decoder are fixed to R = 8 and B = 32, because the transliteration generation performance increased very little beyond these widths based on the experiment using the development set.",
        "We also optimized M , i.e., the number of latent classes for LCT and DM-LST, for each language pair and model in the same way based on the development set."
      ]
    },
    {
      "heading": "6.2 Results",
      "text": [
        "We compared the performance of each transliteration model in Table 1.",
        "For the language pairs En-Ja and En-Ch, all the performance increase in the order of AB < JSC < LCT < DM-LST, showing the superiority our proposed method.",
        "For the language pair En-Ko, the performance for LCT re-ranking considerably decreases compared to JSC.",
        "We suspect this is due to the relatively small number of training set, which caused the excessive fitting to the data.",
        "We also found out that the optimal value of M which maximizes the performance of DM-LST is equal to or smaller than that of LCT.",
        "This goes along with the findings (Yamamoto and Sadamitsu, 2005) that Dirichlet mixture often achieves better language model perplexity with smaller dimensionality compared to other models.",
        "Specific examples in the En-Ja test set whose transliteration is improved by the proposed methods include ?dijon ??????",
        "dijon?",
        "and ?goldenberg ?????????",
        "g?rudenb?gu?.",
        "Conventional methods, including LCT, suggested ?????",
        "diyon?",
        "and ?????????",
        "g?rudenberugu?, meaning that the transliteration model is affected and biased towards non-English pronunciation.",
        "The proposed method can retain the major class of transliteration characteristics (which is English in this case) and can deal with multiple language origins depending on transliteration pairs at the same time.",
        "This trend can be also confirmed in other language pairs, En-Ch and En-Ko.",
        "In En-Ch, the transliterated words of ?covell?",
        "and ?nether-wood?",
        "are improved ?",
        "????",
        "kefuer ???",
        "?",
        "keweier?",
        "and ??????",
        "neitehewude ??",
        "???",
        "neisewude?, respectively.",
        "in En-Ko, the transliterated word of ?darling?",
        "is improved ??",
        "??",
        "dareuling?",
        "?",
        "???",
        "dalling?.",
        "We also observed that ?gutheim ?????",
        "gutehaimu in En-Ch and martina ????",
        "?",
        "mareutina in En-Ko are correctly translated by the proposed method, even though they do not have the English origin.",
        "Generally speaking, however, how these non-English words are pronounced depend on the context, as ?charles?",
        "has different pronunciation in English and in French, with the soft ?sh?",
        "sound at the beginning.",
        "We need external clues to disambiguate such transliteration, such as context information and/or Web statistics."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "In this paper, we proposed the latent semantic transliteration model based on Dirichlet mixture (DM-LST) as the extension to the latent class transliteration model.",
        "The experimental results showed the superior transliteration performance over the conventional methods, since DM-LST can alleviate the overfitting problem and can capture multiple language origins.",
        "One drawback is that it cannot deal with dependencies of higher order of TU n-grams than bigrams.",
        "How to incorporate these dependencies into the latent transliteration models is the future work."
      ]
    }
  ]
}
