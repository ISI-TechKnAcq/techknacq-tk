{
  "info": {
    "authors": [
      "Carlos A. Henríquez Q.",
      "Marta Ruiz Costa-Jussà",
      "Vidas Daudaravicius",
      "Rafael E. Banches",
      "Josée B. Mariño"
    ],
    "book": "Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR",
    "id": "acl-W10-1712",
    "title": "Using Collocation Segmentation to Augment the Phrase Table",
    "url": "https://aclweb.org/anthology/W10-1712",
    "year": 2010
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Using collocation segmentation to augment the phrase table Carlos A. Henrîquez Q. , Marta R. Costa-jussà\"\", Vidas DaudaraviciusJ",
        "Rafael E. Banchs\"\", José B. Marino*",
        "*TALP Research Center, Universität Politèeniea de Catalunya, Barcelona, Spain {carlos.henriquez,jose.marino}@upc.edu ^Barcelona Media Innovation Center, Barcelona, Spain {marta.ruiz,rafael.banchs}@barcelonamedia.org",
        ":",
        "vidas@donelaitis.vdu.lt",
        "This paper describes the 2010 phrase-based statistical machine translation system developed at the TALP Research Center of the UPC in cooperation with BMIC and VMU.",
        "In phrase-based SMT, the phrase table is the main tool in translation.",
        "It is created extracting phrases from an aligned parallel corpus and then computing translation model scores with them.",
        "Performing a collocation segmentation over the source and target corpus before the alignment causes that different and larger phrases are extracted from the same original documents.",
        "We performed this segmentation and used the union of this phrase set with the phrase set extracted from the non-segmented corpus to compute the phrase table.",
        "We present the configurations considered and also report results obtained with internal and official test sets."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The TALP Research Center of the UPC in cooperation with BMIC and VMU participated in the Spanish-to-English WMT task.",
        "Our primary submission was a phrase-based SMT system enhanced with POS tags and our contrastive submission was an augmented phrase-based system using collocation segmentation (Costa-jussà et al., 2010), which mainly is a way of introducing new phrases in the translation table.",
        "This paper presents the description of both systems together with the results that we obtained in the evaluation task and is organized as follows: first, Section 2 and 3 present a brief description of a phrase-based SMT, followed by a general explanation of collocation segmentation.",
        "Section 4 presents the experimental framework, corpus used and a description of the different systems built for the translation task; the section ends showing the results we obtained over the official test set.",
        "Finally, section 5 presents the conclusions obtained from the experiments."
      ]
    },
    {
      "heading": "2. Phrase-based SMT",
      "text": [
        "This approach to SMT performs the translation splitting the source sentence in segments and assigning to each segment a bilingual phrase from a phrase-table.",
        "Bilingual phrases are translation units that contain source words and target words, e.g. < unidad de traduction | translation unit >, and have different scores associated to them.",
        "These bilingual phrases are then sorted in order to maximize a linear combination of feature functions.",
        "Such strategy is known as the log-linear model (Och and Ney, 2003) and it is formally defined as:",
        "where hm are different feature functions with weights Xm.",
        "The two main feature functions are the translation model (TM) and the target language model (LM).",
        "Additional models include POS target language models, lexical weights, word penalty and reordering models among others."
      ]
    },
    {
      "heading": "3. Collocation segmentation",
      "text": [
        "Collocation segmentation is the process of detecting boundaries between collocation segments within a text (Daudaravicius and Marcinkeviciene, 2004).",
        "A collocation segment is a piece of text between boundaries.",
        "The boundaries are established in two steps using two different measures: the Dice score and a Average Minimum Law (AML).",
        "The Dice score is used to measure the association strength between two words.",
        "It has been used before in the collocation compiler XTract (Smadja, 1993) and in the lexicon extraction system Cham-pollion (Smadja et al., 1996).",
        "It is defined as follows:",
        "where f (x, y) is the frequency of co-occurrence of x y f ( x) f ( y) xy",
        "The first step then establishes a boundary between two adjacent words when the Dice score is lower than a threshold t = exp ( – 8).",
        "Such a threshold was established following the results obtained in (Costa-jussà et al., 2010), where an integration of this technique and a SMT system was performed over the Bible corpus.",
        "The second step of the procedure uses the AML.",
        "It defines a boundary between words xj_i and xjwhen:",
        "That is, the boundary is set when the Dice value between words xj and xj_1 is lower than the average of preceding and following values."
      ]
    },
    {
      "heading": "4. Experimental Framework",
      "text": [
        "All systems were built using Moses (Koehn et al., 2007), a state-of-the-art software for phrase-based SMT.",
        "For preprocessing Spanish, we used Freeling (Atserias et al., 2006), an open source library of natural language analyzers.",
        "For English, we used TnT (Brants, 2000) and Moses' tokenizer.",
        "The language models were built using SRILM (Stolcke, 2002).",
        "This year, the translation task provided four different sources to collect corpora for the Spanish-English pair.",
        "Bilingual corpora included version 5 of the Europarl Corpus (Koehn, 2005), the News Commentary corpus and the United Nations corpus.",
        "Additional English corpora was available from the News corpus.",
        "The organizers also allowed the use of the English Gigaword Third and Fourth Edition, released by the LDC.",
        "As for development and internal test, the test sets from 2008 and 2009 translation tasks were available.",
        "For our experiments, we selected as training data the union of the Europarl and the News Commentary.",
        "Development was performed with a section of the 2008 test set and the 2009 test set was selected as internal test.",
        "We deleted all empty lines, removed pairs that were longer than 40 words, either in Spanish or English; and also removed pairs whose ratio between number of words were bigger than 3.",
        "As a preprocess, all corpora were lower-cased and tokenized.",
        "The Spanish corpus was tokenized and POS tags were extracted using Freeling, which split clitics from verbs and also separated words like \"del\" into \"de el\".",
        "In order to build a POS target language model, we also obtained POS tags from the English corpus using the TnT tagger.",
        "Statistics of the selected corpus can be seen in Table 1.",
        "It is important to notice that neither the United Nations nor the Gigaword corpus were used for bilingual training.",
        "Nevertheless, the English part from the United Nations and the monolingual News corpus were used to build the language model of our systems.",
        "We analyzed the content from the internal and official test and realized that they both contained many words that were not seen in the training data.",
        "Table 2 shows the number of unknown words found in both sets, classified according to their POS.",
        "In average, we may expect an unknown word every two sentences in the internal test and more than one per sentence in the official test set.",
        "It can also be seen that most of those unknown words are proper nouns, representing 32% and 79% of the unknown sets, respectively.",
        "Common nouns were the second most frequent type of unknown words, followed by verbs and adjectives.",
        "We submitted two different systems for the translation task.",
        "First a baseline using the training data mentioned before; and then an augmented system, where the baseline-extracted phrase list was extended with additional phrases coming from a segmented version of the training corpus.",
        "We also considered an additional system built with two different decoding path, a standard path from words to words and POS and an alternative path from stems to words and POS in the target side.",
        "At the end, we did not submit this system to the translation task because it did not provide better results than the previous two in our internal test.",
        "Corpora",
        "Spanish",
        "English",
        "Training sent",
        "1,180, 623",
        "1,180, 623",
        "Running words",
        "26,454, 280",
        "25, 291, 370",
        "Vocabulary-",
        "118,073",
        "89, 248",
        "Development sent",
        "1, 729",
        "1, 729",
        "Running words",
        "37, 092",
        "34, 774",
        "Vocabulary-",
        "7,025",
        "6,199",
        "Internal test sent",
        "2, 525",
        "2, 525",
        "Running words",
        "69, 565",
        "65, 595",
        "Vocabulary-",
        "10, 539",
        "8, 907",
        "Official test sent",
        "2,489",
        "-",
        "Running words",
        "66, 714",
        "-",
        "Vocabulary-",
        "10, 725",
        "-",
        "Internal test",
        "Official test",
        "Adjectives",
        "137",
        "72",
        "Common nouns",
        "369",
        "188",
        "Proper nouns",
        "408",
        "2,106",
        "Verbs",
        "213",
        "128",
        "Others",
        "119",
        "168",
        "Total",
        "1246",
        "2662",
        "The set of feature functions used include: source-to-target and target-to-source relative frequencies, source-to-target and target-to-source lexical weights, word and phrase penalties, a target language model, a POS target language model, and a lexicalized reordering model (Tillman, 2004).",
        "4.2.1 Considering stems as an alternate decoding path.",
        "Using Moses' framework for factored translation models we defined a system with two decoding paths: one decoding path using words and the other decoding path using stems in the source language and words in the target language.",
        "Both decoding paths only had a single translation step.",
        "The possibility of using multiple alternative decoding path was developed by Birch et.",
        "al.",
        "(2007).",
        "This system tried to solve the problem with the unknown words.",
        "Because Spanish is morphologically richer than English, this alternative decoding path allowed the decoder translate words that were not seen in the training data and shared the same root with other known words.",
        "4.2.2 Expanding the phrase table using collocation segmentation.",
        "In order to build the augmented phrase table with the technique mentioned in section 3, we segmented each language of the bilingual corpus independently and then, using the collocation segments as words, we aligned the corpus and extracted the phrases from it.",
        "Once the phrases were extracted, the segments of each phrase were split again in words to have standard phrases.",
        "Finally, we use the union of this phrases and the phrases extracted from the baseline system to compute the final phrase table.",
        "A diagram of the whole procedure can be seen in figure 1.",
        "The objective of this integration is to add new phrases in the translation table and to enhance the relative frequency of the phrases that were extracted from both methods.",
        "4.2.3 Language model interpolation.",
        "Because SMT systems are trained with a bilingual corpus, they ended highly tied to the domain the corpus belong to.",
        "Therefore, when the documents we want to translate belong to a different domain, additional domain adaptation techniques are recommended to build the system.",
        "Those techniques usually employ additional corpora that correspond to the domain we want to translate from.",
        "internal test baseline | 24.25 baseline+stem 23.45 augmented 23.9",
        "Table 3: Internal test results.",
        "The test set for this translation task comes from the news domain, but most of our bilingual corpora belonged to a political domain, the Europarl.",
        "Therefore we use the additional monolingual corpus to adapt the language model to the news domain.",
        "The strategy used followed the experiment performed last year in (R. Fonollosa et al., 2009).",
        "We used SRILM during the whole process.",
        "All language models were order five and used modified Kneser-Ney discount and interpolation.",
        "First, we build three different language models according to their domain: Europarl, United Nations and news; then, we obtained the perplexity of each language model over the News Commentary development corpus; next, we used compute-best-mix to obtain weights for each language model that diminish the global perplexity.",
        "Finally, the models were combined using those weights.",
        "In our experiments all systems used the resulting language model, therefore the difference obtained in our results were cause only by the translation model.",
        "We present results from the three systems developed this year.",
        "First, the baseline, which included all the features mentioned in section 4.2; then, the system with an alternative decoding path, called baseline+stem; and finally the augmented system, which integrated collocation segmentation to the baseline.",
        "Internal test results can be seen in table 3.",
        "Automatic scores provided by the WMT 2010 organizers for the official test can be found in table 4.",
        "All BLEU scores are case-insensitive and tokenized except for the official test set which also contains case-sensitive and non-tokenized score.",
        "our case-insensitive and sensitive outputs, respectively.",
        "The highest score was obtained by Unipoints.",
        "test",
        "tCSt cased_detok",
        "baseline",
        "26.1",
        "25.1",
        "augmented",
        "26.1",
        "25.1",
        "Figure 1: Example of the expansion of the phrase table using collocation segmentation.",
        "New phrases added by the collocation-based system are marked with a **.",
        "Once we obtained the translation outputs from the baseline and the augmented system, we performed a manual comparison of them.",
        "Even though we did not find any significant advantages of the augmented system over the baseline, the collocation segmentation strategy chose a better morphological structures in some cases as can be seen in Table 5 (only sentence sub-segments are shown):"
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "We presented two different submissions for the Spanish-English language pair.",
        "The language model for both system was built interpolating two big out-of-domain language models and one smaller in-domain language model.",
        "The first system was a baseline with POS target language model; and the second one an augmented system, that integrates the baseline with collocation segmentation.",
        "Results over the official test set showed no difference in BLEU between these two, even though internal results showed that the baseline obtained a better score.",
        "We also considered adding an additional decoding path from stems to words in the baseline but internal tests showed that it did not improve translation quality either.",
        "The high number of unknown words found in Spanish suggested us that considering in parallel the simple form of stems could help us achieve better results.",
        "Nevertheless, a deeper study of the unknown set showed us that most of those words were proper nouns, which do not have inflection and therefore cannot benefited from stems.",
        "Finally, despite that internal test did not showed an improvement with the augmented system, we submitted it secondary run looking for the effect these phrases could have over human evaluation.",
        "Acknowledgment",
        "The research leading to these results has received funding from the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement number 247762, from the Spanish Ministry of Science and Innovation through the Buceador project (TEC2009-14094-C04-01) and the Juan de la Cierva fellowship program.",
        "The authors also wants to thank the Barcelona Media Innovation Centre for its support and permission to publish this research."
      ]
    }
  ]
}
