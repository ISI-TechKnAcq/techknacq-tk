{
  "info": {
    "authors": [
      "Pierre Boullier",
      "Benoît Sagot"
    ],
    "book": "Tenth International Conference on Parsing Technologies",
    "id": "acl-W07-2213",
    "title": "Are Very Large Context-Free Grammars Tractable?",
    "url": "https://aclweb.org/anthology/W07-2213",
    "year": 2007
  },
  "references": [
    "acl-A00-2036",
    "acl-C88-2121",
    "acl-E93-1036",
    "acl-J95-4002",
    "acl-J97-3004",
    "acl-P89-1018",
    "acl-W05-1501",
    "acl-W05-1522"
  ],
  "sections": [
    {
      "text": [
        "Pierre Boullier & Benoît Sagot",
        "INRIA-Rocquencourt Domaine de Voluceau, Rocquencourt BP 105 78153 Le Chesnay Cedex, France {Pierre.Boullier,Benoit.Sagot}@inria.fr",
        "In this paper, we present a method which, in practice, allows to use parsers for languages defined by very large context-free grammars (over a million symbol occurrences).",
        "The idea is to split the parsing process in two passes.",
        "A first pass computes a sub-grammar which is a specialized part of the large grammar selected by the input text and various filtering strategies.",
        "The second pass is a traditional parser which works with the sub-grammar and the input text.",
        "This approach is validated by practical experiments performed on a Earley-like parser running on a test set with two large context-free grammars."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "More and more often, in real-word natural language processing (NLP) applications based upon grammars, these grammars are no more written by hand but are automatically generated, this has several consequences.",
        "This paper will consider one of these consequences: the generated grammars may be very large.",
        "Indeed, we aim to deal with grammars that have, say, over a million symbol occurrences and several hundred thousands rules.",
        "Traditional parsers are not usually prepared to handle them, either because these grammars are simply too big (the parser's internal structures blow up) or the time spent to analyze a sentence becomes prohibitive.",
        "This paper will concentrate on context-free grammars (CFG) and their associated parsers.",
        "However, virtually all Tree Adjoining Grammars (TAG, see e.g., (Schabes et al., 1988)) used in NLP applications can (almost) be seen as lexicalized Tree Insertion Grammars (TIG), which can be converted into strongly equivalent CFGs (Schabes and Waters, 1995).",
        "Hence, the parsing techniques and tools described here can be applied to most TAGs used for NLP, with, in the worst case, a light over-generation which can be easily and efficiently eliminated in a complementary pass.",
        "This is indeed what we have achieved with a TAG automatically extracted from (Villemonte de La Clergerie, 2005)'s large-coverage factorized French TAG, as we will see in Section 4.",
        "Even (some kinds of) non CFGs may benefit from the ideas described in this paper.",
        "The reason why the runtime of context-free (CF) parsers for large CFGs is damaged relies on a theoretical result.",
        "A well-known result is that CF parsers may reach a worst-case running time of O(\\G\\ x n) where \\G\\ is the size of the CFG and n is the length of the source text.",
        "In typical NLP applications which mainly work at the sentence level, the length of a sentence does not often go beyond a value of say 100, while its average length is around 20-30 words.",
        "In these conditions, the size ofthe grammar, despite its linear impact on the complexity, may be the prevailing factor: in (Joshi, 1997), the author remarks that \"the real limiting factor in practice is the size of the grammar\".",
        "The idea developed in this paper is to split the parsing process in two passes.",
        "A first pass called filtering pass computes a sub-grammar which is the",
        "Proceedings of the 10th Conference on Parsing Technologies, pages 94-105, Prague, Czech Republic, June 2007.",
        "(c 2007 Association for Computational Linguistics",
        "sub-part of the large input grammar selected by the input sentence and various filtering strategies.",
        "The second pass is a traditional parser which works with the sub-grammar and the input sentence.",
        "The purpose is to find a filtering strategy which, in typical practical situations, minimizes on the average the total runtime of the filtering pass followed by the parser pass.",
        "A filtering pass may be seen as a (filtering) function that uses the input sentence to select a sub-grammar out of a large input CFG.",
        "Our hope, using such a filter, is that the time saved by the parser pass which uses a (smaller) sub-grammar will not totally be used by the filter pass to generate this sub-grammar.",
        "It must be clear that this method cannot improve the worst-case parse-time because there exists grammars for which the sub-grammar selected by the filtering pass is the input grammar itself.",
        "In such a case, the filtering pass is simply a waste of time.",
        "Our purpose in this paper is to argue that this technique may profit from typical grammars used in NLP.",
        "To do that we put aside the theoretical view point and we will consider instead the average behaviour of our processors.",
        "More precisely we will study on two large NL CFGs the behaviour of our filtering strategies on a set of test sentences.",
        "The purpose being to choose the best filtering strategy, if any.",
        "By best, we mean the one which, on the average, minimizes the total runtime of both the filtering pass followed by the parsing pass.",
        "Useful formal notions and notations are recalled in Section 2.",
        "The filtering strategies are presented in Section 3 while the associated experiments are reported in Section 4.",
        "This paper ends with some concluding remarks in Section 5."
      ]
    },
    {
      "heading": "2. Preliminaries",
      "text": [
        "A CFG G is a quadruple (N, T, P, S) where N is a non-empty finite set of nonterminal symbols, T is a finite set of terminal symbols, P is a finite set of (context-free rewriting) rules (or productions) and S is a distinguished nonterminal symbol called the axiom.",
        "The sets N and T are disjoint and V = NUT is the vocabulary.",
        "The rules in P have the form A – a, with A G N and a G V*.",
        "For a given string a G V*, its size (length) is noted \\ a\\ .",
        "As an example, for the input string w = a1 • • • an, a G T, we have \\w\\ = n. The empty string is denoted e and we have \\e\\ = 0.",
        "The size \\G\\ of a CFG G is defined by \\G\\ = A^aeP \\ Aa\\.",
        "For G, on strings of V*, we define the binary relation derive, noted ==, by 71A72 ==a 71 if A – a G P and y1;y2 G V *.",
        "The subscript G or even the superscript A – a may be omitted.",
        "As usual, its transitive (resp.",
        "reflexive transitive) closure is noted = (resp.",
        "= ).",
        "We call derivation any derivation is a derivation which starts with the axiom and ends with a terminal string w. In that case we have S =* 7 =* w, and 7 is a sentential form.",
        "The string language defined (generated, recognized) by G is the set of all the terminal strings that w, w G T*}.",
        "We say that a CFG is empty iff its language is empty.",
        "A nonterminal symbol A is nullable iff it can derive the empty string (i.e., A == e).",
        "A CFG is e-free iff its nonterminal symbols are non-nullable.",
        "A CFG is reduced iff every symbol of every production is a symbol of at least one complete derivation.",
        "A reduced grammar is empty iff its production set is empty (P = 0).",
        "We say that a non-empty reduced grammar is in canonical form iff its vocabulary only contains symbols that appear in the productions of P.3,",
        "Two CFGs G and G are weakly equivalent iff they generate the same string language.",
        "They are strongly equivalent iff they generate the same set of structural descriptions (i.e., parse trees).",
        "It is a well known result (See Section 3.2) that every CFG G can be transformed in time linear w.r.t.",
        "\\G\\ into a strongly equivalent (canonical) reduced CFG G'.",
        "For a given input string w G T*, we define its ranges as the set Rw = ] \\ 1 < i < j < \\w\\ + 1}.",
        "If w = w1tw3 G T* is a terminal string, and if t G T U {e} is a (terminal or empty) symbol, the instantiation of t in w is the triple noted t[i..j] where is a range with i = \\w1\\ + 1 and j = i + \\t\\.",
        "More generally, the instantiation of the terminal string w2 in w1w2w3 is noted w2[i..j] with i = \\w1\\ + 1 and j = i + \\w2\\.",
        "Obviously, the instantiation of w itself is then w[1..1 + \\w\\].",
        "Let us consider an input string w = w1 w2w3 and a CFG G. If we have a complete derivation see that A derives w2 (we have A == w2).",
        "Moreover, in this complete derivation, we also know a range in Rw, namely ], which covers the substring w2 which is derived by A (i = \\ w1 \\ + 1 and j = i + \\w2\\).",
        "This is represented by the instantiated nonterminal symbol A[i..j].",
        "In fact, each symbol which appears in a complete derivation may be transformed into its instantiated counterpart.",
        "We thus talk of instantiated productions or (complete) instantiated derivations.",
        "For a given input text w, and a CFG G, let PG be the set of instantiated productions that appears in all complete instantiated derivations.",
        "The pair (PG , S[1..\\w\\ + 1]) is the (reduced) shared parse forest in canonical form.",
        "A finite-state automaton (FSA) is the 5-tuple A = (Q, E, 5, q0, F) where Q is a non empty finite set of states, E is a finite set of terminal symbols, 5 is the transition relation 5 = {(qi; t, qj)\\qj,qj G Q A t G T U {e}}, q0 is a distinguished element of Q called the initial state and F is a subset of Q whose elements are called final states.",
        "The size of A is defined by \\A\\ = \\5\\.",
        "As usual, we define both a configuration as an element of Q x T* and derive a binary relation between",
        "x1 • • • xk • • • xp), then the instantiated production A[i0..ip] – Xi[io..ii ] ••• Xfc [ifc_i..ifc ] ••• Xp [ip_i ..ip ] with io = |wi| + 1, ii = io + |xi|, ..., ik = ik_i + | ... and ip = io + |w21 is an element of PG.",
        "configurations, noted A by (q, tx) A (q',x), iff (q, t, q') G 5.",
        "If w'w'' G T*, we call derivation any sequence of the form (q',w'w'') h ••• h (q'',w'').",
        "If w G T*, the initial configuration is noted c0 and is the pair (q0, w).",
        "A final configuration is noted Cf and has the form (qf, e) with qf G F .A complete derivation is a derivation which starts with C0 and ends in a final configuration Cf .In that case we have",
        "Co h Cf.",
        "The language L(A) defined (generated, recognized) by the FSA A is the set ofall terminal strings w for which there exists a complete derivation.",
        "We say that an FSA is empty iff its language is empty.",
        "Two FSAs A and A' are equivalent iff they defined the same language.",
        "An FSA is e-free iff its transition relation has the form 5 = {(qi, t, qj)\\qi; qj G Q, t G E}, except perhaps for a distinguished transition, the e-transition which has the form (q0,e, qf), qf G F and allows the empty string e to be in L(A).",
        "Every FSA can be transformed into an equivalent e-free FSA.",
        "An FSA A = (Q, E, 5, q0, F) is reduced iff every element of 5 appears in a complete derivation.",
        "A reduced FSA is empty iff we have 5 = 0.",
        "We say that a non-empty reduced FSA is in canonical form iff its set of states Q and its set of terminal symbols E only contain elements that appear in the transition relation 5.",
        "It is a well known result that every FSA A can be transformed in time linear with \\ A\\ into an equivalent (canonical) reduced FSA A .",
        "In many NLP applications the source text cannot be considered as a single string of terminal symbols but rather as a finite set of terminal strings.",
        "These sets are finite languages which can be defined by particular FSAs.",
        "These particular type of FSAs are called directed-acyclic graphs (DAGs).",
        "In a DAG w = (Q, E, 5, q0, F), the initial state q0 is 1 and we assume that there is a single final state f (F = {/}), Q is a finite subset of the positive integers less than or equal to f: Q = < i < f}, E is the set of terminal symbols.",
        "For the transition relation 5, we require that its elements (i, t, j) are such that i < j (there are no loops in a DAG).",
        "Without loss of generality, we will assume that DAGs are e-free reduced FSAs in canonical form and that any DAG w is noted by a triple (E, 5, f) since its initial state is always 1 and its set of states is {i \\ 1 < i < f}.",
        "For a given CFG G, the recognition of an input DAG w is equivalent to the emptiness of its intersection with G. This problem can be solved in time linear in \\G\\ and cubic in \\Q\\ the number of states of w.",
        "If the input text is a DAG, the previous notions of range, instantiations and parse forest easily generalize: the indices i and j which in the string case locate the positions of substrings are changed in the DAG case into DAG states.",
        "For example if A[i0..ip] – X1 [i0• • • Xp[ip-1..ip] is an instantiated production of the parse forest for G = (N, T, P, S) and w = (E, 5, f), we have A – X1 • • • Xp G P and there is a path in the input DAG from state i0 to state ip via states i1,..., ip-1.",
        "Of course, any nonempty terminal string w G T+, may be viewed as a DAG (E, 5, f) where E = {t \\ w = w1tw2 A t G T}, 5 = {(i,t,i + 1) \\ w = w1 tw2 A t G T A i = 1 + \\w1 \\} and f = 1 + \\w\\.",
        "If the input string w is the empty string e, the associated DAG is (E, 5, f) where E = 0, 5 = {(1, e, 2)} and f = 2.",
        "Thus, in the sequel, we will assume that the inputs of our parsers are not strings but DAGs.",
        "As a consequence the size (or length) of a sentence is the size of its DAG (i.e., its number of transitions)."
      ]
    },
    {
      "heading": "3. Filtering Strategies 3.1 Gold Strategy",
      "text": [
        "be an input DAG of size n = \\5\\ and (Fw) = ((PW),S[1..f]) be the reduced output parse forest in canonical form.",
        "From (PW), it is possible to extract a set of (reduced) uninstantiated productions PW = {A – X1 • • • Xp \\ A[i0..ip] – X1[i0..i1 ]X2[i1..i2] ••• Xp[i (PW)}, which, together with the axiom S, defines a new reduced CFG GW = (PW, S) in canonical form.",
        "This grammar is called the gold grammar of G for w, hence the superscript g. Now, if we use GW to reparse the same input DAG w, we will get the same output forest (Fw).",
        "But in that case, we are sure that every production in PW is used in at least one complete derivation.",
        "Now, if this process is viewed as a filtering strategy that computes a filtering function as introduced in Section 1, it is clear that this strategy is size-optimal in the sense that PW is of minimal size, we call it the gold strategy and the associated gold filtering function is noted g. Since we do not want that a filtering strategy looses parses, the result",
        "GW = (PW, S) of any filtering function f must be such that, for every sentence w, P f is a superset of PW.",
        "In other words the recall score of any filtering function f must be of 100%.",
        "We can note that the parsing pass which generates GW may be led by any filtering strategy f.",
        "As usual, the precision score (precision for short) of a filtering strategy f (w.r.t.",
        "the gold case) is, for a given w, defined by the quotient f-yj which expresses the number ofuseful productions selected by f on w (for some G).",
        "However, it is clear that we are interested in strategies that are time-optimal and size-optimal strategies are not necessarily also time-optimal: the time taken at filtering-time to get a smaller grammar will not necessarily be won back at parse-time.",
        "For a given CFG G, an input DAG w and a filtering strategy C, we only have to plot the times taken by the filtering pass and by the parsing pass to make some estimations on their average (median, decile) parse times and then to decide which is the winner.",
        "However, it may well happens that a strategy which has not received the award (with the sample ofCFGs and the test sets tried) would be the winner in another context!",
        "All the following filtering strategies exhibit necessary conditions that any production must hold in order to be in a parse.",
        "An algorithm which takes as input any CFG G = (N, T, P, S) and generates as output a strongly equivalent reduced CFG G and which runs in O(\\G\\) can be found in many text books (See (Hopcroft and Ullman, 1979) for example).",
        "So as to eliminate from all our intermediate sub-grammars all useless productions, each filtering strategy will end by a call to such an algorithm named make-a-reduced-grammar.",
        "The make-a-reduced-grammar algorithm works as follows.",
        "It first finds all productive symbols.",
        "Afterwards it finds all reachable symbols.",
        "A symbol is useful (otherwise useless) if it is both productive and reachable.",
        "A production A – X1 • • • Xp is useful (otherwise useless) iff all its symbols are useful.",
        "A last scan over the grammar erases all useless production and leaves the reduced form.",
        "The canonical form is reached in only retaining in the nonterminal and terminal sets of the sub-grammar the symbols which occur in the (useful) production set.",
        "The basic filtering strategy (b-filter for short) which is described in this section will always be tried the first.",
        "Thus, its input is the couple (G, w) where G = (N, T, P, S) is the large initial CFG and the input sentence w is a reduced DAG in canonical form w = (E, 5, f) of size n. It generates a reduced CFG in canonical form noted Gb = (Pb, S) in which the references to both G and w are assumed.",
        "Besides this b-filter, we will examine in Sections 3.4 and 3.5 two others filtering strategies named a and d. These filters will always have as input a couple (Gc, w) where Gc = (Pc, S) is a reduced CFG in canonical form which has already been filtered by a previous sequence of strategies noted C. They generate a reduced CFG in canonical form noted Gcf = (Pcf, S) with f = a or f = d respectively.",
        "Of course it may happens that Gcf is identical to Gc if the f filter is not effective.",
        "A filtering strategy or a combination of filtering strategies may be applied several times and lead to a filtered grammar of the form say G\" dain which the sequence bada explicits the order in which the filtering strategies have been performed.",
        "We may even repeatedly apply a until a fixed point is reached before applying d, and thus get something of the form Gba°°d.",
        "The idea behind the b-filter is very simple and has largely been used in lexicalized formalisms parsing, in particular in LTAG (Schabes et al., 1988) parsing.",
        "The filter rejects productions of P which contain terminal symbols that do not occur in E (i.e., that are not terminal symbols of the DAG w) and thus takes",
        "T*.",
        "O(\\G\\) time if we assume that the access to the elements of the terminal set E is performed in constant time.",
        "Unlexicalized productions whose right-hand sides are in N* are kept.",
        "It also rejects productions in which several terminal symbol occurs, in an order which is not compatible with the linear order of the input.",
        "Consider for example the set of productions shown in Table 1 and assume that the source text is the terminal string ab.",
        "It is clear that the b-filter will erase production 6 since C is not in the source text.",
        "The execution of the b-filter produces a (non-reduced) CFG G' such that \\G'\\ < \\G\\.",
        "However, it may be the case that some productions ofG are useless, it will thus be the task of the make-a-reduced-grammar algorithm to transform G into its reduced canonical form Gb in time O(\\G'\\).",
        "The worst-case total running time of the whole b-filter pass is thus O(\\G\\ x n).",
        "We can remark that, after the execution of the b-filter, the set of terminal symbols of Gb is a subset of T n E.",
        "As explained before, we assume that the input to the adjacent filtering strategy (a-filter for short) described in this section is a couple (Gc, w) where",
        "Gc = (Nc, Tc, Pc, S) is a reduced CFG in canonical form.",
        "However, the a-filter would also work for a non-reduced CFG.",
        "As usual, we define the symbols of Gc as the elements of the vocabulary",
        "Vc = Nc U Tc.",
        "The idea is to erase productions that cannot be part of any parses for w in using an adjacency criteria: if two symbols are adjacent in a rule, they must derive terminal symbols that are also adjacent in w. To give a (very) simple practical idea of what we mean by adjacency criteria, let us consider again the source string ab and the grammar defined in Table 1 in which the last production has already been erased by the b-filter.",
        "The fact that the B-production ends with a b and that the A-productions all start with an a, implies that production 2 is in a complete parse only if the source text is such that b is immediately followed by a.",
        "Since it is not the case, production 2 can be erased.",
        "More generally, consider a production of the form A – • • • XY • • •.",
        "If for each couple (a, b) G T in which a is a terminal symbol that can terminate (the terminal strings generated by) X and b is a terminal symbol that can lead (the terminal strings generated by) Y, there is no transition on b that can follow a transition on a in the DAG w, it is clear that the production A – • • • XY • • • can be safely erased.",
        "has the previous definition and a' is a terminal symbol that can terminate (the terminal strings generated by) Xp, there is no transition on b that can follow a transition on a' in the DAG w, the production Xp_1 – apXp/ p can be erased if it is not valid in another context.",
        "Xp_i – apXpftp *",
        "In order to formalize these notions we define several binary relations together with their (reflexive) transitive closure.",
        "Moore, 2000), hereafter LC, is a well-known relation since many parsing strategies are based upon it.",
        "We say that X is in the LC of A and we write A l X iff (A, X) G {(B, Y) | B – aY// G P A a => e}.",
        "We can write A l X to enforce how the couple (A, X) may be produced.",
        "For its dual relation, right-corner, noted j, we say that X is in the right corner of A and we write X j A e}.",
        "We can write X j A to enforce how the couple (X, A) may be produced.",
        "V A t G T A X ^ xt A x G T*}).",
        "We define the adjacent ternary relation on V x N * x V noted <-> and we write X Y iff e}.",
        "This means that X and Y occur in that order in the right-hand side of some production and are separated by a nullable string <r. Note that X or Y may or may not be nullable.",
        "On the input DAG w = (£,<$,/), we define the immediately precede relation noted < and we write a < b for a, b G S iff w^bw3 G L(w),w1, w3 G S*.",
        "We also define the precede relation noted <C and we write a <C b for a, b G S iff w1aw2bw3 G L(w),w1 ,w2,w3 G S*.We can note that <C is not the transitive closure of <.n pair (a, b) of A1 is such that the terminal symbol a may terminate a phrase of Xo while the terminal symbol b may lead a phrase of X1 • • • Xp.",
        "Since X0 and Xp are not nullable, A1 is not empty.",
        "If none of its elements (a, b) is such that a < b, the production A – aX0X1 • • • Xp-1Xp7 is useless and can be erased.",
        "Analogously, any pair (a, b) of A2 is such that the terminal symbol a may terminate a phrase of X0X1 • • • Xp-1 while the terminal symbol b may lead a phrase of Xp.",
        "Since X0 and Xp are not nullable, A2 is not empty.",
        "If none of its elements (a, b) is such that a < b, the production A – aX0X1 • • • Xp-1Xp7 is useless and can be erased.",
        "Of course if X1 • • • Xp_1 = e, we have",
        "The previous method has checked some adjacent properties inside the right-hand sides ofproductions.",
        "The following will perform some analogous checks but at the beginning and at the end of the right-hand sides of productions.",
        "Let us go back to Table 1 to illustrate our purpose.",
        "Recall that, with source text ab, productions 6 and 2 have already been erased.",
        "Consider production 4 whose left-hand side is an A, the terminal string ab that it generates ends by b.",
        "If we look for the occurrences of A in the right-hand sides of the (remaining) productions, we only find production 1 which indicates that A is followed by B.",
        "Since the phrases of B all start with b (See production 5) and since in the source text b does not immediately follow another b, production 4 can be erased.",
        "In order to check that the input sentence w starts and ends by valid terminal symbols, we augment the adjacent relation with two elements ($, e, S) and (S, e, $) where $ is a new terminal symbol which is supposed to start and to end every sentence.",
        "Let Z – aU/// be a production in Pc in which U is non-nullable and a e. If X is a non-nullable symbol, we compute the set L = {(a, b) | a tX A Y L Z l U – t b}.",
        "Since Gc is reduced and since $ < S, we are sure that the set X A Y L",
        "Z is non-empty, thus L is also non-empty.",
        "We can associate with each couple (a, b) G L at least one (left) derivation of the form in which w1w2w3w4w5 G Tc*.",
        "These derivations contains all possible usages of the production Z – aUp in a parse.",
        "If for every couple (a, b) G L, the statement a ^ b does not hold, we can conclude that the production Z – aU/ is not used in any parse and can thus be deleted.",
        "Analogously, we can check that the order of terminal symbols is compatible with both a production and its right grammatical context.",
        "Let Z – aU/ be a production in Pc in which U is non-nullable and / e. If Y is a non-nullable symbol, we compute the set R = {(a, b) | a tU j Z J X A Y – t b}.",
        "Since Gc is reduced and since S < $, we are sure that the set Z J X A Y is non-empty, thus R is also non-empty.",
        "To each couple (a, b) G R we can associate at least one (right) derivation of the form in which w5w4w3w2w1 G Tc .",
        "These derivations contains all possible usages of the production Z – aU/ in a partial parse.",
        "If for every couple (a, b) G L, the statement a ^ b does not hold, we can conclude that the production Z – aU/ is not used in any parse and can thus be deleted.",
        "Now, a call to the make-a-reduced-grammar algorithm produces a reduced CFG in canonical form named Gca = (Nca, Tca, Pca, S).",
        "In (Boullier, 2003) the author has presented a method that takes a CFG G and computes a FSA that defines a regular superset of L(G).",
        "However his method would produce intractable gigantic FSAs.",
        "Thus he uses his method to dynamically compute the FSA at parse time on a given source text.",
        "Based on experimental results, he shows that his method called dynamic set automaton (DSA) is tractable.",
        "He uses it to guide an Earley parser (See (Ear-ley, 1970)) and shows improvements over the non guided version.",
        "The DSA method can directly be used as a filtering strategy since the states of the underlying FSA are in fact sets of items.",
        "For a CFG G = (N, T, P, S), an item (or dotted production) is an element of {[A – a./] \\ A – a/ G P}.",
        "A complete item has the form [A – 7.",
        "], it indicates that the production A – 7 has been, in some sense, recognized.",
        "Thus, the complete items of the DSA states gives the set of productions selected by the DSA.",
        "This selection can be further refined if we also use the mirror DSA which processes the source text from right to left and if we only select complete items that both belong to the DSA and to its mirror.",
        "Thus, if we assume that the input to the DSA filtering strategy (d-filter) is a couple (Gc, w) where Gc = (Pc, S) is a reduced CFG in canonical form, we will eventually get a set of productions which is a subset of Pc.",
        "If it is a strict subset, we then apply the make-a-reduced-grammar algorithm which produces a reduced CFG in canonical form named Gcd = (Pcd, s).",
        "The Section 4 will give measures that may help to compare the practical merits of the a and d-filtering strategies."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "The measures presented in this section have been taken on a 1.7GHz AMD Athlon PC with 1.5 Gb of RAM running Linux.",
        "All parsers are written in C and have been compiled with gcc 2.96 with the O2 optimization flag.",
        "We have performed experiments with two large grammars described below.",
        "The first one is an automatically generated CFG, the other one is the CFG equivalent of a TIG automatically extracted from a factorized TAG.",
        "The first grammar, named GT>N, is a variant of the CFG backbone of a large-coverage LFG grammar for French used in the French LFG parser described in (Boullier and Sagot, 2005).",
        "In this variant, the set T of terminal symbols is the whole set of French inflected forms present in the Lefff, a large-coverage syntactic lexicon for French (Sagot et al., 2006).",
        "This leads to as many as 407,863 different terminal symbols and 520,711 lexicalized productions (hence, the average number of categories – which are here non-terminal symbols – for an inflected form is 1.27).",
        "Moreover, this CFG entails a non-neglectible amount of syntactic constraints (including over-generating sub-categorization frame checking), which implies as many as |Pu | = 19, 028 non-lexicalized productions.",
        "All in all, GT>N has 539,739 productions.",
        "The second grammar, named GTIG, is a CFG which represents a TIG.",
        "To achieve this, we applied (Boullier, 2000) s algorithm on the unfolded version of (Villemonte de La Clergerie, 2005)'s factorized TAG.",
        "The number of productions in GTIG is comparable to that of GT>N. However, these two grammars are completely different.",
        "First, GTIG has much less terminal and non-terminal symbols than GT>N. This means that the basic filter may be less efficient on GTIG than on GT>N. Second, the size of GTIGis enormous (more than 10 times that of GT>N), which shows that right-hand sides of GTIG's productions are huge (the average number ofright-hand side symbols is more than 24).",
        "This may increase the usefulness of a-and d-filtering strategies.",
        "Global quantitative data about these grammars is shown in Table 2.",
        "Both grammars, as evoked in the introduction, have not been written by hand.",
        "On the contrary, they are automatically generated from a more abstract and more compact level (a meta-level over LFG for GT>N, and a metagrammar for GTIG).",
        "These grammars are not artificial grammars set up only for this experiment.",
        "On the contrary, they are automatically generated huge real-life CFGs that are variants of grammars used in real NLP applications.",
        "Our test suite is a set of 3093 French journalistic sentences.",
        "These sentences are the general Jemonde part of the EASy parsing evaluation campaign corpus.",
        "Raw sentences have been turned into DAGs of inflected forms known by both grammar/lexicon couples.",
        "This step has been achieved by the pre-syntactic processing chain SxPipe (Sagot and Boul-lier, 2005).",
        "They are all recognized by both grammars.",
        "The resulting DAGs have a median size of 28 and an average size of 31.7.",
        "Before entering into details, let us give here the first important result of these experiments: it was actually possible to build parsers out of GT>N and GT1G and to parse efficiently with the resulting parsers (we shall detail later on efficiency results).",
        "Given the fact that we are dealing with grammars whose sizes are respectively over 1,000,000 and over 12,000,000, this is in itself a very satisfying result.",
        "Let us recall informally that the precision of a filtering strategy is the proportion of productions in the resulting sub-grammar that are in the gold grammar, i.e., that have effectively instantiated counterparts in the final parse forest.",
        "We have applied different strategies so as to compare their precisions.",
        "The results on GT>N and GT1G are summed up in Table 3.",
        "These results give several valuable results.",
        "First, as we expected, the basic b-filter drastically reduces the size of the grammar.",
        "The result is even better on GT>N thanks to its large number of terminal symbols.",
        "Second, both the adjacency a-filter and the DSA d-filter efficiently reduce the size of the grammar: on GT>N, the a-filter eliminates 20% of the productions they receive as input, a bit less for the d-filter.",
        "Indeed, the a-filter performs better than the d-filter introduced in (Boul-",
        "Table 3: Average precision of six different filtering strategies on our test corpus with GT>N and GT1G.",
        "lier, 2003), at least as precision is concerned.",
        "We shall see later that this is still the case on global parsing times.",
        "However, applying the d-filter after the a-filter still removes a non-neglectible amount of productions: each technique is able to eliminate productions that are kept by the other one.",
        "The result of these filters is suprisingly good: in average, after all filters, only approx.",
        "20% of the productions that have been kept will not be successfully instantiated in the final parse forest.",
        "Third, the adjacency filter can be used in its one-pass mode, since almost all the benefit from the full (fix-point) mode is already reached after the first application.",
        "This is practically a very valuable result, since the one-pass mode is obviously faster than the full mode.",
        "However, all these filters do require computing time, and it is necessary to evaluate not only the precision of these filters, but also their execution time as well as the influence they have on the global (including filtering) parsing time .",
        "Filter execution times for the six filtering strategies introduced in Table 3 are illustrated for GT>N in Figure 1.",
        "These graphics show three extremely valuable pieces of information.",
        "First, filtering times are extremely low: the average filtering time for the slowest filter (ba°°d, i.e., basic plus full adjacency plus DSA) on 40-word sentences is around 20 ms. Second, on small sentences, filtering times are virtually zero.",
        "This is important, since it means that there",
        "G",
        "\\N\\",
        "\\T\\",
        "\\P\\",
        "\\Pu\\",
        "\\G\\",
        "qT>N",
        "7,862",
        "407,863",
        "539,739",
        "19,028",
        "1,123,062",
        "GTIG",
        "448",
        "173",
        "493,408",
        "4,338",
        "12,455,767",
        "Strategy",
        "Average",
        "qT>N",
        "jrecision",
        "gtig",
        "no filter",
        "0.04%",
        "0.03%",
        "b",
        "62.87%",
        "39.43%",
        "bd",
        "74.53%",
        "66.56%",
        "ba",
        "77.31%",
        "66.94%",
        "ba°°",
        "77.48%",
        "67.48%",
        "bad",
        "80.27%",
        "77.16%",
        "ba°°d",
        "80.30%",
        "77.41%",
        "gold",
        "100%",
        "100%",
        "b-filter bd-filter ba-filter",
        "Sentence length",
        "ba°°-filter bad-filter ba°° d-filter",
        "is almost no fixed cost to pay when we use these filters (let us recall that without any filter, building efficient parsers for such a huge grammar is highly problematic).",
        "Third, all these filters, at least when used with GT>N, are executed in a time which is linear w.r.t.",
        "the size of the input sentence (i.e., the size of the input DAG).",
        "The results on GT1G lead to the same conclusions, with one exception: with this extremely huge grammar with so long right-hand sides, the basic filter is not as fast as on GT>N (and not as precise, as we will see below, which slows down the make-a-reduced-grammar algorithm since it is applied on a larger filtered grammars).",
        "For example, the median execution time for the basic filter on sentences whose size is approximately 40 is 0.25 seconds, to be compared with the 0.00 seconds reached on GT>N (this zero value means a median time strictly lower than 0.01 seconds, which is the granularity of our time measurments).",
        "Figure 2 and 3 show the global (filtering+parsing) execution time for the 6 different filters.",
        "We only show median times computed on classes of sentences of length 10i to 10(i + 1) – 1 and plotted with a centered x-coordinate (10(i + 1/2)), but results with other percentiles or average times on the same classes draw the same overall picture.",
        "One can see that the results are completely different, showing a strong dependency on the characteristics of the grammar.",
        "In the case of GT>N, the huge number of terminal symbols and the reasonable average size ofright-hand sides ofproductions, the basic filtering strategy is the best strategy: although it is fast because relatively simple, it reduces the grammar extremely efficiently (it has a 60.56% precision, to be compared with the precision of the void filter which is 0.04%).",
        "Hence, for GT>N, our only result",
        "Sentence length",
        "is that this basic filter does allow us to build an efficient parser (the most efficient one), but that refined additionnal filtering strategies are not useful.",
        "The picture is completely different with GT1G.",
        "Contrary to GT>N, this grammar has comparatively very few terminal and non-terminal symbols, and very long right-hand sides.",
        "These two facts lead to a lower precision of the basic filter (39.43%), which keeps many more productions when applied on GT1G than when applied on GT>N, and leads, when applied alone, to the less efficient parser.",
        "This gives to the adjacency filter much more opportunity to improve the global execution time.",
        "However, the complexity of the grammar makes the construction of the DSA filter relatively costly despite its precision, leading to the following conclusion: on GT1G(and probably on any grammar with similar characteristics), the best filtering strategy is the one-pass adjacency strategy.",
        "In particular, this leads to an improvement over the work of (Boullier, 2003) which only introduced the DSA filter.",
        "Incidentally, the extreme size of GT1G leads to much higher parsing times, approximately 10 times higher than with GT>N, which is consistent with the ratio between the sizes of both involved grammars."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "It is a well known result in optimization techniques that the key to practically improve these processes is to reduce their search space.",
        "This is also the case in parsing and in particular in CF parsing.",
        "Many parsers process their inputs from left to right but we can find in the literature other parsing strategies.",
        "In particular, in NLP, (van Noord, 1997) and (Satta and Stock, 1994) propose bidirectional algorithms.",
        "These parsers have the reputation to have a better efficiency than their left-to-right counterpart.",
        "This reputation is not only based upon experimental results (van Noord, 1997) but also upon mathematical arguments in (Nederhof and Satta, 2000).",
        "This is specially true when the productions of the CFG strongly depend on lexical information.",
        "In that case the parsing search space is reduced because the constraints associated to lexical elements are evaluated as early as possible.",
        "We can note that our filtering strategies try to reach the same purpose by a totally different mean: we reduce the parsing search space by eliminating as many productions as possible, including possibly non-lexicalized productions whose irrelevance to parse the current input can not be directly deduced from that input.",
        "We can also remark that our results are not in contradiction with the claims of (Nederhof and Satta, 2000) in which they argue that \"Earley algorithm and related standard parsing techniques [... ] cannot be directly extended to allow left-to-right and correct-prefix-property parsing in acceptable time bound\".",
        "First, as already noted in Section 1, our method does not work for any large CFG.",
        "In order to work well, the first step of our basic strategy must filter out a great amount of (lexicalized) productions.",
        "To do that, it is clear that the set of terminals in the input text must select a small ratio of lexicalized productions.",
        "To give a more concrete idea we advocate that the selected productions produce roughly a grammar of normal size out of the large grammar.",
        "Second, our method as a whole clearly does not process the input text from left-to-right and thus does not enter in the categories studied in (Nederhof and Satta, 2000).",
        "Moreover, the authors bring strong evidences that in case of polynomial-time off-line compilation of the grammar, left-to-right parsing cannot be performed in polynomial time, independently of the size of the lexicon.",
        "Once again, if our filter pass is viewed as an off-line processing of the large input grammar, our output is not a compilation of the large grammar, but a (compilation of a) smaller grammar, specialized in (some abstractions of) the source text only.",
        "In other words their negative results do not necessarily apply to our specific case.",
        "The experiment campaign as been conducted in using an Earley-like parser.",
        "We have also successfully tried the coupling of our filtering strategies with a CYK parser (Kasami, 1967; Younger, 1967) as post-processor.",
        "However the coupling with a GLR parser (See (Satta, 1992) for example) is perhaps more problematic since the time taken to build up the underlying nondeterministic LR automaton from the sub-grammar can be prohibitive.",
        "Though no definitive answer can be made to the question asked in the title, we have shown that, in some cases, the answer is certainly yes."
      ]
    }
  ]
}
