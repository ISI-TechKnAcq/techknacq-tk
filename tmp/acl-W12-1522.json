{
  "info": {
    "authors": [
      "Duo Ding",
      "Florian Metze",
      "Shourabh Rawat",
      "Peter Schulam",
      "Susanne Burger"
    ],
    "book": "INLG 2012 Proceedings of the Seventh International Natural Language Generation Conference",
    "id": "acl-W12-1522",
    "title": "Generating Natural Language Summaries for Multimedia",
    "url": "https://aclweb.org/anthology/W12-1522",
    "year": 2012
  },
  "references": [
    "acl-W09-0613"
  ],
  "sections": [
    {
      "text": [
        "Utica, May 2012. c?2012 Association for Computational Linguistics Generating Natural Language Summaries for Multimedia Duo Ding, Florian Metze, Shourabh Rawat, Peter F. Schulam, Susanne Burger School of Computer Science, Carnegie Mellon University Pittsburgh, PA, USA 15213 {dding, fmetze, srawat, pschulam, sburger}@cs.cmu.edu Abstract In this paper we introduce an automatic system that generates textual summaries of Inter-net-style video clips by first identifying suitable high-level descriptive features that have been detected in the video (e.g. visual concepts, recognized speech, actions, objects, persons, etc.).",
        "Then a natural language generator is constructed using SimpleNLG to compile the high-level features into a textual form.",
        "The generated summary contains information from both visual and acoustic sources, intending to give a general review and summary of the video.",
        "To reduce the complexity of the task, we restrict ourselves to work with videos that show a limited number of ?events?.",
        "In this demo paper, we describe the design of the system and present example outputs generated by the video summarization system."
      ]
    },
    {
      "heading": "1 Introduction The Internet alows us to browse millions of videos. For some of them, the content is well organized with human-generated tags and labels (e.g. wedding ceremony, birthday party, etc.), but the rate at which content is uploaded daily makes it unrealistic to expect that user-provided labels will be sufficient for organizing this information in the future. We believe that automatically generating a brief summary (or ?abstract?) of videos is both an attractive solution to this problem and an exciting challenge for the natural language generation community. Converting audio and video output into natural language to create a human readable summary that facilitates effective browsing, supports classification decisions, or helps differentiating videos from one another without having to watch them in their entirety has both academic and practical value.",
      "text": [
        "In this paper, we introduce an automatic video summary generation system that uses a natural language realization engine (Gatt and Reiter, 2009) to create sentences based on state-of-the-art video classification features.",
        "These features are computed on a large corpus from the TrecVID evaluation (Bao, et al. 2011).",
        "In a recent user study (Ding, et al. 2012), we compared automatically generated and manually generated summaries with respect to several tasks.",
        "The study shows, for example, that more specific information (e.g. ?food?",
        "instead of ?some object?)",
        "and temporal information (some-thing happened first and then?)",
        "is helpful in improving the quality of machine-generated summaries.",
        "This is a first step to implement an automatic system which is not only able to describe videos using natural language, but accomplishes more sophisticated tasks such as differentiating videos, finding supporting evidence for video classification and other tasks.",
        "2 Related Work Significant work has been done in the field of video summarization.",
        "A large part of it is based on the idea that the summarization should be a graphical representation such as visually rich storyboards.",
        "These storyboards intended to help users to efficiently browse the videos, e.g. in the Open-Video Archive (Marchionini, Song et al. 2009).",
        "Christel, et al. (2006) are mainly focusing on the research in user interface designs for video browsing and summarization.",
        "Li, et al. (2010) introduced a maximal marginal relevance algorithm working across video genres to improve the quality of the informative summary for a video, which exploits both audio and video information.",
        "Truong et al. (2007) worked on techniques targeting video data from various domains that were developed to summarize and organize the information and present surrogates to the users.",
        "Tan et al. (2011) recently have",
        "High-level features are extracted from the video using the techniques described in (Bao, et al. 2011).",
        "Visual conceptual features are detected with SVM classifiers trained on the SIN task in TRECVID 2011 using MOSIFT and CSIFT features to describe keyframes.",
        "Other features are also extracted, including event labels, event signatures and the event kit, etc.",
        "Event signatures are relevant features describing a certain event, similar to a fin-gerprint, and the event kit is a textual description of important objects and actions that make up the event.",
        "For features that make use of temporal in-formation, we use a GMM based segmenter to cut the audio of each video into small clips (1-3 se-conds) and give a label to each clip.",
        "3.3 Language Generation Taking a series of features, each of the sentence generators composes these features into a human readable sentence using the SimpleNLG generation tool.",
        "We use SimpleNLG at the lexical level (i.e. orthography, morphology and simple grammar) and at the phrase and sentence level (i.e. phrase element coordination, clause subordinates).",
        "For each set of features, the system generates a sentence specifically mentioning these features.",
        "The VID generator deals with visual concepts, i.e. the probabilities of the occurrence of 346 visual concepts extracted from the video.",
        "A list of visual features (e.g. food, people, room) will be processed as follows: SPhraseSpec p = nlgFactory.createClause(); p.setSubject(?the system\"); p.setVerb(?observe?",
        "); p.setObject(\"food, people, room\"); p.setFeature(Feature.TENSE, Tense.PAST); p.addComplement(?in the video?)",
        "to generate a sentence like: The system observed food, people and room in the video.",
        "Another sentence generator is the ?temporal information generator?, which takes the temporal information and produces a sentence describing what is happening in the video.",
        "We first segment the audio into small clips lasting three seconds each, and assign an audio semantic label to each clip (e.g. music, crowd, cheer, speech).",
        "Using temporal information, we generate a sentence like:"
      ]
    }
  ]
}
