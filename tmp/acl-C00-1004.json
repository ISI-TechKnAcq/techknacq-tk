{
  "info": {
    "authors": [
      "Masayuki Asahara",
      "Yuji Matsumoto"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1004",
    "title": "Extended Models and Tools for High-Performance Part-Of-Speech",
    "url": "https://aclweb.org/anthology/C00-1004",
    "year": 2000
  },
  "references": [
    "acl-A92-1018",
    "acl-J95-4004",
    "acl-P97-1030"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Statistical part-of-speech(POS) taggers achieve high accuracy and robustness when based on large scale manually tagged corpora.",
        "However, enhancements of the learning models are necessary to achieve better performance.",
        "We are developing a learning tool for a Japanese morphological analyzer called ChoSen.",
        "Currently we use a fine-grained POS tag set with about 500 tags.",
        "To apply a normal trigram model on the tag set, we need unrealistic size of corpora.",
        "Even, for a bi-grain model, we cannot prepare a moderate size of an annotated corpus, when we take all the tags as distinct.",
        "A usual technique to cope with such fine-grained tags is to reduce the size of the tag set by grouping the set of tags into equivalence classes.",
        "We introduce the concept of position-wise grouping where the tag set is partitioned into different equivalence classes at each position in the conditional probabilities in the Markov -Model.",
        "Moreover, to cope with the data sparseness problem caused by exceptional phenomena, we introduce several other techniques such as word-level statistics, smoothing of word-level and POS-level statistics and a selective tri-grain model.",
        "To help users determine probabilistic parameters, we introduce an error-driven method for the parameter selection.",
        "We then give results of experiments to see the effect of the tools applied to au existing Japanese morphological analyzer."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Along with the increasing availability of annotated corpora, a number of statistic POS taggers have been developed which achieve high accuracy and robustness.",
        "On the other hand, there is still continuing demand for the improvement of learning models when sufficient quantity of annotated corpora are not available in the users domains or languages.",
        "Flexible tools for easy tuning of learning models are in demand.",
        "We present such tools in this paper.",
        "Our tools are originally intended for use with the Japanese morphological analyzer, ChoSen (Matsumoto et al., 1999), which at present is a statistical tagger based on the variable memory length Markov Model (Ron et al., 1994).",
        "We first give a brief overview of the features of the learning tools.",
        "The part-of-speech tag set we use is a slightly modified version of the IPA POS tag set (RWCP, 2000) with about 500 distinct POS tags.",
        "The real tag set is even larger since sonic words are treated as distinct POS tags.",
        "The size of the tag set is unrealistic for building trigram rules and even bi-grain rules which take all the tags as distinct.",
        "The usual technique for coping with such fine-grained tags is to reduce the size of the tag set by grouping the set of tags into equivalence classes (Jelinek, 1998).",
        "We introduce the concept of position-wise grouping where the tag set is partitioned into different equivalence classes at each position in the conditional probabilities in the Markov Model.",
        "This feature is especially useful for Japanese language analysis since Japanese is a highly conjugated language, where conjugation forms have a great effect on the succeeding morphemes, but have little to do with the preceding morphemes.",
        "Moreover, in colloquial language, a number of contracted expressions are common, where two or more morphemes are contracted into a single word.",
        "The contracted word behaves as belonging to different parts-of-speech by connecting to the previous word or to the next word.",
        "Position-wise grouping enables users to group such words differently according to the positions in which they appear.",
        "Data sparseness is always a serious problem when dealing with a large tag set.",
        "Since it is unrealistic to adopt a simple POS trigram model to our tag set, we base our model on a bi-gram model and augment it with selective tri-grains.",
        "By selective tri-gram, we mean that only special contexts are conditioned by trigram model and are mixed with the ordinary bi-gram model.",
        "We also incorporate some smoothing techniques for coping with the data sparseness problem.",
        "By combining these methods, we constructed the learning tools for a high-performance statistical morphological analyzer that are able to learn the probability parameters with only a moderate size tagged corpus.",
        "The rest of this paper is structured as follows.",
        "Section 2 discusses the basic concepts of the statistical morphological analysis and some problems of the statistical approach.",
        "Section 3 presents the characteristics of the our learning tools.",
        "Section 4 reports the result of some experiments and the accuracy of the tagger in several settings.",
        "Section 5 discusses related works.",
        "Finally, section 6 gives conclusions and discusses future works.",
        "Throughout this paper, we use morphological analysis instead of part-of-speech tagging since Japanese is an agglutinative language.",
        "This is the standard terminology in Japanese literatures."
      ]
    },
    {
      "heading": "2 Preliminaries",
      "text": []
    },
    {
      "heading": "2.1 Statistical morphological analysis",
      "text": [
        "The POS tagging problem or the Japanese morphological analysis problem must do tokenization and find the sequence of POS tags T = for the word sequence 147 = , w, in the input string S. The target is to find T that maximizes the following probability: org max i'(TIT47) Using the Bayes' rule of probability theory, P(T47, T) can be decomposed as a sequence of the products of tag probabilities and word probabilities.",
        "We assumed that the word probability is constrained only by its tag, and that the tag probability is constrained only by its preceding tags, either with the hi-gram or the trigram model:",
        "The values are estimated from the frequencies in tagged corpora using maximum likelihood estimation:",
        "Using these parameters, the most probable tag sequence is determined using the Viterbi algoritInn."
      ]
    },
    {
      "heading": "2.2 Hierarchical Tag Set",
      "text": [
        "We use the IPA POS tag set (RWCP, 2000).",
        "This tag set consist of three elements: the part-of-speech, the type of conjugation and the form of conjugation (the latter two elements are necessary only for words that conjugate).",
        "The PUS tag set has a hierarchical structure: The top POS level consists of 15 categories(e.g., Noun, Verb, ... ).",
        "The second and lower levels are the subdivision level.",
        "For example, Noun is further subdivided into common nouns(general), proper nouns, numerals, and so on.",
        "Proper Noun is subdivided into General, Person, Organization and Place.",
        "Person and Place are subdivided again.",
        "The bottom level of the subdivision level is the word level, which is conceptually regarded as a part of the subdivision level.",
        "In the Japanese language, verbs, adjectives and auxiliary verbs have conjugation.",
        "These are categorized into a fixed set of conjugation types(CTYPE), each of which has a fixed set of conjugation forms(CFORM).",
        "It is known that in Japanese that the CFORM varies according to the words appearing in the succeeding position.",
        "Thus, at the conditional position of the estimated tag probabilities, the CFORM plays an important role, while in the case of other positions, they need not be distinguished.",
        "Figure 1 illustrates the structure of the tag set."
      ]
    },
    {
      "heading": "2.3 Problems in statistical models",
      "text": [
        "On the one hand, most of the problems in statistical natural language processing stem from the sparseness of training data.",
        "In our case, the number of the most fine-grained tags (disregarding the word level) is about 500.",
        "Even when we use the bi-gram model, we stiffer from the data sparseness problem.",
        "The situation is much worse in the case of the trigram model.",
        "This may be remedied by reducing the tag set by grouping the tags into a smaller tag set.",
        "On the other hand, there are various kinds of exceptions in language phenomena.",
        "Some words have different contextual features from others in the same tag.",
        "Such exceptions require a word or some group of words to be taken itself as a distinct part-of-speech or its statistics to be taken in distinct contexts.",
        "In our statistical learning tools, those exceptions are handled by position-wise grouping, word-level statistics, smoothing of word-level and PUS-level, and selective trigram model, which are described in turn in the next section.",
        "These features enable users to or g max P (T1 W) P(T,W) = aril nix p ) (try max P(T,W) org max P(1V1T)P(T)",
        "adjust the balance between fine and coarse grained model settings."
      ]
    },
    {
      "heading": "3 Features of the tools",
      "text": [
        "This section overviews characteristic features of the learning tools for coping with the above mentioned problems."
      ]
    },
    {
      "heading": "3.1 Position-wise grouping of POS tags",
      "text": [
        "Since we use a very fine-grained tag set, it is important to classify them into some equivalence classes to reduce the size of probabilistic parameters.",
        "Moreover, as is discussed in the previous section, some words or POS behaves differently according to the position they appear.",
        "In Japanese, for instance, the CFORM play all important role only to disambiguate the words at their succeeding position.",
        "In other words, the CFORM should be taken into account only when they appear at the position of ti_1 in either bi-gram or trigram model ill 1)(ti 111-4 ) and P (ti I ti _9 , )).",
        "This means that when the statistics of verbs are taken, they should be grouped differently according to the positions.",
        "Note that, we named the positions; The current position means the position of t1 in the hi-gram statistics P(ti I/4_0 or the trigram statistics P (ti t , 14_4 ).",
        "The preceding position means the position of ti_1.",
        "The second preceding position means the position of There are quite a few contracted forms in colloquial expressions.",
        "For example, auxiliary verb \"chau\" is a contracted forms consisting of two words \"te(particle) simau(auxiliary verb)\" and behaves quite differently from other words.",
        "One way to learn its statistical behavior is to collect various usages of the word and add the data to the training data after correctly annotating them.",
        "In contrast, the idea of point-wise grouping provides a nice alternative solution to this problem.",
        "By simply group this word into the same equivalence class of \"te\" for the current position ti and group it into the same equivalent class of \"siman\" for the preceding position ti_1 in 1)(1 [), it learns the statistical behavior from these classes.",
        "We now describe the point-wise grouping in a. more precise way.",
        "For simplicity, we assume bi-gram model.",
        "Let T = {A, B, – } be the original tag set.",
        "We introduce two partitions of the tag set, one is for the current position Te = {A', .73C, • • •}, and the other is for the preceding position T\" = {A\", By, • • •}.",
        "We define the equivalence mapping of the current position: Ic(T T\"), mid another mapping of the preceding position: IP ('r 1\").",
        "Figure 2 shows an example of the partitions by those mappings, where the equivalence mappings ire: = {A A\", B AC, C .-1.\",1) B\", E Be, Is'={A-hrll',13->A\",C->Bs',1)-3B\",F Suppose we express the equivalence class to which the tag t belongs as [t]\" for the current position and [t]\" for the preceding position, then:"
      ]
    },
    {
      "heading": "3.2 Word-level statistics",
      "text": [
        "Some words behave differently from other words even in the same POS.",
        "Especially Japanese particles, auxiliary verbs and some affixes are known to have different contextual behavior.",
        "The tools can define",
        "some words as distinct PUS and their statistics are taken individually.",
        "The tag set T extends to a new tag set Text that defines some words as individual POSs (the word level).",
        "Modification to the probability formulas for such word level tags is straightforward.",
        "Note that the statistics for PUS level should be modified when some words in the same group are individuated.",
        "Suppose that the tags A and B are defined in the T and some words T47„„ ,147„„ E A and 117b1 • ,147b,„ E B are individuated in Text.",
        "We define tags Best Text as follows: We define two smoothing coefficients: A, is the smoothing ratio for the current, position and Ap is the smoothing ratio of the preceding position.",
        "Those values can be defined for each word.",
        "Suppose the word wi is individuated and its PUS is If the current position is smoothed, then the tag probability is defined as follows (note that wi itself is an individuated tag): = ((1 - Ac)P(tilti_i) AePetviiti-0) If the word at the preceding positions is smoothed (assume is the PUS of",
        "To estimate the probability for the connection A-B, the frequency F(Aest, Best) is used rather than the total frequency F(A, B).",
        "Figure 3 illustrate the tag set extension of this situation.",
        "These tag set extension is actually a special case of position-wise grouping.",
        "The equivalence mappings are from all word level tags to Text.",
        "The mapping IC maps all the words in Aest into Aest and maps each of {W01,...,Wa„} into itself.",
        "In the same way, I\" maps all the words in Best into Best and maps each of {147b17 , } into itself."
      ]
    },
    {
      "heading": "3.3 Smoothing of word and POS level statistics",
      "text": [
        "When a word is individuated while its occurrence frequency is not high, We have to accumulate instances to obtain enough statistics.",
        "Another solution is to smooth the word level statistics with PUS level statistics.",
        "In order to back-off the sparseness of the words, we use the statistics of the POS to which the words belong."
      ]
    },
    {
      "heading": "3.4 Selective trigram model",
      "text": [
        "Simple trigram models are not feasible for a large tag set.",
        "As a matter of fact, only limited cases require as long contexts as trigrams.",
        "We propose to take into account only limited trigram instances, which we call selective tri-grains.",
        "Our model is a mixture of such trigram statistics with bi-gram ones.",
        "The idea of mixture of different context length is not new.",
        "Markov Models with variable memory length are proposed by Ron(Ron et al., 1994), in which a mixture model of n-grams with various value of n is presented as well as its learning algorithms.",
        "In such a model, the set of contexts (the set of states of the automata) should be mutually disjoint for the automata to be deterministic and well-defined.",
        "We give a little different interpretation to trigram statistics.",
        "We consider a trigram as an exceptional",
        "context.",
        "When a bi-grain context mid a trigram context have some intersection, the trigram context is regarded as an exception within the hi-grain context.",
        "In this sense, all the contexts are mutually disjoint as well in our model, and it is possible to convert our model into lions formulation.",
        "However, we think that our formulation is more straighforward if the longer contexts are interpreted as exceptions to the shorter contexts.",
        "We assume that the grouping at the current position (T°) share the same grouping of the hi-gram case.",
        "But for the preceding position and the second preceding position, we can define different groupings of tag sets from those of the bi-gram case.",
        "We introduce the two new tag sets for the preceding positions: The tag set of the preceding position: TP {AP' , BP' • • • } The tag set of the second preceding position: Tee' A pr' .",
        ".",
        ".}",
        "We define the equivalence mapping for the preceding position: 17) (T TP ), mid the mapping for the second preceding position: PP (T 'TP1) ).",
        "Assuming that an equivalence classes for t defined by the mapping Im is expressed as [t]PP , the trigram probability is defined naturally as follows:",
        "Figure 4 shows an image of frequency counts for trigram model.",
        "In case some hi-gram context overlaps with a trigram context, the hi-gram statistics are taken by excluding the trigram statistics.",
        "For example, if we include the tri-grain context A – C – 13 in our model, then the statistics of the hi-gram context C – is taken as follows (F stands for true frequency in training corpora while F' stands for estimated frequency to be used for probability calculation): (C, B) = JAC, B) – (A, C , B) Since selection of tri-grain contexts is not easy task, the tools supports the selection based on an error-driven method.",
        "We omit the detail because of the space limitation."
      ]
    },
    {
      "heading": "3.5 Estimation for unseen words in corpus",
      "text": [
        "Since not all the words in the dictionary appear in the training corpus, the occurrence probability of unseen words should be allocated in some way.",
        "There are a number of method for estimating unseen events.",
        "Our current tool adopts Lidstone's law of succession, which add a fixed count to each observation.",
        "At present, the default frequency count (1 is set to (1.5."
      ]
    },
    {
      "heading": "4 Experiments and Evaluation",
      "text": [
        "For evaluating how the proposed extension improves a normal bi-grain model, we conducted several experiments.",
        "We group verbs according to the conjugation forms at the preceding position, take word level statistics for all particles, auxiliary verbs and symbols, each of which is smoothed with the immediately higher POS level.",
        "Selective trigram contexts are defined for discriminating a few notoriously ambiguous particle \"no\" and auxiliary verbs \"nai\" and \"aril.\" This is a very simple extension but suffices for evaluating the effect of the learning tools.",
        "We use 5-fold cross evaluation over the RWCP tagged corpus (RWCP, 2000).",
        "The corpus date size is 37490 sentences(958678 words).",
        "The errors of the corpus are manually modified.",
        "The annotated corpus is divided into the training data set(29992 sentences, 80%) and the test data set(7498 sentences, 20%).",
        "Experiments were repeated 5 times, and the results were averaged.",
        "The evaluation is done at the following 3 levels:",
        "• level': only word segmentation (tokenization) is evaluated • level2: word segmentation and the top level part-of-speech are evaluated • leve13: all information is taken into account for evaluation",
        "Using the tools, we create the following six models: D: normal hi-gram model D.„,: D I word level statistics for particles, etc.",
        "The current position A A A The preceding position",
        "The smoothing rate between the part-of-speech and the words is fixed to 0.9 for each word.",
        "To evaluate the results, we use the F-value defined by the following formulae:",
        "number of words by system output",
        "For each model, we evaluate the F-value (with 13 = 1) for the learning data and test data at each level.",
        "The results are given in the Tables 1 and 2.",
        "From the results the following observation is possible: Smoothing nnprove on grouping dataset in test data slightly.",
        "But in the other environments the accuracy isn't improved.",
        "In this experiment, the smoothing rate for all words is fixed.",
        "We need to make the different rate for each word in the future work.",
        "The grouping performs good result for the test dataset.",
        "It is natural that the grouping is not good for learning dataset since all the word level statistics are learned in the case of learning dataset.",
        "Finally, the selective trigram (only 25 rules added) achieves non-negligible improvement at level2 and level3.",
        "Compared with the normal bi-gram model, it improves about 0.35% on level3 and about 0.2% on level2."
      ]
    },
    {
      "heading": "5 Related work",
      "text": [
        "Cutting introduced grouping of words into equivalence classes based on the set of possible tags to reduce the number of the parameters (Cutting et al., 1992) .",
        "Schmid used the equivalence classes for smoothing.",
        "Their classes define not a partition of POS tags, but mixtures of some PUS tags (Schmid, 1995) .",
        "Brill proposed a transformation-based method.",
        "In the selection of trigram contexts we will use a similar technique (Brill, 1995) .",
        "Haruno constructed variable length models based on the mistake-driven methods, and mixed these tag models.",
        "They do not have grouping or smoothing facilities (Haruno and Matsumoto, 1997).",
        "Kitauchi presented a method to determine refinement of the tag set by a mistake-driven technique.",
        "Their method determines the tag set according to the hierarchical definition of tags.",
        "Word level discrimination and grouping beyond the hierarchical tag structure are out of scope of their method (Kitauchi et al., 1999)."
      ]
    },
    {
      "heading": "6 Conclusion and Future works",
      "text": [
        "We proposed several extensions to the statistical model for Japanese morphological analysis.",
        "We also gave preliminary experiments and showed the effects of the extensions.",
        "Counting some words individually and smoothing them with PUS level statistics alleviate the data sparseness problem.",
        "Position-wise grouping enables an effective refinement of the probability parameter settings.",
        "Using selective trigram provides an easy description of exceptional language phenomena.",
        "In our future work, we will develop a method to refine the models automatically or semi-automatically.",
        "For example, error-driven methods will be applicable to the selection of the words to be individuated and the useful trigram contexts.",
        "For the morphological analyzer ChaSen, we are using the mixture model: Position-wise grouping used for conjugation.",
        "Smoothing of the word level and the PUS level used for particles.",
        "The analyzer and the learning tools are available publicly'."
      ]
    }
  ]
}
