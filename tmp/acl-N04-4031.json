{
  "info": {
    "authors": [
      "Dragomir R. Radev",
      "Hong Qi",
      "Adam Winkel",
      "Daniel Tam"
    ],
    "book": "Human Language Technology Conference and Meeting of the North American Association for Computational Linguistics – Short Papers",
    "id": "acl-N04-4031",
    "title": "Computational Linkuistics: Word Triggers Across Hyperlinks",
    "url": "https://aclweb.org/anthology/N04-4031",
    "year": 2004
  },
  "references": [
    "acl-C00-1027",
    "acl-P96-1041"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "It is known that context words tend to be self-triggers, that is, the probability of a content word to appear more than once in a document, given that it already appears once, is significantly higher than the probability of the first occurrence.",
        "We look at self-triggerability across hyperlinks on the Web.",
        "We show that the probability of a word to appear in a Web document depends on the presence of in documents pointing to .",
        "In Document Model-ing, we will propose the use of a correction factor, , which indicates how much more likely a word is to appear in a document given that another document containing the same word is linked to it."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Given the size of the Web, it is intuitively very hard to find a given page of interest by just following links.",
        "Classic results have shown however, that the link structure of the Web is not random.",
        "Various models have been proposed including power law distributions (the “rich get richer” model), and lexical models.",
        "In this paper, we will investigate how the presence of a given word in a given Web document affects the presence of the same word in documents linked to .",
        "We will use the term Computational Linkuistics to describe the study of hyperlinks for Document Modeling and Information Retrieval purposes."
      ]
    },
    {
      "heading": "1.1 Link structure of the Web",
      "text": [
        "Random graphs have been studied by Erd˝os and R´enyi (Erd˝os and R´enyi, 1960).",
        "In a random graph, edges are added sequentially with both vertices of a new edge chosen randomly.",
        "The diameter of the Web (that is, the average number of links from any given page to another) has been found to be a constant (approximately , where is the number of documents on the Web and is the average document out-degree (i.e., the number of pages linked from the document).",
        "This result was described in (Barab´asi and Albert, 1999) and is based on a corpus of 800 M web pages).",
        "This estimate of would entail that in a random graph model, the size of the Web would be approximately which is 10 M times its actual size.",
        "Clearly, a random graph model is not an appropriate description of the Web.",
        "Instead, it has been shown that due to preferential attachment (Barab´asi and Albert, 1999), the out-degree distribution follows a power law.",
        "The preferential model makes it more likely that a new random edge will connect two vertices that already have a high degree.",
        "Specifically, the degree of pages is distributed according to , where is a constant strictly greater than 0.",
        "(Note this is different from , the distribution of out-degree on random graphs.)",
        "As a result, random walks on the Web graph soon reach well-connected nodes."
      ]
    },
    {
      "heading": "1.2 Lexical structure of the Web",
      "text": [
        "Davison (Davison, 2000) discusses the topical locality hypothesis, namely that new edges are more likely to connect pages that are semantically related.",
        "In Davison’s experiment, semantic and link distances between pairs of pages from a 100 K page corpus were computed.",
        "Davison describes results associating TF*IDF cosine similarity (Salton and McGill, 1983) and link hop distance.",
        "He reports that the cosine similarity between pages selected at random from his corpus is 0.02 whereas that number increases significantly for topologically related pages: 0.31 for pages from the same Web domain, 0.23 for linked pages, and 0.19 for sibling pages (pages pointed to by the same page).",
        "Menczer (Menczer, 2001) introduces the link-content conjecture states that the semantic content of a web page can be inferred from the pages that point to it.",
        "Menczer uses a corpus of 373 K pages and employs a non-linear least squares fit to come up with a semantic model connecting cosine-based semantic similarity and the link distance between two pages and (the shortest directed distance on the hypertext graph from to ).",
        "Menczer reports that and are connected via a power law: .represents noise level in similarity.",
        "Menczer reports empirically determined values of the parameters of the fit as follows: , , and .",
        "Menczer’s results further confirm Davison’s observations that pages adjacent in hyperlink space to a given page are semantically connected.",
        "Our idea has been to investigate the circumstances under which the semantic similarity between linked pages can be explained in terms of the presence of individual words across links."
      ]
    },
    {
      "heading": "1.3 Document modeling",
      "text": [
        "In the computational linguistics and speech communities, the notion of a language model is used to describe a probability distribution over words.",
        "Since a cluster of documents contains a subset of an entire language, a document model is a special case of a language model.",
        "As such, it can be expressed as a conditional probability distribution indicating how likely a word is to appear in a document given some context (e.g., other similar documents, the topic of the document, etc.).",
        "Language models are used in speech recognition (Chen and Goodman, 1996), document indexing (Bookstein and Swanson, 1974; Croft and Harper, 1979) and information retrieval (Ponte and Croft, 1998).",
        "Document models are a special class of language models.",
        "One property of document models is that they can be used to predict some lexical properties of textual documents, e.g., the frequency of a certain word.",
        "Mosteller and Wallace (Mosteller and Wallace, 1984) discovered that content words are ”bursty” - the appearance of a content word significantly increases the probability that the word would appear again.",
        "Church and his colleagues (Church and Gale, 1995; Church, 2000) describe document models based on the distribution of the frequencies of individual words over large document collections.",
        "In (Church and Gale, 1995), Church and Gale compare document models based on the Poisson distribution, the 2-Poisson distribution (Bookstein and Swanson, 1974), as well as generic Poisson mixtures.",
        "A Poisson mixture is described by , where for a given integer non-negative value of .",
        "Church and Gale empirically show that Poisson mixtures are a more accurate model for describing the distribution of words in documents within a corpus.",
        "They obtain the best fits with the Negative Binomial model and the K-mixture (both special cases of Poisson mixtures) (Church and Gale, 1995).",
        "In the Negative Binomial case, (which is the Gamma distribution) whereas in the K-mixture, , where is Dirac’s delta function.",
        "Our study focuses on modeling across hyperlinks.",
        "Documents linked across the web are often written by people with different backgrounds and language usage pattern."
      ]
    },
    {
      "heading": "1.4 Link-based document models",
      "text": [
        "In Church et al.’s experiments, the documents being modeled do not have hyperlinks between them.",
        "When modeling hyperlinked corpora, it is important to decompose the document model into link free and link-dependent components.",
        "The link-free component predicts the probability of a word appearing in a document regardless of the documents that point to .",
        "The link-dependent part makes use of a particular incarnation of the link-content conjecture, namely micro link-content dependency (MLD), which we will propose in this paper."
      ]
    },
    {
      "heading": "1.5 Our framework",
      "text": [
        "In traditional Information Retrieval, the main object that is represented, and searched, is the document.",
        "In our setup, we will be looking at the hyperlink between two documents as the main object to retrieve.",
        "If a page points to page via link , then we will consider as the object to index and the two pages that it links as features describing the link.",
        "For our experiments, we used the 2-Gigabyte wt2g corpus (Hawking, 2002) which contains 247,491 Web documents connected with 3,118,248 links.",
        "These documents contain 948,036 unique words (after Porter-style stemming)."
      ]
    },
    {
      "heading": "2 A link-based document model",
      "text": [
        "It is well known that the distributions of words in text depend on many factors such as genre, topic, author, etc.",
        "Certain words with high content has been found to ”trigger” other words to appear.",
        "Interestingly, the hyperlinks which connect the text on the Web may also affect the word distributions in the hypertext.",
        "For example, if page that contains education points to page , then we would expect a higher probability of seeing education in page than in a random page.",
        "This experiment was designed to discover how the links between pages can trigger words and change the word distributions.",
        "For each stemmed word in wt2g, we compute the following numbers: PagesContainingWord = how many pages in the collection contain the word.",
        "OutgoingLinks = the total number of outgoing links in all the pages that contain the word.",
        "LinkedPagesContainingWord = how many of the linked pages contain the word.",
        "For the latter two measures, only the links inside the collection were considered.",
        "The probability of a word appearing in a random page is computed as where Total Pages = 247,491.",
        "If contains the word and points to a new page , then the probability of the word appearing in is computed as For instance, in the wt2g corpus there are 55,654 pages that contain the word each, and these pages have a total of 46,163 links pointing to the pages in the collection, 15,815 of which have the word each.",
        "Therefore, its prior probability is , and its posterior probability is .",
        "We are interested in the ratio of posterior over prior probability for each stemmed word and would like to see if there is any interesting relationship between this ratio and other linguistic features.",
        "We will look at the ratio (the link effect) which describes how much more likely a linked page is to contain a given word than a random page.",
        "IDF (Inverse Document Frequency) values based on the wt2g corpus are also computed.",
        "We compute IDF using the formula , where is the document frequency (fraction of all documents containing ) and is the number of documents in the collection."
      ]
    },
    {
      "heading": "2.1 Results and Discussion",
      "text": [
        "Table 1 shows the different measures for the 2000 words with lowest IDF.",
        "Each line shows the average values on a chunk of 100 words.",
        "As one can see in the table, the posterior probabilities are always higher than the prior probabilities.",
        "Hypothesis testing shows that the difference between prior and posterior is statistically significant, which verifies our assumption.",
        "It is noticeable that the link effect has the same trend as the IDF values.",
        "The correlation coefficient of these two columns is 0.7112.",
        "It is customary to use IDF as an indicator of words’ content.",
        "Low IDF usually implies a low content value.",
        "We would like to investigate whether link effect can be used instead of IDF for certain IR tasks.",
        "Let’s consider the sample words between and american on table 1.",
        "Intuitively, american has more content than between, but the later has an IDF of 2.37, higher than that of the former (2.36).",
        "However, their link effects agree with intuition: american: 1.97, one standard deviation higher than between: 1.40.",
        "Table 2 compares the link effects for two ranges of sample words with roughly the same IDF values within each range.",
        "It shows the words in the order of IDF and of Link Effect ( ).",
        "As one can see, the link effect tends to be high for content words when IDF value alone cannot discriminate the words.",
        "Figure 1 describes a linear fit of over the 2000 words with the lowest IDF in our corpus.",
        "A very clear trend can be observed, whereby over most words, the value of is almost a constant.",
        "When we looked only at the top 100 or 200 words, the trend was even cleaner.",
        "However, with 2000 words one cannot help but notice that a number of outliers appear in the left hand part of the figure.",
        "We ran a K-Means c (with K=2) to identify two clusters of words.",
        "The clusterer stopped after 32 iterations after identifying the two clusters (Figures 2 and 3), each with a very clear trend.",
        "Their means are 1.86 and 3.57, respectively."
      ]
    },
    {
      "heading": "3 Conclusion",
      "text": [
        "In this paper we discussed some properties of hyperlinked Web documents.",
        "We showed that the probability of a word to appear in a Web document depends on the presence of in documents pointing to ."
      ]
    }
  ]
}
