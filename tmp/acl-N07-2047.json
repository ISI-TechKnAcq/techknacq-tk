{
  "info": {
    "authors": [
      "Zhuoran Wang",
      "John Shawe-Taylor",
      "Sandor Szedmak"
    ],
    "book": "Human Language Technologies 2007: the Conference of the North American Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",
    "id": "acl-N07-2047",
    "title": "Kernel Regression Based Machine Translation",
    "url": "https://aclweb.org/anthology/N07-2047",
    "year": 2007
  },
  "references": [
    "acl-N04-4026",
    "acl-P03-1021"
  ],
  "sections": [
    {
      "text": [
        "Zhuoran Wang and John Shawe-Taylor Sandor Szedmak",
        "London, WC1E 6BT Southampton, SO17 1BJ",
        "United Kingdom United Kingdom",
        "marquées questions aux revenous nous '4s Figure 1: Phrase alignment in SMT We present a novel machine translation framework based on kernel regression techniques.",
        "In our model, the translation task is viewed as a string-to-string mapping, for which a regression type learning is employed with both the source and the target sentences embedded into their kernel induced feature spaces.",
        "We report the experiments on a French-English translation task showing encouraging results."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Fig.",
        "1 illustrates an example of phrase alignment for statistical machine translation (SMT).",
        "A rough linear relation is shown by the co-occurences of phrases in bilingual sentence pairs, which motivates us to introduce a novel study on the SMT task:",
        "If we define the feature space Hx of our source language X as all its possible phrases (i.e. informative blended word «-grams), and define the mapping $x : X – Hx, then a sentence x G X can be expressed by its feature vector <J>x(x) G Hx.",
        "Each component of $x (x) is indexed by a phrase with the value being the frequency of it in x.",
        "The definition of the feature space Hy of our target language Y can be made in a similar way, with corresponding mapping $y : Y – Hy.",
        "Now in the machine translation task, given S = {(x*,y*) : x* G X,y* G Y,i = 1,..., m}, a set of sample sentence pairs where is the translation of x*, we are trying to learn W a matrix represented linear operator, such that:",
        "to predict the translation y for a new sentence x.",
        "Comparing with traditional methods, this model gives us a theoretical framework to capture higher-dimensional dependencies within the sentences.",
        "To solve the multi-output regression problem, we investigate two models, least squares regression (LSR) similar to the technique presented in (Cortes et al., 2005), and maximum margin regression (MMR) introduced in (Szedmak et al., 2006).",
        "The rest of the paper is organized as follows.",
        "Section 2 gives a brief review of the regression models.",
        "Section 3 details the solution to the pre-image problem.",
        "We report the experimental results in Section 4, with discussions in Section 5."
      ]
    },
    {
      "heading": "2. Kernel Regression with Vector Outputs",
      "text": [
        "In the practical learning process, only the inner products of the feature vectors are needed (see Section 2.2, 2.3 and 3), so we can perform the so-called kernel trick to avoid dealing with the very high-dimensional feature vectors explicitly.",
        "That is, for x, z G X, a kernel function is defined as:",
        "Similarly, a kernel function Ky (•, •) is defined in Hy.",
        "In our case, the blended n-spectrum string kernel (Lodhi et al., 2002) that compares two strings by counting how many (contiguous) substrings of length from 1 up to n they have in common, is a good choice for the kernel function to induce our feature spaces Hx and Hy implicitly, even though it brings in some uninformative features (word n-grams) as well, when compared to our original definition.",
        "A basic method to solve the problem in Eq.",
        "1 is least squares regression that seeks the matrix W minimizing the squared loss in Hy on the training set S:",
        "min y WMx where Mx = [ $ x(xi),..., $ x (xm)j, My = [ $y(yi),$ y(ym)], and || • ||f denotes the Frobe-nius norm.",
        "Differentiating the expression and setting it to zero gives:",
        "2WMxMxT - 2MyMxT = where Kx = M]\"Mx = («x (xi ,xj)1<ij<m) is the Gram matrix.",
        "An alternative solution to our regression learning problem is proposed in (Szedmak et al., 2006), called maximum margin regression.",
        "If L2-normalized feature vectors are used in Eq.",
        "1, denoted by Ix(■) and Iy(■), MMR solves the following optimization:",
        "s.t.",
        "< I y (yi), WI x (xi > l - Ci, Ci > 0,i = l,... ,m.",
        "where C > 0 is the regularization coefficient, and £i are the slack variables.",
        "The Lagrange dual form with dual variables a gives:",
        "where kx(-, •) and Ky(•, •) denote the kernel functions associated to the respective normalized feature vectors.",
        "This dual problem can be solved efficiently with a perceptron algorithm based on an incremental subgradient method, of which the bounds on the complexity and achievable margin can be found in (Szedmak etal., 2006).",
        "Then according to Karush-Kuhn-Tucker theory, W is expressed as:",
        "In practice, MMR works better when the distribution of the training points are symmetrical.",
        "So we center the data before normalizing them.",
        "If $ Sx = m YliLi ^x(xi) is the centre of mass of the source sentence sample set {x*} in the feature space, the new feature map is given by $x(•) = $ x(•) – $ Sx.",
        "The similar operation is performed on $ y(•) to obtain $y (•).",
        "Then the L2-normalizations of $x (•) and $y (•) yield our final feature vectors $x( ) and $y (•)."
      ]
    },
    {
      "heading": "3. Pre-image Solution",
      "text": [
        "To find the pre-image sentence y = f -1(x) can be achieved by seeking yt that has the minimum loss between its feature vector $ y (yt) and our prediction f (x).",
        "That is (Eq.",
        "8: LSR, Eq.",
        "9: MMR):",
        "where Y(x) C Y is a finite set covering all potential translations for the given source sentence x, and kx(•) = («x0,x*)i<*<m) and ky(•) = (Ky(•,yi)1<i<m) are m x 1 column matrices.",
        "A proper Y(x) can be generated according to a lexicon that contains possible translations for every component (word or phrase) in x.",
        "But the size of it will grow exponentially with the length of x, which poses implementation problem for a decoding algorithm.",
        "In earlier systems, several heuristic search methods were developed, of which a typical example is Koehn (2004)'s beam search decoder for phrase-based models.",
        "However, in our case, because of the y(y, y) item in Eq.",
        "8 and the normalization operation in MMR, neither the expression in Eq.",
        "8 nor the one in Eq.",
        "9 can be decomposed into a sum of subfunctions each involving feature components in a local area only.",
        "It means we cannot estimate exactly how well a part of the source sentence is translated, until we obtain a translation for the entire sentence, which prevents us doing a straightforward beam search similar to (Koehn, 2004).",
        "To simplify the situation, we restrict the reordering (distortion) of phrases that yield the output sentences by only allowing adjacent phrases to exchange their positions.",
        "(The discussion of this strategy can be found in (Tillmann, 2004).)",
        "We use x[i:j] and y[i:j] to denote the substrings of x and y that begin with the ith word and end with the jth.",
        "Now, if we go back to the implementation of a beam search, the current distortion restriction guarantees that in each expansion of the search states (hypotheses) we have x[1:y translated to a y[1:1y], either like state (a) or like state (b) in Fig. 2, where 1x is the number of words translated in the source sentence, and 1y is the number of words obtained in the translation.",
        "We assume that if y is a good translation of x, then y[1:y is a good translation of x[1:1x] as well.",
        "So we can expect that the squared loss ||W$x (x[1:y) – $ y(y[1:1 ]) || in the LSR is small, or the inner product ($'J(y[1:jw]), W$x(x[1;Min the MMR is large, for the hypothesis yielding a good translation.",
        "According to Eq.",
        "8 and Eq.",
        "9, the hypotheses in the search stacks can thus be reranked with the following score functions (Eq.",
        "10: LSR, Eq.",
        "11: MMR):",
        "(a) pous revenous aux | [quesüons | marquées .",
        "we return to questions",
        "Therefore, to solve the pre-image problem, we just employ the same beam search algorithm as (Koehn, 2004), except we limit the derivation of new hypotheses with the distortion restriction mentioned",
        "(b) Pous revenous aux questions marquées We return to | [marked | questions",
        "above.",
        "However, our score functions will bring more runtime complexities when compared with traditional probabilistic methods.",
        "The time complexity of a naive implementation of the blended n-spectrum string kernel between two sentences and Sj is O(n|si||Sj |), where | • | denotes the length of the sentence.",
        "So the score function in Eq.",
        "11 results in an average runtime complexity of O(mn1yl), where l is the average length of the sentences in the training set.",
        "Note here Kx(x[1:1x],x*) can be precomputed for 1x from 1 to |x| before the beam search, which calls for O(m|x|) space.",
        "The average runtime complexity of the score function in Eq.",
        "10 will be the same if we pre-compute Kx\"kx(x[1:1x])."
      ]
    },
    {
      "heading": "4. Experimental Results",
      "text": [
        "Baseline System To compare with previous work, we take Pharaoh (Koehn, 2004) as a baseline system, with its default settings (translation table size 10, beam size 100).",
        "We train a trigram language model with the SRILM toolkit (Stocke, 2002).",
        "Whilst, the parameters for the maximum entropy model are developed based on the minimum error rate training method (Och, 2003).",
        "In the following experiments, to facilitate comparison, each time we train our regression models and the language model and translation model for Pharaoh on a common corpus, and use the same phrase translation table as Pharaoh's to decode our systems.",
        "According to our preliminary experiments, with the beam size of 100, the search errors of our systems can be limited within 1.5%.",
        "Corpora To evaluate our models, we randomly take 12,000 sentences from the French-English portion of the 1996-2003 Europarl corpus (Koehn, 2005) for scaling-up training, 300 for test (Test), and 300 for the development of Pharaoh (Dev).",
        "Some characteristics of the corpora are summarized in Table 1.",
        "Based on the 4k training corpus, we test the performance of the blended n-spectrum string kernel in LSR and MMR using BLEU score, with n increasing from 2 to 7.",
        "Fig.",
        "3 shows the results.",
        "It can be found that the performance becomes stable when n reaches a certain value.",
        "Finally, we choose the 3-spectrum for LSR, and the 5-spectrum for MMR.",
        "Then we scale up the training set, and compare the performance of our models with Pharaoh in Fig. 4.",
        "We can see that the LSR model performs almost as well as Pharaoh, whose differences of BLEU score are within 0.5% when the training set is larger than 6k.",
        "But MMR model performs worse than the baseline.",
        "With the training set of 12k, it is outperformed by Pharaoh by 3.5%."
      ]
    },
    {
      "heading": "5. Discussions",
      "text": [
        "Although at this stage the main contribution is still conceptual, the capability of our approach to be applied to machine translation is still demonstrated.",
        "Comparable performance to previous work is achieved by the LSR model.",
        "But a main problem we face is to scale-up the training set, as in practice the training set for SMT will be much larger than several thousand sentences.",
        "A method to speed up the training is proposed in (Cortes et al., 2005).",
        "By approximating the Gram matrix with a n x m (n ^ m) low-rank matrix, the time complexity of the matrix inversion operation can be reduced from O(m) to O(nm).",
        "But the space complexity of O(nm) in their algorithm is still too expensive for SMT tasks.",
        "Subset selection techniques could give a solution to this problem, of which we will leave the further exploration to future work."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "The authors acknowledge the support of the EU under the 1ST project No.",
        "FP6-033917.",
        "Vocabulary",
        "Words",
        "Perplexity",
        "Fr",
        "En",
        "Fr",
        "En",
        "Dev",
        "Test",
        "4k",
        "5084",
        "4039",
        "43k",
        "39k",
        "32.25",
        "31.92",
        "6k",
        "6426",
        "5058",
        "64k",
        "59k",
        "30.81",
        "29.03",
        "8k",
        "7377",
        "5716",
        "85k",
        "79k",
        "29.91",
        "28.94",
        "10k",
        "8252",
        "6339",
        "106k",
        "98k",
        "27.55",
        "27.09",
        "12k",
        "9006",
        "6861",
        "127k",
        "118k",
        "27.19",
        "26.41",
        "-",
        "O - Pharaoh – A – LSR - – □ – MMR"
      ]
    }
  ]
}
