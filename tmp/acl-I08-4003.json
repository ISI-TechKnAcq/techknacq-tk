{
  "info": {
    "authors": [
      "Jin-Shea Kuo",
      "Haizhou Li",
      "Chih-Lung Lin"
    ],
    "book": "Proceedings of the Sixth SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-I08-4003",
    "title": "Mining Transliterations from Web Query Results: An Incremental Approach",
    "url": "https://aclweb.org/anthology/I08-4003",
    "year": 2008
  },
  "references": [
    "acl-J98-4003",
    "acl-P04-1021",
    "acl-P06-1010",
    "acl-P06-1142",
    "acl-P98-1069",
    "acl-P99-1067"
  ],
  "sections": [
    {
      "text": [
        "Jin-Shea Kuo Haizhou Li Chih-Lung Lin",
        "Chung-Hwa Telecomm.",
        "Institute for Infocomm Chung Yuan Christian",
        "Laboratories, Taiwan Research, Singapore 119613 University, Taiwan",
        "We study an adaptive learning framework for phonetic similarity modeling (PSM) that supports the automatic acquisition of transliterations by exploiting minimum prior knowledge about machine transliteration to mine transliterations incrementally from the live Web.",
        "We formulate an incremental learning strategy for the framework based on Bayesian theory for PSM adaptation.",
        "The idea of incremental learning is to benefit from the continuously developing history to update a static model towards the intended reality.",
        "In this way, the learning process refines the PSM incrementally while constructing a transliteration lexicon at the same time on a development corpus.",
        "We further demonstrate that the proposed learning framework is reliably effective in mining live transliterations from Web query results."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Transliteration is a process of rewriting a word from one language into another by preserving its pronunciation in its original language, also known as translation-by-sound.",
        "It usually takes place between languages with different scripts, for example, from English to Chinese, and words, such as proper nouns, that do not have \"easy\" or semantic translations.",
        "The increasing size of multilingual content on the Web has made it a live information source rich in transliterations.",
        "Research on automatic acquisition of transliteration pairs in batch mode has shown promising results (Kuo et al., 2006).",
        "In dealing with the dynamic growth of the Web, it is almost impossible to collect and store all its contents in local storage.",
        "Therefore, there is a need to develop an incremental learning algorithm to mine transliterations in an on-line manner.",
        "In general, an incremental learning technique is designed for adapting a model towards a changing environment.",
        "We are interested in deducing the incremental learning method for automatically constructing an English-Chinese (E-C) transliteration lexicon from Web query results.",
        "In the deduction, we start with a phonetic similarity model (PSM), which measures the phonetic similarity between words in two different scripts, and study the learning mechanism of PSM in both batch and incremental modes.",
        "The contributions of this paper include: (i) the formulation of a batch learning framework and an incremental learning framework for PSM learning; (ii) a comparative study of the batch and incremental unsupervised learning strategies.",
        "In this paper, Section 2 briefly introduces prior work related to machine transliteration.",
        "In Section 3, we formulate the PSM and its batch and incremental learning algorithms while in Section 4, we discuss the practical issues in implementation.",
        "Section 5 provides a report on the experiments conducted and finally, we conclude in Section 6."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Much of research on extraction of transliterations has been motivated by information retrieval techniques, where attempts to extracting transliteration pairs from large bodies of corpora have been made.",
        "Some have proposed extracting translations from parallel or comparable bitexts using co-occurrence analysis or a context-vector approach (Fung and Yee, 1998; Nie et al., 1999).",
        "These methods compare the semantic similarities between source and target words without taking their phonetic similarities into account.",
        "Another direction of research is focused on establishing the phonetic relationship between transliteration pairs.",
        "This typically involves the encoding of phoneme-or grapheme-based mapping rules using a generative model trained from a large bilingual lexicon.",
        "Suppose that EW and CW form an EC transliteration pair.",
        "The phoneme-based approach (Knight & Graehl, 1998) first converts EW into an intermediate phonemic representation and then converts the phonemic representation into its Chinese counterpart CW.",
        "The grapheme-based approach, also known as direct orthographical mapping (Li et al., 2004), which treats transliteration as a statistical machine translation problem under monotonic constraints, has also achieved promising results.",
        "Many efforts have also been channeled to tapping the wealth of the Web for harvesting transliteration/translation pairs.",
        "These include studying the query logs (Brill et al., 2001), unrelated corpora (Rapp, 1999), and comparable corpora (Sproat et al.",
        "2006).",
        "To establish cross-lingual correspondence in the harvest, these algorithms usually rely on one or more statistical clues (Lam et al., 2004), such as the correlation between word frequencies, and cognates of similar spelling or pronunciations.",
        "In doing so, two things are needed: first, a robust mechanism that establishes statistical relationships between bilingual words, such as a phonetic similarity model which is motivated by transliteration modeling research; and second, an effective learning framework that is able to adaptively discover new events from the Web.",
        "In Chinese/Japanese/Korean (CJK) Web pages, translated terms are frequently accompanied by their original Latin words, with the Latin words serving as the appositives of the CJK words.",
        "In other words, the EC pairs are always closely collocated.",
        "Inspired by this observation in CJK texts, some algorithms were proposed (Kuo et al., 2006) to search over the close context of an English word in a Chinese predominant bilingual snippet for transliteration.",
        "Unfortunately, many of the reported works have not taken the dynamic nature of the Web into account.",
        "In this paper, we study the learning framework of the phonetic similarity model, which adopts a transliteration modeling approach for transliteration extraction from the Web in an incremental manner."
      ]
    },
    {
      "heading": "3. Phonetic Similarity Model",
      "text": [
        "Phonetic similarity model (PSM) is a probabilistic model that encodes the syllable mapping between EC pairs.",
        "Let ES = {e1,...emeM}be a sequence of English syllables derived from EW and CS = {s1,...sn} be the sequence of Chinese syllables derived from CW, represented by a Chinese character string CW w1,...wnwN .",
        "If each of the English syllables is drawn from a vocabulary of X entries, em e {x1;...,x1}, and each of the Chinese syllable from a vocabulary of Y entries, sn e{>i,...,yJ}, then the EC transliteration can be considered as a generative process formulated by the noisy channel model, which recovers the input CW from the observed output EW.",
        "Applying Bayesian rule, we have Eq.",
        "(1), where P(EW | CW) is estimated to characterize the noisy channel, known as the transliteration probability and P(CW) is a language model to characterize the source language.",
        "P(CW |EW) = P(EW |CW)P(CW)/P(EW) .",
        "(1) Following the translation-by-sound principle, P(EW | CW) can be approximated by the phonetic probability P(ES | CS), which is given by Eq.",
        "(2).",
        "where r is the set of all possible alignment paths between ES and CS.",
        "To find the best alignment path A , one can resort to a dynamic warping algorithm (Myers and Rabiner, 1981).",
        "Assuming conditional independence of syllables in ES and CS, we index of alignment.",
        "We rewrite Eq.",
        "(1) as,",
        "The language model P(CW) in Eq.",
        "(3) can be represented by the n-gram statistics of the Chinese characters derived from a monolingual corpus.",
        "Using bigram to approximate the n-gram model, we have",
        "Removing P(EW) from Eq.",
        "(3) which is not a function of CW, a PSM © now consists of both P(ES | CS) and P(CW) parameters (Kuo et al., 2007).",
        "We now look into the mathematic formulation for the learning of P(ES | CS) parameters from a bilingual transliteration lexicon.",
        "A collection of manually selected or automatically extracted EC pairs can form a transliteration lexicon.",
        "Given such a lexicon for training, the PSM parameters can be estimated in a batch mode.",
        "An initial PSM is bootstrapped using limited prior knowledge such as a small amount of transliterations, which may be obtained by exploiting cooccurrence information (Sproat et al., 2006).",
        "Then we align the EC pairs using the PSM © and derive syllable mapping statistics.",
        "Suppose that we have the event counts ci; = given transliteration lexicon D with alignments A .",
        "We would like to find the parameters",
        "P\\j = P(em = xi I sn = yj ) , < em, sn > A , that maximize the probability, log-likelihood (LL)",
        "It is described as the cross-entropy of the true data distribution c j with regard to the PSM model.",
        "Given an alignment AeA , the MLE estimate of PSM is:",
        "With a new PSM, one is able to arrive at a new alignment.",
        "This is formulated as an expectation-maximization (EM) process (Dempster, 1977), which assumes that there exists a mapping D A, where A is introduced as the latent information, also known as missing data in the EM literature.",
        "The EM algorithm maximizes the likelihood probability P(D | ©) over © by exploiting",
        "P(D|©) = P(D, A |©).",
        "The EM process guarantees non-decreasing likelihood probability P(D | ©) through multiple EM steps until it converges.",
        "In the E-step, we derive the event counts c j and cj by force-aligning all the EC pairs in the training lexicon D using a PSM.",
        "In the M-step, we estimate the PSM parameters © by Eq.(7).",
        "The EM process also serves as a refining process to obtain the best alignment between the EC syllables.",
        "In each EM cycle, the model is updated after observing the whole corpus D .",
        "An EM cycle is also called an iteration in batch learning.",
        "The batch learning process is described as follows and depicted in Figure 1.",
        "Batch Learning Algorithm:",
        "Start: Bootstrap PSM parameters P\\j using prior phonetic mapping knowledge; E-Step: Force-align corpus D using P\\j to obtain",
        "A and hence the counts of ci j and cj; M-Step: Re-estimate P\\j = j / cj using the counts from E-Step; Iterate: Repeat E-Step and M-Step until P(D | ©) converges;",
        "In batch learning all the training samples have to be collected in advance.",
        "In a dynamically changing environment, such as the Web, new samples always appear and it is impossible to collect all of them.",
        "Incremental learning (Zavaliagkos, 1995) is devised to achieve rapid adaptation towards the working environment by updating the model as learning samples arrive in sequence.",
        "It is believed that if the statistics for the E-step are incrementally collected and the parameters are frequently estimated, incremental learning converges quicker because the information from the new data contributes to the parameter estimation more effectively than the batch algorithm does (Gotoh et al., 1998).",
        "In incremental learning, the model is typically updated progressively as the training samples become available and the number of incremental samples may vary from as few as one to as many as they are available.",
        "In the extreme case where all the learning samples are available and the updating is done after observing all of them, the incremental learning becomes batch learning.",
        "Therefore, the batch learning can be considered as a special case of the incremental learning.",
        "The incremental learning can be formulated through maximum a posteriori (MAP) framework, also known as Bayesian learning, where we assume that the parameters © are random variables subject to a prior distribution.",
        "A possible candidate for the prior distribution of Pj is the Dirichlet density over each of the parameters P|; (Bacchiani et al., set, is a positive scalar.",
        "Assuming H is the set of hyperparameters, we have as many hyperparameters h|j e H as the parameters p,; .",
        "The probability of generating the aligned transliteration lexicon is obtained by integrating over the parameter space, P(D) = JP(D | ©)P(©)d© .",
        "This integration can be easily written down in a closed form due to the conjugacy between Dirichlet distribution J~[ .",
        "P| ^|j and the multinomial distribution T~[ Pc,j .",
        "Instead of finding © that maximizes P(D | ©) with MLE, we maximize a posteriori (MAP) probability as follows:",
        "The MAP solution uses a distribution to model the uncertainty of the parameters ©, while the MLE gives a point estimation (Jelinek, 1990; MacKay, 1994).",
        "We rewrite Eq.",
        "(9) as Eq.",
        "(10) using Eq.",
        "(5) and Eq.",
        "(8).",
        "Eq.",
        "(10) can be seen as a Dirichlet function of © given H , or a multinomial function of H given © .",
        "With given prior H, the MAP estimation is therefore similar to the MLE problem which is to find the mode of the kernel density in Eq.(10).",
        "P|j ?.h j + (1 -Afj, (11) where f |j = c,j / cj, A = a /(£.",
        "c,j + a).",
        "One can find that A serves as a weighting factor between the prior and the current observations.",
        "The difference between MLE and MAP strategy lies in the fact that MAP introduces prior knowledge into the parameter updating formula.",
        "Eq.",
        "(11) assumes that the prior parameters H are known and static while the training samples are available all at once.",
        "The idea of incremental learning is to benefit from the continuously developing history to update the static model towards the intended reality.",
        "As is often the case, the Web query results in an on-line application arrive in sequence.",
        "It is of practical use to devise such an incremental mechanism that adapts both parameters and the prior knowledge over time.",
        "The quasi-Bayesian (QB) learning method offers a solution to it (Bai and Li, 2006).",
        "Let's break up a training corpus D into a sequence of sample subsets D = (D1;D2,...,DT} and denote an accumulated sample subset D(t) = (D1;D2,...,Dt},1 <t< T as an incremental corpus.",
        "Therefore, we have D = D(T).",
        "The QB method approximates the posterior probability P(© | D(tby the closest tractable prior density P(© | H(twith H(t-1) evolved from historical corpus D(t-1),",
        "n n ci ,+ahi(t-1)-1 w.",
        "QB estimation offers a recursive learning mechanism.",
        "Starting with a hyperparameter set H(0) and a corpus subset D1, we estimate H(1) and ©gB , then H(2) and ©gB) and so on until H(t) and ©gB as observed samples arrive in sequence.",
        "The updating of parameters can be iterated between the reproducible prior and posterior estimates as in Eq.",
        "(13) and Eq.",
        "(14).",
        "Assuming T ^-oo, we have the following:",
        "Incremental Learning Algorithm:",
        "Start: Bootstrap ©gB and H(0) using prior phonetic mapping knowledge and set t = 1; E-Step: Force-align corpus subset Dt using ©gB1:i, compute the event counts c(tj) and reproduce prior parameters H(t-1) h> H(t).",
        "M-Step: Re-estimate parameters H(t) -» and P|j using the counts from E-Step.",
        "EM cycle: Repeat E-Step and M-Step until P(© | D(t}) converges.",
        "Iterate: Repeat T EM cycles covering the entire data set D in an iteration.",
        "The algorithm updates the PSM as training samples become available.",
        "The scalar factor a can be seen as a forgetting factor.",
        "When a is big, the update of hyperparameters favors the prior.",
        "Otherwise, current observation is given more attention.",
        "As for the sample subset size | Dt |, if we set | Dt |= 100 , each EM cycle updates © after observing every 100 samples.",
        "To be comparable with batch learning, we define an iteration here to be a sequence of EM cycles that covers the whole corpus D. If corpus D has a fixed size | D(T) |, an iteration means T EM cycles in incremental learning.",
        "Mining Transliterations from the Web",
        "Since the Web is dynamically changing and new transliterations come out all the time, it is better to mine transliterations from the Web in an incremental way.",
        "Words transliterated by closely observing common guidelines are referred to as regular transliterations.",
        "However, in Web publishing, translators in different regions may not observe the same guidelines.",
        "Sometimes they skew the transliterations in different ways to introduce semantic implications, also known as wordplay, resulting in casual transliterations.",
        "Casual transliteration leads to multiple Chinese transliteration variants for the same English word.",
        "For example, \"Disney\" may be transliterated into \" ®±J@ /Di-Shi-Ni/\", \" iff /Di-Si-Nai/\" and \"iWff/Di-Si-Nai/\"",
        "Suppose that a sufficiently large, manually validated transliteration lexicon is available, a PSM can be built in a supervised manner.",
        "However, this method hinges on the availability of such a lexicon.",
        "Even if a lexicon is available, the derived model can only be as good as what the training lexicon offers.",
        "New transliterations, such as casual ones, may not be well handled.",
        "It is desirable to adapt the PSM as new transliterations become available, also referred to as the learning-at-work mechanism.",
        "Some solutions have been proposed recently along this direction (Kuo et al., 2006).",
        "However, the effort was mainly devoted to mitigating the need of manual labeling.",
        "A dynamic learning-at-work mechanism for mining transliterations has not been well studied.",
        "Here we are interested in an unsupervised learning process, in which we adapt the PSM as we extract transliterations.",
        "The learning-at-work framework is illustrated in Figure 2.",
        "As opposed to a manually labeled training corpus in Figure 1, we insert into the EM process an automatic transliteration extraction mechanism, search and rank, as shown in the left panel of Figure 2.",
        "The search and rank shortlists a set of transliterations from the Web query results or bilingual snippets.",
        "Search and Rank",
        "The Chinese words are romanized in Hanyu Pinyin.",
        "We obtain bilingual snippets from the Web by iteratively submitting queries to the Web search engines (Brin and Page, 1998).",
        "Qualified sentences are extracted from the results of each query.",
        "Each qualified sentence has at least one English word.",
        "Given a qualified sentence, first, the competing Chinese transliteration candidates are denoted as a set Q , from which we would like to pick the most likely one.",
        "Second, we would like to know if there is indeed a Chinese transliteration CW in the close context of the English word EW.",
        "We propose ranking the candidates using the PSM model to find the most likely CW for a given EW.",
        "The CW candidate that gives the highest posterior probability is considered the most probable candidate CW'.",
        "Search and Rank",
        "M-Step",
        "The next step is to examine if CW' and EW indeed form a genuine EC pair.",
        "We define the confidence of the EC pair as the posterior odds similar to that in a hypothesis test under the Bayesian interpretation.",
        "We have H0 , which hypothesizes that CW' and EW form an EC pair, and H1, which hypothesizes otherwise, and use posterior odd u (Kuo et al., 2006) for hypothesis tests.",
        "Our search and rank formulation can be seen as an extension to a prior work (Brill et al., 2001).",
        "The posterior odd u is used as the confidence score so that EC pairs extracted from different contexts can be directly compared.",
        "In practice, we set a threshold for u to decide on a cutoff point for EC pairs short-listing.",
        "In this way, the search and rank is able to retrieve a collection of transliterations from the Web given a PSM.",
        "Now we can carry out PSM learning as formulated in Section 3 using the transliterations as if they were manually validated.",
        "By unsupervised batch learning, we mean to re-estimate the PSM after search and rank over the whole database, i.e., in each iteration.",
        "Just as in supervised learning, one can expect the PSM performance to improve over multiple iterations.",
        "We report the F-measure at each iteration.",
        "The extracted transliterations also form a new training corpus in next iteration.",
        "In contrast to the batch learning, incremental learning updates the PSM parameters as the training samples arrive in sequence.",
        "This is especially useful in Web mining.",
        "With the QB incremental optimization, one can think of an EM process that continuously re-estimates PSM parameters as the Web crawler discovers new \"territories\".",
        "In this way, the search and rank process gathers qualified training samples Dt after crawling a portion of the Web.",
        "Note that the incremental EM process updates parameters more often than batch learning does.",
        "To evaluate performance of both learning, we define an iteration to be T EM cycles in incremental learning on a training corpus D = D(T) as discussed in Section 3.2."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "To obtain the ground truth for performance evaluation, each possible transliteration pair is manually checked based on the following criteria: (i) only the phonetic transliteration is extracted to form a transliteration pair; (ii) multiple EC pairs may appear in one sentence; (iii) an EW can have multiple valid Chinese transliterations and vice versa.",
        "The validation process results in a collection of qualified EC pairs, also referred to as distinct qualified transliteration pairs (DQTPs), which form a transliteration lexicon.",
        "To simulate the dynamic Web, we collected a Web corpus, which consists of about 500 MB of Web pages, referred to as SET1.",
        "From SET1, 80,094 qualified sentences were automatically extracted and 8,898 DQTPs were further selected with manual validation.",
        "To establish a reference for performance benchmarking, we first initialize a PSM, referred to as seed PSM hereafter, using randomly selected 100 seed DQTPs.",
        "By exploiting the seed PSM on all 8,898 DQTPs, we train a PSM in a supervised batch mode and improve the PSM on SET1 after each iteration.",
        "The performance defined below in precision, recall and F-measure in the 6th iteration is reported in Table 1 and the F-measure is also shown in Figure 3.",
        "precision = # extracted _ DQTPs /# extracted _ pairs, recall = # extracted _ DQTPs /# total _ DQTPs, F – measure = 2 x recall x precision /(recall + precision)",
        "We use this closed test (supervised batch learning) as the reference point for unsupervised experiments.",
        "Next we further implement two PSM learning strategies, namely unsupervised batch and unsupervised incremental learning.",
        "We begin with the same seed PSM.",
        "However, we use transliterations that are extracted automatically instead of manually validated DQTPs for training.",
        "Note that the transliterations are extracted and collected at the end of each iteration.",
        "It may differ from one iteration to another.",
        "After re-estimating the PSM in each iteration, we evaluate performance on SET1.",
        "Precision",
        "Recall",
        "F-measure",
        "Closed-test",
        "0.834",
        "0.663",
        "0.739",
        "Comparing the two batch mode learning strategies in Figure 3, it is observed that learning substantially improves the seed PSM after the first iteration.",
        "Without surprise, the supervised learning consistently outperforms the unsupervised one, which reaches a plateau at 0.679 F-measure.",
        "This performance is considered as the baseline for comparison in this paper.",
        "The unsupervised batch learning presented here is similar to that in (Kuo et al., 2006).",
        "We now formulate an on-line unsupervised incremental learning algorithm:",
        "(i) Start with the seed PSM, set t = 1;",
        "(ii) Extract | Dt | quasi-transliterations pairs followed by E-Step in incremental learning algorithm; (iv) Repeat (ii) and (iii) to crawl over a corpus.",
        "To simulate the on-line incremental learning just described, we train and test on SET1 because of the availability of gold standard and comparison with performance by batch mode.",
        "We empirically set a = 0.5 and study different | Dt | settings.",
        "An iteration is defined as multiple cycles of steps (ii)-(iii) that screen through the whole SET1 once.",
        "We run multiple iterations.",
        "The performance of incremental learning with | Dt |= 100 and | Dt |= 5,000 are reported in Figure 3.",
        "It is observed that incremental learning benefits from more frequent PSM updating.",
        "With | Dt |=100 , it not only attains good F-measure in the first iteration, but also outperforms that of unsupervised batch learning along the EM process.",
        "The PSM updating becomes less frequent for larger | Dt | .",
        "When | Dt | is set to be the whole corpus, then incremental learning becomes a batch mode learning, which is evidenced by | Dt |= 5,000 and it performs close to the batch mode learning.",
        "The experiments in Figure 3 are considered closed tests.",
        "Next we move on to an actual on-line experiment.",
        "Learning from the Live Web",
        "In practice, it is possible to extract bilingual snippets of interest by repeatedly submitting queries to the Web.",
        "With the learning-at-work mechanism, we can mine the query results for up-to-date transliterations in an on-line environment.",
        "For example, by submitting \"Amy\" to search engines, we may get \"Amy-g^/Ai-Mi/\" and, as a by-product, \"Jes-sica-^H-fc/Jie-Xi-Ka/\" as well.",
        "In this way, new queries can be generated iteratively, thus new pairs are discovered.",
        "Following the unsupervised incremental learning algorithm, we start the crawling with the same seed PSM as in Section 5.2.",
        "We adapt the PSM as every 100 quasi-transliterations are extracted, i.e. | Dt | = 100 .",
        "The crawling stops after accumulating 67,944 Web pages, where there are 100 snippets at most in a page, with 2,122,026 qualified sentences.",
        "We obtain 123,215 distinct EC pairs when the crawling stops.",
        "For comparison, we also carry out unsupervised batch learning over the same 2,122,026 qualified sentences in a single iteration under such an on-line environment.",
        "As the gold standard for this live corpus is not available, we randomly select 500 quasi-transliteration pairs for manual checking of precision (see Table 2).",
        "It is found that incremental learning is more productive than batch learning in discovering transliteration pairs.",
        "This finding is consistent with the test results on SET1.",
        "#distinct EC pairs Estimated Precision",
        "The live Web corpus was used in transliteration extraction using active learning (Kuo et al., 2006).",
        "Unsupervised Unsupervised Batch Incremental",
        "Kuo et al.",
        "reported slightly better performance by annotating some samples manually and adapting the learning process in a batch manner.",
        "However, it is apparent that, in an on-line environment, the unsupervised learning is more suitable for discovering knowledge without resorting to human annotation; incremental learning is desirable as it does not require storing all documents in advance."
      ]
    },
    {
      "heading": "6. Conclusions",
      "text": [
        "We have proposed a learning framework for mining EC transliterations using bilingual snippets from a live Web corpus.",
        "In this learning-at-work framework, we formulate the PSM learning method and study strategies for PSM learning in both batch and incremental manners.",
        "The batch mode learning benefits from multiple iterations for improving performance, while the unsupervised incremental one, which does not require all the training data to be available in advance, adapts to the dynamically changing environment easily without compromising the performance.",
        "Unsupervised incremental learning provides a practical and effective solution to transliteration extraction from query results, which can be easily extended to other Web mining applications."
      ]
    }
  ]
}
