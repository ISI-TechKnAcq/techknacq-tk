{
  "info": {
    "authors": [
      "Aravind K. Joshi",
      "Bonnie Lynn Webber",
      "Ralph M. Weischedel"
    ],
    "book": "Workshop on Strategic Computing – Natural Language",
    "id": "acl-H86-1017",
    "title": "Living Up to Expectations: Computing Expert Responses",
    "url": "https://aclweb.org/anthology/H86-1017",
    "year": 1986
  },
  "references": [
    "acl-P84-1029",
    "acl-P84-1030"
  ],
  "sections": [
    {
      "heading": "ABSTRACT",
      "text": [
        "In cooperative man-machine interaction, it is necessary but not sufficient for a system to respond truthfully and informatively to a user's question.",
        "In particular, if the system has reason to believe that its planned response might mislead the user, then it must block that conclusion by modifying its response.",
        "This paper focusses on identifying and avoiding potentially misleading responses by acknowledging types of •informing behavior• usually expected of an expert.",
        "We attempt to give a formal account of several - types of assertions that should be included in response to questions concerning the achievement of some goal (in addition to the simple answer), lest the questioner otherwise be misled."
      ]
    },
    {
      "heading": "1. Introduction]",
      "text": [
        "In cooperative man-machine interaction, it is necessary but not sufficient for a system to respond truthfully and informatively to a user's question.",
        "In particular, if the system has reason to believe that its planned response might mislead the user to draw a false conclusion, then it must block that conclusion by modifying or adding to its response.",
        "Such cooperative behavior was investigated in L51, in which a modification of Grice's Maxim of Quality - •Be truthful' - is proposed: If you, the speaker, plan to say anything which may imply for the hearer something that you believe to be false, then provide further information to block it.",
        "This behavior was studied in the context of interpreting certain definite noun phrases.",
        "In this paper, we investigate this revised principle as applied to responding to users' plan-related questions.",
        "Our overall aim is to:",
        "1. characterize tractable cases in which the system as respondent (R) can anticipate the possibility of the user/questioner (Q) drawing false conclusions from its response and hence alter it so as to prevent this happening; 2. develop a formal method for computing the projected inferences that Q may draw from a",
        "particular response, identifying those factors whose presence or absence catalyzes the inferences; 3. enable the system to generate modifications of its response that can defuse possible false inferences and that may provide additional useful information as well.",
        "In responding to any question, including those related to plans, a respondent (R) must conform to Grice's first Maxim of Quantity as well as the revised Maxim of Quality stated above: Make your contribution as informative as is required (for the current purposes of the exchange).",
        "At best, if R's response is not so informative, it may be seen as uncooperative.",
        "At worst, it may end up violating the revised Maxim of Quality, causing Q to conclude something R either believes to be false or does not know to be true: the consequences could be dreadful.",
        "Our task is to characterize more precisely what this expected informativeness consists of.",
        "In question answering, there seem to be several quite different types of information, over and beyond the simple answer to a question, that are nevertheless expected.",
        "For example, 1.",
        "When a task-related question is posed to an expert (R), R is expected to provide additional information that he recognizes as necessary to the performance of the task, of which the questioner (Q) may be unaware.",
        "Such response behavior was discussed and implemented by Allen Ill in a system to simulate a train information booth attendant responding to requests for schedule and track information.",
        "In this case, not providing the expected additional information is simply uncooperative: Q won't conclude the train doesn't depart at an if fails to volunteer one.",
        "2 respect to discussions and/or arguments, a speaker contradicting another is expected to supp,it his contrary contention.",
        "Again, failing to provide support would simply be viewed as uncoorerative [2,31.",
        "3.",
        "With respect to an expert's responses to questions, if Q expects that R would inform him of P 'f P were true, then Q may interpret R's silence regarding P as implying P is not true.3 Thus if R kflows P to be true, his silence may lead to Q's being misled.",
        "This third type of expected informativeness is the basis for the potentially misleading responses that we are trying to avoid and that constitute the subject of this paper.",
        "What is of interest to us is characterizing the Ps that Q would expect an expert R to inform him of, if they hold.",
        "Notice that these Ps differ from script-based expectations [81, which are based on what is taken to be the ordinary course of events in a situation.",
        "In describing such a situation, if the speaker doesn't explicitly reference some element P of the script, the listener simply assumes it is true.",
        "On the other hand, the Ps of interest here are based on normal cooperative discourse behavior, as set out in Grice's maxims.",
        "If the speaker doesn't make explicit some information P that the listener believes he would possess and inform the listener of, the listener assumes it is false.",
        "In this paper, we attempt to give a formal account of a subclass of Ps that should be included (in addition to the simple answer) in response to questions involving Q's achieving some goal4 - e.g., • Can I",
        "drop CIS577?•, •I want to enrol in CIS577?•, 'How do I get to Marsh Creek on the Expressway?•, etc., lest that response otherwise mislead Q.",
        "In this endeavor, our first step is to specify that knowledge that an expert R must have in order to identify the Ps that Q would expect to be informed of, in response to his question.",
        "Our second step is to formalize that knowledge and show how the system can use it.",
        "Our third step is to show how the system can modify its planned response so as to convey those Ps.",
        "In this paper, Section 2 addresses the first step of this process and Sections 3 and 4 address the second.",
        "The third step we mention here only in passing."
      ]
    },
    {
      "heading": "2. Factors in Computing Likely Informing Behavior]",
      "text": [
        "Before discussing the factors involved in computing this desired system behavior, we want to call attention to the distinction we are drawing between actions and events, and between the stated goal of a question and its intended goal.",
        "We limit the term action to things that Q has some control over.",
        "Things beyond Q's control we will call events, even if performed by other agents.",
        "While events may be likely or even necessary, Q and R nevertheless can do nothing more than wait for them to happen.",
        "This distinction between actions and events shows up in R's response behavior: if an action is needed, R can suggest that Q perform it.",
        "If an event is, R can do no more than inform Q.",
        "Our second distinction is between the stated goal or •S-goal• of a request and its intended goal or •I-goal•.",
        "The former is the goal most directly associated with Q's request, beyond that Q know the information.",
        "That is, we take the S-goal of a request to be the goal directly achieved by using the information.",
        "Underlying the stated goal of a request though may be another goal that the speaker wants to achieve.",
        "This intended goal or •I-goal° may be related to the S-goal of the request in any of a number of ways:",
        "• The I-goal may be the same as the S-goal.",
        "• The I-goal may be more abstract than the S-goal, which addresses only part of the I-goal.",
        "(This is the standard goal/sub-goal relation found in hierarchical planning [141.)",
        "For example, Q's S-goal may be to delete some files (e.g., 'How can I delete all but the last version of FOO.MSS?•), while his I-goal may be to bring his file usage under quota.",
        "This more abstract goal may also involve archiving some other files, moving some into another person's directory, etc.",
        "• The S-goai may be an enabling condition for the I-goal.",
        "For example, Q's S-goal may be to get read/write access to a file, while his I-goal may be to alter it.",
        "• The I-goal may be more general than the S-goal.",
        "For example, Q's S-goal may be to know how to repeat a control-N, while his I-goal may be to know how to effect multiple sequential instances of a control character.",
        "• Conversely, the I-goal may be more specific than the S-goal - for example, Q's S-goal may be to know how to send files to someone on another machine, while his /-goal is just to send a particular file to a local network user, which may allow for a specialized procedure.",
        "Inferring the I-goal corresponding to an S-goal is an active area of research 11, Carberry&3, 10, 111.",
        "We assume for the purposes of this paper that R can successfully do so.",
        "One problem is that the relationship that Q believes to hold between his S-goal and his I-goal may not actually hold: for example, the S-goal",
        "may not fulfill part of the I-goal, or it may not instantiate it, or it may not be a precondition for it.",
        "In fact, the S-goal may not even be possible to effect!",
        "This failure, under the rubric •relaxing the appropriate-query assumption•, is discussed in more detail in 110, 111.",
        "It is also reason for augmenting R's response with appropriate Ps, as we note informally in this section and more formally in the next.",
        "Having drawn these distinctions, we now claim that in order for the system to compute both a direct answer to Q's request and such Ps as he would expect to be informed of, were they true, the system must be able to draw upon knowledge/beliefs about",
        "• the events or actions, if any, that can bring about a goal • their enabling conditions • the likelihood of an event occuring or the enabling conditions for an action holding, with respect to a state • ways of evaluating methods of achieving goals - for example, with respect to simplicity, other consequences (side effects), likelihood of success, etc.",
        "• general characteristics of cooperative expert behavior",
        "The roles played by these different types of knowledge (as well as specific examples of them) are well illustrated in the next section."
      ]
    },
    {
      "heading": "3. Formalizing Knowledge for Expert Response",
      "text": [
        "In this section we give examples of how a formal model of user beliefs about cooperative expert behavior can be used to avoid misleading responses to task-related questions - in particular, what is a very representative set of questions, those of the form •How do I do X?•.",
        "Although we use logic for the model because it is clear and precise, we are not proposing theorem proving as the means of computing cooperative behavior.",
        "In Section 4 we suggest a computational mechanism.",
        "The examples are from a domain of advising students and involve responding to the request •I want to drop CIS577'.",
        "The set of individuals includes not only students, instructors, courses, etc.",
        "but also states.",
        "Since events and actions change states, we represent them as (possibly parameterized) functions from states to states.",
        "All terms corresponding to events or actions will be underlined.",
        "For these examples, the convenient:",
        "the user the expert the current state of the student R believes proposition P R believes that Q believes P event/action e can apply in state S a is a likely event/action in state S P, a proposition, is true in S x wants P to be true following notation is To encode the preconditions and consequences of performing an action, we adopt an axiomatization of STRIPS operators due to rhester83, 7, 151.",
        "The preconditions on an action being applicable are encoded using •holds• and •admissible• (essentially defining eadmissible•).",
        "Namely, if cl, cn are preconditions on an action a,",
        "In particular, we can state the preconditions and consequences of dropping CIS577.",
        "(h acd n are variables, while C stands for CIS577.)",
        "Of course, this only partially solves the frame problem, since there will be implications of pl, pm in general.",
        "For instance, it is likely that one might have an axiom stating that one receives a grade in a course only if the individual is enrolled in the course.",
        "What we claim is: (1) R must give a truthful response addressing at least Q's S-goal; (2) in addition, R may have to provide information in order not to mislead Q; and (3) R may give additional information to be cooperative in other ways.",
        "In the subsections below, we enumerate the cases that R must check in effecting (2).",
        "In each case, we give both a formal representation of the additional information to be conveyed and a possible English gloss.",
        "In that gloss, the part addressing Q's S-goal will appear in normal type, while the additional information will be underlined.",
        "For each case, we give two formulae: a statement of R's beliefs about the current situation and an axiom stating R's beliefs about Q's expectations.",
        "Formulae of the first type have the form RB(P).",
        "Formulae of the second type relate such beliefs to performing an informing action.",
        "They involve a statement of the form"
      ]
    },
    {
      "heading": "RE(P) likely(i, Sc),",
      "text": [
        "where i is an informing act.",
        "For example, if R believes there is a better way to achieve Q's goal, R is likely to inform Q of that better way.",
        "Since it is assumed that Q has this belief, we have QB( RB[PJ likely(i, Se) ).",
        "where we can equate \"Q believes i is likely• with •Q expects i.• Since R has no direct access to Q's beliefs, this must be embedded in R's model of Q's belief space.",
        "Therefore, the axioms have the form (modulo quantifier placement)"
      ]
    },
    {
      "heading": "RBQB( RBIPI likely(i, Sc) ).",
      "text": [
        "An informing act is meant to serve as a command to a natural language generator which selects appropriate lexical items, phrasing, etc.",
        "for a natural language utterance.",
        "Such an act has the form inform-that(R,Q,P) R informs Q that P is true."
      ]
    },
    {
      "heading": "3.1. Failure of enabling conditions",
      "text": [
        "Suppose that it is past the November 15th deadline or that the official records don't show Q enrolled in CIS577.",
        "Then the enabling conditions for dropping it are not met.",
        "That is, R believes Q's S-goal cannot be achieved from Sc."
      ]
    },
    {
      "heading": "3.1.1. A way",
      "text": [
        "If R knows another action b that would achieve Q's goals (cf. formula PI), Q would expect to be informed about it.",
        "If not so informed, Q may mistakenly conclude that there is no other way.",
        "Formula PI states this belief that R has about Q's expectations.",
        "R's full response is therefore •You can't drop 577; you can b.' For instance, b could be changing status to auditor, which may be performed until December 1.",
        "3.1.2.",
        "No way If R doesn't know of any action or event that could achieve Q's goal (cf. 141), Q would expect to be so informed.",
        "Formula 151 states this belief about Q's expectations.",
        "To say only that Q cannot drop the course does not exhibit expert cooperative behavior, since Q would be uncertain as to whether R had considered other alternatives.",
        "Therefore, R's full response is •You can't drop 577; there isn't anything you can do to prevent failing.• Notice that R's analysis of the situation may turn up additional information which a cooperative expert",
        "could provide that does not involve avoiding misleading Q.",
        "For instance, R could indicate enabling conditions that prevent there being a solution: suppose the request to drop the course is made after the November 15th deadline.",
        "Then R would believe the following, in addition to (1) RB(holds(enrolled(Q,C,fall),Sc) & holds(date(Sc)>Nov15,Sc)) More generally, we need a schema such as the following about Q's beliefs:",
        "In this case the response should be 'You can't drop 577; Pi isn't true.• Alternatively, the language generator might paraphrase the whole response as, •if Pi were true, you could drop.• Of course there are potentially many ways to try to achieve a goal: by a single action, by a single event, or by an event and an action, ...",
        "In fact, the search for a sequence of events or actions that would achieve the goal may consider many alternatives.",
        "If all fail, it is far from obvious which blocked condition to notify Q of, and knowledge is needed to guide the choice.",
        "Some heuristics for dealing with that problem :s given in 1121."
      ]
    },
    {
      "heading": "3.2. An nonprciuetive act",
      "text": [
        "Suppose the proposed action does not achieve Q's I-goal, cf. 161.",
        "For example, dropping the course may still mean that failing status would be recorded as a WF (withdrawal while failing).",
        "R may initially plan to answer •You can drop 577 by ...•.",
        "However, Q would expect to be told that his proposed action does not achieve his I-goal.",
        "Formula 171 states R's belief about this expectation.",
        "R's full response is, •You can drop 577 by ....",
        "However, you will still fail.• Furthermore, given the reasoning in section 3.1.1 above, R's full response would also inform Q if there is an action b that the user can take instead."
      ]
    },
    {
      "heading": "3.3. A better way",
      "text": [
        "Suppose R believes that there is a better way to achieve Q's I-goal, cf. 181 - for example, taking an incomplete to have additional time to perform the work, and thereby not losing all the effort Q has already expended.",
        "Q would expect that R, as a cooperative expert, would inform him of such a better way, cf. (9g.",
        "If R doesn't, R risks misleading Q that there isn't one.",
        "Notice that if R doesn't explicitly tell Q that he is presenting a better way (i.e., he just presents the method), Q may be misled that the response addresses his S-goal: i.e., he may falsely conclude that he is being told how to drop the course.",
        "(The possibility shows up clearer in other examples - e.g., if R omits the first sentence of the response below Q: How do I get to Marsh Creek on the Expressway?",
        "R: It's faster and shorter to take Route 30.",
        "Go out Lancaster Ave until....",
        "Thus even when adhering to expert response behavior in terms of addressing an I-goal, we must keep the system aware of potentially misleading aspects of its modified response as well.",
        "Note that R may believe that Q expects to be told the best way.",
        "This would change the second axiom to include within the scope of the existential quantifier (Va){-.",
        "(a=b) (holds(-efail(Q,C), a(Sc)) & admissible(a(Sc)) & better(b,a)1)"
      ]
    },
    {
      "heading": "3.4. The only way",
      "text": [
        "Suppose there is nothing inconsistent about what the user has proposed - i.e., all preconditions are met and it will achieve the user's goal.",
        "R's direct response would simply be to tell Q how.",
        "However, if R notices that that is the only way to achieve the goal (cf. 1101), it could optionally notify Q of that, cf. 1111.",
        "1101 RB((3!a)lholds(-‘fail(Q,C),a(Sc)) & admissible(a(Sc)) & a=drop(Q,C)(SeA)",
        "R's full response is •You can drop 577 by .... That is the only way to prevent failing.•"
      ]
    },
    {
      "heading": "3.5. Something Turning Up",
      "text": [
        "Suppose there is no appropriate action that Q can take to achieve his I-goal.",
        "That is, RB( -1(3 a)[admissible(a(Sc)) & holds(g, a(Sc))]) There may still be some event e out of Q's control that could bring about the intended goal.",
        "This gives several more cases of R's modifying his response."
      ]
    },
    {
      "heading": "3.5.1. Unlikely event",
      "text": [
        "If e is unlikely to occur (cf. 1121), Q would expect R to inform him of e, while noting its implausibility, cf. 1131 1121 RB((3e)ladmissible(e(Sc)) & holds(-fai1(Q,C), e(Sc)) & Sc)))",
        "Thus R's full response is, •You can't drop 577.",
        "If e occurs, you will not fail 577, but e is unlikely.•"
      ]
    },
    {
      "heading": "3.6.2. Likely event",
      "text": [
        "If the event e is likely (cf. 1141), it does not seem necessary to state it, but it is certainly safe to do so.",
        "A formula representing this case follows.",
        "1141 RB(Pelladmissible(qSc)) & holds(--ifail(Q,C),e(Sc)) & likely(e,Sc)1) R's beliefs about Q's expectations are the same as the previous case except that likety(e, Sc) replaces -dikely(e, Sc).",
        "Thus R's full response may be •You can't drop 577.",
        "However, e is likely to occur, in which case you will not fail 577.•",
        "If event e brings about a state in which the enabling conditions of an effective action a are true, cf. 1151",
        "then the same principles about informing Q of the likelihood or unlikelihood of e apply as they did before.",
        "In addition, R must inform Q of a, cf. 1161.",
        "Thus R's full response would be •You can't drop 577.",
        "If e were to occur, which is (unftikely, you could a and thus not fail 577.•"
      ]
    },
    {
      "heading": "4. Reasoning",
      "text": [
        "Our intent in using logic has been to have a precise representation language whose syntax informs R's reasoning about Q's beliefs.",
        "Having computed a full response that conforms to all these expectations, R may go on to 'trim' it according to principles of brevity that we do not discuss here.",
        "Our proposal is that the informing behavior is •pre-compiled•.",
        "That is, R does not reason explicitly about Q's expectations, but rather has compiled the conditions into a case analysis similar to a discrimination net.",
        "For instance, we can represent informally several of the cases in section 3.",
        "If admissible(drop(Q,CASc)) then If -holds(-4ail(Q,C),drop(Q,CASc)) then begin nonproductive act if (3b)(admissible(b(Sc)) & holds(-4:.il(Q,C),b(Sc))1",
        "holds(-'fail(Q,C),b(Sc)) & better(b,111 then a better wa else If (3 b)(admissible(b(Sc)) & holds(-afail(Q,C), b(Sc))1 then a way else no wax.",
        "Note that we are assuming that R assumes the most demanding expectations by Q.",
        "Therefore, R can reason solely within its own space without missing things."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "Since the behavior of expert systems will be interpreted in terms of the behavior users expect of cooperative human experts, we (as system designers) must understand such behavior patterns so as to implement them in our systems.",
        "If such systems are to be truly cooperative, it is not sufficient for them to be simply truthful.",
        "Additionally, they must be able to predict limited classes of false inferences that users might draw from dialogue with them and also to respond in a way to prevent those false inferences.",
        "The current enterprise is a small but non-trivial step in this direction.",
        "In addition to questions about achieving goals, we are investigating other cases where a cooperative expert should prevent false inferences by another agent, including preventing inappropriate default reasoning [6, JWW84nonmonJ.",
        "Future work should include",
        "• identification of additional cases where an expert must prevent false inferences by another agent, • formal statement of a general principle for constaining the search for possible false inferences, and • design of a natural language planning component to carry out the informing acts assumed in this paper."
      ]
    },
    {
      "heading": "ACKNOWLEDGEMENTS",
      "text": [
        "We would like to thank Martha Pollack, Deborah Dahl, Julia Hirschberg, Kathy McCoy and the AAAI program committee reviewers for their comments on this paper."
      ]
    }
  ]
}
