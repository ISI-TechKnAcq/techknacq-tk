{
  "info": {
    "authors": [
      "Thuy Dung Nguyen",
      "Minh-Thang Luong"
    ],
    "book": "Workshop on Semantic Evaluations (SemEval)",
    "id": "acl-S10-1035",
    "title": "WINGNUS: Keyphrase Extraction Utilizing Document Logical Structure",
    "url": "https://aclweb.org/anthology/S10-1035",
    "year": 2010
  },
  "references": [
    "acl-W09-2902"
  ],
  "sections": [
    {
      "text": [
        "WINGNUS: Key phrase Extraction Utilizing Document Logical Structure",
        "We present a system description of the WINGNUS team work for the SemEval-2010 task #5 Automatic Keyphrase Extraction from Scientific Articles.",
        "A key feature of our system is that it utilizes an inferred document logical structure in our candidate identification process, to limit the number of phrases in the candidate list, while maintaining its coverage of important phrases.",
        "Our top performing system achieves an Fi of 25.22% for the combined keyphrases (author and reader assigned) in the final test data.",
        "We note that the method we report here is novel and orthogonal from other systems, so it can be combined with other techniques to potentially achieve higher performance."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Keyphrases are noun phrases (NPs) that capture the primary topics of a document.",
        "While beneficial for applications such as summarization, clustering and indexing, only a minority of documents have manually-assigned keyphrases, as it is a time-consuming process.",
        "Automatic keyphrase generation is thus a focus for many researchers.",
        "Most existing keyphrase extraction systems view this task as a supervised classification task in two stages: generating a list of candidates - candidate identification; and using answer keyphrases to distinguish true keyphrases - candidate selection.",
        "The selection model uses a set of features that capture the saliency of a phrase as a keyphrase.",
        "A major challenge of the keyphrase extraction task lies in the candidate identification process.",
        "A narrow candidate list will overlook some true keyphrases (favoring precision), whereas a broad list will produce more errors and require more processing in latter selection stage (favoring recall).",
        "In our previous system (Nguyen and Kan, 2007), we made use of the document logical structure in the proposed features.",
        "The premise of this earlier work was that keyphrases are distributed non-uniformly in different logical sections of a paper, favoring sections such as introduction, and related work.",
        "We introduced features indicating which sections a candidate occurrs in.",
        "For our fielded system in this task (Kim et al., 2010), we further leverage the document logical structure for both candidate identification and selection stages.",
        "Our contributions are as follows: 1) We suggest the use of Google Scholar-based crawler to automatically find PDF files to enhance logical structure extraction; 2) We provide a keyphrase distribution study with respect to different logical structures; and 3) From the study result, we propose a candidate identification approach that uses logical structures to effectively limit the number of candidates considered while ensuring good coverage."
      ]
    },
    {
      "heading": "2. Preprocessing",
      "text": [
        "Although we have plain text for all test input, we posit that logical structure recovery is much more robust given the original richly-formatted document {e.g., PDF), as font and formatting information can be used for detection.",
        "As a bridge between plain text data provided by the organizer and PDF input required to extract formatting features, we first describe our Google Scholar-based crawler to find PDFs given plain texts.",
        "We then detail on the logical structure extraction process.",
        "Google Scholar-based Paper Crawler",
        "Our crawler takes inputs as titles to query Google Scholar (GS) by means of web scraping.",
        "It processes GS results and performs approximate title matching using character-based Longest Common Subsequence similarity.",
        "Once a matching title with high similarity score (> 0.7 experimentally) is found, the crawler retrieves the list of available PDFs, and starts downloading until one is correctly stored.",
        "We enforce that PDFs accepted should have the OCR texts closely match the provided plain texts in terms of lines and tokens.",
        "In the keyphrase task, we approximate the title inputs to our crawler by considering the first two lines of each plain text provided.",
        "For 140 train and 100 test input documents, the crawler downloaded 117 and 80 PDFs, of which 116 and 76 files are correct, respectively.",
        "This yields an acceptable level of performance in terms of (Precision, Recall) of (99.15%, 82.86%) for train and (95%, 76%) for test data.",
        "Logical Structure Extraction",
        "Logical structure is defined as \"a hierarchy of logical components, for example, titles, authors, affiliations, abstracts, sections, etc.\"",
        "in (Mao et al., 2003).",
        "Operationalizing this definition, we employ an in-house software, called SectLabel (Lu-ong et al., to appear), to obtain comprehensive logical structure information for each document.",
        "SectLabel classifies each text line in a scholarly document with a semantic class (e.g., title, header, bodyText).",
        "Header lines are furthered classified into generic roles {e.g., abstract, intro, method).",
        "A prominent feature of SectLabel is that it is capable of utilizing rich information, such as font format and spatial layout, from an optical character recognition (OCR) output if PDF files are present.",
        "In case PDFs are unavailable, SectLabel still handles plain text based logical structure discovery, but with degraded performance."
      ]
    },
    {
      "heading": "3. Candidate Phrase Identification",
      "text": [
        "Phrase Distribution Study",
        "We perform a study of keyphrase distribution on the training data over different logical structures (LSs) to understand the importance of each section within documents.",
        "These LSs include: title, headers, abstract, introduction (intro), related work (rw), conclusion, and body text (body).",
        "We make a key observation that within a paragraph, important phrases occur mostly in the first n sentences.",
        "To validate our hypothesis, we consider keyphrase distribution over bodyn, which is the subset of all of the body LS, limited to the first n sentences of each paragraph (n = 1,2, 3 experimentally).",
        "Table 1 : Keyphrase distribution over different logical structures computed from the 144 training documents.",
        "The type counts of author-assigned (ath), reader-assigned (rder) and combined (comb) keyphrases are shown.",
        "Sent indicates the number of sentences in each LS.",
        "The Den column gives the density of keyphrases for each LS.",
        "Results in Table 1 show that individual LSs (title, headers, abstract, intro, rw, concl) contain a high concentration (i.e., density > 0.2) of keyphrases, with title and abstract having the highest density, and intro being the most dominant LS in terms of keyphrase count.",
        "With all these LSs and body, we obtain the full setting, covering 1994/2059=96.84% of all keyphrases appearing in the original text, fulltext, while effectively reducing the number of processed sentences by more than two-thirds.",
        "Considering only the first sentence of each paragraph in the body text, bodyi, yields fair keyphrase coverage of 1035/1411=73.35% relative to that of fulltext.",
        "The number of lines to be processed is much smaller, about a third, which validates our aforementioned hypothesis.",
        "Keyphrase Extraction",
        "Results from the keyphrase distribution study motivates us to further explore the use of logical structures (LS).",
        "The idea is to limit the search scope of our candidate identification system while maintaining coverage.",
        "We propose a new ap-",
        "caption, footnote, and reference lines.",
        "Ath",
        "Rder",
        "Com",
        "Sent",
        "Den",
        "title",
        "142",
        "175",
        "251",
        "122",
        "2.06",
        "headers",
        "158",
        "342",
        "425",
        "1,893",
        "0.22",
        "abstract",
        "276",
        "745",
        "897",
        "1,124",
        "0.80",
        "intro",
        "335",
        "984",
        "1,166",
        "4,338",
        "0.27",
        "rw",
        "160",
        "345",
        "443",
        "1,945",
        "0.23",
        "concl",
        "227",
        "488",
        "616",
        "1,869",
        "0.33",
        "body",
        "398",
        "1,175",
        "1,411",
        "39,179",
        "0.04",
        "full",
        "465",
        "1,720",
        "1,994",
        "50,512",
        "0.04",
        "bodyi",
        "333",
        "839",
        "1,035",
        "11,280",
        "0.09",
        "body2",
        "366",
        "980",
        "1,197",
        "20,024",
        "0.06",
        "body3",
        "382",
        "1,042",
        "1,269",
        "26,163",
        "0.05",
        "fulltext",
        "480",
        "1,773",
        "2,059",
        "166,471",
        "0.01",
        "proach, which extracts candidates according to the regular expression rules discussed in (Kim and Kan, 2009).",
        "However, instead of using the whole document text as input, we abridge the input text at different levels from full to minimal.",
        "Table 2: Different levels of abridged inputs computed on the training data.",
        "Cand shows the number of candidate keyphrases extracted for each input type; Com gives the number of correct keyphrases appear as candidates; Recall is computed with respect to the total number of keyphrases in the original texts (2059).",
        "Results in Table 2 show that we could gather a recall of 63.72% when considering a significantly abridged form of the input culled from title, headers, abstract (abs) and introduction (intro) - minimal.",
        "Further adding related work (rw) and conclusion - medium - enhances the recall by 4.95%.",
        "When adding only the first line of each paragraph in the body text, we achieve a good recall of 76.74% while effectively reducing the number of candidate phrases to be process by a half with respect to the fulltext input.",
        "Even though fulhjulh, and full show further improvements in terms of recall, we opt to mtfulh in our experimental runs, which trades off recall for less computational complexity, which may influence downstream classification."
      ]
    },
    {
      "heading": "4. Candidate Phrase Selection",
      "text": [
        "Following (Nguyen and Kan, 2007), we use the Naive Bayes model implemented in Weka (Hall et al., 2009) for candidate phrase selection.",
        "As different learning models have been discussed much previous work, we just list the different features with which we experimented with.",
        "Our featuresare as follows (where n indicates a numeric feature; b, a boolean one):",
        "F1-F3 (n): TFxIDF, term frequency, term frequency of substrings.",
        "F4-F5 (n): First and last occurrences (word offset).",
        "F6 (n): Length of phrases in words.",
        "F7 (b): Typeface attribute (available when PDF is present) - Indicates if any part of the candidate phrase has appeared in the document with bold or italic format, a good hint for its relevance as a keyphrase.",
        "F8 (b): InTitle - shows whether a phrase is also part of the document title.",
        "F9 (n): TitleOverlap - the number of times a phrase appears in the title of other scholarly documents (obtained from a dump of the DBLP database).",
        "F10-F14 (b): Header, Abstract, Intro, RW, Concl - indicate whether a phrase appears in headers, abstract, introduction, related work or conclusion sections, respectively.",
        "F15-F19 (n): HeaderF, AbstractF, IntroF, RWF, ConclF - indicate the frequency of a phrase in the headers, abstract, introduction, related work or conclusion sections, respectively."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "For this task (Kim et al., 2010), we are given two datasets: train (144 docs) and test (100 docs) with detailed answers for train.",
        "To tune our system, we split the train dataset into train and validation subsets: traint (104 docs) and trainv (40 docs).",
        "Once the best setting is derived from traint-trainv, we obtain the final model trained on the full data, and apply it to the test set for the final results.",
        "Our evaluation process is accomplished in two stages: we first experiment different feature combinations by using the input types fulltext and full\\.",
        "We then fix the best feature set, and vary our different abridged inputs to find the optimal one.",
        "Feature Combination",
        "To evaluate the performance of individual features, we define a base feature set, as Fi;4, and measure the performance of each feature added separately to the base.",
        "Results in Table 3 have highlighted the set of positive features, which is F3 5)6)i3 i6.",
        "From the positive set Fs^, 13,16, we tried different combinations for the two input types shown System F Score System F Score in Table 4.",
        "The results indicate that while fulltext obtains the best performance withF3 6,5 added, usingshows superior performance at 28.18% F Score with F3;6 added.",
        "Hence, we have identified our best feature set as F13 4 6.",
        "Input",
        "Description",
        "Cand",
        "Com",
        "Recall",
        "minimal",
        "title + headers + abs + intro",
        "30,702",
        "1,312",
        "63.72%",
        "medium",
        "min + rw + conclusion",
        "44,975",
        "1,414",
        "68.67%",
        "fulli",
        "med+bodyi",
        "73,958",
        "1,580",
        "76.74%",
        "fulla",
        "med+body2",
        "90,624",
        "1,635",
        "79.41%",
        "full3",
        "med + body3",
        "101,006",
        "1,672",
        "81.20%",
        "full",
        "med+body",
        "121,378",
        "1,737",
        "84.36%",
        "fulltext",
        "original text",
        "148,411",
        "1,766",
        "85.77%",
        "Abridged Inputs",
        "Table 5 gives the performance for the abridged inputs we tried with the best feature set F13 4 6.",
        "All/Mi, fulh, fulh and full show improved performance compared to those on the fulltext.",
        "We achieve our best performance with/Mi at 28.18% F Score.",
        "These results validate the effectiveness of our approach in utilizing logical structure for the candidate identification.",
        "We report our results submitted in Table 6.",
        "These figures are achieved using the best feature combination Fi;3;4;6."
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "We have described and evaluated our keyphrase extraction system for the SemEval-2 Task #5.",
        "With the use of logical structure in the candidate identification, our system has demonstrated its superior performance over systems that do not use such information.",
        "Moreover, we have effectively reduced the numbers of text lines and candidate",
        "Table 5: Performance over different abridged inputs using the best feature set Fi;3;4;6.",
        "\"@N\" indicates the number of top N keyphrase matches.",
        "Table 6: Final results on the test data.",
        "phrases to be processed in the candidate identification and selection respectively by about half.",
        "Our system takes advantage of the logical structure analysis but not to the extent we had hoped.",
        "We had hypothesized that formatting features (F7) such as bold and italics, would help discriminate key phrases, but in our limited experiments for this task did not validate this.",
        "Similarly, external knowledge should help in the keyphrase task, but the prior knowledge about keyphrase likelihood (Fg) in DBLP hurt performance in our tests.",
        "We plan to further explore these issues for the future.",
        "Input",
        "@5",
        "@10",
        "@15",
        "Fscore",
        "min",
        "62",
        "110",
        "145",
        "23.75%",
        "med",
        "79",
        "130",
        "158",
        "25.88%",
        "fulh",
        "84",
        "135",
        "172",
        "28.18%",
        "full2",
        "90",
        "132",
        "164",
        "26.86%",
        "full3",
        "89",
        "134",
        "162",
        "26.54%",
        "full",
        "84",
        "130",
        "164",
        "26.86%",
        "fulltext",
        "82",
        "127",
        "158",
        "25.88%",
        "base",
        "23.42%",
        "+ Fn",
        "23.42%",
        "+ F2",
        "21.13%",
        "+ F12",
        "23.42%",
        "+ F3",
        "24.57%",
        "+ F13",
        "23.75%",
        "+ F5",
        "24.08%",
        "+ F14",
        "22.28%",
        "+ F6",
        "25.06%",
        "+ F15",
        "22.11%",
        "+ F7",
        "23.42%",
        "+ F16",
        "23.59%",
        "+ F8",
        "22.77%",
        "+ F17",
        "22.60%",
        "+ Fg",
        "22.28%",
        "+ F18",
        "23.26%",
        "+ F10",
        "23.42%",
        "+ F19",
        "21.95%",
        "fulltext",
        "fulli",
        "base (Fi;4)",
        "23.42%",
        "22.60%",
        "+ F3;6",
        "25.88%",
        "28.18%",
        "+ F3;6,5",
        "26.21%",
        "26.21%",
        "+ F3;6,5,13",
        "24.90%",
        "26.21%",
        "+ F3;6,5,16",
        "24.24%",
        "26.70%",
        "+ F3;6,5,13,16",
        "23.42%",
        "26.70%"
      ]
    }
  ]
}
