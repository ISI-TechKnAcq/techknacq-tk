{
  "info": {
    "authors": [
      "Hua-Ping Zhang",
      "Qun Liu",
      "Xueqi Cheng",
      "Hao Zhang",
      "Hong-Kui Yu"
    ],
    "book": "SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-W03-1709",
    "title": "Chinese Lexical Analysis Using Hierarchical Hidden Markov Model",
    "url": "https://aclweb.org/anthology/W03-1709",
    "year": 2003
  },
  "references": [
    "acl-C02-1012",
    "acl-C02-1080",
    "acl-J00-3004",
    "acl-P97-1041",
    "acl-W02-1808",
    "acl-W02-1815",
    "acl-W02-1817"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents a unified approach for Chinese lexical analysis using hierarchical hidden Markov model (HHMM), which aims to incorporate Chinese word segmentation, Part-Of-Speech tagging, disambiguation and unknown words recognition into a whole theoretical frame.",
        "A class-based HMM is applied in word segmentation, and in this level unknown words are treated in the same way as common words listed in the lexicon.",
        "Unknown words are recognized with reliability in role-based HMM.",
        "As for disambiguation, the authors bring forth an n-shortest-path strategy that, in the early stage, reserves top N segmentation results as candidates and covers more ambiguity.",
        "Various experiments show that each level in HHMM contributes to lexical analysis.",
        "An HHMM-based system ICTCLAS was accomplished.",
        "The recent official evaluation indicates that ICTCLAS is one of the best Chinese lexical analyzers.",
        "In a word, HHMM is effective to Chinese lexical analysis."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Word is the independent and meaningful atom in natural language.",
        "Unlike English and Spanish, there is no delimiter to mark word boundaries and no explicit definition of words in some Asian languages.",
        "As for Chinese language processing, the fundamental task is word segmentation, which transforms Chinese character string into words sequence.",
        "It is prerequisite to POS tagger, parser and other deep processing, and the lexical result is the basis of further applications such as machine translation, information retrieval and information extraction.",
        "Since the first system CDWS appeared in 1983, word segmentation has been researched intensively.",
        "Many solutions were proposed and could be broadly categorized into rules-based approaches that make use of linguistic knowledge and statistical approaches that train on corpus after machine learning.",
        "The classic rule-based approaches include maximum matching and shortest path (SP), which achieve the minimum number of segmented words.",
        "Zhang and Liu (2002) present an extended SP algorithm named \"n-shortest paths\".",
        "Some researchers introduce more complicated rules, such as error-driven learning (Hockenmaier and Brew, 1998) and parsing (Wu and Jiang, 1998).",
        "Rule is the only feasible way to segment words unless necessary resources such as large amount of corpus are available.",
        "With the development of hand-corrected resource, statistical approaches became more popular.",
        "The language models commonly applied are n-gram (Zhang and Liu, 2002; Gao et al., 2001), EM (Peng and Schuurmans, 2001), and channel noise model.",
        "As far as we know, however, there is yet neither purely rule-based system nor purely statistical one.",
        "It tends to tackle Chinese lexical problem with mixture of rules and statistical information.",
        "On one hand, trainable rules (Palmer, D. 1997) seem more adaptive and efficient in that rule-based approaches benefit from frequency of rule occurrence, on the other hand, statistical solutions employ rules to detect ambiguity, numeric expression, time and other named entities.",
        "Apart from the above approaches, we also notice some other promising ideas such as compression-based (Teahan et al., 2001), classifier-based (Xue and Susan, 2002) and self-supervised segmentation without lexicon.",
        "According to recent reports, word segmentation has achieved good result in precision, especially on texts that do not contain ambiguity or out-of-vocabulary words.",
        "However, segmentation ambiguity and unknown words1 cause bottlenecks and greatly degrade performance in word segmentation.",
        "Ambiguous or unknown string is hard to be correctly segmented; at the same time, it also influences on segmenting its neighboring words.",
        "What's worse, ambiguity often occurs with unknown words.",
        "Take \" A # VN X\" f ] fq M g M IA \"(Clinton said to Netanyahu) as exemplification, \" f ] fq M g ffl\"(Netanyahu) is unknown transliterated personal name, and both \" X\" f ] \" (�or home) and \"MIA\" (talk nonsense) has two ambiguous segmentations: split into halves or not.",
        "Here, it's difficult to identify unknown word\"f ]fq�g�\" because of the ambiguities, while disambiguation is also difficult to accomplish before unknown words detection.",
        "Therefore, the final lexical result is very likely to be\"A#VN�X\"f ]�fq�g��IA\" instead of \"A#VN�X\"�f ]fq�g��IA\".",
        "Historically, much effort has been made in the two sub-problems of word segmentation.",
        "Almost all previous solutions (Chunyu et al.",
        "2002; Zhang, 1998; Zheng, 1999) of disambiguation attempt to cover each possible case with trivial rules, while recently statistical approaches are applied in some special categories of ambiguity.",
        "For instance, vector space model was applied in combinational ambiguity (Luo et al.",
        "2002).",
        "Concerning unknown word, we only need focus on unknown named entities, including personal name (PER), location name (LOC), and organization name (ORG).",
        "The motivation in named entity recognition is to utilize its components and contexts.",
        "Like word segmentation and disambiguation, the usual approach is to apply rules (Sun, 1993;Tan, 1999;Luo and Ji, 2001; Luo and Song, 2001).",
        "Recognition rules are summarized on name libraries or different linguistic phenomena.",
        "Compared with rules-based approach, machine learning from large corpus seems easy but better in performance.",
        "The statistical approaches proposed recently include hidden Markov model (Zhang and Liu, 2002; Zhang et al.",
        "2002), agent-based (Ye, 2003), class-based trigram model (Sun et al., 2002).",
        "After nearly 20 years of hard work, rapid progresses are made on word segmentation, disambiguation and unknown word recognition research individually.",
        "To the best of our knowledge, however, all the achievement has not",
        "ever, all the achievement has not integrated into a unified model with a general theoretical basis.",
        "In previous lexical analyzers, so-called word segmentation algorithm actually only employs on common words listed in the lexicon, while disambiguation and unknown word recognition have their own independent mechanism and become distinct processes from segmentation.",
        "Without scientific quantification, unknown words and disambiguation result could not compete with other segmentation candidates.",
        "In a word, previous work lacks a whole frame incorporating the different subtasks in lexical analysis, while there is also no consistent mechanism to evaluate various lexical results Therefore, previous lexical system is difficult achieve better performance on real texts that contain irregular character strings mentioned above.",
        "This paper presents an HHMM-based approach for Chinese lexical analysis.",
        "It aims to utilize a general model to proceed all steps in lexical analysis, including word segmentation, disambiguation, unknown words recognition and part-of speech (POS) tagging.",
        "In the preprocessing, top n segmentation candidates covering the possible ambiguity are provided using n-shortest-path algorithm (Zhang and Liu, 2002).",
        "Then, simple unknown named entities like personal names and location names are identified on the candidate set using class-based HMM.",
        "Following that, a higher level of HMM could be employed on recognizing organization and other recursive named entity, which includes another simple unknown word.",
        "Unknown words recognized with credible probability are added to class-based HMM for word segmentation.",
        "In this level of HHMM, unknown words and ambiguity are treated in the same way as common words.",
        "POS tagging is the top level in HHMM.",
        "After HHMM- based approach applied, Chinese lexical analysis system ICTCLAS achieves well in segmentation and POS tagging.",
        "The official evaluation, which was held by the National Foundation of 973 Plan of China, shows that ICTCLAS rank top and it is one of the best Chinese lexical analyzers.",
        "The structure of this paper is as follows.",
        "The next section reviews HHMM and presents the framework of HHMM-based Chinese lexical analysis.",
        "Then we explain the class-based HMM for word segmentation.",
        "Next we detail role-based unknown words recognition and n-shortest-path disambiguation.",
        "The following section describes various experiments designed to evaluate lexical analysis performance and contribution from different level in HHMM.",
        "2 HHMM and Chinese lexical analysis",
        "3) Only the bottom HMM can observe the symbols.",
        "The corresponding symbol emission probabilities"
      ]
    },
    {
      "heading": "2.1 An overview of HHMM are B(qD )=(bk (qD ))) , where bk (qD)=P(ok IqD )",
      "text": [
        "Hidden Markov model (HMM, L.R.",
        "Rabiner, 1989) has become the method of choice for modeling stochastic processes and sequence in natural language processing, because HMM is very rich in mathematical structure and hence can form theoretical basis for use.",
        "However, compared with the sophisticated phenomena in natural language, traditional HMM seems hard to use due to the multiplicity of length scales and recursive nature of the sequences.",
        "Therefore Shai Fine et al. (1998) proposed hierarchical hidden Markov model, which is a recursive and generalized HMM.",
        "Based on Shai's work, we give a formal description of HHMM.",
        "An HHMM is specified by a six-tuple (S , O , II, A, B, D), where D is the depth of levels, S and O are the finite set of states and the final output alphabet or intermediate output, and II ,A and B are the probabilities of the initial state, state transitions and emissions of symbol or intermediate output, respectively.",
        "The contrast between traditional HMM and HHMM lies in: 1) The state set S can be classified into different subsets according to its level.",
        "A state in S is annotated with qd (0<d<D, 0<i<l Sd 1), where d is i the level index, i is the state index and Sd is the set of state in level d. When d=D, qd is called i terminal state because its observation is symbols, or else, it is called internal state whose observation is from its child HMM in (d+1)th level.",
        "2) Every internal state q d (0<d<D) has its child states, which form an independent HMM.",
        "In the child HMM, the state transitive probabilities are (qd)=(a..(qd)),and,a..(qd)=P(qd+1lqd+ ii ii J i And the initial distribution vector is like",
        "and a. is in symbol set.",
        "For the d (d<D) level HMM, state sequence in its child HMM could be viewed as its observation.",
        "The emission probabilities could be estimated as above.",
        "All in all, HHMM includes D levels of HMM while each level is independent HMM.",
        "Moreover, each HMM only links with its parent and child.",
        "The whole parameters set of HHMM is denoted by"
      ]
    },
    {
      "heading": "2.2 Framework of HHMM-based lexical analysis",
      "text": [
        "As illustrated in Figure 1, HHMM-based Chinese lexical analysis comprises five levels: atom segmentation, simple and recursive unknown words recognition, class-based segmentation and POS tagging.",
        "In the whole frame, class-based segmentation graph, which is a directed graph designed for word segmentation, is an essential intermediate data structure that links disambiguation, unknown words recognition with word segmentation and POS tagging.",
        "Atom segmentation, the bottom level of HHMM, is an initial step.",
        "Here, atom is defined to be the minimal segmentation unit that cannot be split in any stage.",
        "The atom consists of Chinese character, punctuation, symbol string, numeric expression and other non-Chinese char string.",
        "Any word is made up of an atom or more.",
        "Atom segmentation is to segment original text into atom sequence and it provides pure and simple source for its parent HMM.",
        "For instance, a sentence like \"2002.9, ICTCLAS (6, n Eh{ ,qfF44MMtli\" (The free source codes of ICTCLAS was distributed in September, 2002) would be segmented as atom sequence \"2002.9/, /ICTCLAS/(6,/n/Eh/{ ,/q/fF /44M/M/tli/\".",
        "In this HMM, the original symbol is",
        "Given the atom sequence A=(al ,... aj, let W=(wl ,... wm) be the words sequence, C= (cl ,... cm) be a corresponding class sequence of W, and W# be the choice of word segmentation with the maximized probability, respectively.",
        "Then, we could get:",
        "detail of operation in that it's a simple application on the basis of HMM.",
        "POS tagging using HMM is also skipped because role tagging, which presented in section 5, is similar to it in nature.",
        "The other levels of HHMM will be provided in the next parts.",
        "3.",
        "Weight on the directed edge is – logp (ci I ci-1); 4.",
        "\"Et4T,,\" (Mao Ze-Dong) is personal name outside the lexicon.The node \" Et4T,,/PER\" and the related edges with dash line is inserted after unknown words recognition.",
        "For convenience, we often use the negative log probability instead of the proper form.",
        "That is: M",
        "According to the word class definition, if wi is listed in lexicon, then ci is wi, and p(wiIci) is equal to 1.0.",
        "Otherwise, p(wiIci) is probability that class ci initially activates wi, and it could be estimated in its child HMM for unknown words recognition.",
        "As demonstrated in Figure 3, we provide the process of class-based word segmentation on \"Et 4T,, 1893*01\" (Mao Ze-Dong was born in the year of 1893).",
        "The significance of our method is: it covers the possible ambiguity.",
        "Moreover, unknown words, which are recognized in the following steps, can be added into the segmentation graph and proceeded as any other common words.",
        "After transformation through class-based HMM, word segmentation becomes single-source shortest paths problem.",
        "Hence the best choice Woof word segmentation is easy to find using Djikstra's algorithm."
      ]
    },
    {
      "heading": "4 NSP-based disambiguation strategy",
      "text": [
        "Segmentation ambiguous error is made mainly because of improper decision in the earlier stage.",
        "For example, overlapping ambiguity in \"ffiLf � /)A/ 3}'T/ffl\" (When combining into molecule) and combining ambiguity in \"9A_/n/1k/_T_/_L/�/ �\"(The person has naevi on his hand) are difficult to solve only in the initial stage of word segmentation.",
        "However, it's simple to find the correct result among the possible candidates in POS tagging or further processes.",
        "Therefore, the initial process should not make the final decision, but provide candidates covering the correct segmentation.",
        "We take n-shortest-path (NSP, Zhang and Liu, 2002) algorithm as the disambiguation strategy.",
        "NSP, which selects n shortest paths, is an extension of Djikstra's algorithm.",
        "The motivation in disambiguation using NSP is covering more ambiguity with top n results in rough segmentation, which is the initial step in lexical analysis and produces candidate results.",
        "Considering efficiency and performance, rough segmentation coverage, which is percentage of correct results, should be much higher while the average size of candidate set should be as small as possible.",
        "Compared with NSP, full segmentation, which produces all the possible segmentation paths, suffers from large amount of candidates, while other approaches lose so many correct results.",
        "As shown in Table 1, NSP-based rough segmentation enjoys two good properties: higher coverage and fewer candidates.",
        "In other word, NSP is effective strategy for disambiguation.",
        "1) MM: maximum matching; SP: shortest path; ML: Maximum likelihood; FS: Full segmentation 2) Max size and AV size is the maximum and average size of segmentation candidate set, respectively; 3) Coverage=# of correctly segmented/# of sentence*100% 4) The size of testing set is 2 million Chinese characters.",
        "5 Unknown words recognition using role-based HMM",
        "The task includes: locating the boundary of a unknown word wi, identifying the word class ci, and computing the probability p(wilci), which is required in class-based segmentation.",
        "Here, we introduce two levels of HMM to recognize simple and recursive unknown words on the rough segmentation set."
      ]
    },
    {
      "heading": "5.1 Role set for unknown words recognition",
      "text": [
        "In the same way of class-based HMM for word segmentation, here we classify word class into various role according to its linguistic features shown in unknown words recognition.",
        "In table 2, we present a simplified role set for unknown personal name recognition.",
        "Role is similar as word class.",
        "Their difference is: a word has only a word class, but a word class has one role or more."
      ]
    },
    {
      "heading": "5.2 Role tagging and Recognizing Unknown words recognition",
      "text": [
        "Given a word sequence W=(wl ,... wn) , we could get its class result C=(cl ,... cn).",
        "Now we could tag W with role R=(rl ,... rn), where all roles are from the same set.",
        "Among all the roles sequence, we select the sequence R# with the maximum probability as the final choice.",
        "Through the same induction detailed in section 3, we could get It is a tagging process and we make use of Viterbi algorithm (L.R.Rabiner, 1988) that selects the global optimum among all the state sequences.",
        "Here, tagging word class sequence\" EE / 4 / ,, , /TIME/0-'t\" (Mao Ze-Dong was born in sometime.)",
        "with personal roles, we could get R#=\"EE/C 4/D ,, ,/E T�ME/B 0-'t/Z\" through Vitebi selection.",
        "Unknown words are recognized through maximum pattern matching on role sequence.",
        "For instance, \"C\", \"D\",\"E\" is surname, first and second token of 2-Hanzi given name, respectively.",
        "So token sequence tagged with role \"CDE\" is likely to form a traditional Chinese personal name.",
        "There-fore,\"EE4,, ,\" will be recognized as a Chinese personal name according to its roles.",
        "Let wi be recognized unknown word and ci be the word class, we estimate the probability p (wiI ci) with the following formula: where wi is made up of tokens from pth to (p+k-1)th.",
        "Hencep(EE4,, ,IPER)=p(EE�C) p(41D) p(,, ,IE) p(DIC)p(EID).",
        "Finally unknown word \"EE4,, ,\" and p(EE4,, , IPER) can be added into the class-based HMM, shown as dashed area in Figure 3."
      ]
    },
    {
      "heading": "5.3 Recursive unknown word recognition",
      "text": [
        "Organization name like \"frCK,* RAW-A ' C ��\"(Memorial Hall of Zhou En-Lai and Deng Yun-Chao) and some sophisticated location name like \" � n,' ,,�\"(Zhang Zi-Zhong Road) often include one or more unknown words.",
        "We call them \"recursive unknown word\".",
        "Our solution is: Firstly, recognizing non-recursive unknown words in the lower level of role-based HMM, then revising the word class sequence with the recognized results; next applying another role HMM to recognize the recursive ones.",
        "Take the original word class sequence \"frC/K,/*/ R/ A/W-A/ ' /C��\"as exemplification.",
        "In the first step, \"frCK,*\" and \" AW-A ' \" would be recognized as personal name.",
        "Then, the original class sequence could be replaced with \"PER/R/PER/C",
        "��\".Based on the revised class result, the higher role-based HMM could recognize the recursive unknown word \" M E,��������\" as an organization name.",
        "Our method utilizes previous results and greatly reduces data sparseness.",
        "The role training set is transformed from corpus tagged with POS.",
        "Zhang and Liu (2002) provided the algorithm for role data conversion, model training, named entity recognition and the other procedures in role-based HMM."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": [
        "An HHMM-based Chinese lexical system ICTCLAS was accomplished.",
        "The following experiments are performed on ICTCLAS.",
        "As commonly used, we conduct our evaluations on terms of segmentation accuracy (SEG), accuracy of POS tagging (TAG1) with 24 tags, accuracy of POS tagging (TAG2) with 48 tags, precision of named entity recognition (P), recall of named entity recognition (R) and F-measure (F) that is weighted combination of P and R. They are calculated as following: SEG= # of correctly segmented words/ # of words; TAG1= # of correctly tagged 24-tag POS/ # of words; TAG2= # of correctly tagged 48-tag POS/ # of words; P= # of correct recognized NE/# of recognized NE; R= # of correct recognized NE/# of NE X 100%; F_ , Rx Px (\"J62) here 0 is assigned with 1, and F R+PxJ62 is called F-1."
      ]
    },
    {
      "heading": "6.1 Chinese Lexical analysis and HHMM",
      "text": [
        "On 1,108,049-word news corpus from the People 's Daily, we conduct four experiments:",
        "1) BASE: ICTCLAS with only class-based segmentation and POS tagging; 2) +PER: Adding role-based HMM for personal name recognition to BASE; 3) +LOC: Adding role-based HMM for location name recognition to +PER; 4) +ORG: Adding role-based HMM for location name recognition to +LOC.",
        "Figure 4 gives the contrast among the four experiments in performance.",
        "It indicates that: firstly, every level in HHMM contributes to lexical analysis.",
        "For instance, SEG increases from 96.55% to 97.96% after personal HMM is added.",
        "If all levels of HMM are integrated, ICTCLAS achieves 98.25% SEG, 95.63% TAG1 and 93.38% TAG2.",
        "Secondly, low levels in HHMM benefits from the higher one.",
        "After organization recognition is applied, F-1 value of organization adds by 25.91%, furthermore, the performance of segmentation, POS tagging and recognition of personal and location name improves, too.",
        "It is because high level not only solves its own problem, but also helps the lower HMMfilter improper candidate.",
        "For exa m-ple, in the sentence\"���7JcTRWU\"(The water in Liu village is sweet),\"��\"(Liu village) is very likely to be incorrectly recognized as a personal name in +PER experiment.",
        "However, it will be revised as a location name in +LOC experiment."
      ]
    },
    {
      "heading": "6.2 Official evaluation on ICTCLAS",
      "text": [
        "On July 6, 2002, ICTCLAS participated the official evaluation, which was held by the National Foundation of 973 Project of China.",
        "The open evaluation is conducted on real texts from six domains.",
        "The performance of ICTCLAS lists as Table 3."
      ]
    },
    {
      "heading": "BASE +PER +LOC +ORG",
      "text": [
        "best Chinese lexical analyzer."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "Our contributions are: 1) Applying HHMM to different lexical tasks, including word segmentation, POS tagging, unknown words recognition, and disambiguation.",
        "2) Using class-based HMM for word segmentation, which integrates common words and unknown ones into a unified frame.",
        "3) Proposing NSP strategy for segmentation disambiguation.",
        "4) Bringing forth role-based HMM to recognize simple and recursive unknown words.",
        "Various experiments show that each level in HHMM contributes to the final performance.",
        "Evaluation on ICTCLAS confirms that HHMM-based Chinese lexical analysis is effective."
      ]
    },
    {
      "heading": "8 Acknowledgements",
      "text": [
        "The authors wish to thank Prof. Shiwen Yu of Peking University for the training corpus.",
        "And we acknowledge our debt to Gang Zou, Dr. Bin Wang, Dr. Jian Sun, Ji-Feng Li and other colleagues.",
        "Huaping Zhang would especially express gratitude to his graceful girl friend Feifei and her family for their encouragement during the hard work.",
        "We also thank three anonymous reviewers for their elaborate and helpful comments."
      ]
    }
  ]
}
