{
  "info": {
    "authors": [
      "Jun Suzuki",
      "Yutaka Sasaki",
      "Eisaku Maeda"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C02-1119",
    "title": "SVM Answer Selection for Open-Domain Question Answering",
    "url": "https://aclweb.org/anthology/C02-1119",
    "year": 2002
  },
  "references": [
    "acl-C02-1053",
    "acl-C02-1054",
    "acl-J96-1002",
    "acl-N01-1025",
    "acl-W01-0509"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents an answer selection method based on Support Vector Machines (SVM) for Open-Domain Question Answering (QA).",
        "Selecting and ranking plausible answers from a large number of candidates in documents is one of the most critical parts of QA systems.",
        "It is extremely difficult to find good evaluation functions or rules for the answer selection.",
        "To overcome this issue, we apply SVM to answer selection.",
        "We evaluate the performance measured by mean reciprocal rank (MRR) and the correct ratio of answer ranked first.",
        "The results show that the proposed SVM-based method offers a statistically significant increase in performance compared to other machine learning methods such as decision tree learning (C4.5) boosting with decision tree learning (C5.0), and the maximum entropy method."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Question Answering (QA) involves the extraction of answers to a question from large-scale documents.",
        "For instance, if a QA system is given the question \"When was Queen Victoria born?",
        "\", it should answer \"1832\".",
        "Question Answering has been studied intensely all over the world since the start of the Question Answering Track at TREC-8 (1999).",
        "The definition of QA tasks at the TREC QA-Track has been revised and extended year after year.",
        "At first, QA research focused on the Passage Retrieval method as used at TREC-8.",
        "That is, the QA task eras to answer a question in the form of strings of 50 bytes or 250 bytes excerpted from a large set of news wire articles.",
        "Recently, however, the QA task is considered to be to extract exact answers to a question.",
        "Typically, question answering systems use the following components: Question analysis analyzes a given question sentence and determines the question type and keywords.",
        "In addition, some systems find the question focus of a given question.",
        "Text retrieval finds the top N paragraphs (or documents) that match the output of question analysis, such as keywords and question types.",
        "Answer candidate extraction extracts answer candidates from the relevant documents retrieved by the text retrieval component.",
        "Answer selection selects answers to the question from among the answer candidates based on the result of question analysis.",
        "We have studied a Japanese.",
        "QA system SAIQA (Sasaki et al., 2001), which has above.",
        "four components and an article summarization module to provide a justification of the, answer.",
        "Now we arc developing a trainable, QA system SAIQA-II based on Support Vector Machines (SVM) technique.",
        "(Sasaki et al., 2002; Hirao et al., 2002; Isozaki and Kazawa, 2002).",
        "This paper focuses on the answer selection part and proposes SVM based answer selection method.",
        "In answer selection, selecting and ranking plausible answers from a large number of candidates is the key to success.",
        "It is, however, very difficult to find good evaluation functions or rules that work well in all fields because.",
        "there arc many system parameters that must be carefully tuned in order to achieve good answer selection.",
        "Our solution is to apply SVM to determine.",
        "the best answer selection function the SVM has achieved high performance in the fields of Natural Language Processing, such as chunk-ing(Kudo and Matsumoto, 2001) and text cat-egorization(Joachims, 1998; Taira and Haruno, 1999).",
        "We utilize a QA test collection of Japanese question sentences whose answers are named entities (exact answers), such as dates and person names.",
        "While Japanese is an agglutinative language such as Chinese and Thai, the exact definition of a named entity has been already discussed and defined in IREX1.",
        "Before we present the SVM approach, we define the question answering task addressed in this paper.",
        "Basically, our question answering system follows the definition set by the TREC QA-Track.",
        "In addition, we adopt some additional conditions to evaluate the answer selection part.",
        "1.",
        "Answers to questions are named entities.",
        "2.",
        "The answer exists in at least one of the documents in the set.",
        "In the following sections, we will show hoer to apply SVM to answer selection and its performance.",
        "It is compared against the baseline method and other machine learning approaches (decision tree learning, boosting with decision tree learning and maximum entropy method)."
      ]
    },
    {
      "heading": "2 Support Vector Machines (SVM)",
      "text": []
    },
    {
      "heading": "2.1 Key Ideas of SVM",
      "text": [
        "This section briefly introduces the machine learning methodology of Support Vector Machines (Vapnik, 1995; Cortes and Vapnik, 1995).",
        "SVM offer the following advantages over conventional statistical learning algorithms (i.e., decision tree learning, maximum entropy method):",
        "1 high generalization performance even with feature vectors of high dimension, and 2 the ability to manage kernel functions that map input data to higher dimensional space without increasing computational complexity.",
        "The explanation of SVM starts with a set of l training data (x1 y1), • • •, (x1, yi) where",
        "The optimal hyperplane w • x + b = 0(w C Rn, b C R) separates the training data into two classes.",
        "The basic idea of SVM is to maximize the margin between the positive and negative examples.",
        "Figure 1 shows training examples linearly separated into two classes.",
        "In general, it is not necessary to separate training examples into each class.",
        "Variable �qi (> 0) is introduced for misclassification errors.",
        "This optimization problem is defined as",
        "The first term in equation (1) specifies the size of the margin and the second term represents the cost of the misclassification.",
        "The decision function f (x) can be written as:",
        "We calculate the kernel function defined as",
        "sifier.",
        "Using a Kernel function, we can rewrite equation (4) as:",
        ": positive example : negative example"
      ]
    },
    {
      "heading": "2.2 Application to QA Tasks",
      "text": [
        "From the viewpoint of machine learning, answer selection is defined as the task of training and classifying answer candidates into positives (correct answers) and negatives (incorrect answers) for a given question.",
        "To apply SVM, we have to prepare a set of training examples that contain feature vectors xi of i-th answer candidates.",
        "For each question, a QA system analyzes the question, retrieves documents related to the question, then lists the answer candidates.",
        "The system parameters that were computed in the process are recorded and used to create feature vectors for each answer candidate.",
        "We used the g(x) in equation (5) to rank the answer candidates g(x) represents the distance of x from the optimal hyperplane normalized by the margin (Figure 1)."
      ]
    },
    {
      "heading": "3 Feature Vectors",
      "text": []
    },
    {
      "heading": "3.1 Preparation for Feature Extraction",
      "text": [
        "As with typical QA systems, question analysis, text retrieval and answer candidate extraction arc performed before the feature extraction.",
        "the following three steps arc performed:",
        "(1) For each question, the question analysis module analyzes the question and obtains key",
        "words (KW), question types (QT), a question focus (QF), numerical units (QU) such as \"litter\" or \"piece.",
        "\", and auxiliary terms (AT) which is quoted term and a sequences of katakana words in Japanese.",
        "(2) The text retrieval module collects the documents that contain at least one keyword or named entity.",
        "(3) All named entities that match the question types are extracted from the retrieved documents.",
        "After these steps, the features for the answer selection are extracted using the results of question analysis (1) and the extracted answer candidates (3)."
      ]
    },
    {
      "heading": "3.2 Method of Feature Extraction",
      "text": [
        "Tables 1 and 2 show the features used in this paper.",
        "Table.",
        "1 shows the features extracted using the window function, explained in Section 3.2.1.",
        "Table, 2 shows the remaining features.",
        "Foreign words are expressed in katakana in Japanese.",
        "The, answer selection module calculates several parameters using a window function that we define based on the phrase (bunsetsu) unit, the sentence unit, and the paragraph unit (Table 1).",
        "Let D be a document.",
        "Let wi, bi, si, and pi be sequentially numbered words, bunsetsu, sentences, and paragraphs in D, respectively.",
        "We define window function TV (k) with metavariable T C {p, S1, w} as: Here, we treat wi as a singleton set.",
        "The window function forms a set of words of interest for subsequent processing.",
        "Figure 2 shows an example of the window function.",
        "For example, W2(k) includes all words in the region from the (i – 2)-th sentence to the (i + 2)-th sentence.",
        "We use A = 1, 5.... , 20 (every five) for bunsetsu analysis, A = 0,1 2, 3 for the sentence analysis, and A = 0 for the paragraph analysis.",
        "This is necessary due to the analysis error of sentence boundaries and parts of speech, and the difficulty of context understanding or semantic analysis in Natural Language Processing.",
        "Some parameters are real numbers (type real in Tables 1 and 2 in the range of 0 to 1 after normalization.",
        "These real values are quantized into five bins.",
        "Integer values (type int) are also quantized into five bins.",
        "The bin widths are uniform.",
        "As a result of this operation, our feature vectors contain only boolean features.",
        "We use the semantic categories of the semantic attribute system of \"Goi-Taikei A Japanese Lexicon\" (Ikehara et al., 1997).",
        "This semantic attribute system is used for calculating the similarity of keywords (SC in Table 1).",
        "Thus, semantic relations are included in the feature vector."
      ]
    },
    {
      "heading": "4 Experimental Settings",
      "text": []
    },
    {
      "heading": "4.1 Evaluation Method",
      "text": [
        "We adopted a recent standard style used by TREC QA-Track, and call this style answer (6)",
        "class feature type answer candidate (AC) word length int normalized position in the document real matching with Part-of-Speeches bool matching with attached function words bool numerical unit (QU) entity bool question type (Q7') pair of Q'1' and NE of AC bool ranking.",
        "The evaluation measure for answer ranking is the mean reciprocal rank (MRR), which is the same as that used by TREC QA-Track(1999).",
        "This score is simply the rank position of the first correct answer.",
        "If the first correct answer is ranked n, the score is 1/n (first = score 1, second = score 1/2, .. fifth = score 1/5).",
        "The evaluation method used is ten fold cross validation, divided into ten sets.",
        "Nine sets were used for training and the remaining one for testing.",
        "In addition, the ratio of each category in each set is even."
      ]
    },
    {
      "heading": "4.2 Data Set",
      "text": [
        "The following experiments used the, QA test collection constructed in (Sasaki et al., 2001) as the data sets.",
        "The style of questions is almost the same as the TREC QA-Track style, except all questions are written in Japanese.",
        "All questions used in the evaluation have at least one correct answer in the retrieved documents (some questions had no answers because, of erroneous named entities analysis or text retrieval).",
        "This allows us to evaluate just the Answer Selection parts that we focus on.",
        "the number of questions for each question type used is shown in Table, 3 the category",
        "\"others\" includes 25 categories such as TIME, MONEY, PERCENT, and AGE.",
        "We also evaluated certain categories, PERSON, ORGANIZATION, LOCATION and DATE, considering only questions in the target category.",
        "the average number of answer candidates for each question was 145.12; the number of correct answer candidates was one or a few the feature.",
        "vector consisted of 3081 features."
      ]
    },
    {
      "heading": "4.3 Comparison Methods",
      "text": [
        "To estimate the performance of our answer selection method, we compared it against one.",
        "baseline method and three machine learning methods as described below.",
        "culated from the sum of the distance between the answer candidate and each keyword."
      ]
    },
    {
      "heading": "Decision Tree Learning (C4.5)",
      "text": [
        "the learning algorithm is C4.5 (Quinlan, 1993) and the default values for learning parameters were used the evidence value.",
        "eras calculated for the purpose of ranking.",
        "Boosting with Decision Tree Learning (C5.0) The learning algorithm is C5.0 with 100-rounds boosting (Freund and Shapire, 1996) the evidence value was calculated for the ranking as well as C4.5."
      ]
    },
    {
      "heading": "Maximum Entropy Method (Berger et al., 1996) (ME)",
      "text": [
        "The ranking scores were equated to the probability of answer correctness."
      ]
    },
    {
      "heading": "Support Vector Machines (SVM)",
      "text": [
        "We selected the 2-degree polynomial kernel because of its excellent performance in preliminary experiments that considered 1 2, 3, 4,5-degree polynomial kernels and y = 0.0001, 0.001, 0.1,1 RBF kernels.",
        "The same question analysis module and text retrieval module were used in all methods; the machine learning methods used the same feature vectors."
      ]
    },
    {
      "heading": "5 Results",
      "text": [
        "Figure 3 shows the results of answer selection.",
        "The error bars represent the standard deviation between each fold.",
        "The results of evaluating all questions and every category show that SVM answer selection offers the best performance.",
        "The results of the baseline method indicate that category PERSON probably includes easier questions since questions against this category can be found by using just keywords.",
        "We performed a statistical significance test based on Tukey's multiple comparison method on the machine learning methods.",
        "The significance test results arc shown in Table 4.",
        "The asterisk (*) represents a 5% significant difference between methods, while (**) represents a 1% significant difference.",
        "Our approach became statistically different (superior) from the other methods in most of comparisons.",
        "Figure 4 shows the ratio of questions where the answer ranked first is the correct answer.",
        "SVM answer selection offers the best performance as the same as MRR results.",
        "The performance in providing the correct answer at the first run would be seen as more important than ranking answers.",
        "In fact, the QA task is considered to be the task of providing exact answers instead of ranked answers.",
        "In addition, the performance of SVM answer selection was compared to the hand-crafted ranking function of a QA system (Sasaki et al., 2001).",
        "The SVM answer selection method had a 0.446 point higher MRR value."
      ]
    },
    {
      "heading": "6 Discussion and Related Work",
      "text": [
        "In the TREC QA-Track, only a few systems took the machine learning approach (It-tycheriah et al., 2001).",
        "This system demonstrates answer selection with the maximum entropy method using 168 features after feature selection, and so it shares our view of using machine learning.",
        "Other research on adapting machine learning for answer selection was undertaken by (Ng et al., 2001), using four features for the feature vector and C5.0 for the learning algorithm.",
        "In contrast to their work, this paper adopted the SVM instead of the maximum entropy method or C5.0, as well as a different method of feature extraction, which suits the SVM, that offers good performance against large scale features.",
        "In applying SVM, the strategy of feature extraction can be different from the other methods.",
        "It is a better approach to place many features in the feature vectors since answer selection is a very complicated and sensitive process."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "This paper presented a question answering system based on Support Vector Machines that offers high performance in answer selection.",
        "Answer selection experiments were conducted on 1358 questions.",
        "The experimental results showed that the proposed SVM answer selection method had statistically better performance compared to other machine learning methods such as decision tree learning (C4.5), boosting with decision tree learning (C5.0), and maximum entropy method."
      ]
    },
    {
      "heading": "Acknowledgement",
      "text": [
        "We would like to thank all the members of the Knowledge Processing Research Group for valuable comments and discussions."
      ]
    }
  ]
}
