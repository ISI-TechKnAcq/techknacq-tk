{
  "info": {
    "authors": [
      "Wei Hu",
      "Nobuyuki Shimizu",
      "Hiroshi Nakagawa",
      "Huanye Sheng"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C08-1044",
    "title": "Modeling Chinese Documents with Topical Word-Character Models",
    "url": "https://aclweb.org/anthology/C08-1044",
    "year": 2008
  },
  "references": [],
  "sections": [
    {
      "text": [
        "As Chinese text is written without word boundaries, effectively recognizing Chinese words is like recognizing collocations in English, substituting characters for words and words for collocations.",
        "However, existing topical models that involve collocations have a common limitation.",
        "Instead of directly assigning a topic to a collocation, they take the topic of a word within the collocation as the topic of the whole collocation.",
        "This is unsatisfactory for topical modeling of Chinese documents.",
        "Thus, we propose a topical word-character model (TWC), which allows two distinct types of topics: word topic and character topic.",
        "We evaluated TWC both qualitatively and quantitatively to show that it is a powerful and a promising topic model."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Topic models (Blei et al., 2003; Griffiths & Steyvers 2004, 2007) are a class of statistical models in which documents are expressed as mixtures of topics, where a topic is a probability distribution over words.",
        "A topic model is a generative model for documents: it specifies a probabilistic procedure for generating documents.",
        "To make a new document, we choose a distribution over topics.",
        "Then, for each word in this document, we randomly select a topic from the distribution, and draw a word from the topic.",
        "Once we have a topic model, we can invert the generating process, inferring the set of topics that was responsible for generating a collection of documents.",
        "Although most topic models treat a document as a bag-of-words, the assumption has obvious shortcomings.",
        "Suppose there are many documents about art and musicals in New York.",
        "Then, we find a topic represented by words such as \"art\", \"musical\", and \"New\", instead of getting \"New York\".",
        "The bag-of-words assumption makes the topic model split a collocation – a phrase with meaning beyond the individual words – into individual words that have a different meaning.",
        "One example of a collocation is the phrase \"white house\".",
        "In politics, it carries a special meaning beyond a house that is white, whereas \"yellow house\" does not.",
        "While it is reasonable for English, the equivalent bag-of-characters assumption is especially troublesome for modeling Chinese documents, where almost all basic vocabularies are the equivalents of English collocations.",
        "In Chinese, some of the most commonly used couple-of-thousand characters are combined to make up a word, and no word boundary is given in the text.",
        "Effectively, Chinese words are like collocations in English.",
        "The difficulty is that there are overwhelmingly more of them, enough to render a bag-of-character assumption unreasonable for Chinese.",
        "Therefore, a topical model for Chinese should be capable of detecting the boundary between two words, as well as assigning a topic to each word.",
        "While topic models for Chinese documents bear some similarity to collocation models in English, existing topical collocation discovery models, such as the LDA (latent Dirichelt allocation) Collocation model (LDACOL) (Griffiths et (Wang et al., 2007), do not directly assign a topic to a collocation.",
        "These models find the boundaries of phrases and assign a topic to each word.",
        "The problem is in the next step – the topic of the collocation is exactly the same as one of the words.",
        "This is like saying that the topic of \"white house\" is the same as either that of \"white\" or \"house\".",
        "We propose a new topical model, the topical word-character model (TWC), which aims to overcome these limitations.",
        "We evaluated the model both quantitatively and qualitatively.",
        "For the quantitative analysis, we compared the performance of TWC and TNG using a standard measure perplexity.",
        "For the qualitative analysis, we evaluated TWC's ability to discover Chinese words and assign topics in comparison with TNG.",
        "The rest of the paper is organized as follows.",
        "Section 2 reviews topic models that aim to include collocations explicitly in the model and analyzes their limitations.",
        "Section 3 presents our new model TWC.",
        "Section 4 gives details of our consideration on inference for TWC.",
        "Section 5 presents our qualitative and quantitative experiments.",
        "Section 6 concludes with a summary and briefly mentions future work."
      ]
    },
    {
      "heading": "2. Topic Models for Collocation Discovery",
      "text": [
        "Since Chinese word discovery is similar to English collocation discovery, we first review some related topic models for collocation discovery.",
        "Although collocation discovery has long been studied, most methods are based on frequency or variance.",
        "LDACOL is an attempt to model collocations in a topical scheme.",
        "Starting from the LDA topic model, LDACOL introduces special random variables X .",
        "Variable xi = 1 implies that the corresponding word wi and previous word wi-1 belong to the same phrase, while xi = 0 implies otherwise.",
        "Thus, LDACOL can decide the length of a phrase dynamically.",
        "TNG is a powerful generalization of LDACOL.",
        "Its graphical model is shown in Figure 1.",
        "variables: a sequence of words w, a sequence of topics z , and a sequence of indicators X .",
        "TNG assumes the following generative process for documents.",
        "1.",
        "For each document d, draw 0d ~ Dirichlet(a).",
        "2.",
        "For each topic z, draw <pz ~ Dirichlet(/(.",
        "3.",
        "For each topic z and each word w, draw azw~ Dirichlet(<5).",
        "4.",
        "For each topic z and each word w, draw t//zw~ Beta(y).",
        "5.",
        "For each word wdJ in document d: (a) draw xdJ ~ Bernoulli( ^dd 1),",
        "The model is defined in terms of three sets of where a, (3, 8 are Dirichlet priors and y is a Beta prior, zdii denotes the ith topic assignment in document d, wdi denotes the ith word in document d, and xdi denotes the indicator between wdi-1 and wdi.",
        "Note that the variable xdi = 1 implies that word wd,i-1 and its neighbor wd,i belong to the same phrase, while xd i = 0 implies otherwise.",
        "However, the topics assigned to them (zd i-1 and zd i) are not required to be identical to each other.",
        "To decide the topic of a phrase, we can simply take the first (or last) word's topic or the most common topic in the phrase.",
        "The authors of TNG prefer to choose the last word's topic as the phrase topic because the last noun in a collocation is usually the \"head noun\" in English.",
        "However, this simple strategy may be ineffective when we apply TNG to Chinese documents.",
        "The topics of \"kbS\" (game) and \"W^S\" (tournament) should be represented by their last characters while those of \" (farmer) and \"rfx>\\lL\" (agriculture) should be represented by their first characters.",
        "And occasionally, the topic of a Chinese word is not identical to any topic of its component characters.",
        "For example \" (Bluetooth) is neither a color nor a tooth.",
        "To overcome the limitation of TNG, we must discard its underlying assumption: that the topic of a whole word is the same as the topic of at least one of its components."
      ]
    },
    {
      "heading": "3. Modeling Word Topic and Character Topic",
      "text": [
        "This section describes our topical word-character model (TWC), which models two distinct types of topics: word topic and character topic.",
        "To solve the problem associated with the \"5£ >p\" (Bluetooth) example, we need to distinguish between the topics of characters and words.",
        "Therefore, we introduce a new type of topic for words in addition to the topics assigned to characters.",
        "When generating a Chinese character, we first draw a word topic and then choose a character topic.",
        "A schematic description of this model is shown in Figure 2.",
        "t^f Character topic",
        "Here, we use random variables z and t to denote word and character topics, respectively.",
        "Note that the word topic and character topics have a hierarchical treelike structure (upper layer in Figure 2), whereas character topics and characters form a hidden Markov model (HMM) (lower layer in Figure 2).",
        "There are some indicators in the upper-right corner of each character topic in Figure 2.",
        "They help us to tell whether the current character belongs to the same word as the previous one.",
        "Now the question left is how to probabilistically draw these indicators, i.e., how to determine the length of the Markov chain.",
        "There are two ways to set the values of the indicators.",
        "One is similar to that applied in the hidden semi-Markov model (HSMM), which generates the duration of a segment from the state.",
        "Accordingly, we could first choose the length of a word from the distribution associated with the word topic and then assign 0 or 1 to each indicator.",
        "The other method is to directly draw indicators from the distribution associated with the previous character and topic, just as LDACOL and TNG do.",
        "The difference between these two methods is that the former determines the length of a word in advance while the latter increases the length dynamically.",
        "We prefer the second choice because it takes into consideration a lot of context information.",
        "In fact, our experimental results indicate that it has better performance.",
        "The formal definition of our model with word and character topics is as follows.",
        "TWC has four sets of variables: a sequence of characters c, a sequence of character topics t , a sequence of word topics z , and a sequence of indicators x .",
        "A document is generated via the following procedure.",
        "1.",
        "For each document d, draw 0d ~ Dirichlet(a);"
      ]
    },
    {
      "heading": "2.. For each word topic z, draw q z ~ DirichletO O;",
      "text": [
        "3.",
        "For each word topic z and each character topic t, draw azt ~ Dirichlet(8);",
        "4.",
        "For each word topic z, each character topic t and each character c, draw t//ztc ~ Beta(y);"
      ]
    },
    {
      "heading": "5.. For each character topic t, draw n ~ Dirichlet(^);",
      "text": [
        "6.",
        "For each character cd,,i in document d: (a) draw xdii ~ Bernoulli( y/z t Cj i); (d) draw cdii ~ Discrete( ntd.",
        ").",
        "Here, a, (, 8, r are Dirichlet priors and y is a Beta prior, zdii denotes the ith word topic assignment in document d, tdii denotes the ith character topic assignment in document d, cd i denotes the ith character in document d, and xdi denotes the indicator between cdM and cdJ.",
        "Note that compared with the schematic model in Figure 2, each character has its corresponding word topic in the TWC model.",
        "This is because we cannot decide how many words there will be in a document and how many characters there will be in a certain word in advance.",
        "In other words, the structure of the ideal model is not fixed.",
        "Therefore, we duplicate word topic variables for each character."
      ]
    },
    {
      "heading": "4. Inference with TWC",
      "text": [
        "Many approximate inference techniques such as variational methods, expectation propagation, and Gibbs sampling can be applied to graphical models.",
        "We use Gibbs sampling to perform our Bayesian inference in TWC.",
        "Gibbs sampling is a simple and widely applicable Markov chain Monte Carlo (MCMC) algorithm.",
        "In a traditional procedure, variables are sequentially sampled from their distributions conditioned on all other variables in the model.",
        "An extension of the basic approach is to choose blocks of variables first and then sample jointly from the variables in each block in turn, conditioned on the remaining variables; this is called blocking Gibbs sampling.",
        "When sampling for TWC, we separate variables into three types of blocks in the following manner (as shown in Figure 4)."
      ]
    },
    {
      "heading": "1.. character variables t i",
      "text": []
    },
    {
      "heading": "2.. indicators x i , whose value is 1 after n iterations",
      "text": [
        "3. word topics zi, zi+1, ..., zi+l-1 and indicator xi, satisfying xi=xi+i=1 and xj=0 (J from i to after n iterations",
        "Note that variables 6, q>,a, \\j/ and rj are not sampled.",
        "This is because we can integrate them out according to their conjugate priors.",
        "We only need to sample variables z , x , and t .",
        "Before discussing the inference of conditional probabilities, let us analyze our partition strategy in detail.",
        "We will explain the reasons for (1) sampling zi, zi+1, zi+l-1 together and (2) sampling z and xi together",
        "Assume that we draw zi, zi+1, ..., zi+i-1 one by one, and it is now time to sample zi+1 according to the conditional probability where z_{i+r) denotes a word topic except zi+1.",
        "Recall step 6-b in the generative TWC model: it says \"If xd4=1, then zd4= zdj_\", which implies P( zt+1\\zi, x,+1 = 1) = I (zt+1 = zt).",
        "As this probability is a factorial of the target probability, it follows that",
        "P(z,+1 = J \\ +1^ x t, ^a, p,, y, £) = 0 for all J ^ zt.",
        "In other words, zi+1 should be equal to zi and not change during sampling.",
        "It seems that step 6-b in the generative model causes the problem.",
        "But supposing that we do not set zi+1 to zi; it is still more reasonable to sample z together.",
        "According to our partition principle,",
        "1, x+i is a continuous indicator sequence whose head and tail are both 0 and the rest are 1, which implies that character string ci, ci+1, ci+l_1 forms a word and has the same word topic.",
        "Recall the schematic model in Figure 2: the word topic and character topics have a treelike structure and each word has only one word topic node.",
        "We add some auxiliary duplicates just because the ideal model is not fixed.",
        "Therefore, it is natural to sample the word topic together with its duplicates."
      ]
    },
    {
      "heading": "2.. Why do we sample z and x i together?",
      "text": [
        "Let us consider the probability of converting xifrom 0 to 1 in the current sampling iteration.",
        "Assume that the number of word topics is 3, zi-1=2, and where other variables and priors are omitted.",
        "If we first sample z and next sample xi, then the probability of drawing 1 for xi is 1/6, according to the multiplication principle.",
        "If we sample z and xi together, the probability of drawing 1 for xi is P( zr =... = zl+l _1 = , xr =).Since",
        "P(z, =... = zt+i_1 = 2,x, = 1) = 1 / 4 > 1 / 6 .",
        "In conclusion, the model is more likely to form long words, if we sample z and xi together.",
        "This is preferred because both TNG and TWC tend to produce shorter words than we would like.",
        "For each type of block, we need to work out the corresponding conditional probability.",
        "where Fd_.",
        "denotes the character topic assignments except tdi, xd_i denotes the indicators except xdi, and zd__r) denotes the word topic assignments except zdJ (J from i to Details of the derivation of these conditional probabilities are provided in Appendix A.1."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "In this section, we discuss our evaluation of TWC in Chinese document modeling and Chinese word and topic discovery.",
        "To evaluate the generalization performance of our model, we trained both TWC and TNG on a Chinese corpus and computed the perplexity of the held-out test set.",
        "Perplexity, which indicates the uncertainty in predicting a single character, is a standard measure of performance for statistical models of natural language.",
        "A lower perplexity score indicates better generalization performance.",
        "Formally, the perplexities for TWC and TNG are defined as follows.",
        "perplexityTWC (DUst) where Dtest is the testing data, D is the number of documents in Dtest, Nd is the number of characters in document d, 6d z is simply set to 1/Z (Z is number of word topics), and p, a, yy, rj are posterior estimates provided by applying TWC to training data.",
        "Details of the parameter estimation for TWC are provided in Appendix A.2.",
        "perplexityTNG ( Dtest ) where Dtest, D, and Nd are the same as defined for the TWC perplexity, 6d z is simply set to 1/T (T is number of topics), and p, a, y are posterior estimates provided by applying TNG to training data.",
        "Now, the remaining question is how to work out the likelihood function in the definition of perplexity.",
        "The likelihood function can be obtained by marginalizing latent variables, but the time complexity is exponential.",
        "Therefore, we propose an efficient method of computing the likelihood that is similar to the forward algorithm for an HMM.",
        "Details of the forward approach to computing likelihood for TWC and TNG are provided in Appendix B.",
        "In our experiments, we used a subset of Chinese corpus LDC2005T14.",
        "The dataset contains 6000 documents with 4476 unique characters and 2,454,616 characters.",
        "We evaluated both TWC and TNG using 10-fold cross validation.",
        "In each experiment, both models ran for 500 iterations on 90% of the data and computed the complexity for the remaining 10% of the data.",
        "TWC used fixed Dirichlet (Beta) priors a=1, P=1, 3=1, Y=0.1 and =0.01 while TNG used a=1, p=0.01, 3=0.01, and =0.1.",
        "No.",
        "of topics (TNG) No.",
        "of charcter topics * no.",
        "of word topics (TWC)",
        "The results of these computations are shown in Figure 5.",
        "Note that the abscissa for TWC is the number of word topics (Z) multiplied by the number of character topics (T), while the abscissa for TNG is the number of topics (T).",
        "They both represent the number of partitions into which the model classifies characters.",
        "Chance performance results in a perplexity equal to the number of unique characters, which was 4476 in our experiments.",
        "Therefore, both TWC and TNG are competitive models.",
        "And the lower curve shows that TWC is much better than",
        "TNG.",
        "We also found that both perplexity curves increased with the number of partitions.",
        "In other words, both models suffer from overfitting issues.",
        "This is because the documents in a test set are very likely to contain words that do not appear in any of the documents in the training set.",
        "Such words will have a very low probability, which is inversely proportional to the number of partitions.",
        "Therefore, the perplexity of TWC increased from 7.3513 (Z*T=2*2) to 8.9953 (Z*T=10*10), while that of TNG increased from 20.3789 (T=5) to 193.6065 (T=100).",
        "As shown in the previous subsection, TWC is a competitive method for topically modeling Chinese documents.",
        "Next, we show its ability to extract Chinese words and topics in comparison with TNG.",
        "In our qualitative experiments, the task of Chinese word and topic discovery was addressed as a supervised learning problem, where a set of words with their topical assignments was given as a seed set.",
        "Each seed can be viewed as some constraints imposed on the TWC and TNG models.",
        "For example, suppose that \"ttJJJft\" (teacher) together with its assignment \"education\" is a seed.",
        "This assumption implies that the indicator between characters \"It\" and \"JJJJ5\" is 1 and that the (word) topic for each character is \"education\".",
        "We make use of such constraints in a simple but effective way.",
        "In each sampling iteration, we first sample all variables as usual and then reset observed variables according to the constraints.",
        "We used 8000 Chinese documents in the Chinese Gigaword corpus (LDC2005T14) provided by the Linguistic Data Consortium for our experiments.",
        "The dataset contains 4651 unique characters and 3,295,810 characters.",
        "The number of word topics in TWC, the number of character topics in TWC, and the number of topics in TNG were all set to 15.",
        "Furthermore, 16 seeds scattered in 4 distinct topics were given, as listed in Table 1 column \"seed\".",
        "Dirichlet (Beta) priors were set to the same values as described in the previous subsection.",
        "Word and topic assignments were extracted after running 300 Gibbs sampling iterations on the training corpus together with the seed set.",
        "For the TNG model, we took the first character's topic as the topic of the word.",
        "We omitted one-character words and ranked the extracted words using the following formula occt (W) where occi(W) represents how many words were assigned to (word) topic i.",
        "The top-50 extracted words are presented in Table 1.",
        "We find found that both TWC and TNG could assemble many common words used in corresponding topics.",
        "And the TWC model had advantages over the TNG model in the following three respects.",
        "First, TNG drew more words related to the seeds.",
        "In Table 1, highly related words are marked in pink (underline) and partly related words are marked in blue (italic).",
        "It is clear that the TWC column is more colorful than the TNG column.",
        "Secondly, we found that many words extracted by TNG had the same prefix.",
        "For example, consider the topic \"agriculture\": there are 14 words marked with superscript 1 in Table 1.",
        "They all have the prefix This is because we took the first character's topic as the topic of the word.",
        "Although this strategy is beneficial in some cases, such as for words with prefix \"^<\", it is detrimental in other cases.",
        "For example, \" lEfjS:\" (sugar cane) and \"Izffl^\" (Gansu) have the same prefix and topic assignment, but the latter is a name of a province in China and is not related to agriculture.",
        "Similarly, even though the character string \"fp^f-\" does not form a Chinese word, this string \"fp^\" and (Iran) are classified in the same cluster.",
        "Compared with TNG, TWC can also extract words whose topics are identical to the topic of any character.",
        "For example, the topics \" §E&?",
        "*\" (freestyle swimming), \" MO*\" (medley swimming), and \" \" (butterfly stroke) depend on their suffixes.",
        "Thirdly, although TNG stands for \"topical n-gram model\", it infrequently draws words containing more than two characters.",
        "On the other hand, the TWC model extracts many n-character words, such as \" ItH^^^fJff\" (president of United States, George Bush), \" ©AMr*\" (individual medley), and \"H^^H\" (four per-",
        "£^(football) ©H(player) ftff(match) (championship)",
        "ip:fj (school) Sffi (teacher) ip:^.",
        "(student) (education)",
        "(war) dr© (solider) MM (general) (weapon)",
        "cent).",
        "This is partly due to our sampling strategy, discussed in subsection 4.1, which increases the probability of forming long words.",
        "We also found that some extracted character strings were very close to real Chinese words.",
        "For instance, \"f^U\" is a substring of \"W^S\" (tournament); \"H^zfe^©' is a suffix of \"^H^zfe ^\" (Chinese player), \" ^H^^\" (American player), and \" ^H^^\" (French player); and \"7^Ff\" is a substring of \"+7^Ff\" (100 thousand kilograms) and \"£77©^jr\" (50 thousand kilograms).",
        "(Such substrings are marked with superscript 2 in Table 1.)",
        "We believe that this result occurred because the training corpus was not large enough and that TWC will achieve better performance with a large dataset."
      ]
    },
    {
      "heading": "6. Conclusion and Future Work",
      "text": [
        "In this paper, we presented a topical word-character (TWC) model, which models two distinct types of topics: word topic and character topic.",
        "The experimental results show that TWC is a powerful approach to modeling Chinese documents according to the standard evaluation measure of perplexity.",
        "We also demonstrated TWC's ability to detect words and assign topics.",
        "Since TWC is a straightforward improvement that removes the limitations of existing topical collocation models, we expect that its application to English collocation will also result in higher performance.",
        "Symbols used here are defined as follows.",
        "C is the number of unique characters, T is the number of character topics, and Z is the number of word topics.",
        "Nd denotes the number of characters in a document",
        "d.",
        "I () is an indicator function, taking the value 1 when its argument is true, and 0 otherwise.",
        "qd,z0 represents how words are assigned to topic z in document d; pz t,c,k represents how many times an indicator is k given the previous character c, the previous character topic t, and the previous word topic z; represents how many times a character topic is t given a word topic z and the corresponding indicator 0; rriz^tj represents how many times a character topic is t given a word topic z, the previous character topic v, and the corresponding indicator 1; and rtc represents how many times character c is assigned to character topic t.",
        "Similarly, tive distribution over new word topics, new indicators, new character topics, and new characters.",
        "To compute the likelihood function for TWC, a quaternion function gi is defined as follows: (formula has a broken character) where Z is the number of word topics and T is the number of character topics.",
        "The function gi can be rewritten in a recursive manner.",
        "[l (r = J) Ku s = Similarly we can define function hi to help compute the likelihood for TNG.",
        "(formula has a broken character)",
        "After each Gibbs sampling iteration.",
        "we obtain posterior estimates 6.",
        "<p. a. y/ and r by where the symbols are the same as those defined in Appendix A.1.",
        "These values correspond to the predic-",
        "Blei.",
        "D. M.. Ng.",
        "A. Y.. and Jordan.",
        "M. J.",
        "2003.",
        "Latent Dirichlet allocation.",
        "Journal of Machine Learning Research, 3:993-1022.",
        "Griffiths.",
        "T. L. and Steyvers.",
        "M. 2004.",
        "Finding Scientific Topics.",
        "Proceedings of the National Academy of Sciences, 101 (suppl.",
        "1).",
        "5228-5235.",
        "Steyvers.",
        "M. and Griffiths.",
        "T. L. 2007.",
        "Probabilistic topic models.",
        "Latent Semantic Analysis: A Road to Meaning.",
        "Laurence Erlbaum.",
        "Griffiths.",
        "T. L.. Steyvers.",
        "M.. and Tenenbaum.",
        "J.",
        "B. T. 2007.",
        "Topics in Semantic Representation Psychological Review, 114(2).",
        "211-244.",
        "Wang.",
        "X.. McCallum.",
        "A.. and Wei.",
        "X.",
        "2007.",
        "Topical N-grams: Phrase and Topic Discovery.",
        "with an Application to Information Retrieval.",
        "Proceedings of the 7th IEEE International Conference on Data Mining (ICDM 2007).",
        "np)ßnptt -nn",
        ",-' ) k = 1 ^d.l-'.td.l-'.Cd.l-1."
      ]
    }
  ]
}
