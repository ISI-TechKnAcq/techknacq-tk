{
  "info": {
    "authors": [
      "Alexander Clark",
      "Franck Thollard"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C04-1013",
    "title": "Partially Distribution-Free Learning of Regular Languages from Positive Samples",
    "url": "https://aclweb.org/anthology/C04-1013",
    "year": 2004
  },
  "references": [
    "acl-J97-2003",
    "acl-P99-1070"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Regular languages are widely used in NLP today in spite of their shortcomings.",
        "Efficient algorithms that can reliably learn these languages, and which must in realistic applications only use positive samples, are necessary.",
        "These languages are not learnable under traditional distribution free criteria.",
        "We claim that an appropriate learning framework is PAC learning where the distributions are constrained to be generated by a class of stochastic automata with support equal to the target concept.",
        "We discuss how this is related to other learning paradigms.",
        "We then present a simple learning algorithm for regular languages, and a self-contained proof that it learns according to this partially distribution free criterion."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Regular languages, especially generated by deterministic finite state automata are widely used in Natural Language processing, for various different tasks (Mohri, 1997).",
        "Efficient learning algorithms, that have some guarantees of correctness, would clearly be useful.",
        "Existing algorithms for learning deterministic automata, such as (Carrasco and Oncina, 1994) have only guarantees of identification in the limit (Gold, 1967), generally considered not to be a good guide to practical utility.",
        "Unforunately the prospects for learning according to the more useful PAC-learning criterion are poor after the well known result of (Kearns and Valiant, 1989).",
        "Distribution-free learning criteria require algorithms to learn for every possible combination of concept and distribution.",
        "Under this worst-case analysis many simple concept classes are unlearnable.",
        "However in many situations it is more realistic to assume that there is some relationship between the concept and the distribution, and furthermore in general only positive examples will be available.",
        "There are two ways of modelling this.",
        "The simplest is to study the learnability of distributions (Kearns et al., 1994; Ron et al., 1995).",
        "In this case the samples are drawn from the distribution that is being learned.",
        "The choice of error function then becomes critical – the most natural (and difficult) being the Kullback-Leibler divergence.",
        "This means that any successful algorithm must produce hypotheses that assign a non-zero probability to every string.",
        "If what we are interested in is learning the underlying non-probabilistic concept then these hypotheses will be useless.",
        "We have elsewhere proved (Clark and Thollard, 2004) a suitable result similar to that of (Ron et al., 1995), bounding the divergence, but that proof involves some more elaborate technical machinery.",
        "The second way is to consider a traditional concept-learning problem, but to restrict the class of distributions to some set that only generates positive examples, and has some relation to the target concept.",
        "It is this latter possibility that we explore here.",
        "In the particular case of learning languages we will have an instance space of E* for some finite alphabet E, and we shall have a concept class, in this paper, corresponding to the class of all regular languages.",
        "In a distribution-free setting this is not learnable from positive and negative samples, nor a fortiori from positive samples alone.",
        "In our partially distribution-free framework however, we are able to prove learnability with an additional parameter in the sample complexity polynomial, that bounds a simple property of the distribution.",
        "We are able to present a simple stand alone proof for this well studied class of languages.",
        "The rest of the paper is structured as follows.",
        "Section 2 argues for a modified version of PAC learning as being an appropriate learning framework for a range of NLP problems.",
        "After defining some notation in Section 3 we then define an algorithm that learns regular languages (Section 4) and then in Section 5 prove that it does so according to this modified PAC-learnability criterion.",
        "We conclude with a critical analysis of our results."
      ]
    },
    {
      "heading": "2 Appropriateness",
      "text": [
        "Regular languages are widely used in a number of different applications drawn from numerous domains such as computational biology, robotics etc.",
        "In many of these areas, efficient learning algorithms are desirable but in each the exact requirements will be different since the sources of information, and the desired properties of the algorithms vary widely.",
        "We argue here that learning algorithms in NLP have certain special properties that make the particular learnability result we study here useful.",
        "The most important feature in our opinion is the necessity for learning from positive examples only.",
        "Negative examples in NLP are rarely available.",
        "Even in a binary classification problem, there will often be some overlap between the classes so that examples labelled with − are not necessarily negative examples of the class labelled with +.",
        "For this reason alone we consider a traditional distribution-free PAC-learning framework to be wholly inappropriate.",
        "An essential part of the PAC-learning framework is a sort of symmetry between the positive and negative examples.",
        "Furthermore, there are a number of negative results which rule out distribution free learning of regular languages (Kearns et al., 1994).",
        "A related problem is that in the sorts of learning situations that occur in practice in NLP problems, and also those such as first language acquisition that one wishes to model formally, the distribution of examples is dependent on the concept being learned.",
        "Thus if we are modelling the acquisition of the grammar of a language, the positive examples are the grammatical, or perhaps acceptable, sentences of the target language.",
        "The distribution of examples is clearly highly dependent on the particular language, simply as a matter of fact, in that the sentences in the sample are generated by people who have acquired the language.",
        "It thus seems reasonable to require the distribution to be drawn from some limited class that depends on the target concept and generates only positive examples – i.e. where the support of the distribution is identical to the positive part of the target concept.",
        "Our proposal is that when the class of languages is defined by some simple class of automata, we can consider only those distributions generated by the corresponding stochastic automata.",
        "The set of distributions is restricted and thus we call this partially distribution free.",
        "Thus when learning the class of regular languages, which are generated by deterministic finite-state automata, we select the class of distributions which are generated by PDFAs.",
        "Similarly, context free languages are normally defined by context-free grammmars which can be extended again to probabilistic or stochastic context free grammars.",
        "Formally, for every class of languages, L, defined by some formal device define a class of distributions, D, defined by a stochastic variant of that device.",
        "Then for each language L, we select the set of distributions whose support is equal to the language:",
        "Samples are drawn from one of these distributions.",
        "There are two technical problems here: first, this doesn’t penalise over-generalisation.",
        "Since the distribution is over positive examples, negative examples have zero weight – which would give a hypothesis of all strings zero error.",
        "We therefore need some penalty function over negative examples or alternatively require the hypothesis to be a subset of the target, and use a one-sided loss function as in Valiant’s original paper (Valiant, 1984), which is what we do here.",
        "Secondly, this definition is too vague.",
        "The exact way in which you extend the “crisp” language to a stochastic one can have serious consequences.",
        "When dealing with regular languages, for example, though the class of languages defined by deterministic automata is the same as that defined by non-deterministic languages, the same is not true for their stochastic variants.",
        "Additionally, one can have exponential blow-ups in the number of states when determinizing automata.",
        "Similarly, with context free languages, (Abney et al., 1999) showed that converting between two parametrisations of models for stochastic context free languages are equivalent but that there are blow-ups in both directions.",
        "It is interesting to compare this to the PAC-learning with simple distributions model (Denis, 2001).",
        "There, the class of distributions is limited to a single distribution derived from algorithmic complexity theory.",
        "There are a number of reasons why this is not appropriate.",
        "First there is a computational issue: since Kol-mogorov complexity is not computable, sampling from the distribution is not possible, though a lower bound on the probabilities can be defined.",
        "Secondly, there are very large constants in the sample complexity polynomial.",
        "Finally and most importantly, there is no reason to think that in the real world, samples will be drawn from this distribution; in some sense it is the easiest distribution to learn from since it dominates every other distribution up to a multiplicative factor.",
        "We reject the identification in the limit paradigm introduced by (Gold, 1967) as unsuitable for three reasons.",
        "First it is only an asymptotic bound that says nothing about the performance of the algorithms on finite amounts of data; secondly because it must learn under all presentations of the data even when these are chosen by an adversary to make it hard to learn, and thirdly because it has no bounds on the amount of computation allowed.",
        "An alternative way to conceive of this problem is to consider the task of learning distributions directly (Kearns et al., 1994), a task related to probability density estimation and language modelling, where the algorithm is given examples drawn from a distribution and must approximate the distribution closely according to some distance metric: usually the Kullback-Leibler divergence or the variational distance.",
        "We consider the choice between the distribution-learning analysis, and the analysis we present here to depend on what the underlying task or phenomena to be modelled is.",
        "If it is the probability of the event occurring, then the distribution modelling analysis is better.",
        "If on the other hand it concerns binary judgments about the membership of strings in some set then the analysis we present here is preferable.",
        "The result of (Kearns et al., 1994) shows up a further problem.",
        "Under a standard cryptographic assumption the class of acyclic PDFAs over a two-letter alphabet are not learnable since the class of noisy parity functions can be embedded in this simple subclass of PDFAs.",
        "(Ron et al., 1995) show that this can be circumvented by adding an additional parameter to the sample complexity polynomial, the dis-tinguishability, which we define below."
      ]
    },
    {
      "heading": "3 Preliminaries",
      "text": [
        "We will write a for letters and s for strings.",
        "We have a finite alphabet E, and E* is the free monoid generated by E, i.e. the set of all strings with letters from E, with A the empty string (identity).",
        "For s E E* we define |s |to be the length of s. The subset of E* of strings of length d is denoted by Ed.",
        "A distribution or stochastic language D over E* is a function D : E* – * [0, 1] such that EsEE* D(s) = 1.",
        "The L,,,, norm between two distributions is defined as maxs |D1 (s) − D2 (s) |.",
        "For a multiset of strings S we write Sˆ for the empirical distribution defined by that multiset – the maximum likelihood estimate of the probability of the string.",
        "A probabilistic deterministic finite state automaton is a mathematical object that stochastically generates strings of symbols.",
        "It has a finite number of states one of which is a distinguished start state.",
        "Parsing or generating starts in the start state, and at any given moment makes a transition with a certain probability to another state and emits a symbol.",
        "We have a particular symbol and state which correspond to finishing.",
        "A PDFA A is a tuple (Q, E, q0, qf, (, T, -y) , where",
        "• Q is a finite set of states, • E is the alphabet, a finite set of symbols, • q0 E Q is the single initial state, • q f V Q is the final state, • C V E is the final symbol, • 7-: Q × E U {(} -* Q U {qf} is the transition function and • Y : Q × E U {(} -* [0, 1] is the next symbol probability function.",
        "-y(q, a) = 0 when T(q, a) is not defined.",
        "We will sometimes refer to automata by the set of states.",
        "All transitions that emit C go to the final state.",
        "In the following T and y will be extended to strings recursively in the normal way.",
        "The sum of the output transition from each states must be one: so for all q E Q",
        "Assuming further that there is a non zero probability of reaching the final state from each state: i.e. bq E Q]s E E* : T(q, s() = q f n -y(q, s() > 0 (2) the PDFA then defines a probability distribution over E*, where the probability of generating a string s E E* is PA(s) = -y(q0, s().",
        "We will write L(A) for the support of this distribution, L(A) = {s E E* : PA(s) > 01.",
        "We will also define Pq (s) = -y(q, s() which we call the suffix distribution of the state q.",
        "We say that two states q, q' are µ- distinguishable if L,,,, (Pq, Pq,) > µ for some µ > 0.",
        "An automaton is µ-distinguishable iff every pair of states is µ-distinguishable.",
        "Since we can merge states q, q' which have L,,,, (Pq, Pq�) = 0, we can assume without loss of generality that every PDFA has a non-zero distinguishability.",
        "Note that ry(q0, s) where s E E* is the prefix probability of the string s, i.e. the probability that the automaton will generate a string that starts with s. We will use a similar notation, neglecting the probability function for (non-probabilistic) deterministic finite-state automata (DFAs)."
      ]
    },
    {
      "heading": "4 Algorithm",
      "text": [
        "We shall first state our main result.",
        "Theorem 1 For any regular language L, when samples are generated by a PDFA A where L(A) = L, with distinguishability µ and number of states n, for any c, S > 0, the algorithm LearnDFA will with probability at least 1 – S return a DFA H which defines a language L(H) that is a subset of L with PA(L(A) – L(H)) < c. The algorithm will draw a number of samples bounded by a polynomial in JEJ, n,1/µ,1/E,1/S, and the computation is bounded by a polynomial in the number of samples and the total length of the strings in the sample.",
        "We now define the algorithm LearnDFA.",
        "We incrementally construct a sequence of DFAs that will generate subsets of the target language.",
        "Each state of the hypothesis automata will represent a state of the target and will have attached a multiset of strings that approximates the distribution of strings generated by that state.",
        "We calculate the following quantities m0 and N from the input parameters.",
        "We start with an automaton that consists of a single state and no transitions, and the attached multiset is a sample of strings from the target.",
        "At each step we sample N strings from the target distribution.",
        "This re-sampling ensures the independence of all of the samples, and allows us to apply bounds in a straightforward way.",
        "For each state u in the hypothesis automaton and letter Q in the alphabet, such that there is no arc labelled with Q out of u we construct a candidate node (u, Q) which represents the state reached from u by the transition labelled with Q.",
        "For each string in the sample, we trace the corresponding path through the hypothesis.",
        "When we reach a candidate node, we remove the preceding part of the string, and add the rest to the multiset of the candidate node.",
        "Otherwise, in the case when the string terminates in the hypothesis automaton we discard the string.",
        "After we have done this for every string in the sample, we select a candidate node (u, Q) that has a multiset of size at least m0.",
        "If there is no such candidate node, the algorithm terminates, Otherwise we compare this candidate node with each of the nodes already in the hypothesis.",
        "The comparison we use calculates the L,,,,-norm between the empirical distributions of the two multisets and says they are similar if this distance is less than µ/4.",
        "We will make sure that with high probability these empirical distributions are close in the L,,,,-norm to the suffix distributions of the states they represent.",
        "Since we know that the suffix distributions of different states will be at least µ apart, we can be confident that we will only rarely make mistakes.",
        "If there is a node, v, which is similar then we conclude that v and (u, Q) represent the same state.",
        "We therefore add an arc labelled with Q leading from u to v. If it is not similar to any node in the hypothesis, then we conclude that it represents a new node, and we create a new node u' and add an arc labelled with Q leading from u to u'.",
        "In this case we attach the multiset of the candidate node to the new node in the hypothesis.",
        "Intuitively this multiset will be a sample from the suffix distribution of the state of the target that it represents.",
        "We then discard all of the candidate nodes and their associated multisets, but keep the multisets attached to the states of the hypothesis, and repeat.",
        "We can now prove that this algorithm has the properties we claim.",
        "We use one technical lemma that we prove in the appendix.",
        "Lemma 1 Given a distribution D over E*, for any µ' < 1/2, when we independently draw a number of samples m more than m0 = 2µ log µ , into a multiset S then L,,,,(ˆS, D) < µ' with probability greater than 1 - S'.",
        "Let H0, H1, ... , Hk be the sequence of finite automata, the states labelled with multisets, generated by the algorithm when samples are generated by a target PDFA A.",
        "We will say that a hypothesis automaton Hi is µ-good if there is a bijective function 4b from a subset of states of A including q0, to all the states of Hi such that 4b(q0) is the root node of Hi, and if there is an edge in Hi such that T(u, a) = v then 7-(4b-1 (u), a) = 4b-1(v) i.e. if Hi is isomorphic to a subgraph of the target that includes the root.",
        "If 4b(q) = u then we say that u represents q.",
        "In this case the language generated by Hi is a subset of the target language.",
        "Additionally we require that for every state v in the hypothesis, the corresponding multiset satisfies L,,,,(ˆSv, PD-, (v)) < µ/4.",
        "When a multiset satisfies this we will say it is µ-good.",
        "We will extend the function 4b to candidate nodes in the obvious way, and also the definition of µ-good.",
        "Definition 1 (Good sample) We say that a sample of size N is µ-c-good given a good hypothesis DFA H and a target A if all the candidate nodes with multisets larger than the threshold m0 are µ-good, and that if PA(L(A) - L(H)) > c then the number of strings that exit the hypothesis automaton is more than 12NPA(L(A) - L(H))."
      ]
    },
    {
      "heading": "5.1 Approximately Correct",
      "text": [
        "We will now show if all the samples are good, that for all i-0, 1, ... , k, the hypothesis Hi will be good, and that when the algorithm terminates the final hypothesis will have low error.",
        "We will do this by induction on the index i of the hypothesis Hi.",
        "Clearly H0 is good.",
        "Suppose Hi_1 is good, and we draw a good sample.",
        "Consider a candidate node (u, a) with multiset greater than m0.",
        "Since the previous hypothesis was good, this will be a representative of a state q and thus the multiset will be a sequence of independent draws from the suffix distribution of this state Pq.",
        "Thus L,,,,(ˆSu,Q, Pq) < µ/4 by the goodness of the sample.",
        "We compare it to a state in the hypothesis v. If this state is a representative of the same state in the target v, then L,,,,(ˆSv, Pq) < µ/4 (by the goodness of the multisets), the triangle inequality shows that L,,,, (ˆSu,,, ˆSv) < µ/2, and therefore the comparison will return true.",
        "On the other hand, let us suppose that v is a representative of a different state qv.",
        "We know that L,,,,(ˆSu,Q,Pq) < µ/4 and L,,,, (ˆSv, Pq,) < µ/4 (by the goodness of the multisets), and L,,,, (Pq, Pq,) > µ (by the µ-distinguishability of the target).",
        "By the triangle inequality L,,,, (Pq, Pq) < L,,,, (ˆSu,,, Pq) +",
        "L�( ˆSu,�,ˆSv) > µ/2 and the comparison will return false.",
        "In these cases Hi will be good.",
        "Alternatively there is no candidate node above threshold in which case the algorithm terminates, and i = k. The total number of strings that exit the hypotheis must then be less than n E m0 since there are at most njEj candidate nodes each of which has multiset of size less than m0.",
        "By the definition of N and the goodness of the sample PA(L(A) - L(H)) < c. Since it is good and thus defines a subset of the target language, this is a suitably close hypothesis."
      ]
    },
    {
      "heading": "5.2 Probably Correct",
      "text": [
        "We must now show that by setting m0 sufficiently large we can be sure that with probability greater than 1 - S all of the samples will be good.",
        "We need to show that with high probability a sample of size N will be good for a given hypotheis G. We can assume that the hypothesis is good at each step.",
        "Each step of the algorithm will increase the number of transitions in the active set by at least 1.",
        "There are at most njEj transitions in the target; so there are at most n IE I + 2 steps in the algorithm since we need an initial step to get the multiset for the root node and another at the end when we terminate.",
        "So we want to show that a particular sample will be good with probability at least",
        "There are two sorts of errors that can make the sample bad.",
        "First, one of the multisets could be bad, and secondly too few strings might exit the graph.",
        "There are at most njEj candidate nodes, so we will make the probability of getting a bad multiset less than S/2nIEI(nIEI + 2), and we will make the probability of the second sort of error less than S/2(nJEJ + 2).",
        "First we bound the probability of getting a bad multiset of size m0.",
        "This will be satisfied if we set µ' = µ/4 and S' = S/2njEj(njEj + 2), and use Lemma 1.",
        "We next need to show that at each step the number of strings that exit the graph will be not too far from its expectation, if PA(L(A) – L(H)) > E. We can use Chernoff bounds to show that the probability too few strings exit the graph will be less than S/2(nJEJ + 2)",
        "which will be satisfied by the value of N defined earlier, as can be easily verified."
      ]
    },
    {
      "heading": "5.3 Polynomial complexity",
      "text": [
        "Since we need to draw at most n IE I + 2 samples of size N the overall sample complexity will be (nJEJ + 2)N, which ignoring log factors gives a sample complexity of O(n2 JEJ2µ-2E1), which is quite benign.",
        "It is easy to see that the computational complexity is polynomial.",
        "Producing an exact bound is difficult since it depends on the length of the strings.",
        "The precise complexity also depends on the relative magnitudes of µ, JEJand so on.",
        "The complexity is dominated by the cost of the comparisons.",
        "We can limit each multiset comparison to at most m0 strings, which can be compared naively with m20 string comparisons or much more efficiently using hashing or sorting.",
        "The number of nodes in the hypothesis is at most n, and the number of candidate nodes is at most nJEJ, so the number of comparisons at each step is bounded by n21EI and thus the total number of multiset comparisons by n2JEJ(nJEJ+2).",
        "Construction of multisets can be performed in time linear in the sample size.",
        "These observations suffice to show that the computation is polynomially bounded."
      ]
    },
    {
      "heading": "6 Discussion",
      "text": [
        "The convergence of these sorts of algorithms has been studied before in the identification in the limit framework, but previous proofs have not been completely convincing (Carrasco and Oncina, 1999), and this criterion gives no guide to the practical utility of the algorithms since it applies only asymptotically.",
        "The partially distribution free learning problem we study here is novel.",
        "as is the extension of the results of (Ron et al., 1995) to cyclic automata and thus to infinite languages.",
        "Before we examine our results critically, we would like to point out some positive aspects of the algorithm.",
        "First, this class of algorithms is in practice efficient and reliable.",
        "This particular algorithm is designed to have a provably good worst-case performance, and thus we anticipate its average performance on naturally occurring data to be marginally worse than comparable algorithms.",
        "We have established that we can learn an exponentially large family of infinite languages using polynomial amounts of data and computation.",
        "Mild properties of the input distributions suffice to guarantee learnability.",
        "The algorithm we present here is however not intended to be efficient or cognitively plausible: our intention was to find one that allowed a simple proof.",
        "The major weakness of this approach in our opinion is that the parameter n in the sample complexity polynomial is the number of states in the PDFA generating the distribution, and not the number of states in the minimal FA generating the language.",
        "Since determinisation of finite automata can cause exponential blow ups this is potentially a serious problem, depending on the application domain.",
        "A second problem is the need for a distinguishability parameter, which again in specific cases could be exponentially small.",
        "An alternative to this is to define a class of µ-distinguishable automata where the distinguishability is bounded by an inverse polynomial in the number of states.",
        "Formally this is equivalent, but it has the effect of removing the parameter from the sample complexity polynomial at the cost of having a further restriction on the class of distributions.",
        "Indeed we can deal with the previous objection in the same way if necessary by requiring the number of states in the generating PDFA to be bounded by a polynomial in the minimal number of states needed to generate the target language.",
        "However both of these limitations are unavoidable given the negative results previously discussed."
      ]
    },
    {
      "heading": "References",
      "text": []
    },
    {
      "heading": "Appendix",
      "text": [
        "Proof of Lemma 1.",
        "We write p(s) for the true probability and ˆp(s) = c(s)/m for the empirical probability of the string in the sample – i.e. the maximum likelihood estimate.",
        "We want to bound the probability over an infinite number of strings, which rules out a naive application of Hoeffding bounds.",
        "It will suffice to show that every string with probability less than µ'/2 will have empirical probability less than µ', and that all other strings will have probability within µ' of their true values.",
        "The latter is straightforward: since there are at most 2/µ' of these frequent strings.",
        "For any given frequent string s, by Hoeffding bounds:",
        "So the probability of making an error on a frequent string is less than 4/µ'e-2m0µ12.",
        "Consider all of the strings whose probability",
        "We define Srare = U�k=1 Sk.",
        "The Chernoff bound says that for any S > 0, for the sum of n bernouilli variables with prob p and",
        "Now we bound each group separately, using the binomial Chernoff bound where n = mµ' > mp (which is true since p < µ') Pr [ˆp(s) >µ,] :5(p) n en – mp (8) n This bound decreases with p, so we can replace this for all strings in Sk with the upper bound for the probability, and we can replace m with m0.",
        "Pr [Is ESk:ˆp(s)>µ'] < ISk2-2k22 – m0µ' < 8 2 – k2 – m0µ' µ Using the factor of the form 2 – k, we can sum over all of the k.",
        "Putting these together we can show that the probability of the bound being exceeded will be 4e-2m0µ'2 + 8 2 – m0µ' < 12 e-2m0µ'2 (9) µ' µ µ' This will be less than S' if",
        "which establishes the result."
      ]
    }
  ]
}
