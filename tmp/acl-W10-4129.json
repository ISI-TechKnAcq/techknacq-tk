{
  "info": {
    "authors": [
      "Chongyang Zhang",
      "Zhigang Chen",
      "Guoping Hu"
    ],
    "book": "Proceedings of the Joint Conference on Chinese Language Processing",
    "id": "acl-W10-4129",
    "title": "A Chinese Word Segmentation System Based on Structured Support Vector Machine Utilization of Unlabeled Text Corpus",
    "url": "https://aclweb.org/anthology/W10-4129",
    "year": 2010
  },
  "references": [
    "acl-W02-1801",
    "acl-W06-0127"
  ],
  "sections": [
    {
      "text": [
        "An Double Hidden HMM and an CRF for Segmentation Tasks with",
        "Pinyin's Finals",
        "Huixing Jiang Zhe Dong",
        "Center for Intelligence Science and Technology Beijing University of Posts and Telecommunications Beijing, China",
        "We have participated in the open tracks and closed tracks on four corpora of Chinese word segmentation tasks in CIPS-SIGHAN-2010 Bake-offs.",
        "In our experiments, we used the Chinese inner phonology information in all tracks.",
        "For open tracks, we proposed a double hidden layers' HMM (DHHMM) in which Chinese inner phonology information was used as one hidden layer and the BIO tags as another hidden layer.",
        "N-best results were firstly generated by using DHHMM, then the best one was selected by using a new lexical statistic measure.",
        "For close tracks, we used CRF model in which the Chinese inner phonology information was used as features."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Chinese language has many characteristics not possessed by other languages.",
        "One obvious is that the written Chinese text does not have explicit word boundaries like western languages.",
        "So word segmentation became very significative for Chinese information processing, and is usually considered as the first step of any further processing.",
        "Identifying words has been a basic task for many researchers who have devoted themselves on Chinese text processing.",
        "The biggest characteristic of Chinese language is its trinity of sound, form and meaning (Pan, 2002).",
        "Hanyu Pinyin is the form of sound for Chinese text and the Chinese phonology information is explicit expressed by Pinyin which is the inner features of Chinese Characters.",
        "And it naturally contributes to the identification of Out-Of-Vacabulary words (OOV).",
        "In our work, Chinese phonology information is used as basic features of Chinese characters in all models.",
        "For open tracks, we propose a new double hidden layers HMM in which a new phonology information is built in as a hidden layer, a new lexical association is proposed to deal with the OOV questions and domains' adaptation questions.",
        "And for closed tracks, CRF model has been used , combined with Chinese inner phonology information.",
        "We used the CRF++ package Version 0.43 byTakuKudo.",
        "In the rest sections of this paper, we firstly introduce the Chinese phonology in Section 2.",
        "Then in the Section 3, the models used in our tasks are presented.",
        "And the experiments and results are described in Section 4.",
        "Finally, we give the conclusions and make prospect on future work."
      ]
    },
    {
      "heading": "2. Chinese Phonology",
      "text": [
        "Hanyu Pinyin is the form of sound for Chinese text and the Chinese phonology information is explicit expressed by Pinyin.",
        "It is currently the most commonly used romanization system for Standard Mandarin.",
        "Hanyu means the Chinese language, and Pinyin means \"phonetics\", or more literally, \"spelling sound\" or \"spelled sound\" (wikipedia, 2010).",
        "The system has been employed to teach Mandarin as home language or as second language by China, Malaysia, Singapore et.al.",
        "Pinyin has been the most Chinese character's input method for computers and other devices.",
        "The romanization system was developed by a government committee in the People's Republic of China, and approved by the Chinese government on February 11, 1958.",
        "The International Organization for Standardization adopted pinyin as the international standard in 1982, and since then it has been adopted by many other organizations(wikipedia, 2010).",
        "In this system, pinyin is composed by initials(pinyin: shengmu), finals(pinyin: yunmu) and tones(pinyin: sheng-diao) instead of consonants and vowels used in European language.",
        "For example, the Pinyin of \"t\" is \"zhong1\" composed by \"zh\", \"ong\" and \"1\".",
        "In which \"zh\" is initial, \"ong\" is final and \"1\" is the tone.",
        "Every language has its rhythm and rhyme, so Chinese is no exception.",
        "The rhythm system are the driving force from the unconscious habit of language(Edward, 1921).",
        "And the Pinyin's finals contribute the Chinese rhythm system, Which is the basic assumption our research based on."
      ]
    },
    {
      "heading": "3. Algorithms",
      "text": [
        "Generally the task of segmentation can be viewed as a sequence labeling problem.",
        "We first define a tag set as TS = {B, I, E, S}, shown in Table 1.",
        "For a given piece of Chinese sentence,",
        "where Xi,i = 1,... ,T is a Chinese character.",
        "Suppose that we can give each Chinese character Pinyin's final yi.",
        "And suppose the label sequence of X is S = s\\s2 ... sT, where si € TS is the tag of xi.",
        "Then what we want to di",
        "The model is described in Fig. 1.",
        "For a given piece of Chinese character strings, One hidden layer is label sequence S. Another hidden layer is Pinyin's finals sequence Y.",
        "The observation layer is the given piece of Chinese characters X.",
        "For the piece \"^^SifridfeJ-S-W\" of the example described in the experiments section, firstly, the TS tags are labeled to it.",
        "And its result is \"^/S 3t/B a/E mjVS i/B dfe/E Jt/B -Sr/I W/E\".",
        "Then the tags are combined sequentially to get the finally result J-SW\".",
        "In this section, A novel HMM solution is presented firstly for open tracks.",
        "Then the CRF solution for closed tracks is introduced.",
        "For transition probability, second-order Markov model is used to estimate probability of the double hidden sequences as described in (2).",
        "For emission probability, we keep the firstorder Markov assumption as shown in (5).",
        "Based on the work of (Jiang, 2010), a word lattice is also built firstly, then in the second step, the backward A* algorithm is used to find the top N results instead of using the backward viterbi algorithm to find the top one.",
        "The backward A* search algorithm is described as follow (Wang, 2002; Och, 2001).",
        "fined in (1).",
        "Label",
        "Explanation",
        "B",
        "beginning character of a word",
        "I",
        "inner character of a word",
        "E",
        "end character of a word",
        "S",
        "a single character as a word",
        "Given two random Chinese characters X and Y and assume that they appears in an aligned region of the corpus.",
        "The distribution of the two random Chinese characters could be depicted by a 2 by 2 contingency table shown in Fig. 2(Chang, 2002).",
        "tation(Lafferty, 2001; Zhao, 2006).",
        "In the closed tracks of the paper, we also use it.",
        "We adopted two main kinds of features: n-gram features and Pinyin's finals features.",
        "The n-gram feature set is quite orthodox, they are, namely, C-2, C-1, C0, C1, C2, C-2C-1, C-1C0, C0C1, C1C2.",
        "The Pinyin's finals feature set is the same as n-gram feature set.",
        "They are described in Table.",
        "2.",
        "In Fig. 2, a is the counts of X and Y co-occur; b is the counts of the cases that X occurs but Y does not; c is the counts of the cases that X does not occur but Y does; d is the counts of the cases that both X and Y do not occur.",
        "The Log-likelihood rate is calculated by (4).",
        "For the N-best result described in sec. 3.1.1, they can be re-ranked by (5).",
        "where scoreh is the negative log value of P (S, Y X ).",
        "K is the number of breaks in X and xk is the left Chinese character of the k break and yk is the right Chinese character of the k break.",
        "X is the regulatory factor(in our experiments X = 0.45).",
        "Bigger value of LLR(xk ,yk) means stronger ability in combining of the two characters xk and yk, then they should not be segmented.",
        "Conditional random field, as statistical sequence labeling model, has been used widely in segmen-",
        "Templates",
        "Category",
        "N-gram: Unigram N-gram: Bigram Phonetic: Unigram Phonetic: Bigram"
      ]
    },
    {
      "heading": "4. Experiments and Results",
      "text": [
        "We build a basic words dictionary for DHHMM and a Pinyin's finals dictionary for both DHHMM and CRF from The Grammatical Knowledge-base of Contemporary Chinese(Yu, 2001).",
        "For the finals dictionary, we give each Chinese character a final extracted from its Pinyin.",
        "When it comes to a polyphone, we just combine its all finals simply to one.",
        "For example, \" t {ong}\", \" J.",
        "{a&ai&i}\".",
        "The training corpus (5,769 KB) we used is the Labeled Corpus provided by the organizer.",
        "We firstly add the Pinyin's finals to each Chinese character of it, then we train the parameters of",
        "DHHMM and CRF model on it.",
        "And the test corpus contains four domains: Literature (A), Computer (B), Medicine (C) and Fi-nance(D).",
        "The LLR function's parameters{a, b, c, d} are counted from the current test corpus A, B, C, or D. It's means that for segmenting A, the LLR parameters are counted from A, so the same for segmenting B, C and D.",
        "Y",
        "-,7",
        "X",
        "a",
        "b",
        "c",
        "The date, time, numbers and symbols information are easily identified by rules.",
        "We propose four regular expressions' processes, in which the regular expressions' processes are handled one after another in order of date, time, numbers and symbols.",
        "By now, a rough segmentation can be done.",
        "For a character stream, the date, time, numbers and symbols are firstly identified, then the whole stream can be divided by these units to some pieces of character strings which will be segment by the models described in sec. 3.",
        "For example, a character stream \"2009 4 ^8^319, $Ti:te.Jt-£-^12$4& 9 » \" will be divided to \"20094_fft_8^_31 E)_, -«Slfrifef^",
        "12_9 _» \".",
        "Then the pieces \"fft\", * S ifr idfe $-£-^s\", \"^4^ 9 \" will be segmented sequentially by the models described in Section3.",
        "We evaluate our system by Precision Rate(6), Recall Rate(7), F1 measure(8) and OOV(Out-Of-Vocabulary) Recall rate(9).",
        "P C(correct words in segmented result) C(words in segmented result)",
        "r C(correct words in segmented result) C(words in standard result)",
        "Or C(correct OOV in segmented result) C(OOV in standard result)",
        "In (6-9), C(■ ■ ■) is the count of (■ ■ ■).",
        "Table 3 are the results of the DHHMM on open tracks.",
        "In Table 3, OOV RR is the recall rate of OOV, I VRR is the recall rate of IV(In Vocabulary).",
        "Since the CRF segmenter will not always return a valid tag sequence that can be translated into segmentation result, some corrections should be made if such error occurs.",
        "We devised a dynamic programming routine to tackle this problem: first we compute the valid tag sequence that closest to",
        "Table 3: Results of open tracks using DHHMM: Literature (A), Computer (B), Medicine (C) and Finance(D) the output of CRF segmenter (by term closest, we mean least hamming distance), ifthere is a tie, we choose the one has the least 'S' tags, if the tie still exists, we choose the one that comes lexicographically earlier (B < I < E < S, described in Table.",
        "1).",
        "Table 4 are the results of the CRF on closed tracks.",
        "Table 4: Results of closed tracks using CRF: Literature (A), Computer (B), Medicine (C) and Fi-nance(D)",
        "From the results of Table 3 and Table 4, we can observe that the CRF model outperforms the DHHMM by average 2.72% in F1 measure.",
        "In the other hand, from Table 5, we can see that the computation cost in DHHMM is less than half of the time cost and lower one-fifth memory cost than",
        "CRF model.",
        "A",
        "B",
        "C",
        "D",
        "R",
        "0.893",
        "0.918",
        "0.917",
        "0.928",
        "P",
        "0.918",
        "0.896",
        "0.907",
        "0.934",
        "F1",
        "0.905",
        "0.907",
        "0.912",
        "0.931",
        "OOV RR",
        "0.803",
        "0.771",
        "0.704",
        "0.808",
        "IV RR",
        "0.899",
        "0.945",
        "0.943",
        "0.939",
        "A",
        "B",
        "C",
        "D",
        "R",
        "0.945",
        "0.946",
        "0.94",
        "0.956",
        "P",
        "0.946",
        "0.914",
        "0.928",
        "0.952",
        "F1",
        "0.946",
        "0.93",
        "0.934",
        "0.954",
        "OOV RR",
        "0.816",
        "0.808",
        "0.761",
        "0.849",
        "IV RR",
        "0.954",
        "0.971",
        "0.962",
        "0.966"
      ]
    },
    {
      "heading": "5. Conclusions and Future works",
      "text": [
        "This paper has presented a double hidden lawyers HMM for Chinese word segmentation task in SIGHAN bakeoff 2010.",
        "It firstly created N top results and then select the best one from it by a new lexical association.",
        "Chinese phonology (specially by Pinyin's final in text) is very useful inner information ofChinese language, which is the first time used in our models.",
        "We have used it in both DHHMM and CRF model.",
        "In future work, there are lots of improvements can be done.",
        "Firstly, which polyphone's finals should be used in a given context is a visible question.",
        "And the strategy to train the parameter X described in 3.1.2 can also be improved."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This research has been partially supported by the National Science Foundation of China (NO.",
        "leading our discuss, Li Sun, Peng Zhang, Yaojing Chen, Zhixu Lin, Gan Lin, Guannan Fang for their useful helps in this work.",
        "Ye-Yi Wang, Alex Waibel.",
        "2002.",
        "Decoding Algorithm in Statistical Machine Translation, Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics and Eighth Conference of the European Chapter of the Association for Computational Linguistics: 366-372.",
        "Progress of the Grammatical Knowledge-base of Contemporary Chinese, ZHONGWEN XINXI XUEBAO, 2001 Vol.01.",
        "John Lafferty, A.Mccallum, F.Pereira.",
        "2001.",
        "Conditional Random Field: Probabilitic Models for Segmenting and Labeling Sequence Data., Proceedings of the Eighteenth International Conference on Machine Learning: 282-289.",
        "Hai Zhao, Changning Huang, Mu Li.",
        "2006.",
        "An Improved Chinese Word Segmentation System with Conditional Random Field, Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing (SIGHAN-5)(Sydney, Australia):162-165."
      ]
    }
  ]
}
