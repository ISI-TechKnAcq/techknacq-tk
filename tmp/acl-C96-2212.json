{
  "info": {
    "authors": [
      "Akira Ushioda"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C96-2212",
    "title": "Hierarchical Clustering of Words",
    "url": "https://aclweb.org/anthology/C96-2212",
    "year": 1996
  },
  "references": [
    "acl-C96-1020",
    "acl-J92-4003"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "'Fins paper describes a data-driven method for hierarchical clustering of words in which a large vocabulary of Nii-glish words is clustered bottom--up, with respect to corpora ranging in size from 5 to 50 million words, using a greedy algorithm that tries to riiinimize average loss of mutual information of adjacent classes.",
        "The resulting hierarchical clusters of words are then naturally transformed to a bit-string representation of (i.e. word bits for) all the words in the vocabulary.",
        "Introducing word bits into the Ant Decision-Tree POS l'agger is shown to significantly reduce the tagging error rate.",
        "Portability of word bits from one domain to another is also disscussed."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "One of the, fundamental issues concerning corpus-based Ni,P is the data sparseness problem.",
        "In view of the effectiveness of class-based n-grain language models against the data sparseness problem (Kneser and Ney 1993), it is expected that classes of words are also useful for NI,P tasks in such a way that statistics on classes are used whenever statistics on individual words are unavailable or unreliable.",
        "An ideal type of clusters for NI,P is the one which guarantees mutual substitutability, in terms of both syntactic and semantic soundness, among words in the saute class.",
        "Furthermore, clustering is much more useful if the clusters are of variable granularity, or hierarchical.",
        "We will consider a tree representation of all the words in the vocabulary in which the root node represents the whole vocabulary and a leaf node represents a word in the vocabulary.",
        "Also, any set of nodes in the tree constitutes a partition (or clustering) of the vocabulary if there exists one and only one node in the set along the path from the root node to each leaf node.",
        "In the following sections, we will describe a method of creating binary tree representation of words and present results of evaluating and comparing the quality of the hierarchical clusters obtained front texts of very different sizes."
      ]
    },
    {
      "heading": "2 Word Bits Construction",
      "text": [
        "Our word hits construction algorithm is a modification and an extension of the mutual information clustering algorithm proposed by Brown et al.",
        "(1992).",
        "We will first illustrate the difference between the original formulae and the ones we used, and then introduce the word hits construction algarithM.",
        "We will use the same notation ;is in Brown et al.",
        "to make the couiparison easier."
      ]
    },
    {
      "heading": "2.1 Mutual Information Clustering Algorithm",
      "text": [
        "Mutual information clustering method employs a bottum-up merging procedure with the average mutual information (A MI) of adjacent classes in the text as an objective function.",
        "In the initial stage, each word in the vocabularly of size l/ is assigned to its own distinct class.",
        "We then merge two classes if the merging of them induces minimum AM I reduction among all pairs of classes, and we repeat the merging step until the number of the classes is reduced to the predefined number C. Time complexity of this basic algorithm is 0(V5) when implemented straightforwardly.",
        "By Storing the result of all the trial merges at the previous merging step, however, the time complexity can be reduced to 0(1/ 3) as shown below.",
        "Suppose that, starting with V classes, we have already made V - k merges, leaving k classes, G(I), , Ck(k).",
        "The AMI at this stage is given by the following equations.",
        "In equation qi,„'s are summed over the entire k x k class bigram table in which (1,m) cell represents qk(1,m).",
        "In this merging step we investigate a trial merge of Ck(i) and (.'kU) for all class pairs j), and compute the A MI reduction E /1, lk(i, j) effected by this merge, where 14;(i, j) is the AMI after the merge.",
        "Suppose that the pair (CA;(i),Ck(j)) was chosen to merge, that is, Lk (i, j) 14(1,7n) for all",
        "to calculate 1)(1, in) for all the pairs (1, m).",
        "Here we use the superscript (i, j) to indicate that (Ck(i), CAW) was merged in the previous merging step.",
        "Now note that the difference between L(1:'.71) (1, m) and L,k(1, in) only comes from the terms which are affected by merging the pair",
        "Some part of the summation region of 4iji)(1, rn) and Ik cancels out with a part of /(i'ii) or a part of 4(1, m).",
        "Let ii(„iji)(d, in), Ik(/,m),and k 1 Ik denote the values of 1(i'ii) (1, rn), /), (1, m), and respectively, after all the common terms among them which can be canceled are canceled out.",
        "Then, we have",
        "Because equation 3 is expressed as the summation of a fixed number of q's, its value can be calculated in constant time, whereas the calculation of equation 1 requires 0(V2) time.",
        "Therefore, the total time complexity is reduced by 0(V2).",
        "The summation regions of I's in equation 3 are illustrated in Figure 1.",
        "Brown et al.",
        "seem to have ignored the second term of the right hand side of equation 3 and used only the first term to calculate .1Vi (1, m) – Lk (1, m) 1 .",
        "However, since the second term has as much weight as the first term, we used equation 3 to make the model complete.",
        "Even with the 0073) algorithm, the calculation is not practical for a large vocabulary of order :104 or higher.",
        "Brown et al.",
        "proposed the following Actually, it is the first term of equation 3 times (-1) that appeared in their paper, but we believe that it is simply due to a misprint.",
        "method, which we also adopted.",
        "We first make V singleton classes out of the V words in the vocabulary and arrange the classes in descending order of frequency, then define the merging region as the first C +1 positions in the sequence of classes.",
        "At each merging step, merging of only the classes in the merging region is considered, thus reducing the number of trial merges from 0(V2) to 0(C2).",
        "After each actual merge, the most frequent singleton class outside of the merging region is shifted into the region.",
        "With this algorithm, the time complexity is reduced to 0(C2 V)."
      ]
    },
    {
      "heading": "2.2 Word Bits Construction Algorithm",
      "text": [
        "The simplest way to construct a tree structured representation of words is to construct a dendro-gram from the record of the merging order.",
        "A simple example with a five-word vocabulary is shown in Figure 2.",
        "If we apply this method to the above 0(C2 V) algorithm, however, we obtain for each class an extremely unbalanced, almost left branching subtree.",
        "The reason is that after classes in the merging region are grown to a certain size, it is much less expensive, in terms of AM1, to merge a singleton class with lower frequency into a higher frequency class than merging two higher frequency classes with substantial sizes.",
        "A new approach we adopted is as follows.",
        "1.",
        "MI-clustering: Make C classes using the mutual information clustering algorithm with the merging",
        "region constraint mentioned in (2.1).",
        "2.",
        "Outer-clustering: Replace all words in the text",
        "with their class token2 and execute binary merging without the merging region constraint until all the classes are merged into a singe class.",
        "Make a dendrogram out of this process.",
        "This dendrogratn, Droo, constitutes the upper part of the final tree.",
        ":3.",
        "Inner-clustering: Let {C(.1), 0(2), ..., C(C)} be the set of the classes obtained at step I.",
        "For each i (I < i < C) do the following.",
        "(a) Replace all words in the text except those in 0(i) with their class token.",
        "Define a new vocabulary V' U where Vi = {all the words in",
        "C(i)}, V2 = {Ci C2, Cc }, and Ci is a token for 0(j) for I < j < C. Assign each element in V' to its own class and execute binary merging with a merging constraint such that only those classes which only contain elements of Vi can be merged.",
        "(b) Repeat merging until all the elements in V1 are put in a single class.",
        "Make a dendrograni /..),„,b out of the merging process for each class.",
        "This dendrogram constitutes a subtree for each class with a leaf node representing each word in the class.",
        "4.",
        "Combine the dendrograms by substituting each leaf node of D„01 with coresponding This algorithm produces a balanced binary tree representation of words in which those words which are close in meaning or syntactic feature come close in position.",
        "Figure 3 shows an example of 1.",
        "),„b for one class out of 500 classes constructed using this algorithm with a vocabulary of the 70,000 most frequently occurring words in the Wall Street Journal Corpus.",
        "Finally, by tracing the path from the root node to a leaf node and assigning a bit to each branch with zero or one representing a left or right branch, respectively, we can assign a bit-string (word bits) to each word in the vocabulary."
      ]
    },
    {
      "heading": "3 Experiments",
      "text": [
        "We used plain texts from six years of the WSJ Corpus to create word bits.",
        "The sizes of the texts are 5 million words (MW), 1.0MW, 20MW, and 50M W. The vocabulary is selected as the 70,000 most frequently occurring words in the entire corpus.",
        "We set the number C of classes to 500.",
        "The obtained hierarchical clusters are evaluated via the error rate of the A'I'R Decision-Tree Part-Of-Speech Tagger which is based on SPNEFER (Magerman 1994).",
        "The tagger employs a set of 443 syntactic tags.",
        "In the training phase, a set of events are extracted from the training texts.",
        "An event is a set of feature-value pairs or question-answer pairs.",
        "A feature can be any attribute of the context in which the current word word(0) appears; it is conveniently expressed as a question.",
        "Figure 4 shows an example of an event, with a current word \"like\".",
        "The last pair in the event is a special item which shows the answer, i.e., the correct tag of the current, word.",
        "The first three lines show questions about, identity of words around the current word and tags for previous words.",
        "These questions are called basic questions and always used.",
        "The second type of questions, word bits questions, are on clusters and word hits such as what is the 2911 bit of the previous word's word bits?.",
        "The third type of questions are called linguist's questions and these are compiled by an expert grammarian.",
        "Out of the set of events, a decision tree is constructed whose leaf nodes contain conditional probability distributions of tags, conditioned by the feature values.",
        "In the test phase the system looks up conditional probability distributions of tags for each word in the test text and chooses the most probable tag sequences using beam search.",
        "We used WSJ texts and the ATR, corpus (Black et al.",
        "1996) for the tagging experiment.",
        "Both corpora use the A'I'R syntactic tag set.",
        "Since the ATR, corpus is still in the process of development, the size of the texts we have at hand for this experiment is rather minimal considering the large size of the tag set.",
        "Table I shows the sizes of texts used for the experiment.",
        "Figure 5 shows the tagging error rates plotted against various clustering"
      ]
    },
    {
      "heading": "Clustering Text Size (Million Words)",
      "text": [
        "text sizes.",
        "Out of the three types of questions, basic questions and word bits questions are always used in this experiment.",
        "To see the effect of introducing word bits information into the tagger, we performed a separate experiment in winch a randomly generated bit-string is assigned to each wore and basic questions and word bits questions are used.",
        "The results are plotted at zero clustering text size.",
        "For both WSJ texts and ATR corpus, the tagging error rate dropped by more than 30% when using word bits information extracted from the 5MW text, and increasing the clustering text size further decreases the error rate.",
        "At 50MW, the error rate drops by 43%.",
        "This shows the improvement of the quality of the hierarchical clusters with increasing size of the clustering text.",
        "In Figure 5, introduction of linguistic questions4 is also shown to significantly reduce the error rates for the WSJ corpus.",
        "The dependency of the error rates on the clustering text size is quite similar to the case in which no linguistic questions are used, indicating the effectiveness of combin-3Since a distinctive bit-string is assigned to each word, the tagger also uses a bit-string as an ID number for each word in the process, In this control experiment bit-strings are assigned in a random way, but no two words are assigned the same word bits.",
        "Random word bits are expected to give no class information to the tagger except for the identity of words.",
        "'The linguistic questions we used here are still in the initial stage of development and are by no means comprehensive.",
        "ing automatically created word bits and handcrafted linguistic questions.",
        "Figure 5 also shows that reshuffling the classes several times just after step 1 (MI-clustering) of the word hits construction process further improves the word bits.",
        "One round of reshuffling corresponds to moving each word in the vocabulary from its original class to another class whenever the movement increases the A.M1, starting from the most frequent word through the least frequent one.",
        "The figure shows the error rates with zero, two, and five rounds of reshuffling'.",
        "Overall high error rates are attributed to the very large tag set and the small training set.",
        "Another notable point in the figure is that introducing word bits constructed from WSJ texts is as effective for tagging ATR, texts as it is for tagging WSJ texts even though these texts are from very different domains.",
        "To that extent, the obtained hierarchical clusters are considered to be portable across domains."
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "We presented an algorithm for hierarchical clustering of words, and conducted a clustering experiment using large texts of:varying sizes.",
        "High quality of the obtained clusters are confirmed by the POS tagging experiments.",
        "By introducing word bits into the ATR Decision-Tree POS Tagger, the tagging error rate is reduced by up to 43%.",
        "The hierarchical clusters obtained from WSJ texts are also shown to be useful for tagging ATR, texts which are from quite different domains than WSJ texts."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We thank John Lafferty for his helpful suggestions."
      ]
    }
  ]
}
