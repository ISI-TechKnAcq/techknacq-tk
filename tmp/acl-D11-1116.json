{
  "info": {
    "authors": [
      "Stephen Tratz",
      "Eduard Hovy"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1116",
    "title": "A Fast, Accurate, Non-Projective, Semantically-Enriched Parser",
    "url": "https://aclweb.org/anthology/D11-1116",
    "year": 2011
  },
  "references": [
    "acl-A00-2018",
    "acl-A97-2005",
    "acl-C96-1058",
    "acl-D07-1096",
    "acl-D07-1101",
    "acl-D09-1058",
    "acl-E06-1011",
    "acl-H05-1066",
    "acl-J02-3001",
    "acl-J05-1004",
    "acl-J93-2004",
    "acl-J96-1002",
    "acl-N07-1051",
    "acl-N09-1073",
    "acl-N09-3017",
    "acl-N10-1115",
    "acl-N10-1138",
    "acl-P05-1013",
    "acl-P05-1022",
    "acl-P06-1033",
    "acl-P06-1055",
    "acl-P07-1031",
    "acl-P08-1068",
    "acl-P09-1040",
    "acl-P10-1001",
    "acl-P10-1070",
    "acl-P10-1110",
    "acl-P98-1013",
    "acl-W04-2705",
    "acl-W06-2920",
    "acl-W07-2005",
    "acl-W07-2018",
    "acl-W08-1301",
    "acl-W08-2121",
    "acl-W09-1201",
    "acl-W09-3811"
  ],
  "sections": [
    {
      "text": [
        "Stephen Tratz and Eduard Hovy",
        "Information Sciences Institute University of Southern California Marina del Rey, California 90292",
        "Dependency parsers are critical components within many NLP systems.",
        "However, currently available dependency parsers each exhibit at least one of several weaknesses, including high running time, limited accuracy, vague dependency labels, and lack of non-projectivity support.",
        "Furthermore, no commonly used parser provides additional shallow semantic interpretation, such as preposition sense disambiguation and noun compound interpretation.",
        "In this paper, we present a new dependency-tree conversion of the Penn Treebank along with its associated fine-grain dependency labels and a fast, accurate parser trained on it.",
        "We explain how a non-projective extension to shift-reduce parsing can be incorporated into non-directional easy-first parsing.",
        "The parser performs well when evaluated on the standard test section of the Penn Treebank, outperforming several popular open source dependency parsers; it is, to the best of our knowledge, the first dependency parser capable of parsing more than 75 sentences per second at over 93% accuracy."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Parsers are critical components within many natural language processing (NLP) systems, including systems for information extraction, question answering, machine translation, recognition of textual entailment, summarization, and many others.",
        "Unfortunately, currently available dependency parsers suffer from at least one of several weaknesses including high running time, limited accuracy, vague dependency labels, and lack of non-projectivity support.",
        "Furthermore, few parsers include any sort of additional semantic interpretation, such as interpretations for prepositions, possessives, or noun compounds.",
        "In this paper, we describe 1) a new dependency conversion (Section 3) of the Penn Treebank (Marcus, et al., 1993) along with the associated dependency label scheme, which is based upon the Stanford parser's popular scheme (de Marneffe and Manning, 2008), and a fast, accurate dependency parser with non-projectivity support (Section 4) and additional integrated semantic annotation modules for automatic preposition sense disambiguation and noun compound interpretation (Section 5).",
        "We show how Nivre's (2009) swap-based reordering technique for non-projective shift-reduce-style parsing can be integrated into the non-directional easy-first framework of Goldberg and Elhadad (2010) to support non-projectivity, and we report the results ofour parsing experiments on the standard test section of the PTB, providing comparisons with several freely available parsers, including Goldberg and Elhadad's (2010) implementation, MALTPARSER (Nivre et al., 2006), MSTParser (McDonald et al., 2005; McDonald and Pereira, 2006), the Charniak (2000) parser, and the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007).",
        "The experimental results show that the parser is substantially more accurate than Goldberg and El-hadad's original implementation, with fairly similar overall speed.",
        "Furthermore, the results prove that Stanford-granularity dependency labels can be learned by modern dependency parsing systems when using our Treebank conversion, unlike the Stanford conversion, for which Cer et al.",
        "(2010) show that this isn't the case.",
        "The optional semantic annotation modules also perform well, with the preposition sense disambiguation module exceeding the accuracy of the previous best reported result for fine-grained preposition sense disambiguation (85.7% vs Hovy et al.",
        "'s (2010) 84.8%), the possessives interpretation system achieving over 85% accuracy, and the noun compound interpretation system performing similarly to an earlier version described by Tratz and Hovy (2010) at just over 79% accuracy."
      ]
    },
    {
      "heading": "2. Background",
      "text": [
        "The NLP community has recently seen a surge of interest in dependency parsing, with several CoNLL shared tasks focusing on it (Buchholz and Marsi, 2006; Nivre et al., 2007).",
        "One of the main advantages of dependency parsing is the relative ease with which it can handle non-projectivity.",
        "Additionally, since each word is linked directly to its head via a link that, ideally, indicates the syntactic dependency type, there is no difficulty in determining either the syntactic head of a particular word or the syntactic relation type, whereas these issues often arise when dealing with constituent parses.",
        "Unfortunately, most currently available dependency parsers produce relatively vague labels or, in many cases, produce no labels at all.",
        "While the Stanford fine-grain dependency scheme (de Marn-effe and Manning, 2008) has proven to be popular, recent experiments by Cer et al.",
        "(2010) using the Stanford conversion of the Penn Treebank indicate that it is difficult for current dependency parsers to learn.",
        "Indeed, the highest scoring parsers trained using the MSTParser (McDonald and Pereira, 2006) and MaltParser (Nivre et al., 2006) parsing suites achieved only 78.8 and 81.1 labeled attachment Fi, respectively.",
        "This contrasted with the much higher performance obtained using a constituent-to-dependency conversion approach with accurate, but much slower, constituency parsers such as the Char-niak and Johnson (2005) and Berkeley (Petrov et al., 2006; Petrov and Klein, 2007) parsers, which achieved 89.1 and 87.9 labeled Fi scores, respectively.",
        "Though there are many syntactic parsers than can reconstruct the grammatical structure of a text, there are few, if any, accurate and widely accepted systems that also produce shallow semantic analysis of the text.",
        "For example, a parser may indicate that, in the case of 'ice statue', 'ice' modifies 'statue' but will not indicate that 'ice' is the substance of the statue.",
        "Similarly, a parser will indicate which words a preposition connects but will not give any semantic interpretation (e.g., 'the boy with the pirate hat' – wearing or carrying, 'wash with cold water' – means, 'shave with the grain' – in the same direction as).",
        "While, in some cases, it may be possible to use the output from a separate system for this purpose, doing so is often difficult in practice due to a wide variety of complications, including programming language differences, alternative data formats, and, sometimes, other parsers."
      ]
    },
    {
      "heading": "3. Dependency Conversion",
      "text": [
        "Most recent English dependency parsers produce one of three sets of dependency types: unlabeled, some variant of the coarse labels used by the CoNLL dependency parsing shared-tasks (Buchholz and Marsi, 2006; Nivre et al., 2007) (e.g., ADV, NMOD, PMOD), or Stanford's dependency labels (de Marneffe and Manning, 2008).",
        "Unlabeled dependencies are clearly too impoverished for many tasks.",
        "Similarly, the coarse labels of the CoNLL tasks are not very specific; for example, the same relation, NMOD, is used for determiners, adjectives, nouns, participle modifiers, relative clauses, etc.",
        "that modify nouns.",
        "In contrast, the Stanford relations provide a more reasonable level of granularity.",
        "Our dependency relation scheme is similar to Stanford's basic scheme but has several differences.",
        "It introduces several new relations including ccinit \"initial coordinating conjunction\", cleft \"cleft clause\", combo \"combined term\", extr \"extraposed element\", infmark \"infinitive marker 'to' \", objcomp \"object complement\", postloc \"post-modifying location\", sccomp \"clausal complement of 'so' \", vch \"verbal chain\" and whadvmod \"wh- adverbial modifier\".",
        "The nsubjpass, csubjpass, and auxpass relations of Stanford's are left out because adding them up front makes learning more difficult and the fact",
        "Table 1: Dependency scheme with differences versus basic Stanford dependencies highlighted.",
        "Bold indicates the relation does not exist in the Stanford scheme.",
        "Italics indicate the relation appears in Stanford's scheme but not ours.",
        "that a nsubj, csubj, or aux is passive can easily be determined from the final tree.",
        "Stanford's aux dependencies are replaced using verbal chain (vch) links; conversion of these to Stanford-style aux dependencies is also trivial as a post-processing step.",
        "The attr dependency is excluded because it is redundant with the cop relation due to different handling of copula, and the dependency scheme does not have an abbrev label because this information is not provided by the Penn Treebank.",
        "The dependency scheme with differences with Stanford highlighted is presented in",
        "Table 1.",
        "Figure 1: Example comparing Stanford's (top) handling of copula and coordinating conjunctions with ours (bottom).",
        "In addition to using a slightly different set of dependency names, a handful of relations, notably cop, conj, and cc, are treated in a different manner.",
        "These differences are illustrated by Figure 1.",
        "The Stanford scheme's treatment of copula may be one reason why dependency parsers have trouble learning and applying it.",
        "Normally, the head of the clause is a verb, but, under Stanford's scheme, if the verb happens to be a copula, the complement of the copula (cop) is treated as the head of the clause instead.",
        "A three-step process is used to convert the Penn Treebank (Marcus, et al., 1993) from constituent parses into dependency trees labeled according to the dependency scheme presented in the prior section.",
        "The first step is to apply the noun phrase structure patch created by Vadas and Curran (2007), which adds structure to the otherwise flat noun phrases (NPs) of the Penn Treebank (e.g., '(metal soup pot cover)' would become '(metal (soup pot) cover)').",
        "The second step is to apply a version of Johansson and Nugues' (2007) constituent-to-dependency converter with some head-finding rule modifications; these rules, with changes highlighted",
        "abbrev",
        "abbreviation",
        "csubjpass",
        "clausal subject (passive)",
        "pobj",
        "prepositional object",
        "acomp",
        "adjectival complement",
        "det",
        "determiner",
        "poss",
        "possessive",
        "advcl",
        "adverbial clause",
        "dobj",
        "direct object",
        "possessive",
        "possessive marker",
        "advmod",
        "adverbial modifier",
        "extr",
        "extraposed element",
        "postloc",
        "post-modifying location",
        "agent",
        "'by' agent",
        "expl",
        "'there' expletive",
        "preconj",
        "pre conjunct",
        "amod",
        "adjectival modifier",
        "infmark",
        "infinitive marker ('to')",
        "predet",
        "predeterminer",
        "appos",
        "appositive",
        "infmod",
        "infinite modifier",
        "prep",
        "preposition",
        "attr",
        "attributive",
        "iobj",
        "indirect object",
        "prt",
        "particle",
        "aux",
        "auxillary",
        "mark",
        "subordinate clause marker",
        "punct",
        "punctuation",
        "auxpass",
        "auxillary (passive)",
        "measure",
        "measure modifier",
        "purpcl",
        "purpose clause",
        "cleft",
        "cleft clause",
        "neg",
        "negative",
        "quantmod",
        "quantifier modifier",
        "cc",
        "coordination",
        "nn",
        "noun compound",
        "rcmod",
        "relative clause",
        "ccinit",
        "initial CC",
        "nsubj",
        "nominal subject",
        "rel",
        "relative",
        "ccomp",
        "clausal complement",
        "nsubjpass",
        "nominal subject (passive)",
        "sccomp",
        "clausal complement of 'so'",
        "combo",
        "combination term",
        "num",
        "numeric modifier",
        "tmod",
        "temporal modifier",
        "compl",
        "complementizer",
        "number",
        "compound number",
        "vch",
        "verbal chain",
        "conj",
        "conjunction",
        "objcomp",
        "object complement",
        "whadvmod",
        "wh- adverbial",
        "cop",
        "copula complement",
        "parataxis",
        "parataxis",
        "xcomp",
        "clausal complement w/o subj",
        "csubj",
        "clausal subject",
        "partmod",
        "participle modifier",
        "(WH)?NPlNXlNMLlNAC FWlNMLlNN* JJR $]# CDlFW QP JJlNAC JJS PRP ADJP RB[SR] VBGlDTlWP",
        "RB NP-e SlSBARlUCPlPP SINVlSBARQlSQ UH VPlNP VBlVBP",
        "ADJPlJJP NNS QP NN $l# JJ VBN VBG (ADlJ)JP ADVP JJR NPlNML JJS DT FW RBR RBS SBAR RB",
        "ADVP RBlRBRlJJlJJR RBS FW ADVP TO CD IN NPlNML JJS NN",
        "PRN S* VP NN*|NX|NML NP W* PPlIN ADJPlJJ ADVP RB NAC VP INTJ",
        "QP $l# NNS NN CD JJ RB DT NCD QP IN CC JJR JJS",
        "SBARQ SQ S SBARQ SINV FRAG",
        "SQ VBZ VBD VBP VB MD *-PRD SQ VP FRAG X",
        "UCP [QNVP]PlS*lUCPl NMLl PR[NT]lRRClNXlNAClFRAGlINTJlAD[JV]Pl LSTlWH*lX",
        "VP VBDlAUX VBN MD VBZ VB VBG VBP VP POS *-PRD ADJP JJ NN NNS NPlNML",
        "WHADJP CC JJ WRB ADJP",
        "WHADVP CC WRBlRB",
        "X [QNVP]PlS*lUCPl NMLl PR[NT]lRRClNXlNAClFRAGlINTJlAD[JV]Pl LSTlWH*lXlCONJP",
        "LST LS : DTlNNlSYM",
        "Figure 2: Modified head-finding rules.",
        "Underline indicates that the search is performed in a left-to-right fashion instead of the default right-to-left order.",
        "NML and JJP are both products of Vadas and Curran's (2007) patch.",
        "Bold indicates an added or moved element; for the original rules, see the paper by Johansson and Nugues (2007).",
        "in bold, are provided in Figure 2.",
        "Finally, an additional script makes additional changes and converts the intermediate output into the dependency scheme.",
        "This dependency conversion has several advantages to it.",
        "Using the modified head-finding rules for Johansson and Nugues' (2007) converter results in fewer buggy trees than were present in the CoNLL shared tasks, including fewer trees in which words are headed by punctuation marks.",
        "For sections 221, there are far fewer generic dep/DEP relations (2,765) than with the Stanford conversion (34,134) or the CoNLL 2008 shared task conversion (23,811).",
        "Also, the additional conversion script contains various rules for correcting part-of-speech (POS) errors using the syntactic structure as well as additional rules for some specific word forms, mostly common words with inconsistent taggings.",
        "Many of these changes cover part-of-speech problems discussed by Manning (2011), including VBD/VBN, VBZ/NNS, NNP/NNPS, and IN/WDT/DT issues.",
        "In total, the script changes over 9,500 part-of-speech tags, with the most common change being to change preposition tags (IN) into adverb tags (RB) for cases where there is no prepositional complement/object.",
        "The top fifteen of these changes are presented in Table 2.",
        "The conversion script contains a variety of additional rules for modifying the parse structure and fixing erroneous trees as well, including cases where one or more POS tags were incorrect and, as such, the initial dependency parse was flawed.",
        "Quick manual inspections of the changes suggested that the vast majority are accurate.",
        "In the final output from the conversion, the number of sentences with one or more words dependent on non-projective arcs in sections 2-21 is 3,245 – about 8.1% of the dataset.",
        "About 1.3% of this, or 556 of sentences, is due to the secondary conversion script, with sentences containing approximate currency amounts (e.g., about $ 10) comprising the bulk of difference.",
        "For these, the quantifying text (e.g., about, over, nearly), is linked to the number following the currency symbol instead of to the currency symbol as it was in the CoNLL 2008 task.",
        "Original",
        "New",
        "# of changes",
        "IN",
        "RB",
        "1128",
        "JJ",
        "NN",
        "787",
        "VBD",
        "VBN",
        "601",
        "RB",
        "IN",
        "462",
        "VBN",
        "VBD",
        "441",
        "NN",
        "JJ",
        "409",
        "NNPS",
        "NNP",
        "405",
        "IN",
        "WDT",
        "388",
        "VBG",
        "NN",
        "223",
        "DT",
        "IN",
        "220",
        "RB",
        "JJ",
        "214",
        "VB",
        "VBP",
        "184",
        "NN",
        "NNS",
        "169",
        "RB",
        "NN",
        "157",
        "NNS",
        "VBZ",
        "148"
      ]
    },
    {
      "heading": "4. Parser 4.1 Algorithm",
      "text": [
        "The parsing approach is based upon the non-directional easy-first algorithm recently presented by Goldberg and Elhadad (2010).",
        "Their original algorithm behaves as follows.",
        "For a sentence oflength n, the algorithm performs a total of n steps.",
        "In each step, one of the unattached tokens is added as a child to one of its current neighbors and is then removed from the list of unprocessed tokens.",
        "When only one token remains unprocessed, it is designated as the root.",
        "Provided that only a constant number of potential attachments need to be re-evaluated after each step, which is the case if one restricts the context for feature generation to a constant number of neighboring tokens, the algorithm can be implemented to run in O(n log n).",
        "However, since only O(n) dot products must be calculated by the parser and these have a large constant associated with them, the running time will rival O(n) parsers for any reasonable n, and, thus, a naive O(n) implementation will be nearly as fast as a priority queue implementation in practice.",
        "The algorithm has a couple potential advantages over standard shift-reduce style parsing algorithms.",
        "The first advantage is that performing easy actions first may make the originally difficult decisions easier.",
        "The second advantage is that performing parse actions in a more flexible order than left-to-right/right-to-left shift-reduce parsing reduces the chance of error propagation.",
        "Unfortunately, the original algorithm does not support non-projective trees.",
        "To extend the algorithm to support non-projective trees, we introduce move-right and move-left operations similar to the stack-to-buffer swaps proposed by Nivre (2009) for shift-reduce style parsing.",
        "Thus, instead of attaching a token to one of its neighbors at each step, the algorithm may instead decide to move a token past one of its neighbors.",
        "Provided that no node is allowed to be moved past a token in such a way that a previous move operation is undone, there can be at most O(n) moves and the overall worst-case complexity becomes O(n log n).",
        "While theoretically slower, this has a limited impact upon actual parsing times in practice, especially for languages with relatively fixed word order such as English.",
        "Though Goldberg and Elhadad's (2010) original implementation only supports unlabeled dependencies, the algorithm itself is in no way limited in this regard, and it is simple enough to add labeled dependency support by treating each dependency label as a specific type of attach operation (e.g., attach_as_nsubj), which is the method used by this implementation.",
        "Pseudocode for the non-directional easy-first algorithm with non-projective support is given in Algorithm 1.",
        "input : w\\ ... wn, #the sentence m, #the model k, #the context width actions, #the list of parse actions </>, #the feature generator output: tree #a collection of dependency arcs words = copyOf(s) ; stale = copyOf (s); cache; #cache of action scores while lwordsl > 1 do for w G stale do for act G actions do",
        "stale.remove(w) ; best = arg max cache[w,a\\",
        "a G actions&valid(a) ,w Swords",
        "if isMove(best) then",
        "words.index(getTokenToMove(best)) ; words.move (i, isMoveLeft(best) ?",
        "-1 arc = createArc(best) ; tree.add(arc) ; i = words.index(getChild( arc)) ; words.remove(i) ; for x G -k,...,k do",
        "stale.add(words.get(index+x)) ; return tree",
        "Algorithm 1: Modified version of Goldberg and Elhadad's (2010) Easy-First Algorithm with non-projective support.",
        "One of the key aspects of the parser is the complex set of features used.",
        "The feature set is based off the features used by Goldberg and Elhadad (2010) but has a significant number of extensions.",
        "Various feature templates are specifically designed to produce features that help with several syntactic issues including preposition attachment, coordination, adverbial clauses, clausal complements, and relative clauses.",
        "Unfortunately, there is insufficient space in this paper to describe them all here.",
        "However, a list of feature templates will be provided with the parser download.",
        "Several of the feature templates use unsupervised word clusters created with the Brown et al.",
        "(1992) hierarchical clustering algorithm.",
        "The use of this algorithm was inspired by Koo et al.",
        "(2008), who used the top branches of the cluster hierarchy as features.",
        "However, unlike Koo et al.",
        "'s (2008) parser, the finegrained cluster identifiers are used instead of just the top 4-6 branches of the cluster hierarchy.",
        "The 175 word clusters utilized by the parser were created from the New York Times corpus (Sandhaus, 2008).",
        "Some examples from the clusters are presented in Figure 3.",
        "The ideal number of such clusters was not thoroughly investigated.",
        "while where when although despite unless unlike ... why what whom whatever whoever whomever whence ... based died involved runs ended lived charged born ... them him me us himself themselves herself myself ... really just almost nearly simply quite fully virtually ... know think thought feel believe knew felt hope mean ... into through on onto atop astride Saturday/Early thru ... Ms. Mr. Dr. Mrs. Judge Miss Professor Officer Colonel... John President David J. St. Robert Michael James George ... wife own husband brother sister grandfather beloved ... often now once recently sometimes clearly apparently ... everyone it everybody somebody anybody nobody hers ... around over under among near behind outside across ... Clinton Bush Johnson Smith Brown Williams King ... children companies women people men things students ...",
        "The parsing model is trained using a variant of the structured perceptron training algorithm used in the original Goldberg and Elhadad (2010) implementation.",
        "The general idea of the algorithm is to iterate over the sentences and, whenever the model predicts an incorrect action, update the model weights.",
        "Following Goldberg and Elhadad, parameter averaging is used to reduce overfitting.",
        "Our implementation varies slightly from that of Goldberg and Elhadad (2010).",
        "The difference is that, at any particular step for a given sentence, the algorithm continues to update the weight vector as long as any invalid action is scored higher than any valid action, not just the highest scoring valid action; unfortunately, this change significantly slowed down the training process.",
        "In early experiments, this change produced a slight improvement in accuracy though it also slowed training significantly.",
        "In later experiments using additional feature templates, this change ceased to have any notable impact on the overall accuracy, but it was kept anyway.",
        "The oracle used to determine whether a move operation should be considered legal during the training phase is similar to Nivre et al.",
        "'s (2009) improved oracle based upon maximal projective subcomponents.",
        "As an additional restriction, during training, move actions were only considered valid either if no other action was valid or if the token to be moved already had all its children attached and moving it caused it to be adjacent to its parent.",
        "This fits with Nivre et al.",
        "'s (2009) intuition that it is best to delay word reordering as long as possible.",
        "To enhance the speed for practical use, the parser uses constraints based upon the part-of-speech tags of the adjacent word pairs to eliminate invalid dependencies from even being evaluated.",
        "A relation is only considered between a pair of words if such a relation was observed in the training data between a pair of words with the same parts-of-speech (with the exception of the generic dep dependency, which is permitted between any POS tag pair).",
        "Early experiments utilizing similar constraints showed an improvement in parsing speed of about 16% with no significant impact on accuracy, regardless of whether the constraints were enforced during training.",
        "Table 3: Parsing results for section 23 of the Penn Treebank (punctuation excluded).",
        "Results in parentheses were produced using gold POS tags.",
        "^Eisner (1996) algorithm with non-projective rewriting and second order features.",
        "* Results not directly comparable; see text.",
        "*Labeled dependencies not available/comparable.",
        "The following split of the Penn Treebank (Marcus, et al., 1993) was used for the experiments: sections 2-21 for training, 22 for development, and 23 for testing.",
        "For part-of-speech (POS) tagging, we used an in-house SVM-based POS tagger modeled after the work of Giménez and Marquez (2004) .",
        "The training data was tagged in a 10-fold fashion; each fold was tagged using a tagger trained from the nine remaining folds.",
        "The development and test sections were tagged by an instance of the tagger trained using the entire training set.",
        "The full details of the POS tagger are outside the scope of this paper; it is included with the parser download.",
        "The final parser was trained for 31 iterations, which is the point at which its performance on the development set peaked.",
        "One test run was performed with non-projectivity support disabled in order to get some idea of the impact of the move operations on the parser's overall performance; also, since the parsers used for comparison had no access to the unsupervised word clusters, an additional instance of the parser was trained with every word treated as belonging to the same cluster so as to facilitate a more fair comparison.",
        "Seven different dependency parsing models were trained for comparison using the following open source parsing packages: Goldberg and Elhadad's (2010)'s non-directional easy-first parser, Malt-Parser (Nivre et al., 2006), and MSTParser (McDonald and Pereira, 2006).",
        "The model trained using Goldberg and Elhadad's (2010) easy-first parser serves as something of a baseline.",
        "The four MaltParser parsing models used the arc-eager, arc-standard, stack-eager, and stack-lazy algorithms.",
        "One of the MSTParser models used the Chu-Liu-Edmonds maximum spanning tree approach, and the other used the Eisner (1996) algorithm with second order features and a non-projective rewriting post-processing step.",
        "Unfortunately, it is not possible to directly compare the parser's accuracy with most popular constituent parsers such as the Charniak (2000) and Berkeley (Petrov et al., 2006; Petrov and Klein, 2007) parsers both because they do not produce functional tags for subjects, direct objects, etc., which are required for the final script of the constituent-to-dependency conversion routine, and because they determine part-of-speech tags in conjunction with the parsing.",
        "However, it is possible to compute approximate unlabeled accuracy scores by training the constituent parsers on the NP-patched (Vadas and Curran, 2007) version of the data and then running the test output through just the first conversion script – that is, the modified version of Johansson and Nugues' (2007) converter.",
        "System",
        "Arc Accuracy",
        "Perfect Sentences",
        "Non-Proj Arcs",
        "Labeled",
        "Unlabeled",
        "Labeled",
        "Unlabeled",
        "Labeled",
        "Unlabeled",
        "This Work",
        "This Work„0 clusters This WoRKm0ves disabled",
        "92.1 (93.3)",
        "93.7 (94.3)",
        "38.4 (42.5)",
        "46.2 (48.5)",
        "66.5 (69.7)",
        "69.3 (71.7)",
        "91.8 (93.1)",
        "93.4 (94.1)",
        "38.2 (42.3)",
        "45.5 (47.3)",
        "67.3 (70.9)",
        "69.3 (72.5)",
        "91.7 (92.9)",
        "93.3 (93.9)",
        "37.1 (40.8)",
        "44.2 (46.2)",
        "21.1 (21.1)",
        "22.7 (21.9)",
        "Non-Dir Easy First",
        "*",
        "91.2 (92.0)",
        "*",
        "37.8 (39.4)",
        "*",
        "15.1 (16.3)",
        "ElSNERtMST",
        "Chu-Liu-Edmondsmst",
        "90.9 (92.2)",
        "92.8 (93.5)",
        "32.1 (35.6)",
        "40.6 (42.3)",
        "62.5 (65.3)",
        "63.7 (66.9)",
        "90.0 (91.2)",
        "91.8 (92.5)",
        "28.4 (31.3)",
        "35.0 (36.4)",
        "62.9 (65.3)",
        "64.1 (66.5)",
        "ARC-EAGERMalt ARC-STANDARDMaltSTACK-EAGERMalt STACK-LAZYMalt",
        "89.8 (91.1)",
        "91.3 (92.1)",
        "31.6 (34.2)",
        "37.4 (38.5)",
        "19.5 (19.5)",
        "20.3 (19.9)",
        "88.3 (89.5)",
        "89.7 (90.4)",
        "31.4 (34.1)",
        "36.1 (37.3)",
        "13.1 (12.0)",
        "13.9 (12.7)",
        "90.0 (91.2)",
        "91.5 (92.3)",
        "34.5 (37.5)",
        "40.4(41.9)",
        "51.8 (53.8)",
        "53.8 (55.4)",
        "90.4 (91.7)",
        "91.9 (92.8)",
        "34.8 (37.7)",
        "40.6 (42.5)",
        "61.8 (63.3)",
        "63.3 (65.3)",
        "Charniak*",
        "*",
        "93.2",
        "*",
        "43.5",
        "*",
        "32.3",
        "Berkeley*",
        "*",
        "93.3",
        "*",
        "43.6",
        "*",
        "34.3",
        "The results of the experiment are given in Table 3, including accuracy for individual arcs, non-projective arcs only, and full sentence match.",
        "Punctuation is excluded in all the result computations.",
        "To determine whether an arc is non-projective, the following heuristic was used.",
        "Traverse the sentence in a depth-first search, starting from the imaginary root node and pursuing child arcs in order of increasing absolute distance from their parent.",
        "Whenever an arc being traversed is found to cross a previously traversed arc, mark it as non-projective and continue.",
        "To evaluate the impact of part-of-speech tagging error, results for parsing using the gold standard part-of-speech tags are also included.",
        "We also measured the speed of the parser on the various sentences in the test collection.",
        "For reasonable sentence lengths, the parser scales quite well.",
        "The scatterplot depicting the relation between sentence length and parsing time is presented in Figure 5.",
        "Charniak Berkeley Eisner C-L-E G & E This Work",
        "Figure 4: Parse times for Penn Treebank section 23 for the parsers on a PC with a 2.4Ghz Q6600 processor and 8GB RAM.",
        "MALTPARSER ran substantially slower than the others, perhaps due to its use of polynomial kernels, and isn't shown.",
        "(C-L-E - Chu-Liu-Edmonds, G&E -",
        "Goldberg and Elhadad (2010)).",
        "The parser achieves 92.1% labeled and 93.7% un-labeled accuracy on the evaluation, a solid result and about 2.5% higher than the original easy-first implementation of Goldberg and Elhadad (2010).",
        "Furthermore, the parser processed the entire test section in just over 30 seconds – a rate of over 75 sentences per second, substantially faster than most of the other parsers.",
        "Not surprisingly, the results for non-projective arcs are substantially lower than the results for all arcs, and the systems that are designed to handle them outperformed the strictly projective parsers in this regard.",
        "The negative effect of part-of-speech tagging error appears to impact the different parsers about the same amount, with a loss of .6% to .8% in unlabeled accuracy and 1.1% to 1.3% in labeled accuracy.",
        "The 93.2% and 93.3% accuracy scores achieved by the Charniak and Berkeley parsers are not too different from the 93.7% result, but, of course, it is important to remember that these scores are not directly comparable.",
        "sentence length"
      ]
    },
    {
      "heading": "5. Shallow Semantic Annotation",
      "text": [
        "To create a more informative parse, the parser includes four optional modules, a preposition sense disambiguation (PSD) system, a work-in-progress 's-possessive interpretation system, a noun compound interpretation system, and a PropBank-based semantic role labeling system.",
        "Taken together, these integrated modules enable the parsing system to produce substantially more informative output than a traditional parser.",
        "Preposition Sense Disambiguation The PSD system is a newer version of the system described by Tratz and Hovy (2009) and Hovy et al.",
        "(2010); it achieves 85.7% accuracy on the SemEval-2007 fine-grain PSD task (Litkowski and Hargraves, 2007), which is a statistically significant (p<=0.05; upper-tailed z test) increase over the previous best reported result for this dataset, Hovy et al.",
        "'s (2010) 84.8%.",
        "Noun Compound Interpretation The noun compound interpretation system is a newer version of the system described by Tratz and Hovy (2010) with similar accuracy (79.6% vs 79.3% using 10-fold cross-validation).",
        "Possessives Interpretation The possessive interpretation system assigns interpretations to 's possessives (e.g., John's arm – PART-OF, Mowgli's capture – PATIENT/THEME).",
        "The current system achieves over 85.0% accuracy, but it is important to note that the annotation scheme, automatic classifier, and dataset are all still under active development.",
        "PropBank SRL The PropBank-based semantic role labeling system achieves 86.8 combined Fi measure for automatically-generated parse trees calculated over both predicate disambiguation and argument/adjunct classification (89.5 F1 on predicate disambiguation, 85.6 Fi on argument and adjuncts corresponding to dependency links, and 86.8 F1); this score is not directly comparable to any previous work due to some differences, including differences in both the parse tree conversion and the Prop-Bank conversion.",
        "The most similar work is that of the CoNLL shared task work (Surdeanu et al., 2008; Hajic et al., 2009)."
      ]
    },
    {
      "heading": "6. Related Work",
      "text": [
        "Non-projectivity.",
        "There are two main approaches used in recent NLP literature for handling non-projectivity in parse trees.",
        "The first is to use an algorithm, like the one presented in this paper, that has inherent support for non-projective trees.",
        "Examples of this include the Chu-Liu-Edmonds' approach for maximum spanning tree (MST) parsing based reordering method for shift-reduce parsing.",
        "The second approach is to create an initial projective parse and then apply transformations to introduce non-projectivity into it.",
        "Examples of this include McDonald and Pereira's (2006) rewriting of projective trees produced by the Eisner (1996) algorithm, and Nivre and Nilsson's (2005) pseudo-projective approach that creates projective trees with specially marked arcs that are later transformed into non-projective dependencies.",
        "Descriptive dependency labels.",
        "While most recent dependency parsing research has used either vague labels, such as those of the CoNLL shared tasks, or no labels at all, some descriptive dependency label schemes exist.",
        "By far the most prominent of these is the Stanford typed dependency scheme (de Marneffe and Manning, 2008).",
        "Another descriptive scheme that exists, but which is less widely used in the NLP community, is the one used by Tapanainen and Järvinen's parser (1997).",
        "Unfortunately, the Stanford dependency conversion of the Penn Treebank has proven difficult to learn for current dependency parsers (Cer et al., 2010), and there is no publicly available dependency conversion according to Tapanainen and Järvinen's scheme.",
        "Faster parsing.",
        "While the fastest reasonable parsing algorithms are the O(n) shift-reduce algorithms, such as Nivre's (2003) algorithm and an expected linear time dynamic programming approach presented by Huang and Sagae (2010), a few other fast alternatives exist.",
        "Goldberg and Elhadad's (2010) easy-first algorithm is one such example.",
        "Another example, is Roark and Hollingshead's (2009) work that uses chart constraints to achieve linear time complexity for constituency parsing.",
        "Effective features for parsing.",
        "A variety of work has investigated the use of more informative features for parsing.",
        "This includes work that integrates second and even third order features (McDonald et al., 2006; Carreras, 2007; Koo and Collins, 2010).",
        "Also, some work has incorporated unsupervised word clusters as features, including that ofKoo unsupervised word clusters created using the Brown et al.",
        "(1992) hierarchical clustering algorithm.",
        "Semantically-enriched output.",
        "The 2008 and",
        "Hajic et al., 2009), which required participants to build systems capable of both syntactic parsing and Semantic Role Labeling (SRL) (Gildea and Juraf-sky, 2002), are the most notable attempts to encourage the development of parsers with additional semantic annotation.",
        "These tasks relied upon Prop-",
        "Bank (2005) and NomBank (2004) for the semantic roles.",
        "A variety of other systems have focused on FrameNet-based (1998) SRL instead, including those that participated in the SemEval-2007 Task 19 (Baker et al., 2007) and work by Das et al.",
        "(2010)."
      ]
    },
    {
      "heading": "7. Conclusion",
      "text": [
        "In this paper, we have described a new high-quality dependency tree conversion of the Penn Treebank (Marcus, et al., 1993) along with its labeled dependency scheme and presented a parser that is fast, accurate, supports non-projective trees and provides rich output, including not only informative dependency labels similar to Stanford's but also additional semantic annotation for prepositions, possessives, and noun compound relations.",
        "We showed how the easy-first algorithm of Goldberg and Elhadad (Goldberg and Elhadad, 2010) can be extended to support non-projective trees by adding move actions similar to Nivre's (2009) swap-based reordering for shift-reduce parsing and evaluated our parser on the standard test section of the Penn Treebank, comparing with several other freely available parsers.",
        "The Penn Treebank conversion process fixes a number of buggy trees and part-of-speech tags and produces dependency trees with a relatively small percentage of generic dep dependencies.",
        "The experimental results show that dependency parsers can generally produce Stanford-granularity labels with high accuracy when using the new dependency conversion of the Penn Treebank, something which, according to the findings of Cer et al.",
        "(2010), does not appear to be the case when training and testing dependency parsers on the Stanford conversion.",
        "The parser achieves high labeled and unlabeled accuracy in the evaluation, 92.1% and 93.7%, respectively.",
        "The 93.7% result represents a 2.5% increase over the accuracy of Goldberg and Elhadad's (2010) implementation.",
        "Also, the parser proves to be quite fast, processing section 23 of the Penn Treebank in just over 30 seconds (a rate of over 75 sentences per second).",
        "The parsing system is capable of not only producing fine-grained dependency relations, but can also produce shallow semantic annotations for prepositions, possessives, and noun compounds by using several optional integrated modules.",
        "The preposition sense disambiguation (PSD) module achieves 85.7% accuracy on the SemEval-2007 PSD task, exceeding the previous best published result of 84.8% by a statistically significant margin, the possessives module is over 85% accurate, the noun compound interpretation module achieves 79.6% accuracy on Tratz and Hovy's (2010) dataset.",
        "The PropBank SRL module achieves 89.5 F1 on predicate disambiguation and 85.6 F1 on argument and adjuncts corresponding to dependency links, for an overall F1 of 86.8.",
        "Combined with the core parser, these modules allow the system to produce a substantially more informative textual analysis than a standard parser."
      ]
    },
    {
      "heading": "8. Future Work",
      "text": [
        "There are a variety of ways to extend and improve upon this work.",
        "We would like to change our handling of coordinating conjunctions to treat the coordinating conjunction as the head because this has fewer ambiguities than the current approach and also add the ability to produce traces for WH- words.",
        "It would also be interesting to examine the impact on final parsing accuracy of the various differences between our dependency conversion and Stanford's.",
        "To aid future NLP research work, the code, including the treebank converter, part-of-speech tagger, parser, and semantic annotation add-ons, will be made publicly available for download via http://www.isi.edu."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We would like to thank Richard Johansson for providing us with the code for the pennconverter consituent-to-dependency converter.",
        "We would also like to thank Dirk Hovy and Anselmo Penas for many valuable dicussions and suggestions."
      ]
    }
  ]
}
