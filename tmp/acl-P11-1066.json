{
  "info": {
    "authors": [
      "Guangyou Zhou",
      "Li Cai",
      "Zhao Jun",
      "Kang Liu"
    ],
    "book": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    "id": "acl-P11-1066",
    "title": "Phrase-Based Translation Model for Question Retrieval in Community Question Answer Archives",
    "url": "https://aclweb.org/anthology/P11-1066",
    "year": 2011
  },
  "references": [
    "acl-D10-1010",
    "acl-J04-4002",
    "acl-J93-2003",
    "acl-P05-1033",
    "acl-P08-1019",
    "acl-P09-1082",
    "acl-W04-3252"
  ],
  "sections": [
    {
      "text": [
        "Phrase-Based Translation Model for Question Retrieval in Community",
        "Question Answer Archives",
        "Guangyou Zhou, Li Cai, Jun Zhao,* and Kang Liu",
        "National Laboratory of Pattern Recognition Institute of Automation, Chinese Academy of Sciences 95 Zhongguancun East Road, Beijing 100190, China {gyzhou,lcai,jzhao,kliu}@nlpr.ia.ac.cn",
        "Community-based question answer (Q&A) has become an important issue due to the popularity of Q&A archives on the web.",
        "This paper is concerned with the problem of question retrieval.",
        "Question retrieval in Q&A archives aims to find historical questions that are semantically equivalent or relevant to the queried questions.",
        "In this paper, we propose a novel phrase-based translation model for question retrieval.",
        "Compared to the traditional word-based translation models, the phrase-based translation model is more effective because it captures contextual information in modeling the translation of phrases as a whole, rather than translating single words in isolation.",
        "Experiments conducted on real Q&A data demonstrate that our proposed phrase-based translation model significantly outperforms the state-of-the-art word-based translation model."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Over the past few years, large scale question and answer (Q&A) archives have become an important information resource on the Web.",
        "These include the traditional Frequently Asked Questions (FAQ) archives and the emerging community-based Q&A services, such as Yahoo!",
        "Answers, Live QnA, and Baidu Zhidao.",
        "Correspondence author: jzhao@nlpr.ia.ac.cn http://answers.yahoo.com/ http://qna.live.com/ http://zhidao.baidu.com/",
        "Community-based Q&A services can directly return answers to the queried questions instead of a list of relevant documents, thus provide an effective alternative to the traditional adhoc information retrieval.",
        "To make full use of the large scale archives of question-answer pairs, it is critical to have functionality helping users to retrieve historical answers (Duan et al., 2008).",
        "Therefore, it is a meaningful task to retrieve the questions that are semantically equivalent or relevant to the queried questions.",
        "For example in Table 1, given question Q 1, Q2 can be returned and their answers will then be used to answer Qi because the answer of Q2 is expected to partially satisfy the queried question Q1.",
        "This is what we called question retrieval in this paper.",
        "The major challenge for Q&A retrieval, as for",
        "Qi : How to get rid of stuffy nose?",
        "Expected:",
        "Q2 : What is the best way to prevent a cold?",
        "Not Expected:",
        "Q3: How do I air out my stuffy room?",
        "Q4: How do you make a nose bleed stop quicker?",
        "most information retrieval models, such as vector space model (VSM) (Salton et al., 1975), Okapi model (Robertson et al., 1994), language model (LM) (Ponte and Croft, 1998), is the lexical gap (or lexical chasm) between the queried questions and the historical questions in the archives (Jeon et al., 2005; Xue et al., 2008).",
        "For example in Table 1, Qi and Q2 are two semantically similar questions, but they have very few words in common.",
        "This problem is more serious for Q&A retrieval, since the question-answer pairs are usually short and there is little chance of finding the same content expressed using different wording (Xue et al., 2008).",
        "To solve the lexical gap problem, most researchers regarded the question retrieval task as a statistical machine translation problem by using IBM model 1 (Brown et al., 1993) to learn the word-to-word translation probabilities (Berger and Lafferty, 1999; Jeon et al., and Gurevych, 2009).",
        "Experiments consistently reported that the word-based translation models could yield better performance than the traditional methods (e.g., VSM.",
        "Okapi and LM).",
        "However, all these existing approaches are considered to be context independent in that they do not take into account any contextual information in modeling word translation probabilities.",
        "For example in Table 1, although neither of the individual word pair (e.g., \"stuffy\"/\"cold\" and \"nose\"/\"cold\") might have a high translation probability, the sequence of words \"stuffy nose\" can be easily translated from a single word \"cold\" in Q2with a relative high translation probability.",
        "In this paper, we argue that it is beneficial to capture contextual information for question retrieval.",
        "To this end, inspired by the phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004), we propose a phrase-based translation model (P-Trans) for question retrieval, and we assume that question retrieval should be performed at the phrase level.",
        "This model learns the probability of translating one sequence of words (e.g., phrase) into another sequence of words, e.g., translating a phrase in a historical question into another phrase in a queried question.",
        "Compared to the traditional word-based translation models that account for translating single words in isolation, the phrase-based translation model is potentially more effective because it captures some contextual information in modeling the translation of phrases as a whole.",
        "More precise translation can be determined for phrases than for words.",
        "It is thus reasonable to expect that using such phrase translation probabilities as ranking features is likely to improve the question retrieval performance, as we will show in our experiments.",
        "Unlike the general natural language translation, the parallel sentences between questions and answers in community-based Q&A have very different lengths, leaving many words in answers unaligned to any word in queried questions.",
        "Following (Berger and Lafferty, 1999), we restrict our attention to those phrase translations consistent with a good wordlevel alignment.",
        "Specifically, we make the following contributions:",
        "• we formulate the question retrieval task as a phrase-based translation problem by modeling the contextual information (in Section 3.1).",
        "• we linearly combine the phrase-based translation model for the question part and answer part (in Section 3.2).",
        "• we propose a linear ranking model framework for question retrieval in which different models are incorporated as features because the phrase-based translation model cannot be interpolated with a unigram language model (in Section",
        "3.3).",
        "• finally, we conduct the experiments on community-based Q&A data for question retrieval.",
        "The results show that our proposed approach significantly outperforms the baseline methods (in Section 4).",
        "The remainder of this paper is organized as follows.",
        "Section 2 introduces the existing state-of-the-art methods.",
        "Section 3 describes our phrase-based translation model for question retrieval.",
        "Section 4 presents the experimental results.",
        "In Section 5, we conclude with ideas for future research."
      ]
    },
    {
      "heading": "2. Preliminaries 2.1 Language Model",
      "text": [
        "The unigram language model has been widely used for question retrieval on community-based Q&A data (Jeon et al., 2005; Xue et al., 2008; Cao et al., 2010).",
        "To avoid zero probability, we use Jelinek-Mercer smoothing (Zhai and Lafferty, 2001) due to its good performance and cheap computational cost.",
        "So the ranking function for the query likelihood language model with Jelinek-Mercer smoothing can be written as:",
        "where q is the queried question, D is a document, C is background collection, X is smoothing parameter.",
        "#(t, D) is the frequency of term t in D, \\D\\ and \\C\\ denote the length of D and C respectively.",
        "Previous work (Berger et al., 2000; Jeon et al., 2005; Xue et al., 2008) consistently reported that the word-based translation models (Trans) yielded better performance than the traditional methods (VSM, Okapi and LM) for question retrieval.",
        "These models exploit the word translation probabilities in a language modeling framework.",
        "Following Jeon et al.",
        "(2005) and Xue et al.",
        "(2008), the ranking function can be written as:",
        "where P(w\\t) denotes the translation probability from word t to word w.",
        "Xue et al.",
        "(2008) proposed to linearly mix two different estimations by combining language model and word-based translation model into a unified framework, called TransLM.",
        "The experiments show that this model gains better performance than both the language model and the word-based translation model.",
        "Following Xue et al.",
        "(2008), this model can be written as:",
        "D: ... for good cold home remedies ... document",
        "E: [for, good, cold, home remedies] segmentation",
        "F: [fori, best2, stuffy nose3, home remedy4] translation",
        "q: best home remedy for stuffy nose queried question",
        "3 Our Approach: Phrase-Based Translation Model for Question Retrieval",
        "Phrase-based machine translation models (Koehn 2004) have shown superior performance compared to word-based translation models.",
        "In this paper, the goal of phrase-based translation model is to translate a document D into a queried question q.",
        "Rather than translating single words in isolation, the phrase-based model translates one sequence of words into another sequence of words, thus incorporating contextual information.",
        "For example, we might learn that the phrase \"stuffy nose\" can be translated from \"cold\" with relative high probability, even though neither of the individual word pairs (e.g., \"stuffy\"/\"cold\" and \"nose\"/\"cold\") might have a high word translation probability.",
        "Inspired by the work of (Sun et al., 2010; Gao et al., 2010), we assume the following generative process: first the document D is broken into K non-empty word sequences ti,..., tK, then each t is translated into a new non-empty word sequence w1;..., wK, and finally these phrases are permutated and concatenated to form the queried questions q, where t and w denote the phrases or consecutive sequence of words.",
        "To formulate this generative process, let E denote the segmentation of D into K phrases t1}..., tK, and let F denote the K translation phrases w1}..., wK we refer to these (ti, wi) pairs as bi-phrases.",
        "Finally, let M denote a permutation of K elements representing the final reordering step.",
        "Figure 1 describes an example of the generative procedure.",
        "Next let us place a probability distribution over rewrite pairs.",
        "Let B(D, q) denote the set of E, F, M triples that translate D into q.",
        "Here we assume a uniform probability over segmentations, so the phrase-based translation model can be formulated as:",
        "As is common practice in SMT, we use the maximum approximation to the sum:",
        "Although we have defined a generative model for translating D into q, our goal is to calculate the ranking score function over existing q and D, rather than generating new queried questions.",
        "Equation (8) cannot be used directly for document ranking because q and D are often of very different lengths, leaving many words in D unaligned to any word in q.",
        "This is the key difference between the community-based question retrieval and the general natural language translation.",
        "As pointed out by Berger and Laf-ferty (1999) and Gao et al.",
        "(2010), document-query translation requires a distillation of the document, while translation of natural language tolerates little being thrown away.",
        "Thus we attempt to extract the key document words that form the distillation of the document, and assume that a queried question is translated only from the key document words.",
        "In this paper, the key document words are identified via word alignment.",
        "We introduce the \"hidden alignments\" A = ai.. .aj ... aJ, which describe the mapping from a word position j in queried question to a document word position i = aj.",
        "The different alignment models we present provide different decompositions of P(q, AID).",
        "We assume that the position of the key document words are determined by the Viterbi alignment, which can be obtained using IBM model 1 as follows:",
        "A = argmaxP (q,AD) arg maxP(wj \\ta. )",
        "Given A, when scoring a given Q&A pair, we restrict our attention to those E, F, M triples that are consistent with A, which we denote as B(D, q, A).",
        "Here, consistency requires that if two words are aligned in A, then they must appear in the same biphrase (ti, wj).",
        "Once the word alignment is fixed, the final permutation is uniquely determined, so we can safely discard that factor.",
        "Thus equation (8) can be written as:",
        "For the sole remaining factor P(FD,E), we make the assumption that a segmented queried question F = wi,..., wK is generated from left to right by translating each phrase ti, tK independently:",
        "where P(wk |tk) is a phrase translation probability, the estimation will be described in Section 3.3.",
        "To find the maximum probability assignment efficiently, we use a dynamic programming approach, somewhat similar to the monotone decoding algorithm described in (Och, 2002).",
        "We define aj to be the probability of the most likely sequence of phrases covering the first j words in a queried question, then the probability can be calculated using the following recursion: (1) Initialization:",
        "3.2 Phrase-Based Translation Model for Question Part and Answer Part",
        "In Q&A, a document D is decomposed into (q, a), where q denotes the question part of the historical question in the archives and a denotes the answer part.",
        "Although it has been shown that doing Q&A retrieval based solely on the answer part does not perform well (Jeon et al., 2005; Xue et al., 2008), the answer part should provide additional evidence about relevance and, therefore, it should be combined with the estimation based on the question part.",
        "In this combined model, P(q|q) and P(q|a) are calculated with equations (12) to (14).",
        "So P(q|D) will be written as:",
        "where ßi + ß2 = 1.",
        "In equation (15), the relative importance of question part and answer part is adjusted through ßi and ß2.",
        "When ßi = 1, the retrieval model is based on phrase-based translation model for the question part.",
        "When ß2 = 1, the retrieval model is based on phrase-based translation model for the answer part."
      ]
    },
    {
      "heading": "3. 3 Parameter Estimation",
      "text": []
    },
    {
      "heading": "3. 3 1 Parallel Corpus Collection",
      "text": [
        "In Q&A archives, question-answer pairs can be considered as a type of parallel corpus, which is used for estimating the translation probabilities.",
        "Unlike the bilingual machine translation, the questions and answers in a Q&A archive are written in the same language, the translation probability can be calculated through setting either as the source and the other as the target.",
        "In this paper, P (a|q) is used to denote the translation probability with the question as the source and the answer as the target.",
        "P(q|a) is used to denote the opposite configuration.",
        "For a given word or phrase, the related words or phrases differ when it appears in the question or in the answer.",
        "Following Xue et al.",
        "(2008), a pooling strategy is adopted.",
        "First, we pool the question-answer pairs used to learn P (a|q) and the answer-question pairs used to learn P(q|a), and then use IBM model 1 (Brown et al., 1993) to learn the combined translation probabilities.",
        "Suppose we use the collection {(q, a)i,..., (q, a)m} to learn P(a|q) and use the collection {(a, q)i,..., (a, q)m} to learn P(q|a), used here to learn the combination translation probability PPooi(wj\\tj)."
      ]
    },
    {
      "heading": "3. 3 2 Parallel Corpus Preprocessing",
      "text": [
        "Unlike the bilingual parallel corpus used in SMT, our parallel corpus is collected from Q&A archives, which is more noisy.",
        "Directly using the IBM model 1 can be problematic, it is possible for translation model to contain \"unnecessary\" translations (Lee et al., 2008).",
        "In this paper, we adopt a variant of TextRank algorithm (Mihalcea and Tarau, 2004) to identify and eliminate unimportant words from parallel corpus, assuming that a word in a question or answer is unimportant if it holds a relatively low significance in the parallel corpus.",
        "Following (Lee et al., 2008), the ranking algorithm proceeds as follows.",
        "First, all the words in a given document are added as vertices in a graph G. Then edges are added between words if the words co-occur in a fixed-sized window.",
        "The number of co-occurrences becomes the weight of an edge.",
        "When the graph is constructed, the score of each vertex is initialized as 1, and the PageRank-based ranking algorithm is run on the graph iteratively until convergence.",
        "The TextRank score of a word w in document D at kth iteration is defined as follows:",
        "where d is a damping factor usually set to 0.85, and eitj is an edge weight between i and j.",
        "We use average TextRank score as threshold: words are removed if their scores are lower than the average score of all words in a document."
      ]
    },
    {
      "heading": "3. 3 3 Translation Probability Estimation",
      "text": [
        "After preprocessing the parallel corpus, we will calculate P(w|t), following the method commonly used in SMT (Koehn et al., 2003; Och, 2002) to extract bi-phrases and estimate their translation probabilities.",
        "First, we learn the word-to-word translation probability using IBM model 1 (Brown et al., 1993).",
        "Then, we perform Viterbi word alignment according to equation (9).",
        "Finally, the bi-phrases that are consistent with the word alignment are extracted using the heuristics proposed in (Och, 2002).",
        "We set the maximum phrase length to five in our experiments.",
        "After gathering all such bi-phrases from the training data, we can estimate conditional relative frequency estimates without smoothing:",
        "where N(t, w) is the number of times that t is aligned to w in training data.",
        "These estimates are",
        "Table 2: Phrase translation probability examples.",
        "Each column shows the top 5 target phrases learned from the word-aligned question-answer pairs.",
        "useful for contextual lexical selection with sufficient training data, but can be subject to data sparsity issues (Sun et al., 2010; Gao et al., 2010).",
        "An alternate translation probability estimate not subject to data sparsity is the so-called lexical weight estimate (Koehn et al., 2003).",
        "Let P(w\\t) be the word-to-word translation probability, and let A be the word alignment between w and t. Here, the word alignment contains (i, j) pairs, where i g 1... \\w\\ and j g 0 ... \\t\\, with 0 indicating a null word.",
        "Then we use the following estimate:",
        "We assume that for each position in w, there is either a single alignment to 0, or multiple alignments to non-zero positions in t. In fact, equation (18) computes a product of per-word translation scores; the per-word scores are the averages of all the translations for the alignment links of that word.",
        "The word translation probabilities are calculated using IBM 1, which has been widely used for question retrieval (Jeon et al., 2005; Xue et al., 2008; Lee et al., 2008; Bernhard and Gurevych, 2009).",
        "These word-based scores of bi-phrases, though not as effective in contextual selection, are more robust to noise and sparsity.",
        "A sample of the resulting phrase translation examples is shown in Table 2, where the top 5 target phrases are translated from the source phrases according to the phrase-based translation model.",
        "For example, the term \"explorer\" used alone, most likely refers to a person who engages in scientific exploration, while the phrase \"internet explorer\" has a very different meaning.",
        "Unlike the word-based translation models, the phrase-based translation model cannot be interpolated with a unigram language model.",
        "Following (Sun et al., 2010; Gao et al., 2010), we resort to a linear ranking framework for question retrieval in which different models are incorporated as features.",
        "We consider learning a relevance function of the following general, linear form:",
        "where the feature vector 3>(q, D) is an arbitrary function that maps (q, D) to a real value, i.e., *(q, D) g R. 6 is the corresponding weight vector, we optimize this parameter for our evaluation metrics directly using the Powell Search algorithm (Paul et al., 1992) via cross-validation.",
        "The features used in this paper are as follows:",
        "3>pt (q,D,A) = logP (q \\ D), where P (q \\ D) is computed using equations (12) to (15), and the phrase translation probability P(w t) is estimated using equation (17).",
        "• Inverted Phrase translation features (IPT):",
        "(D, q, A) = logP (D \\ q), where P (D \\ q) is computed using equations (12) to (15) except that we set ß2 = 0 in equation (15), and the phrase translation probability P(w t) is estimated using equation (17).",
        "3>lw (q, D, A) = logP (q \\ D), here P (q \\ D) is computed by equations (12) to (15), and the phrase translation probability is computed as lexical weight according to equation (18).",
        "• Inverted Lexical weight feature (ILW):",
        "&ILW (D, q,A) = logP (D \\ q), here P (D \\ q) is computed by equations (12) to (15) except that we set ß2 = 0 in equation (15), and the phrase translation probability is computed as lexical weight according to equation (18).",
        "$PA(q,D,B) = Y% \\ak - bk-1 - 1 \\, where B is a set of K bi-phrases, ak is the start position of the phrase in D that was translated",
        "source",
        "stuffy nose",
        "internet explorer",
        "1",
        "stuffy nose",
        "internet explorer",
        "2",
        "cold",
        "ie",
        "3",
        "stuffy",
        "internet browser",
        "4",
        "sore throat",
        "explorer",
        "5",
        "sneeze",
        "browser",
        "into the kth phrase in queried question, and bk-i is the end position of the phrase in D that was translated into the (k – 1)th phrase in queried question.",
        "The feature, inspired by the distortion model in SMT (Koehn et al., 2003), models the degree to which the queried phrases are reordered.",
        "For all possible B, we only compute the feature value according to the Viterbi alignment, B = argmaxß P (q, B \\ D).",
        "We find B using the Viterbi algorithm, which is almost identical to the dynamic programming recursion of equations (12) to (14), except that the sum operator in equation (13) is replaced with the max operator.",
        "• Unaligned word penalty features (UWP): &uWP(q, D), which is defined as the ratio between the number of unaligned words and the total number of words in queried questions.",
        "• Language model features (LM): &LM (q,D,A) = logPLM (q \\ D), where PLM (q \\ D) is the unigram language model with Jelinek-Mercer smoothing defined by equations (1) and (2).",
        "3>wt (q,D) = logP (q \\ D), where P (q \\ D) is the word-based translation model defined by equations (3) and (4)."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We collect the questions from Yahoo!",
        "Answers and use the getByCategory function provided in Yahoo!",
        "Answers API to obtain Q&A threads from the Yahoo!",
        "site.",
        "More specifically, we utilize the resolved questions under the top-level category at Yahoo!",
        "Answers, namely \"Computers & Internet\".",
        "The resulting question repository that we use for question retrieval contains 518,492 questions.",
        "To learn the translation probabilities, we use about one million question-answer pairs from another data set.",
        "In order to create the test set, we randomly select 300 questions for this category, denoted as \"CI_TST\".",
        "To obtain the ground-truth of question retrieval, we employ the Vector Space Model (VSM) (Salton et al., 1975) to retrieve the top 20 results and obtain manual judgements.",
        "The top 20 results don't include the queried question itself.",
        "Given a returned result by VSM, an annotator is asked to label it with \"relevant\" or \"irrelevant\".",
        "If a returned result is considered semantically equivalent to the queried question, the annotator will label it as \"relevant\"; otherwise, the annotator will label it as \"irrelevant\".",
        "Two annotators are involved in the annotation process.",
        "If a conflict happens, a third person will make judgement for the final result.",
        "In the process of manually judging questions, the annotators are presented only the questions.",
        "Table 3 provides the statistics on the final test set.",
        "We evaluate the performance of our approach using Mean Average Precision (MAP).",
        "We perform a significant test, i.e., a t-test with a default significant level of 0.05.",
        "Following the literature, we set the parameters X = 0.2 (Cao et al., 2010) in equations (1), (3) and (5), and a = 0.8 (Xue et al., 2008) in equation (6).",
        "We randomly divide the test questions into five subsets and conduct 5-fold cross-validation experiments.",
        "In each trial, we tune the parameters ß\\ and ß2 with four of the five subsets and then apply it to one remaining subset.",
        "The experiments reported below are those averaged over the five trials.",
        "Table 4 presents the main retrieval performance.",
        "Row 1 to row 3 are baseline systems, all these methods use word-based translation models and obtain the state-of-the-art performance in previous work (Jeon et al., 2005; Xue et al., 2008).",
        "Row 3 is similar to row 2, the only difference is that TransLM only considers the question part, while Xue et al.",
        "(2008) incorporates the question part and answer part.",
        "Row 4 and row 5 are our proposed phrase-based translation model with maximum phrase length of five.",
        "Row 4 is phrase-based translation model purely based on question part, this model is equivalent to setting p1 = 1 in equation (15).",
        "Row 5 is the phrase-based combination model which linearly combines the question part and answer part.",
        "As expected, different parts can play different roles: a phrase to be translated in queried questions may be translated from the question part or answer part.",
        "All these methods use pooling strategy to estimate the translation probabilities.",
        "There are some clear trends in the result of Table 4:",
        "#queries",
        "#returned",
        "#relevant",
        "CLTST",
        "300",
        "6,000",
        "798",
        "(1) Word-based translation language model (TransLM) significantly outperforms word-based translation model of Jeon et al.",
        "(2005) (row 1 vs. row 2).",
        "Similar observations have been made by Xue et al.",
        "(2008).",
        "(2) Incorporating the answer part into the models, either word-based or phrase-based, can significantly improve the performance of question retrieval (row 2 vs. row 3; row 4 vs. row 5).",
        "(3) Our proposed phrase-based translation model (P-Trans) significantly outperforms the state-of-the-art word-based translation models (row 2 vs. row 4 and row 3 vs. row 5, all these comparisons are statistically significant at p < 0.05).",
        "Our proposed phrase-based translation model, due to its capability of capturing contextual information, is more effective than the state-of-the-art word-based translation models.",
        "It is important to investigate the impact of the phrase length on the final retrieval performance.",
        "Table 5 shows the results, it is seen that using the longer phrases up to the maximum length of five can consistently improve the retrieval performance.",
        "However, using much longer phrases in the phrase-based translation model does not seem to produce significantly better performance (row 8 and row 9 vs. row 10 are not statistically significant).",
        "Question-answer pairs collected from Yahoo!",
        "answers are very noisy, it is possible for translation models to contain \"unnecessary\" translations.",
        "In this paper, we attempt to identify and decrease the proportion of unnecessary translations in a translation model by using TextRank algorithm.",
        "This kind of \"unnecessary\" translation between words will eventually affect the bi-phrase translation.",
        "Table 6 shows the effectiveness of parallel corpus preprocessing.",
        "Row 11 reports the average number of translations per word and the question retrieval performance when only stopwords are removed.",
        "When using the TextRank algorithm for parallel corpus preprocessing, the average number of translations per word is reduced from 69 to 24, but the performance of question retrieval is significantly improved (row 11 vs. row 12).",
        "Similar results have been made by Lee et al.",
        "(2008).",
        "The correspondence of words or phrases in the question-answer pair is not as strong as in the bilingual sentence pair, thus noise will be inevitably introduced for both P(a|q) and P(q|a).",
        "To see how much the pooling strategy benefit the question retrieval, we introduce two baseline methods for comparison.",
        "The first method (denoted as P(a|q)) is used to denote the translation probability with the question as the source and the answer as the target.",
        "The second (denoted as p(a|q)) is used to denote the translation probability with the answer as the source and the question as the target.",
        "Table 7 provides the comparison.",
        "From this Table, we see that the pooling strategy significantly outperforms the two baseline methods for question retrieval (row 13 and row 14 vs. row 15).",
        "#",
        "Methods",
        "Trans Prob",
        "MAP",
        "#",
        "Systems",
        "MAP",
        "~r",
        "Jeon et al.",
        "(2005)",
        "Ppool",
        "0.289",
        "6",
        "P-Trans (l = 1)",
        "0.352",
        "2",
        "TransLM",
        "Ppool",
        "0.324",
        "7",
        "P-Trans (l = 2)",
        "0.373",
        "3",
        "Xue et al.",
        "(2008)",
        "Ppool",
        "0.352",
        "8",
        "P-Trans (l = 3)",
        "0.386",
        "4",
        "P-Trans (pi = 1, l = 5)",
        "Ppool",
        "0.366",
        "9",
        "P-Trans (l = 4)",
        "0.390",
        "5",
        "P-Trans (l = 5)",
        "Ppool",
        "0.391",
        "10",
        "P-Trans (l = 5)",
        "0.391",
        "Model",
        "#",
        "Methods",
        "Average",
        "MAP",
        "P-Trans (l = 5)",
        "11",
        "Initial",
        "69",
        "0.380",
        "12",
        "TextRank",
        "24",
        "0.391"
      ]
    },
    {
      "heading": "5. Conclusions and Future Work",
      "text": [
        "In this paper, we propose a novel phrase-based translation model for question retrieval.",
        "Compared to the traditional word-based translation models, the proposed approach is more effective in that it can capture contextual information instead of translating single words in isolation.",
        "Experiments conducted on real Q&A data demonstrate that the phrase-based translation model significantly outperforms the state-of-the-art word-based translation models.",
        "There are some ways in which this research could be continued.",
        "First, question structure should be considered, so it is necessary to combine the proposed approach with other question retrieval methods (e.g., (Duan et al., 2008; Wang et al., 2009; Bunescu and Huang, 2010)) to further improve the performance.",
        "Second, we will try to investigate the use of the proposed approach for other kinds of data set, such as categorized questions from forum sites and FAQ sites."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was supported by the National Natural Science Foundation of China (No.",
        "60875041 and No.",
        "61070106).",
        "We thank the anonymous reviewers for their insightful comments.",
        "We also thank Maoxi Li and Jiajun Zhang for suggestion to use the alignment toolkits.",
        "Model",
        "#",
        "Trans Prob",
        "MAP",
        "P-Trans (l = 5)",
        "13",
        "p (a|q)",
        "0.387",
        "14",
        "p (q |a)",
        "0.381",
        "15",
        "Ppool",
        "0.391"
      ]
    }
  ]
}
