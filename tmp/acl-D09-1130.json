{
  "info": {
    "authors": [
      "Minwoo Jeong",
      "Chin-Yew Lin",
      "Gary Geunbae Lee"
    ],
    "book": "EMNLP",
    "id": "acl-D09-1130",
    "title": "Semi-supervised Speech Act Recognition in Emails and Forums",
    "url": "https://aclweb.org/anthology/D09-1130",
    "year": 2009
  },
  "references": [
    "acl-J00-3003",
    "acl-N03-1030",
    "acl-N06-1027",
    "acl-P04-1085",
    "acl-W04-3239",
    "acl-W04-3240"
  ],
  "sections": [
    {
      "text": [
        "Minwoo Jeong** Chin-Yew Lin* Gary Geunbae Lee*",
        "*Pohang University of Science & Technology, Pohang, Korea ^Microsoft Research Asia, Beijing, China",
        "In this paper, we present a semi-supervised method for automatic speech act recognition in email and forums.",
        "The major challenge of this task is due to lack of labeled data in these two genres.",
        "Our method leverages labeled data in the Switchboard-DAMSL and the Meeting Recorder Dialog Act database and applies simple domain adaptation techniques over a large amount of unlabeled email and forum data to address this problem.",
        "Our method uses automatically extracted features such as phrases and dependency trees, called subtree features, for semi-supervised learning.",
        "Empirical results demonstrate that our model is effective in email and forum speech act recognition."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Email and online forums are important social media.",
        "For example, thousands of emails and posts are created daily in online communities, e.g., Usenet newsgroups or the TripAdvisor travel forum , in which users interact with each other using emails/posts in complicated ways in discussion threads.",
        "To uncover the rich interactions in these email exchanges and forum discussions, we propose to apply speech act recognition to email and forum threads.",
        "Despite extensive studies of speech act recognition in many areas, developing speech act recognition for online forms of conversation is very challenging.",
        "A major challenge is that emails and forums usually have no labeled data for training statistical speech act recognizers.",
        "Fortunately, labeled speech act data are available in other domains (i.e., telephone and meeting conversations",
        "This work was conducted during the author's internship at Microsoft Research Asia.",
        "http://tripadvisor.com/",
        "in this paper) and large unlabeled data sets can be collected from the Web.",
        "Thus, we focus on the problem of how to accurately recognize speech acts in emails and forums by making maximum use of data from existing resources.",
        "Recently, there are increasing interests in speech act recognition of online text-based conversations.",
        "Analysis of speech acts for online chat and instant messages and have been studied in computer-mediated communication (CMC) and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Rosé et al., 2008).",
        "In natural language processing, Cohen et al.",
        "(2004) and Feng et al.",
        "(2006) used speech acts to capture the intentional focus of emails and discussion boards.",
        "However, they assume that enough labeled data are available for developing speech act recognition models.",
        "A main contribution of this paper is that we address the problem of learning speech act recognition in a semi-supervised way.",
        "To our knowledge, this is the first use of semi-supervised speech act recognition in emails and online forums.",
        "To do this, we make use of labeled data from spoken conversations (Jurafsky et al., 1997; Dhillon et al., 2004).",
        "A second contribution is that our model learns subtree features that constitute discriminative patterns: for example, variable length n-grams and partial dependency structures.",
        "Therefore, our model can capture both local features such as n-grams and non-local dependencies.",
        "In this paper, we extend subtree pattern mining to the semi-supervised learning problem.",
        "This paper is structured as follows.",
        "Section 2 reviews prior work on speech act recognition and Section 3 presents the problem statement and our data sets.",
        "Section 4 describes a supervised method of learning subtree features that shows the effectiveness of subtree features on labeled data sets.",
        "Section 5 proposes semi-supervised learning techniques for speech act recognition and Section 6 demonstrates our method applied to email and on-",
        "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1250-1259, Singapore, 6-7 August 2009.",
        "©2009 ACL and AFNLP",
        "line forum thread data.",
        "Section 7 concludes this paper with future work."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Speech act theory is fundamental to many studies in discourse analysis and pragmatics (Austin, 1962; Searle, 1969).",
        "A speech act is an illo-cutionary act of conversation and reflects shallow discourse structures of language.",
        "Recent research on spoken dialog processing has investigated computational speech act models of human-human and human-computer conversations (Stol-cke et al., 2000) and applications of these models to CMC and distance learning (Twitchell et al., 2004; Nastri et al., 2006; Rosé et al., 2008).",
        "Our work in this paper is closely related to prior work on email and forum speech act recognition.",
        "Cohen et al.",
        "(2004) proposed the notion of 'email speech act' for classifying the intent of an email sender.",
        "They defined verb and noun categories for email speech acts and used supervised learning to recognize them.",
        "Feng et al.",
        "(2006) presented a method of detecting conversation focus based on the speech acts of messages in discussion boards.",
        "Extending Feng et al.",
        "(2006)'s work, Ravi and Kim (2007) applied speech act classification to detect unanswered questions.",
        "However, none of these studies have focused on the semi-supervised speech act recognition problem and examined their methods across different genres.",
        "The speech processing community frequently employs two large-scale corpora for speech act annotation: Switchboard-DAMSL (SWBD) and Meeting Recorder Dialog Act (MRDA).",
        "SWBD is an annotation scheme and collection of labeled dialog act data for telephone conversations (Juraf-sky et al., 1997).",
        "The main purpose of SWBD is to acquire stochastic discourse grammars for training better language models for automatic speech recognition.",
        "More recently, an MR DA corpus has been adapted from SWBD but its tag set for labeling meetings has been modified to better reflect the types of interaction in multi-party face-to-face meetings (Dhillon et al., 2004).",
        "These two corpora have been extensively studied, e.g., (Stolcke et al., 2000; Ang et al., 2005; Galley et al., 2004).",
        "We also use these for our experiments.",
        "This paper focuses on the problem of semi-supervised speech act recognition.",
        "The goal of semi-supervised learning techniques is to use auxiliary data to improve a model's capability to recognize speech acts.",
        "The approach in Tur et al.",
        "(2005) presented semi-supervised learning to employ auxiliary unlabeled data in call classification, and is closely related to our work.",
        "However, our approach uses the most discriminative subtree features, which is particularly attractive for reducing the model's size.",
        "Our problem setting is closely related to the domain adaptation problem (Ando and Zhang, 2005), i.e., we seek to obtain a model that analyzes target domains (emails and forums) by adapting a method that analyzes source domains (SWBD and MR DA).",
        "Recently, this type of domain adaptation has become an important topic in natural language processing."
      ]
    },
    {
      "heading": "3. Problem Definition",
      "text": [
        "We define speech act recognition to be the task that, given a sentence, maps it to one of the speech act types.",
        "Figure 1 shows two examples of our email and forum speech act recognition.",
        "El~6 are all sentences in an email message.",
        "Fl~3, F4~5, and F6 are three posts in a forum thread.",
        "A sentence interacts alone or with others, for example, F6 agrees with the previous post (F4~5).",
        "To gain insight into our work, it is useful to consider that E2, 3 and Fl, 4, 6 are summaries of two discourses.",
        "In particular, Fl denotes a question and F4 and F6 are corresponding answers.",
        "More recently, using speech acts has become an appealing approach in summarizing the discussions (Galley et al., 2004; McKeown et al., 2007).",
        "Next, we define speech act category based on MR DA.",
        "Dhillon et al.",
        "(2004) included definitions of speech acts for colloquial style interactions (e.g., backchannel, disruption, and floorgrabber), but these are not applicable in emails and forums.",
        "After removing these categories, we define 12 tags (Table 1).",
        "Dhillon et al.",
        "(2004) provides detailed descriptions of each tag.",
        "We note that our tag set definition is different from (Cohen et al., 2004; Feng et al., 2006; Ravi and Kim, 2007) for two reasons.",
        "First, prior work primarily interested in the domain-specific speech acts, but our work use domain-independent speech act tags.",
        "Second, we focus on speech act recognition on the sentence-level.",
        "E2: - will there be anything happening at the conference related to this W3C User interest group?",
        "QY E3: I do not see anything on the program yet, but I suspect we could at least have an informal SIG S E4: - a chance to meet others and bring someone like me up to speed on what is happening.",
        "S E5: There will be many competing activities, so the sooner we can set this up the more likely I can attend.",
        "S",
        "Fl: If given a choice, should I choose Huangpu area, or should I choose Pudong area?",
        "QR",
        "F2: Both location are separated by a Huangpu river, not sure which area is more convenient for sight seeing?",
        "QW F3: Thanks in advance for reply!",
        "P",
        "F4: Stay on the Puxi side of the Huangpu river and visit the Pudong side by the incredible tourist tunnel.",
        "AC F5: If you stay on the Pudong side add half an hour to visit the majority of the tourist attractions.",
        "S",
        "F6: I definitely agree with previous post.",
        "AA",
        "Figure 1 : Examples of speech act recognition in emails and online forums.",
        "Tags are defined in Table 1.",
        "Tag Description",
        "The goal of semi-supervised speech act recognition is to learn a classifier using both labeled and unlabeled data.",
        "We formally define our problem as follows.",
        "Let x = {xj} be a forest, i.e., a set of trees that represents a natural language structure, for example, a sequence of words and a dependency parse tree.",
        "We will describe this in more detail in Section 4.",
        "Let y be a speech act.",
        "Then, we define T>l = {xi,j/i}™=1 as the set of labeled training data, and T>u = {^i}\\=n+i as tb-e set of unlabeled training data where I = n + m and m is the number of unlabeled data instances.",
        "Our goal is to find a learning method to minimize the classification errors in T>l and Vu-",
        "In this paper, we separate labeled (T>l) and unlabeled data (Vu).",
        "First we use SWBD and MR DA as our labeled data.",
        "We automatically map original annotations in SWBD and MR DA to one of the 12 speech acts.",
        "Inter-annotator agreement k in both data sets is ~ 0.8 (Jurafsky et al., 1997; Dhillon et al., 2004).",
        "For evaluation purposes, we divide labeled data into three sets: training, development, and evaluation sets (Table 2).",
        "Of the 1,155 available conversations in the SWBD corpus, we use 855 for training, 100 for development, and 200 for evaluation.",
        "Among the 75 available meetings in the MR DA corpus, we exclude two meetings of different natures (btrOOl and btr002).",
        "Of the remaining meetings, we use 59 for training, 6 for development, and 8 for evaluation.",
        "Then we merge multi-segments utterances that belong to the same speaker and then divide all data sets into sentences.",
        "As stated earlier, our unlabeled data consists of email (EMAIL) and online forum (FORUM) data.",
        "For the EMAIL set, we selected 22,391 emails from Enron data (discussionJhreads, all-documents, and calendar folders).",
        "For the FORUM set, we crawled 11,602 threads and 55,743 posts from the TripAdvisor travel forum site (Beijing, Shanghai, and Hongkong forums).",
        "As our evaluation sets, we used 40 email threads of the BC3 corpus for EMAIL and 100 threads selected from the same travel forum site for FORUM.",
        "Every sentences was automatically segmented by the MSRA sentence boundary detector (Table 2).",
        "Annotation was performed by two human annotators, and inter-annotator agreements were n = 0.79 for EMAIL and n = 0.73 for FORUM.",
        "Overall performance of automatic evaluation measures usually depends on the distribution of tags.",
        "In both labeled and unlabeled sets, the most frequent tag is the statement (S) tag (Figure 2).",
        "Distributions of tags are similar in training and development sets of SWBD and MRDA.",
        "A",
        "Accept response",
        "AA",
        "Acknowledge and appreciate",
        "AC",
        "Action motivator",
        "P",
        "Polite mechanism",
        "QH",
        "Rhetorical question",
        "QO",
        "Open-ended question",
        "QR",
        "Or/or-clause question",
        "QW",
        "Wh-question",
        "QY",
        "Yes-no question",
        "R",
        "Reject response",
        "S",
        "Statement",
        "u",
        "Uncertain response"
      ]
    },
    {
      "heading": "4. Speech Act Recognition",
      "text": [
        "Previous work in speech act recognition used a large set of lexical features, e.g., bag-of-words, bigrams and trigrams (Stolcke et al., 2000; Cohen et al., 2004; Ang et al., 2005; Ravi and Kim, 2007).",
        "However, these methods create a large number of lexical features that might not be necessary for speech act identification.",
        "For example, a Wh-question \"What site should we use to book a Beijing-Chonqing flight?\"",
        "can be predicted by two discriminative features, \"(<s>, WRB) – > QW\" and \"(?, </s>) QW\" where <s> and </s> are sentence start and end symbols, and WRB is a part-of-speech tag that denotes a Wh-adverb.",
        "In addition, useful features could be of various lengths, i.e. not fixed length n-grams, and non-adjacent.",
        "One key idea of this paper is a novel use of subtree features to model these for speech act recognition.",
        "To exploit subtree features in our model, we use a subtree pattern mining method proposed by Kudo and Matsumoto (2004).",
        "We briefly introduce this algorithm here.",
        "In Section 3.1, we defined x = {xj} as the forest that is a set of trees.",
        "More precisely, Xj is a labeled ordered tree where each node has its own label and is ordered left-to-right.",
        "Several types of labeled ordered trees",
        "Figure 3: Representations of tree: (a) bag-of-words, (b) n-gram, (c) word pair, and (d) dependency tree.",
        "A node denotes a word and a directed edge indicates a parent-and-child relationship.",
        "are possible (Figure 3).",
        "Note that S-expression can be used instead for computation, for example (a (b (c (d) ) ) ) for the n-gram (Figure 3(b)).",
        "Moreover, we employ a combination of multiple trees as the input of the subtree pattern mining algorithm.",
        "We extract subtree features from the forest set {xi}.",
        "A subtree t is a tree if t ç x.",
        "For example, (a) , (a (b) ) , and (b (c (d) ) ) are subtrees of Figure 3(b).",
        "We define the subtree feature as a weak learner:",
        " – y otherwise, where we assume a binary case y e y = {+1,-1} for simplicity.",
        "Even though the approach in Kudo and Matsumoto (2004) and ours are simiar, there are two clear distinctions.",
        "First, our method employs multiple tree structures, and uses different constraints to generate subtree candidates.",
        "In this paper, we only restrict generating the dependency subtrees which should have 3 or more nodes.",
        "Second, our method is of interest for semi-supervised learning problems.",
        "To learn subtree features, Kudo and Matsumoto (2004) assumed supervised data {(xj,^)}.",
        "Here, we describe the supervised learning method and will describe our semi-supervised method in Section 5.",
        "Set",
        "SWBD",
        "MRDA",
        "Training",
        "96,553",
        "50,865",
        "Development",
        "12,299",
        "8,366",
        "Evaluation",
        "24,264",
        "10,492",
        "Set",
        "EMAIL",
        "FORUM",
        "Unlabeled",
        "122,125",
        "297,017",
        "Evaluation",
        "2,267",
        "3,711",
        "80%-70%-",
        "n ruMi n moi im",
        "60%-",
        "50%-40%-",
        "30%-",
        "20%-10%-0%",
        "Given training examples, we construct a ensemble learner F(x) = J2k WÙ/fc> tk, x), where Xkis a coefficient for linear combination.",
        "A final classifier /z,(x) can be derived from the ensemble learner, i.e., /i(x) = sgn (F(x)).",
        "As an optimization framework (Mason et al., 2000), the objective of boosting learning is to find F such that the cost of functional is minimized for some non-negative and monotonically decreasing cost function C : R – > R and the weight e R+.",
        "In this paper, we use the AdaBoost algorithm (Schapire and Singer, 1999); thus the cost function is defined as C(z) = e~z.",
        "Constructing an ensemble learner requires that the user choose a base learner, /(y,i,x), to maximize the inner product – (VC(F),f) (Mason et al., 2000).",
        "Finding /(y,i,x) to maximize – (VC(F), /) is equivalent to searching for f(y,t,x) to minimize 2^^.^.«;; - 1, where Wi for i e T>l, is the empirical data distribution w\\ at step k. It is defined as:",
        "From Eq.",
        "3, a proper base learner (i.e., subtree) can be found by maximizing weighted gain, where",
        "Thus, subtree mining is formulated as the problem of finding (i, y) = arg max gain(i, y).",
        "We need to search with respect to a non-monotonic score function (Eq.",
        "4), thus we use the monotonie bound, gain(i, y) < ß(t), where",
        "Table 3: Result of supervised learning experiment; columns are micro-averaged F\\ score with macro-averaged Fi score in parentheses.",
        "MaxEnt: maximum entropy model; BoW: bag-of-words model; Ngram: n-gram model; +PosTag, +deptree, +speaker indicate that the components were added individually onto Ngram.",
        "'*' indicates results significantly better than the Ngram model (p < 0.001).",
        "The subtree set is efficiently enumerated using a branch-and-bound procedure based on ß(t) (Kudo and Matsumoto, 2004).",
        "After finding an optimal base leaner, f(y, t, x), we need to set the coefficient to form a new ensemble, F(xj) <- F(xj) + Xkf(t, y,xj).",
        "In AdaBoost, we choose",
        "After K iterations, the boosting algorithm returns the ensemble learner F(x) which consists of a set of appropriate base learners f(y, t, x).",
        "We verified the effectiveness of using subtree features on the SWBD and MRDA data sets.",
        "For boosting learning, one typically assumes a% = \\.",
        "In addition, the number of iterations, which relates to the number of patterns, was determined by a development set.",
        "We also used a one-vs.-all strategy for the multi-class problem.",
        "Precision and recall were computed and combined into micro-and macro-averaged F\\ scores.",
        "The significance of our results was evaluated using the McNemar paired test (Gillick and Cox, 1989), which is based on individual labeling decisions to compare the correctness of two models.",
        "All experiments were implemented in C++ and executed in Windows XP on a PC with a Dual 2.1 GHz Intel Core2 processor and 2.0 Gbyte of main memory.",
        "Model",
        "SWBD",
        "MRDA",
        "MaxEnt",
        "92.76 (63.54)",
        "82.48 (57.19)",
        "bow",
        "91.32 (54.47)",
        "82.17 (55.42)",
        "Ngram",
        "92.60 (58.43)",
        "83.30 (57.53)",
        "+postag",
        "92.69 (60.07)",
        "83.60 (58.46)",
        "+deptree",
        "92.67 (61.75)",
        "*83.57 (57.45)",
        "+speaker",
        "*92.86 (63.13)",
        "83.40 (58.20)",
        "All",
        "*92.87 (63.77)",
        "83.49 (59.04)",
        "We show that use of subtree features is effective to solve the supervised speech act recognition problem.",
        "We also compared our model with the state-of-the-art maximum entropy classifier (MaxEnt).",
        "We used bag-of-words, bigram and trigram features for maxent, which modeled 702k (SWBD) and 460k (MRDA) parameters (i.e., patterns), and produced micro-averaged F\\ scores of 92.76 (macro-averaged F\\ = 63.54) for SWBD and 82.48 (macro-averaged Fi = 57.19) for MRDA.",
        "In contrast, our method generated approximately 4k to 5 k patterns on average with similar or greater F\\ scores (Table 3); hence, compared to maxent, our model requires fewer calculations and is just as accurate.",
        "The n-gram model (ngram) performed significantly better than the bag-of-words model (Mc-Nemar test; p < 0.001) (Table 3).",
        "Unlike MaxEnt, Ngram automatically selects a relevant set of variable length n-gram features (i.e., phrase features).",
        "To this set, we separately added two syntax type features, part-of-speech tag n-gram (PosTag) and dependency parse tree (DepTree) automatically parsed by Minipar, and one discourse type feature, speaker n-gram (Speaker).",
        "Although some micro-averaged F\\ are not statistically significant between the original ngram and the models that include PosTag, DepTree or Speaker, macro-averaged F\\ values indicate that minor classes can take advantage of other structures.",
        "For example, in the result of SWBD (Figure 4), DepTree and Speaker models help to predict uncertain responses (U), whereas Ngram and postag cannot do this."
      ]
    },
    {
      "heading": "5. Semi-supervised Learning",
      "text": [
        "Our goal is to eventually make maximum use of existing resources in SWBD and MRDA for email/forum speech act recognition.",
        "We call the model trained on the mixed data of these two corpora Baseline.",
        "We use All features in constructing the Baseline for the semi-supervised experiments.",
        "While this model gave promising results using SWBD and MRDA, language used in emails and forums differs from that used in spoken conversation.",
        "For example, 'thanx' is an expression commonly used as a polite mechanism in online communications.",
        "To adapt our model to understand this type of difference between spoken and online text-based conversations, we should induce new patterns from unlabeled email and forum data.",
        "We describe here two methods of semi-supervised learning.",
        "First, we bootstrap the Baseline model using automatically predicted unlabeled examples.",
        "However, using all of the unlabeled data results in noisy models; therefore filtering or selecting datais very important in practice.",
        "To this end, we only select similar examples by criterion, d(xj, Xj) < r or k nearest neighbors where Xj G Vl and Xj e Vu.",
        "In practice, r or k are fixed.",
        "In our method, examples are represented by trees; hence we use a \"tree edit distance\" for calculating d(xi,Xj) (Shasha and Zhang, 1990).",
        "Selected examples are evaluated using baseline, and using subtree pattern mining runs on the augmented data (i.e. unlabeled).",
        "We call this method bootstrap.",
        "Our second method is based on a principle of semi-supervised boosting learning (Bennett et al., 2002).",
        "Because we have no supervised guidance for Vu, our objective functional to find F is defined as:",
        "This cost functional is non-differentiable.",
        "To solve it, we introduce pseudo-labels y where y = sgn(F(-x.))",
        "and |-F(x)| = yF(x).",
        "Using the same derivation in Section 4.2, we obtain the following gain function and update rules:",
        "Intuitively, an unlabeled example that has a high-confidence |F(x)| at the current step, will probably receive more weight at the next step.",
        "That is, similar instances become more important when learning and mining subtrees.",
        "This semi-supervised boosting learning iteratively generates pseudo-labels for unlabeled data and finds the value of F that minimizes training errors (Bennett et al., 2002).",
        "Also, the algorithm infers new features from unlabeled data, and these features are iteratively re-evaluated by the current ensemble learner.",
        "We call this method SemiBoost."
      ]
    },
    {
      "heading": "6. Experiment 6.1 Setting",
      "text": [
        "We describe specific settings used in our experiment.",
        "Because we have no development set, we set the maximum number of iterations K at 10,000.",
        "At most K patterns can be extracted, but this seldom happens because duplicated patterns are merged.",
        "Typical settings for semi-supervised boosting are ai = 1 and ßi = 0.5, that is, we penalize the weights for unlabeled data.",
        "For efficiency, Baseline model used 10% of the SWBD and MRDA data, selected at random.",
        "We observed that this data set does not degrade the results of semi-supervised speech act recognition.",
        "For Bootstrap and SemiBoost, we selected k = 100 nearest neighbors of unlabeled examples for each labeled example using tree edit distance, and then used 24,625 (SWBD) and 54,961 (MRDA) sentences for the semi-supervised setting.",
        "All trees were combined as described in Section 4.3 (All model).",
        "In EMAIL and FORUM data we added different types of discourse features: message type (e.g., initial or reply posts), authorship (e.g., an identification of 2nd or 3rd posts written by the same author), and relative position of a sentence.",
        "In Figure 1, for example, Fl~3 is an initial post, and F4~5 and F6 are reply posts.",
        "Moreover, Fl, F4, and F6 are the first sentence in each post.",
        "Table 4: Results of speech act recognition on online conversations; columns are micro-averaged Fi score with macro-averaged scores in parentheses.",
        "'*' indicates that the result is significantly better than Baseline (p < 0.001).",
        "These features do not occur in SWBD or MRDA because these are utterance-by-utterance conversations.",
        "First, we show that our method of semi-supervised learning can improve modeling of the speech act of emails and forums.",
        "As our baseline, Baseline achieved a micro-averaged Fi score of ~ 79 for both data sets.",
        "This implies that SWBD and MRDA data are useful for our problem.",
        "Using unlabeled data, semi-supervised methods Bootstrap and SemiBoost perform better than baseline (Table 4; Figure 5).",
        "To verify our claim, we evaluated the supervised speech act recognition on EMAIL and FORUM evaluation sets with 5-fold cross validation (Supervised in Table 4).",
        "In particular, our semi-supervised speech act recognition is competitive with the supervised model in FORUM data.",
        "The difference in performance between supervised results in EMAIL and FORUM seems to indicate that the latter is a more difficult data set.",
        "However, our SemiBoost method were able to come close to the supervised FORUM results (81.76 vs. 83.67).",
        "This is also close to the range of supervised MRDA data set (Fi = 83.49 for All, Table 3).",
        "Moreover, we analyzed a main reason of why transfer results were competitive in the FORUM but not in the EMAIL.",
        "This might be due to the mismatch in the unlabeled data, that is, we used different email collections, the BC3 corpus (email communication of W3C on w3.org sites), for evaluation while used Enron data for adaption.",
        "We also conjecture that the discrepancy between EMAIL and FORUM is probably due to the more heterogeneous nature of the FORUM data where anyone can post and reply while EMAIL (Enron or",
        "Model",
        "EMAIL",
        "FORUM",
        "Baseline Bootstrap SemiBoost Supervised",
        "78.87 (37.44) *83.11 (44.90) *82.80 (44.64)",
        "90.95 (75.71)",
        "78.93 (35.57) 79.09 (44.38) *81.76 (44.21) 83.67 (40.68)",
        "(a) EMAIL (b) FORUM",
        "Figure 5 : Result of the semi-supervised learning method",
        "BC3) might have a more fix set of participants.",
        "The improvement of less frequent tags is prominent, for example 25% for action motivator (AC), 40% for polite mechanism (P), and 15% for rhetorical question (QR) error rate reductions were achieved in FORUM data (Figure 5(b)).",
        "Therefore, the semi-supervised learning method is more effective with small amounts of labeled data (i.e., less frequent annotations).",
        "We believe that despite their relative rarity, these speech acts are more important than the statement (S) in some applications, e.g., summarization.",
        "Next, we give a qualitative analysis for better interpretation of our problem and results.",
        "Due to limited space, we focus on FORUM data, which can potentially be applied to many applications.",
        "Of the top ranked patterns extracted by Semi-Boost (Figure 6(a)), subtree patterns of n-gram, part-of-speech, dependency parse trees are most discriminative.",
        "The patterns from unlabeled data have relatively lower ranks, but this is not surprising.",
        "This indicates that Baseline model provides the base knowledge for semi-supervised speech act recognition.",
        "Also, unlabeled data for EMAIL and FORUM help to induce new patterns or adjust the model's parameters.",
        "As a result, the semi-supervised method is better than the Baseline when an identical number of patterns is modeled (Figure 6(b)).",
        "For this result, we conclude that our method successfully transfers knowledge from a source domain (i.e., SWBD and MRDA) to a target domain (i.e., EMAIL and FORUM); hence it can be a solution to the domain adaption problem.",
        "Finally, we determine the main reasons for error (in SEMIBOOST), to gain insights that may allow development of better models in future work (Figure 6(c)).",
        "We sorted speech act tags by their semantics and partitioned the confusion matrix into question type (Q*) and statement, which are two high-level speech acts.",
        "Most errors occur in the similar categories, that is, language usage in question discourse is definitely distinct from that in statement discourse.",
        "From this analysis, we believe that more advanced techniques (e.g. two-stage classification and learning with hierarchy-augmented loss) can improve our model."
      ]
    },
    {
      "heading": "7. Conclusion",
      "text": [
        "Despite the increasing interest in online text-based conversations, no study to date has investigated semi-supervised speech act recognition in email and forum threads.",
        "This paper has addressed the problem of learning to recognize speech acts using labeled and unlabeled data.",
        "We have also contributed to the development of a novel application of boosting subtree mining.",
        "Empirical results have demonstrated that semi-supervised learning of speech act recognition with subtree features improves the performance in email and forum data sets.",
        "An attractive future direction is to exploit prior knowledge for semi-supervised speech act recognition.",
        "Druck et al.",
        "(2008) described generalized expectation criteria in which a discriminative model can employ the labeled features and unlabeled instances.",
        "Using prior knowledge, we expect that our model will effectively learn useful patterns from unlabeled data.",
        "As work progresses on analyzing online text-based conversations such as emails, forums, and online chats, the importance of developing models for discourse without annotating much new data will become more important.",
        "In the future, we plan to explore other related problems such as adjacency pairs (Levinson, 1983) and discourse parsing (Soricut and Marcu, 2003) for large-scale online forum data.",
        "BASELINE",
        "BOOTSTRAP",
        "SEMIBOOST",
        "(a) Example patterns (b) Learning behavior (c) Confusion matrix",
        "Acknowledgement",
        "We would like to thank to anonymous reviewers for their valuable comments, and Yunbo Cao, Wei Lai, Xinying Song, Jingtian Jing, and Wei Wu for their help in preparing our data.",
        "Tag",
        "Example pattern",
        "A",
        "(ROOT (yep))",
        "AA",
        "(<s> (wow (.",
        "(</s>))))",
        "AC",
        "(WRB (VB (NN (PRP)))",
        "P",
        "(thanks)",
        "QH",
        "(cares (?))",
        "QO",
        "(ROOT (think) (?))",
        "QR",
        "(ROOT(or)(?))",
        "QW",
        "(ROOT (rel=sub (what)))",
        "QY",
        "(<s>(do»",
        "R",
        "(nay)",
        "S",
        "(ROOT (U (,))(?))",
        "U",
        "(it (is (possible (.))))",
        "A",
        "14",
        "0",
        "0",
        "1",
        "0",
        "3",
        "21",
        "2",
        "0",
        "0",
        "0 0",
        "R",
        "0",
        "3",
        "0",
        "0",
        "2",
        "0",
        "19",
        "0",
        "0",
        "0",
        "0 0",
        "u",
        "0",
        "0",
        "1",
        "0",
        "1",
        "2",
        "20",
        "0",
        "0",
        "0",
        "0 0",
        "AA",
        "1",
        "0",
        "0",
        "3",
        "1",
        "0",
        "12",
        "0",
        "0",
        "0",
        "0 0",
        "P",
        "0",
        "0",
        "0",
        "2",
        "22",
        "3",
        "0",
        "0",
        "0 0",
        "1",
        "AC",
        "1",
        "0",
        "0",
        "0",
        "6",
        "4",
        "4",
        "0",
        "0 0",
        "True",
        "S",
        "15",
        "2",
        "0",
        "4",
        "21",
        "14",
        "2",
        "0",
        "1 1",
        "QY",
        "1",
        "0",
        "0",
        "0",
        "0",
        "7",
        "33",
        "6",
        "6",
        "5 0",
        "QW",
        "0",
        "0",
        "0",
        "0",
        "0",
        "6",
        "23",
        "19",
        "0",
        "10 7",
        "QR",
        "0",
        "0",
        "0",
        "0",
        "0",
        "3",
        "9",
        "16",
        "1",
        "13",
        "0 1",
        "QO QH",
        "0 0",
        "0 0",
        "0 0",
        "0 0",
        "0 0",
        "1",
        "0",
        "0 0",
        "18 0",
        "5 2",
        "0 0",
        "9 1 0 1",
        "A",
        "R",
        "u",
        "AA",
        "P",
        "AC",
        "S",
        "QY QW QR QO QH"
      ]
    }
  ]
}
