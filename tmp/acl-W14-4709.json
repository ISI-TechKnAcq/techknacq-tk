{
  "info": {
    "authors": [
      "Laurianne Sitbon",
      "Lance De Vine"
    ],
    "book": "CogALex",
    "id": "acl-W14-4709",
    "title": "Predicting sense convergence with distributional semantics: an application to the CogaLex 2014 shared task",
    "url": "https://aclweb.org/anthology/W14-4709",
    "year": 2014
  },
  "references": [
    "acl-J10-4006"
  ],
  "sections": [
    {
      "text": [
        "Zock/Rapp/Huang (eds.",
        "Abstract",
        "This paper presents our system to address the CogALex-IV 2014 shared task of identifying a single word most semantically related to a group of 5 words (queries).",
        "Our system uses an implementation of a neural language model and identifies the answer word by finding the most semantically similar word representation to the sum of the query representations.",
        "It is a fully unsupervised system which learns on around 20% of the UkWaC corpus.",
        "It correctly identifies 85 exact correct targets out of 2,000 queries, 285 approximate targets in lists of 5 suggestions."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "How humans draw associations between words or concepts has been the object of many studies by psy-chologists, and for many years computer scientists have attempted to model this human mental lexicon by means of symbolic methods (Enguix et al., 2014) or statistical models (Baroni and Lenci, 2013).",
        "These models and methods have in turn been used to improve natural language processing systems (Lewis and Steedman, 2013), search technologies (Deerwester et al., 1990) and have since been evaluated in the view of supporting such systems more than helping users directly.",
        "The Shared Task CogALex-IV 2014 aims to evaluate how these models can support a user with deficiencies in their lexical access.",
        "The task is set as one of retrieving one target word when being presented with 5 cue (associated) words.",
        "After submissions of all systems, the organisers revealed that the cue words were the 5 words most often associated with the target words.",
        "They have been collected from a large number of users who were presented with the target word and invited to produce one associate.",
        "In this paper we present our preliminary investigations to address the task with a neural net language model learning representations for words on the UkWaC corpus (M. Baroni and Zanchetta, 2009).",
        "We propose a strict evaluation (accuracy of finding the target word) as well as a retrieval based evaluation that we believe is closer to the aim of helping user find their words.",
        "2 Approach and methodology 2.1 Neural Net Language Model In 2003 Bengio et.",
        "al. (Bengio et al., 2003) introduced a neural net based method for language modelling that learns simultaneously 1) a distributed representation for each word and 2) the probability function for word sequences, expressed in terms of the distributed representations.",
        "Generalization to unseen word sequences is obtained because such sequences receive a high probability if they are composed of words that are similar to words from an already seen sequence.",
        "An outcome of this approach is the learning of ?word embeddings?, which are vectors representing the meanings of words relative to other words (via a mechanism akin to word distribution).",
        "For this task, we used our own implementation of the continuous Skip-Gram neural language model introduced by (Mikolov et al., 2013).",
        "We refer to this model hereafter as skip-gram.",
        "The implementation is similar to the word2vec software package.",
        "Neither ",
        "64 sub-sampling nor negative sampling were used.",
        "A small context term window radius of size two and a vector dimensionality of 128 were used.",
        "We use the cosine between the word embedding representations (vectors) to estimate the similarity between the words in the evaluation task.",
        "The parameters were not tuned for the task and so it is probable that further improvements can be made.",
        "2.2 Combined similarity Once semantic vectors are created with skip-gram it allows us to measure the distances between words and retrieve the words most similar to another word, or those with a vector most similar to any vector, such as the sum of several word vectors.",
        "In the CogALex-IV shared task, we are provided with 5 words (cues) associated to a target word to be found.",
        "If we consider that these words are effectively a unique semantic context for the word to be found, then it makes sense to add their vectors and find the unique word most similar.",
        "This approach is of course inspired by vectorial models of information retrieval and adopted widely when testing distributional models for more than single words (see for example (Deerwester et al., 1990)).",
        "However we found that this strategy has limitations, because in the case of some polysemous words, some of the cues were from radically different contexts, and therefore summing up the vector did not necessarily make sense.",
        "For such situations, it makes more sense to find the lists of words most related to each of the cues, and then combine these lists.",
        "To do this we first selected 10 candidate targets for each cue, which are the 10 words with a representation most similar to the cue, according to the cosine between their words embeddings and that of the cue.",
        "We then ranked the words according to their number of occurrences in the 5 lists.",
        "We did not consider the distance as measured by cosine similarity (the actual value) because while cosine is a good measure to rank terms by similarity, we do not believe that this leads to an absolute estimate for actual semantic similarity.",
        "Additionally, we chose not to assign weights to the terms depending on which cue they were associated to as there was no reason to believe that the cues were ordered in any way (that is, by manual inspection, we did not find that cues early in the lists were most likely to lead to the target than cues later in the list were).",
        "The results were not as good then as those with the summed vectors.",
        "We then adopted a third strategy, which was to consider the sum of the cues as a 6th cue when generating the lists of candidates, but also to decide on priority when selecting a unique target in case there are several candidates ranked first with an equal number of occurrences in the 6 lists.",
        "On the training set, this allowed us to find 92 correct answers for the 2,000 cases.court law judge judges courts SUMcourts laws judges appellants court courtsheriff legislation pettiti judge rackets courtstribunal jurisprudence court defendants magistrates judgeprosecution statutes sheriff respondents badminton judgesjudge statute prosecutor panellists sharia sheriffjusticiary litigation dredd jury squash lawjudicature antiunion jury organizers tribunals magistratesconsistory sharia coroner complainants prosecutors prosecutionleet criminology appellant winners proceedings prosecutorsmagistrates arbitration defendant plaintiffs parliaments prosecutorprosecutor llm magistrate magistrates rulings tribunalcontactfulhambaronsregulation complainant appellant law consistoryappeal courts magistrates senatus prosecution rulingspalace penal appeal chairmen leagues judicature Figure 1: Example of lists of 14 most similar words for the 5 cues ?court law judge judges courts?",
        "and their sum vector Figure 1 shows an example where the cues are ?court law judge judges courts?",
        "and the target was 65 magistrates.",
        "We present for each cue as well as for the sum of all cues the lists of 14 most similar terms.",
        "In gray the words that were cues or plural of a cue were ignored.",
        "In bold we show how ?sheriff?",
        "would have been picked if we considered the sum only, while when considering the individual sets of similar terms in addition to the sum we could find that magistrates was a more likely target.",
        "2.3 Training corpus The corpus we used for learning the word representations is the UKWaC corpus (M. Baroni and Zanchetta, 2009).",
        "This is the corpus suggested by the organisers of the CogALex-IV 2014 Shared task, and contains web pages from the UK domain.",
        "We pre-processed the corpus by a) lower-casing all terms, b) replacing contractions with their expansion, eg.",
        "?it?s?",
        "becomes ?it is?, c) removing all punctuation and d) replacing all digits with the single character ?7?.",
        "The Skip-gram model that we used is able to scale to the whole UKWaC corpus (approximately 2 billion terms) but because of time constraints we selected only the first 20% of the corpus, and then processed the remainder of the corpus, adding to our corpus subset all sentences containing words that were present in the training set but not in the initial 20% subset.",
        "This was to ensure that representations for all the words in the training set could be learned.",
        "3 Results 3.1 Shared task evaluation The evaluation proposed in the shared task is the exact accuracy of a single proposed target for each query composed of 5 words.",
        "There were 2,000 queries in each of the training and test set, and the results are expressed in total number.",
        "All our results according to this metric are situated between 4% and 5%.",
        "We have included in table 1 the results according to the task metric, on the training and on the test set, for both the sum vectors and the combination of results from a sum and the individual words.",
        "The latter is the one that was submitted to the shared task.",
        "Method Train Test Sum 81 75 Combination 84 85 Table 1: Accuracy of the methods on the training and on the test corpus 3.2 Retrieval evaluation We now consider a task where a system would support a user in finding a word that they describe using the 5 associations.",
        "In such a tip-of-the-tongue context, users would immediately recognise the word they are looking for when presented in a list and also if it is presented with a different inflection (ie.",
        "?run?",
        "instead of ?running?).",
        "Therefore, if presented a list of words containing the target word or variation of the target word, the outcome of such a system would be considered successful.",
        "While it would be impractical to consider very long lists in a usability context, we have measured the accuracy for lists of 2, 3, 4 and 5 words, with a measure of 1 where the word (or at least one of its inflections) is in the list and 0 otherwise.",
        "In other words, the accuracy is the number of target words that appear as is or as an inflection in suggestion lists of varying sizes.",
        "The results presented on Figure 2 show that taking inflections into account leads to only marginal im-provements, but more importantly considering additional targets (as a list) can really improve outcomes for the users, with almost 13% of targets being retrieved in lists of 5 suggestions with the combined approach.",
        "4 Discussion and conclusion The accuracy of our approach, even when considering lists of 5 suggestions and inflections of words, show that results are still very low if one would consider a usable assistance system for users with lexical access issues.",
        "This is consistent with previous findings on a similar task in French (Sitbon et al., 2008).",
        "66 50 ?",
        "100 ?",
        "150 ?",
        "200 ?",
        "250 ?",
        "300 ?",
        "1 ?",
        "2 ?",
        "3 ?",
        "4 ?",
        "5 ?",
        "train-?",
        "?sum ?",
        "train-?",
        "?comb ?",
        "50 ?",
        "100 ?",
        "150 ?",
        "200 ?",
        "250 ?",
        "300 ?",
        "1 ?",
        "2 ?",
        "3 ?",
        "4 ?",
        "5 ?",
        "test-?",
        "?sum ?",
        "test-?",
        "?comb ?",
        "Figure 2: Total number of targets on the left, or inflections of target on the right, out of 2,000, found in lists of 1 to 5 results, in the training set and in the test set This work suggested that a combination of resources encoding various types of semantic relations would be best, along with user models.",
        "CogALex-IV task was not based on associations drawn by a single user, but rather by majority associations drawn by many users, so this would not apply to the task specifically.",
        "However we believe that including definitional associations such as that drawn from an ESA model on the Wikipedia would be a way to dramatically improve the accuracy, at least when considering lists of results.",
        "Additionally it would be interesting to inspect a number of variables to weigh the contribution of each cue (depending on their specificity for example).",
        "In this paper we found that adding the vectors representing each word let to better results than only considering the words individually.",
        "This mode of combination is one of many proposed by (Mitchell and Lapata, 2010) and in future work we will experiment with alternative combination models.",
        "Finally, an area for future work would be to consider cleaning up the dataset so as to avoid effects such as several cues being inflections of one another (i.e.. ?courts?",
        "and ?court?)",
        "or even the target being an inflection of one of the cues, as we have observed in the CogALex-IV dataset.",
        "References"
      ]
    }
  ]
}
