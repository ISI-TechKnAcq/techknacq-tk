{
  "info": {
    "authors": [
      "Liheng Xu",
      "Kang Liu",
      "Siwei Lai",
      "Yubo Chen",
      "Jun Zhao"
    ],
    "book": "ACL",
    "id": "acl-P13-1173",
    "title": "Mining Opinion Words and Opinion Targets in a Two-Stage Framework",
    "url": "https://aclweb.org/anthology/P13-1173",
    "year": 2013
  },
  "references": [
    "acl-C10-1074",
    "acl-C10-2167",
    "acl-D07-1114",
    "acl-D09-1159",
    "acl-D12-1123",
    "acl-H05-1043",
    "acl-P09-1079",
    "acl-P10-1041",
    "acl-P10-1060",
    "acl-P12-1043",
    "acl-W00-1213",
    "acl-W03-1014",
    "acl-W09-2307"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper proposes a novel two-stage method for mining opinion words and opinion targets.",
        "In the first stage, we propose a Sentiment Graph Walking algorithm, which naturally incorporates syntactic patterns in a Sentiment Graph to extract opinion word/target candidates.",
        "Then random walking is employed to estimate confidence of candidates, which improves extraction accuracy by considering confidence of patterns.",
        "In the second stage, we adopt a self-learning strategy to refine the results from the first stage, especially for filtering out high-frequency noise terms and capturing the long-tail terms, which are not investigated by previous methods.",
        "The experimental results on three real world datasets demonstrate the effectiveness of our approach compared with state-of-the-art unsupervised methods."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Opinion mining not only assists users to make informed purchase decisions, but also helps business organizations understand and act upon customer feedbacks on their products or services in real-time.",
        "Extracting opinion words and opinion targets are two key tasks in opinion mining.",
        "Opinion words refer to those terms indicating positive or negative sentiment.",
        "Opinion targets represent aspects or attributes of objects toward which opinions are expressed.",
        "Mining these terms from reviews of a specific domain allows a more thorough understanding of customers?",
        "opinions.",
        "Opinion words and opinion targets often co-occur in reviews and there exist modified relations (called opinion relation in this paper) between them.",
        "For example, in the sentence ?It has a clear screen?, ?clear?",
        "is an opinion word and ?screen?",
        "is an opinion target, and there is an opinion relation between the two words.",
        "It is natural to identify such opinion relations through common syntactic patterns (also called opinion patterns in this paper) between opinion words and targets.",
        "For example, we can extract ?clear?",
        "and ?screen?",
        "by using a syntactic pattern ?Adj-{mod}-Noun?, which captures the opinion relation between them.",
        "Although previous works have shown the effectiveness of syntactic patterns for this task (Qiu et al., 2009; Zhang et al., 2010), they still have some limitations as follows.",
        "False Opinion Relations: As an example, the phrase ?everyday at school?",
        "can be matched by a pattern ?Adj-{mod}-(Prep)-{pcomp-n}-Noun?, but it doesn't bear any sentiment orientation.",
        "We call such relations that match opinion patterns but express no opinion false opinion relations.",
        "Previous pattern learning algorithms (Zhuang et al., 2006; Kessler and Nicolov, 2009; Jijkoun et al., 2010) often extract opinion patterns by frequency.",
        "However, some high-frequency syntactic patterns can have very poor precision (Kessler and Nicolov, 2009).",
        "False Opinion Targets: In another case, the phrase ?wonderful time?",
        "can be matched by an opinion pattern ?Adj-{mod}-Noun?, which is widely used in previous works (Popescu and Et-zioni, 2005; Qiu et al., 2009).",
        "As can be seen, this phrase does express a positive opinion but unfortunately ?time?",
        "is not a valid opinion target for most domains such as MP3.",
        "Thus, false opinion targets are extracted.",
        "Due to the lack of ground-truth knowledge for opinion targets, non-target terms introduced in this way can be hardly filtered out.",
        "Long-tail Opinion Targets: We further notice that previous works prone to extract opinion targets with high frequency (Hu and Liu, 2004; Popescu and Etzioni, 2005; Qiu et al., 2009; Zhu et al., 2009), and they often have difficulty in identifying the infrequent or long-tail opinion targets.",
        "To address the problems stated above, this paper proposes a two-stage framework for mining opinion words and opinion targets.",
        "The underlying motivation is analogous to the novel idea ?Mine the Easy, Classify the Hard?",
        "(Dasgupta and Ng, 2009).",
        "In our first stage, we propose a Sentiment Graph Walking algorithm to cope with the false opinion relation problem, which mines easy cases of opinion words/targets.",
        "We speculate that it may be helpful to introduce a confidence score for each pattern.",
        "Concretely, we create a Sentiment Graph to model opinion relations among opinion word/target/pattern candidates and apply random walking to estimate confidence of them.",
        "Thus, confidence of pattern is considered in a unified process.",
        "Patterns that often extract false opinion relations will have low confidence, and terms introduced by low-confidence patterns will also have low confidence accordingly.",
        "This could potentially improve the extraction accuracy.",
        "In the second stage, we identify the hard cases, which aims to filter out false opinion targets and extract long-tail opinion targets.",
        "Previous supervised methods have been shown to achieve state-of-the-art results for this task (Wu et al., 2009; Jin and Ho, 2009; Li et al., 2010).",
        "However, the big challenge for fully supervised method is the lack of annotated training data.",
        "Therefore, we adopt a self-learning strategy.",
        "Specifically, we employ a semi-supervised classifier to refine the target results from the first stage, which uses some highly confident target candidates as the initial labeled examples.",
        "Then opinion words are also refined.",
        "Our main contributions are as follows: ?",
        "We propose a Sentiment Graph Walking algorithm to mine opinion words and opinion targets from reviews, which naturally incorporates confidence of syntactic pattern in a graph to improve extraction performance.",
        "To our best knowledge, the incorporation of pattern confidence in such a Sentiment Graph has never been studied before for opinion words/targets mining task (Section 3).",
        "?",
        "We adopt a self-learning method for refining opinion words/targets generated by Sentiment Graph Walking.",
        "Specifically, it can remove high-frequency noise terms and capture long-tail opinion targets in corpora (Section 4).",
        "?",
        "We perform experiments on three real world datasets, which demonstrate the effectiveness of our method compared with state-of-the-art unsupervised methods (Section 5)."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "In opinion words/targets mining task, most unsupervised methods rely on identifying opinion relations between opinion words and opinion targets.",
        "Hu and Liu (2004) proposed an association mining technique to extract opinion words/targets.",
        "The simple heuristic rules they used may potentially introduce many false opinion words/targets.",
        "To identify opinion relations more precisely, subsequent research work exploited syntax information.",
        "Popescu and Etzioni (2005) used manually complied syntactic patterns and Pointwise Mutual Information (PMI) to extract opinion words/targets.",
        "Qiu et al. (2009) proposed a bootstrapping framework called Double Propagation which introduced eight heuristic syntactic rules.",
        "While manually defining syntactic patterns could be time-consuming and error-prone, we learn syntactic patterns automatically from data.",
        "There have been extensive works on mining opinion words and opinion targets by syntactic pattern learning.",
        "Riloff and Wiebe (2003) performed pattern learning through bootstrapping while extracting subjective expressions.",
        "Zhuang et al. (2006) obtained various dependency relationship templates from an annotated movie corpus and applied them to supervised opinion words/targets extraction.",
        "Kobayashi et al. (2007) adopted a supervised learning technique to search for useful syntactic patterns as contextual clues.",
        "Our approach is similar to (Wiebe and Riloff, 2005) and (Xu et al., 2013), all of which apply syntactic pattern learning and adopt self-learning strategy.",
        "However, the task of (Wiebe and Riloff, 2005) was to classify sentiment orientations in sentence level, while ours needs to extract more detailed information in term level.",
        "In addition, our method extends (Xu et al., 2013), and we give a more complete and in-depth analysis on the aforementioned problems in the first section.",
        "There were also many works employed graph-based method (Li et al., 2012; Zhang et al., 2010; Hassan and Radev, 2010; Liu et al., 2012), but none of previous works considered confidence of patterns in the graph.",
        "In supervised approaches, various kinds of models were applied, such as HMM (Jin and Ho, 2009), SVM (Wu et al., 2009) and CRFs (Li et al., 2010).",
        "The downside of supervised methods was the difficulty of obtaining annotated training data in practical applications.",
        "Also, classifiers trained",
        "on one domain often fail to give satisfactory results when shifted to another domain.",
        "Our method does not rely on annotated training data."
      ]
    },
    {
      "heading": "3 The First Stage: Sentiment Graph",
      "text": []
    },
    {
      "heading": "Walking Algorithm",
      "text": [
        "In the first stage, we propose a graph-based algorithm called Sentiment Graph Walking to mine opinion words and opinion targets from reviews."
      ]
    },
    {
      "heading": "3.1 Opinion Pattern Learning for Candidates Generation",
      "text": [
        "For a given sentence, we first obtain its dependency tree.",
        "Following (Hu and Liu, 2004; Popescu and Etzioni, 2005; Qiu et al., 2009), we regard all adjectives as opinion word candidates (OC) and all nouns or noun phrases as opinion target candidates (TC).",
        "A statistic-based method in (Zhu et al., 2009) is used to detect noun phrases.",
        "Then candidates are replaced by wildcards ?<OC>?",
        "or ?<TC>?.",
        "Figure 1 gives a dependency tree example generated by Minipar (Lin, 1998).",
        "p red s det",
        "?The style of the screen is gorgeous?.",
        "We extract two kinds of opinion patterns: ?OC-TC?",
        "pattern and ?TC-TC?",
        "pattern.",
        "The ?OC-TC?",
        "pattern is the shortest path between an OC wildcard and a TC wildcard in dependency tree, which captures opinion relation between an opinion word candidate and an opinion target candidate.",
        "Similarly, the ?TC-TC?",
        "pattern captures opinion relation between two opinion target candidates.1 Words in opinion patterns are replaced by their POS tags, and we constrain that there are at most two words other than wildcards in each pattern.",
        "In Figure 1, there are two opinion patterns marked out by dash lines: ?<OC>{pred}(VBE){s}<TC>?",
        "for the ?OC-TC?",
        "type and ?<TC>{mod}(Prep){pcompn}<TC>?",
        "for the ?TC-TC?",
        "type.",
        "After all pat-1We do not identify the opinion relation ?OC-OC?",
        "because this relation is often unreliable.",
        "terns are generated, we drop those patterns with frequency lower than a threshold F ."
      ]
    },
    {
      "heading": "3.2 Sentiment Graph Construction",
      "text": [
        "To model the opinion relations among opinion words/targets and opinion patterns, a graph named as Sentiment Graph is constructed, which is a weighted, directed graph G = (V,E,W ), where ?",
        "V = {Voc ?",
        "Vtc ?",
        "Vp} is the set of vertices in G, where Voc, Vtc and Vp represent the set of opinion word candidates, opinion target candidates and opinion patterns, respectively.",
        "is the weighted, bidirectional edge set in G, where Epo and Ept are mutually exclusive sets of edges connecting opinion word/target vertices to opinion pattern vertices.",
        "Note that there are no edges between Voc and Vtc.",
        "?",
        "W : E ?",
        "R+ is the weight function which assigns non-negative weight to each edge.",
        "For each (e : va ?",
        "vb) ?",
        "E, where va, vb ?",
        "V , the weight function w(va, vb) = freq(va, vb)/freq(va), where freq(?)",
        "is the frequency of a candidate extracted by opinion patterns or co-occurrence frequency between"
      ]
    },
    {
      "heading": "3.3 Confidence Estimation by Random Walking with Restart",
      "text": [
        "We believe that considering confidence of patterns can potentially improve the extraction accuracy.",
        "Our intuitive idea is: (i) If an opinion word/target is with higher confidence, the syntactic patterns containing this term are more likely to be used to express customers?",
        "opinion.",
        "(ii) If an opinion pattern has higher confidence, terms extracted by this pattern are more likely to be correct.",
        "It's a reinforcement process.",
        "We use Random Walking with Restart (RWR) algorithm to implement our idea described above.",
        "Let Moc p denotes the transition matrix from Voc to Vp, for vo ?",
        "Voc, vp ?",
        "Vp, Moc p(vo, vp) = w(vo, vp).",
        "Similarly, we have Mtc p, Mp oc, Mp tc.",
        "Let c denotes confidence vector of candidates so ctoc, cttc and ctp are confidence vectors for opinion word/target/pattern candidates after walking t steps.",
        "Initially c0oc is uniformly distributed on a few domain-independent opinion word seeds, then the following formula are updated iteratively until cttc and ctoc converge:",
        "where MT is the transpose of matrix M and ?",
        "is a small probability of teleporting back to the seed vertices which prevents us from walking too far away from the seeds.",
        "In the experiments below, ?",
        "is set 0.1 empirically."
      ]
    },
    {
      "heading": "4 The Second Stage: Refining Extracted",
      "text": []
    },
    {
      "heading": "Results Using Self-Learning",
      "text": [
        "At the end of the first stage, we obtain a ranked list of opinion words and opinion targets, in which higher ranked terms are more likely to be correct.",
        "Nevertheless, there are still some issues needed to be addressed: 1) In the target candidate list, some high-frequency frivolous general nouns such as ?thing?",
        "and ?people?",
        "are also highly ranked.",
        "This is because there exist many opinion expressions containing non-target terms such as ?good thing?, ?nice people?, etc.",
        "in reviews.",
        "Due to the lack of ground-truth knowledge for opinion targets, the false opinion target problem still remains unsolved.",
        "2) In another aspect, long-tail opinion targets may have low degree in Sentiment Graph.",
        "Hence their confidence will be low although they may be extracted by some high quality patterns.",
        "Therefore, the first stage is incapable of dealing with the long-tail opinion target problem.",
        "3) Furthermore, the first stage also extracts some high-frequency false opinion words such as ?every?, ?many?, etc.",
        "Many terms of this kind are introduced by high-frequency false opinion targets, for there are large amounts of phrases like ?every time?",
        "and ?many people?.",
        "So this issue is a side effect of the false opinion target problem.",
        "To address these issues, we exploit a self-learning strategy.",
        "For opinion targets, we use a semi-supervised binary classifier called target refining classifier to refine target candidates.",
        "For opinion words, we use the classified list of opinion targets to further refine the extracted opinion word candidates."
      ]
    },
    {
      "heading": "4.1 Opinion Targets Refinement",
      "text": [
        "There are two keys for opinion target refinement: (i) How to generate the initial labeled data for target refining classifier.",
        "(ii) How to properly represent a long-tail opinion target candidate other than comparing frequency between different targets.",
        "For the first key, it is clearly improper to select high-confidence targets as positive examples and choose low-confidence targets as negative examples2, for there are noise with high confidence and long-tail targets with low confidence.",
        "Fortunately, a large proportion of general noun noises are the most frequent words in common texts.",
        "Therefore, we can generate a small domain-independent general noun (GN) corpus from large web corpora to cover some most frequently used general noun examples.",
        "Then labeled examples can be drawn from the target candidate list and the GN corpus.",
        "For the second key, we utilize opinion words and opinion patterns with their confidence scores to represent an opinion target.",
        "By this means, a long-tail opinion target can be determined by its own contexts, whose weights are learnt from contexts of frequent opinion targets.",
        "Thus, if a long-tail opinion target candidate has high contextual support, it will have higher probability to be found out in despite of its low frequency.",
        "Creation of General Noun Corpora.",
        "1000 most frequent nouns in Google-1-gram3 were selected as general noun candidates.",
        "On the other hand, we added all nouns in the top three levels of hyponyms in four WordNet (Miller, 1995) synsets ?object?, ?person?, ?group?",
        "and ?measure?",
        "into the GN corpus.",
        "Our idea was based on the fact that a term is more general when it sits in higher level in the WordNet hierarchy.",
        "Then inapplicable candidates were discarded and a 3071-word English",
        "GN corpus was created.",
        "Another Chinese GN corpus with 3493 words was generated in the similar way from HowNet (Gan and Wong, 2000).",
        "Generation of Labeled Examples.",
        "Let T = {Y+1,Y?1} denotes the initial labeled set, where N most highly confident target candidates but not in our GN corpora are regarded as the positive example set Y+1, other N terms from GN corpora which are also top ranked in the target list are selected as the negative example set Y?1.",
        "The reminder unlabeled candidates are denoted by T ?.",
        "Feature Representation for Classifier.",
        "Given T and T ?",
        "in the form of {(xi, yi)}.",
        "For a target candidate ti, xi = (o1, .",
        ".",
        ".",
        ", on, p1, .",
        ".",
        ".",
        ", pm)T represents its feature vector, where oj is the opinion word feature and pk is the opinion pattern feature.",
        "The value of feature is defined as follows,",
        "where conf(?)",
        "denotes confidence score estimated by RWR, freq(?)",
        "has the same meaning as in Section 3.2.",
        "Particularly, freq(ti, oj , pk) represents the frequency of pattern pk extracting opinion target ti and opinion word oj .",
        "Target Refinement Classifier: We use support vector machine as the binary classifier.",
        "Hence, the classification problem can be formulated as to find a hyperplane < w, b > that separates both labeled set T and unlabeled set T ?",
        "with maximum margin.",
        "The optimization goal is to minimize over",
        "where yi, y?j ?",
        "{+1,?1}, xi and x?j represent feature vectors, C and C?",
        "are parameters set by user.",
        "This optimization problem can be implemented by a typical Transductive Support Vector Machine (TSVM) (Joachims, 1999)."
      ]
    },
    {
      "heading": "4.2 Opinion Words Refinement",
      "text": [
        "We use the classified opinion target results to refine opinion words by the following equation,",
        "where T is the opinion target set in which each element is classified as positive during opinion target refinement, s(ti) denotes confidence score exported by the target refining classifier.",
        "Particularly,",
        "pk freq(ti, oj , pk).",
        "A higher score of s(oj) means that candidate oj is more likely to be an opinion word."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": []
    },
    {
      "heading": "5.1 Datasets and Evaluation Metrics",
      "text": [
        "Datasets: We select three real world datasets to evaluate our approach.",
        "The first one is called Customer Review Dataset (CRD) (Hu and Liu, 2004) which contains reviews on five different products (represented by D1 to D5) in English.",
        "The second dataset is pre-annotated and published in COAE084, where two domains of Chinese reviews are selected.",
        "At last, we employ a benchmark dataset in (Wang et al., 2011) and named it as Large.",
        "We manually annotated opinion words and opinion targets as the gold standard.",
        "Three annotators were involved.",
        "Firstly, two annotators were required to annotate out opinion words and opinion targets in sentences.",
        "When conflicts happened, the third annotator would make the final judgment.",
        "The average Kappa-values of the two domains were 0.71 for opinion words and 0.66 for opinion targets.",
        "Detailed information of our datasets is shown in Table 1.",
        "corpora.",
        "Stemming and fuzzy matching are also performed following previous work (Hu and Liu, 2004).",
        "Evaluation Metrics: We evaluate our method by precision(P), recall(R) and F-measure(F)."
      ]
    },
    {
      "heading": "5.2 Our Method vs. the State-of-the-art",
      "text": [
        "Three state-of-the-art unsupervised methods are used as competitors to compare with our method.",
        "Hu extracts opinion words/targets by using adjacency rules (Hu and Liu, 2004).",
        "DP uses a bootstrapping algorithm named as Double Propagation (Qiu et al., 2009).",
        "Zhang is an enhanced version of DP and employs HITS algorithm (Kleinberg, 1999) to rank opinion targets (Zhang et al., 2010).",
        "Ours-Full is the full implementation of our method.",
        "We employ SVMlight (Joachims, 1999) as the target refining classifier.",
        "Default parameters are used except the bias item is set 0.",
        "Ours-Stage1 only uses Sentiment Graph Walking algorithm which does?t have opinion word and opinion target refinement.",
        "All of the above approaches use same five common opinion word seeds.",
        "The choice of opinion seeds seems reasonable, as most people can easily come up with 5 opinion words such as ?good?, ?bad?, etc.",
        "The performance on five products of CRD dataset is shown in Table 2 and Table 3.",
        "Zhang does not extract opinion words so their results for opinion words are not taken into account.",
        "We can see that Ours-Stage1 achieves superior recall but has some loss in precision compared with DP and Zhang.",
        "This may be because the CRD dataset is too small and our statistic-based method may suffer from data sparseness.",
        "In spite of this, Ours-Full achieves comparable F-measure with DP, which is a well-designed rule-based method.",
        "The results on two larger datasets are shown in Table 4 and Table 5, from which we can have the following observation: (i) All syntax-based-methods outperform Hu, showing the importance of syntactic information in opinion relation identification.",
        "(ii) Ours-Full outperforms the three competitors on all domains provided.",
        "(iii) Ours-Stage1 outperforms Zhang, especially in terms of recall.",
        "We believe it benefits from our automatical pattern learning algorithm.",
        "Moreover, Ours-Stage1 do not loss much in precision compared with Zhang, which indicates the applicability to estimate pattern confidence in Sentiment Graph.",
        "(iv) Ours-Full achieves 4-9% improvement in precision over the most accurate method, which shows the effectiveness of our second stage."
      ]
    },
    {
      "heading": "5.3 Detailed Discussions",
      "text": [
        "This section gives several variants of our method to have a more detailed analysis.",
        "Ours-Bigraph constructs a bi-graph between opinion words and targets, so opinion patterns are not included in the graph.",
        "Then RWR algorithm is used to only assign confidence to opinion word/target candidates.",
        "Ours-Stage2 only contains the second stage, which doesn't apply Sentiment Graph Walking algorithm.",
        "Hence the confidence score conf(?)",
        "in Equations (4) and (5) have no values and they are set to 1.",
        "The initial labeled examples are exactly the same as Ours-Full.",
        "Due to the limitation of space, we only give analysis on opinion target extraction results in Figure 3.",
        "We can see that our graph-based methods (Ours-Bigraph and Ours-Stage1) achieve higher recall than Zhang.",
        "By learning patterns automatically, our method captures opinion relations more efficiently.",
        "Also, Ours-Stage1 outperforms Ours-Bigraph, especially in precision.",
        "We believe it is because Ours-Stage1 estimated confidence of patterns so false opinion relations are reduced.",
        "Therefore, the consideration of pattern confidence is beneficial as expected, which alleviates the false opinion relation problem.",
        "On another hand, we find that Ours-Stage2 has much worse performance than Ours-Full.",
        "This shows the effectiveness of Sentiment Graph Walking algorithm since the confidence scores estimated in the first stage are indispensable and indeed key to the learning of the second stage.",
        "Figure 4 shows the average Precision@N curve of four domains on opinion target extraction.",
        "Ours-GN-Only is implemented by only removing 50 initial negative examples found by our GN corpora.",
        "We can see that the GN corpora work quite well, which find out most top-ranked false opinion targets.",
        "At the same time, Ours-Full has much better performance than Ours-GN-Only which indicates that Ours-Full can filter out more noises other than the initial negative examples.",
        "Therefore, our self-learning strategy alleviates the shortcoming of false opinion target problem.",
        "Moreover, Table 5 shows that the performance of opinion word extraction is also improved based on the classified results of opinion targets.",
        "ID Pattern Example #Ext.",
        "Conf.",
        "PrO PrT #1 <OC>{mod}<TC> it has a clear screen 7344 0.3938 0.59 0.66 #2 <TC>{subj}<OC> the sound quality is excellent 2791 0.0689 0.62 0.70 #3 <TC>{conj}<TC> the size and weight make it convenient 3620 0.0208 N/A 0.67 #4 <TC>{subj}<TC> the button layout is a simplistic plus 1615 0.0096 N/A 0.67 #5 <OC>{pnmod}<TC> the buttons easier to use 128 0.0014 0.61 0.34 #6 <TC>{subj}(V){s}(VBE){subj}<OC> software provided is simple 189 0.0015 0.54 0.33 #7 <OC>{mod}(Prep){pcomp-c}(V){obj}<TC> great for playing audible books 211 0.0013 0.43 0.48",
        "dence score estimated by RWR and PrO/PrT stand for precisions of extraction on opinion words/targets of a pattern respectively.",
        "Opinion words in examples are in bold and opinion targets are in italic.",
        "Figure 5 gives the recall of long-tail opinion targets5 extracted, where Ours-Full is shown to have much better performance than Ours-Stage1 and the three competitors.",
        "This observation proves that our method can improve the limitation of long-tail opinion target problem.",
        "Table 6 shows some examples of opinion pattern and their extraction accuracy on MP3 reviews in the first stage.",
        "Pattern #1 and #2 are the two most high-confidence opinion patterns of ?OC-TC?",
        "type, and Pattern #3 and #4 demonstrate two typical ?TC-TC?",
        "patterns.",
        "As these patterns extract too many terms, the overall precision is very low.",
        "We give Precision@400 of them, which is more meaningful because only top listed terms in the extracted results are regarded as opinion targets.",
        "Pattern #5 and #6 have high precision on opinion words but low precision on opinion targets.",
        "This observation demonstrates the false opinion target problem.",
        "Pattern #7 is a pattern example that extracts many false opinion relations and it has low precision for both opinion words and opinion targets.",
        "We can see that Pattern #7 has 5Since there is no explicit definition for the notion ?long-tail?, we conservatively regard 60% opinion targets with the lowest frequency as the ?long-tail?",
        "terms.",
        "a lower confidence compared with Pattern #5 and #6 although it extracts more words.",
        "It's because it has a low probability of walking from opinion seeds to this pattern.",
        "This further proves that our method can reduce the confidence of low-quality patterns.",
        "Finally, we study the sensitivity of parameters when recall is fixed at 0.70.",
        "Figure 6 shows the precision curves at different N initial training examples and F filtering frequency.",
        "We can see that the performance saturates when N is set to 50 and it does not vary much under different F , showing the robustness of our method.",
        "We thus set N to 50, and F to 3 for CRD, 5 for COAE08 and 10 for Large accordingly."
      ]
    },
    {
      "heading": "6 Conclusion and Future Work",
      "text": [
        "This paper proposes a novel two-stage framework for mining opinion words and opinion targets.",
        "In the first stage, we propose a Sentiment Graph Walking algorithm, which incorporates syntactic patterns in a Sentiment Graph to improve the extraction performance.",
        "In the second stage, we propose a self-learning method to refine the result of first stage.",
        "The experimental results show that our method achieves superior performance over state-of-the-art unsupervised methods.",
        "We further notice that opinion words are not limited to adjectives but can also be other type of word such as verbs or nouns.",
        "Identifying all kinds of opinion words is a more challenging task.",
        "We plan to study this problem in our future work."
      ]
    },
    {
      "heading": "Acknowledgement",
      "text": []
    }
  ]
}
