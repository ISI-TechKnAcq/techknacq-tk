{
  "info": {
    "authors": [
      "John J. Kovarik"
    ],
    "book": "Chinese Language Processing Workshop",
    "id": "acl-W00-1217",
    "title": "How Should a Large Corpus Be Built? - A Comparative Study of Closure in Annotated Newspaper Corpora from Two Chinese Sources, Towards Building a Larger Representative Corpus Merged from Representative Sublanguage Collections",
    "url": "https://aclweb.org/anthology/W00-1217",
    "year": 2000
  },
  "references": [],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This study measures comparative lexical and syntactic closure rates in annotated Chinese newspaper corpora from the Academica Sinica Balanced Corpus and the University of Penn-sylvania's Chinese Treebank.",
        "It then draws inferences as to how large such corpora need be to be representative models of subject-matter-constrained language domains within the same genre.",
        "Future large corpora should be built incrementally only by combining smaller representative sublanguage collections."
      ]
    },
    {
      "heading": "1 Prior Work",
      "text": [
        "Practically speaking, earlier attempts at building corpora, such as the IBM/Lancaster approach, have taken an all-inclusive perspective toward text selection proposing that (Garside and McEnery, 1993) raw texts for parsed corpora should come from a variety of sources.",
        "The IBM/Lancaster group used the Canadian Hansards collection of parallel parsed English and French sentences as a base of English parsed sentences and then focused on the Computer Manuals domain, in which they attempted to randomly select texts with some additional non-Computer Manual material selected as \"a measure of 'light relief' \" supposedly for the benefit of the annotators.",
        "A broad approach was also used in the Hong Kong element of the International Corpus of English (ICE) project (Green-baum, 1992) which sought to assemble a range of both spoken and written texts along with a range of both formal and informal situations to provide a reasonably large, well-documented and detailed snapshot of the use of educated English.",
        "(Bolt, 1994) Both the IBM/Lancaster approach and the ICE project build millions of tokens worth of corpora.",
        "But from a more principled perspective, Douglas Biber, in speaking on representativeness in corpus design, pointed out that the linguistic characterization of a corpus should include both its central tendency and its range of variation.",
        "(Biber, 1993) Similarly Geoffrey Leech has stated that a corpus, in order to be representative, must somehow capture the magnitude of languages not only in their lexis but also in their syntax.",
        "(Leech, 1991) This suggests we should build corpora focusing on how well they can approach lexical and syntactic closure, rather than by merely fixating on ever larger amounts of text.",
        "To build representative corpora why not first select representative texts constrained by genre of writing?",
        "In general one possible technique for methodical corpus selection would be to build large corpora out of representative sub-collections constrained by genre and subject matter.",
        "Zelig Harris said, \"Certain proper subsets of the sentences of a language may be closed under some or all of the operations defined in the language, and thus constitute a sublanguage of it.\"",
        "(Harris, 1968) Calling this an inductive definition of sublanguage Satoshi Sekine has embarked on studies involving new trends in the analysis of sublanguages (Sekine, 1994).",
        "Both Harris and Sekine recognized that sublanguages are an efficient way to observe and measure the properties of natural language in smaller, representative blocks.",
        "Getting down to specifics, McEnery and Wilson (McEnery and Wilson, 1996) have hypothesized that genres of writing, such as the style used in newspapers and similar printed publications to report news stories, represent a constrained subset of a natural language.",
        "Thus newspaper texts constitute a sublanguage – a version of a natural language which does not display all of the creativity of that natural lan",
        "guage.",
        "The newspaper sublanguage can be further constrained by subject matter to divide it into smaller, more manageable subsets.",
        "A key mathematical feature of a sublanguage is that it will show a high degree of closure at various levels of description, setting it apart from unconstrained natural language.",
        "This closure property of a sublanguage is analogous to the mathematical property of transitive closure.",
        "McEnery and Wilson used the closure property to measure and compare rates of lexical and syntactic closure in three corpora: the IBM computer manual corpus, the Canadian Hansards, and the American Printing House for the Blind corpus.",
        "To date, however, there has been little work in a similar vein in other languages."
      ]
    },
    {
      "heading": "2 Overview",
      "text": [
        "This work applies the methodology of McEnery and Wilson to examine closure rates in a comparative study of all available tagged Chinese newspaper corpora.",
        "First I define lexical and syntactic closure for this study in section 3.",
        "Then, section 4 begins this study with an examination of all the newspaper texts of the Academica Sinica Balanced Corpus (ASBC).",
        "Section 5 extends this study to an examination of"
      ]
    },
    {
      "heading": "3 Lexical and Syntactic Closure",
      "text": []
    },
    {
      "heading": "3.1 Tokenization in Chinese",
      "text": [
        "It should be pointed out that Chinese is an agglutinative, not an inflected language.",
        "Moreover, while Chinese tokens can concatenate, Chinese has no extensive morphology like many Indo-European languages.",
        "Chinese, of course, has no white space separating lexemes, as a result, all Chinese text must first be segmented into word lengths.",
        "However, once a text has been segmented, no stemming is needed so each segmented Chinese word can be counted as it occurs without the need of finding its lemma."
      ]
    },
    {
      "heading": "3.2 • Lexical Closure",
      "text": [
        "Lexical closure is that property of a collection of text whereby given a representative sample, the number of new lexical forms seen among every additional 1,000 new tokens begins to level off at a rate below 10 per cent."
      ]
    },
    {
      "heading": "3.3 Syntactic Closure",
      "text": [
        "Syntactic closure is that property of a collection of text whereby given a representative sample of a type of text, then the number of new syntactic forms seen among every additional 1,000 new tokens begins to level off.",
        "A syntactic form is the combination of token plus type.",
        "Thus syntactic closure approaches as the number of new grammatical uses for a previously observed token plus the number of new tokens, regardless of syntactic use, level off to a growth rate below 10 per cent."
      ]
    },
    {
      "heading": "4 Academica Sinica Balanced Corpus (ASBC)",
      "text": [
        "While it is common practice to attempt to build huge annotated corpora, it is of course very tedious, very expensive, and especially challenging for annotators to maintain consistency over such a huge task.",
        "Consequently one must hope that once an annotated corpus of newspaper texts is created, it can be statistically measured and confirmed to be a representative sample.",
        "I first measured lexical and syntactic closure rates in all ASBC newspaper texts but found that when viewed as a whole this newspaper sub-collection of the ASBC does not approach closure (see graphs below)."
      ]
    },
    {
      "heading": "Tokens",
      "text": [
        "This raises the question how can we hope for NLP applications to learn on large corpora if they themselves never approach statistical closure, never approach being statistically confirmed as a representative model of the language?",
        "I then focused downward on subsections of the newspaper corpus-grouping them by similar filename.",
        "I searched the ASBC corpus looking for files of annotated newspaper text and found a total of 57 files (18.7 Mb); my findings are summarized in the following table.",
        "The large single file, named \"SSLA\", dealt with a wide assortment of subject matter and thus was significantly different from the other 3 newspaper collections.",
        "Not only was its individual file size rather large; it was not even close to the size and homogeneity of the other three newspaper multi-file collections.",
        "I rejected it from further study.",
        "The other sub-collections were more similar.",
        "Topically speaking, the ASBC \"A\" newspaper collection was focused primarily on news (77 per cent) while at the same time focusing narrowly on academic events in 1994.",
        "The ASBC \"C\" newspaper collection was less narrowly focused on news (73.5 per cent) but expanded its focus to other than academia while limiting itself to events of 1993.",
        "The ASBC \"T\" newspaper collection, however, spanned the period 1991 through 1995 and dealt with many different subjects, the most frequent of which were sports, news, and domestic politics, but even each of these most frequent subjects only represented 9 per cent of the whole.",
        "Let us consider the three ASBC newspaper sub-collections (\"A\", \"C\", and \"T\" filenames) to be potentially representative sublanguages.",
        "If we can observe relatively high degrees of closure at various levels of description, we can propose that such sub-collections are representative sublanguages within the newspaper genre of Chinese natural language.",
        "Conversely, those which do not have a high degrees of closure are definitely not sublanguage corpora and not of further interest for this study.",
        "The following graphs depict the observed lexical and syntactic closure rates of the three ASBC newspaper sub-collections under study."
      ]
    },
    {
      "heading": "Tokens",
      "text": [
        "It appears that the ASBC \"A\" newspaper sub-collections does approach lexical closure; while the \"C\" and \"T\" newspaper sub-collections definitely do not.",
        "It appears that the ASBC \"A\" newspaper sub-collection also approaches syntactic closure; while the \"C\" and \"T\" newspaper sub-collections do not."
      ]
    },
    {
      "heading": "5 UPenn Chinese Treebank",
      "text": []
    },
    {
      "heading": "6 Findings",
      "text": [
        "The UPenn data initially approaches lexical and syntactic closure at a rate which can be favorably compared with the ASBC \"A\" Chinese newspaper sub-corpus.",
        "By the time 59,000 tokens of the UPenn corpus were tagged, only 56 new token+tag combinations were observed in the preceding 1,000 tokens and 27 of those were new proper nouns.",
        "In comparison, at this point the dosure rate in the ASBC \"A\" corpus was not quite as good (see table below).",
        "But continuing to the 69,000 token mark, the ASBC \"A\" closure rate had overtaken that of the UPenn CTB.",
        "Interestingly the graphs for both the ASBC \"A\" and the UPenn CTB data reveal two-humped curves.",
        "At approximately the 60,000 token mark on both UPenn CTB graphs the curve was starting to flatten only to suddenly shift into a sharper climb."
      ]
    },
    {
      "heading": "ROCLIns A Collecdon Lexical Cloeure",
      "text": [
        "!I 11 11111mill"
      ]
    },
    {
      "heading": "TOkilri",
      "text": []
    },
    {
      "heading": "UPenn CTB Lexical Crowe",
      "text": [
        "An investigation of the UPenn CTB data revealed that the vast majority of the documents at this point dealt with international aspects of the Chinese economy 1, whereas previously the vast majority of documents had focused on Chinese domestic economic growth 2.",
        "And similarly on the ASBC \"A\" collection closure graphs around the 30,000 token mark, both 'Headlines of articles in UPenn CTB from 66,800-68,300 tokens – 66809:",
        "curves start to flatten only to suddenly around the 40,000 token mark shift back into a climb until about 70,000 tokens.",
        "An investigation of the ASBC data also showed a subtle shift in subject matter, most of the documents from 29,000 to 39,000 tokens were short notes and simple bulletins on, shifts in academic positions, whereas the later data had more long stories on subjects of greater complexity 3, whereas the later data had more long stories on subjects of greater complexity 4.",
        "Thus the ASBC \"A\" collection, more so than the UPenn CTB corpus, did eventually seem to approach closure.",
        "By the time we reach 80,000 tokens, the ASBC \"A\" collection only saw 14 new tokens-tag combinations in the last thousand tokens of new text(see table below).",
        "Six of those 14 new combinations were nouns, seven were verbs, and one was a preposition 6, having nearly reached lexical closure on newspa",
        "X3:FAEiEWVis.5.14.-Ak 29444: 111 E -Iklat • 29948: Mt7Fiet2gi3 • 31003: l*M44P4P4M• • 31499: ttn-gvPa 31890: IRM41A3M ; 32246: trATaa .",
        "3Topics of articles in ASBC \"A\" from 29,300-32,000 tokens 29306: 29337: 29382: AA: k*30V/14.1 29537: 29581: AA: 4N8 29636: iffit: i-Xt*Sc • 30087: AA: ; 30153: fue, rtgligt4J 1001VfifFA • 30342: 30402: 30449: 30478: 30539: AA: A44-bt ; 30590: 30677: 30772: 30822: 30944: AA 444 ; 30976: 31093: AA: **CI* .",
        "4Topics of articles in ASBC \"A\" from 56,000-57,000 tokens 56183: A: a*IiiMifigiatithff • 56232: ffIE: /ft-II-423/1 ; 56274: ifit: smmi.",
        "56323: fue: 1-44MEMVEC fki** • 56475: AS ffiti",
        "Na Na MCA, Na II, Nc Hat P VA VC A, VC /, VC NV, VF VF 411, VJ #t; Total tags: 7, Total new items: 14. per articles regarding academics.",
        "In contrast the CTB collection instead logged 146 new token--tag combinations at the 80,000 token mark and its new vocabulary ranged widely across 12 different parts of speech 6.",
        "While the majority of this new vocabulary were nouns, this continuing influx of new words was due primarily to the late inclusion of international news in the CTB's collection of newspaper articles regarding Chinese economics."
      ]
    },
    {
      "heading": "7 Corpus Building Implications",
      "text": [
        "The fact that the UPenn Chinese Treebank data approaches lexical and syntactic closure at rates comparable to the ASBC \"A\" file newspaper collection suggests that if the UPenn data had been selected more narrowly, it might have reached closure for the economics domain in the newspaper genre even sooner.",
        "Some day corpus linguistics may only need much smaller collections of annotated corpora than is the practice today, relying on new directions in sublanguage research.",
        "In the case of the ASBC \"A\" collection, for example, a robust learning structure should be able to build a useable model on 100,000 words worth of data like this which exhibits strong tendencies to lexical and syntactic closure in the Chinese newspaper genre constrained to a given domain.",
        "If the UPenn CTB were enlarged by the infusion of more news stories on international aspects of Chinese economic development, the CTB might better reach lexical and syntactic closure.",
        "The following graph shows that the blind addition of 20K additional Chinese economic news stories does not aid closure much.",
        "This additional data spanned many topics not seen in the original 100K collection.",
        "If it had been selected precisely to aid closure by measuring its potential contribution before extensive hand annotation, the result could have been better."
      ]
    },
    {
      "heading": "20K Augmented GIB Lexical Omura 20K Augmented CTB Syntactic Closure",
      "text": []
    },
    {
      "heading": "Tokens",
      "text": [
        "Nevertheless, this expanded CTB collection is sufficiently improved that the rate of closure toward of the expanded collection is better (see table below).",
        "Consequently, this expanded CTB collection is sufficiently improved that merging it with the ASBC \"A\" collection results in a far more measurably representative larger corpus as shown in the final two graphs below.",
        "The creation of such measurably representative large corpora out of such smaller, better focused sublanguage building blocks would be cheaper and faster without the resulting tools developed against such corpora suffering much degradation in speed or accuracy."
      ]
    },
    {
      "heading": "Combined ASBC 'A\" and UPenn CM Lexical Closure",
      "text": [
        "III Tokens"
      ]
    },
    {
      "heading": "8 Discussion",
      "text": [
        "Since only two Chinese tagged corpora are available at the present time, only about 200,000 words of Chinese corpora have been so studied.",
        "But to see more work in this vein, one need only consult McEnery and Wilson's study of three English corpora: the IBM computer manual corpus, the Canadian Hansards, and the American Printing House for the Blind corpus (McEnery and Wilson, 1996).",
        "Their study (exhaustively detailed in Chapter 6 of their book) spans more than 2.2 million words of tagged English text in three different domains.",
        "Consequently the total of 2 3 million words from McEnery and Wilson's results in tagged English texts when combined with these results in tagged Chinese newspaper texts should satisfy any who might argue that there is insufficient data upon which to draw some general conclusions.",
        "This paper does not argue that the two closure measures used are the only measures possible.",
        "The argument here is simply that these two closure measures are used to spot when a sublanguage corpus approaches closure-that is, when the curve of new types and new combinations of type with token begins to flatten at a rate below ten percent.",
        "One can readily point out that no natural language corpus can ever guarantee closure.",
        "The best anyone can aspire to do today, given the current state of our art, is to only approach closure."
      ]
    },
    {
      "heading": "9 References References",
      "text": []
    }
  ]
}
