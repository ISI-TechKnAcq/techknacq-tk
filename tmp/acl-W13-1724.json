{
  "info": {
    "authors": [
      "Victoria Bobicev"
    ],
    "book": "BEA",
    "id": "acl-W13-1724",
    "title": "Native Language Identification with PPM",
    "url": "https://aclweb.org/anthology/W13-1724",
    "year": 2013
  },
  "references": [
    "acl-J00-3004",
    "acl-N12-1033"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper reports on our work in the NLI shared task 2013 on Native Language Identification.",
        "The task is to automatically detect the native language of the TOEFL essays authors in a set of given test documents in English.",
        "The task was solved by a system that used the PPM compression algorithm based on an n-gram statistical model.",
        "We submitted four runs; word-based PPMC algorithm with normalization and without, character-based PPMC algorithm with normalization and without.",
        "The worst result was obtained on training and testing data during the evaluation procedure using the character-based PPM method and normalization: accuracy = 31.9%; the best one was macroaverage F-measure = 0.708 with the word-based PPMC algorithm without normalization."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "With the emergence of user-generated web content, text author profiling is being increasingly studied by the NLP community.",
        "Various works describe experiments aiming to automatically discover hidden attributes of text which reveal author's gender, age, personality and others.",
        "While English remains one of the main global languages used for communication, interchange of information and ideas, English texts written by different language speakers differ considerably.",
        "This is yet another characteristic of the author that can be learned from a text.",
        "While a great number of works have presented investigations in this area there was no common ground to evaluate different techniques and approaches to Native Language Identification.",
        "NLI shared task 2013 on Native Language Identification provides a playground and a corpus for such an evaluation.",
        "We participated in this shared task with the PPM compression algorithm based on a character-based and word-based n-gram statistical model."
      ]
    },
    {
      "heading": "2 Related work",
      "text": [
        "The task of Native Language Identification is to automatically detect text's author's native language when having only English text written by this author.",
        "It is generally a subtask of text classification or, more closely, text author profiling when various stylometric text features are used for certain author's characteristics (gender, age, education, cultural background, etc.)",
        "detection (Bergsma et al., 2012; Argamon et al., 2009).",
        "This task is mostly solved by machine-learning algorithms, such as SVM (Witten and Frank, 2005).",
        "However, the algorithm itself is not the most influential choice for better performance but rather the set of features used for learning.",
        "This set can consist of character, word and PoS n-grams, functional words, punctuation, specific errors, syntactic structures, and others.",
        "Some works investigate the influence of thousands of features of very different types (Koppel et al., 2011; Abbasi and Chen, 2008).",
        "Extraction of all these features requires a substantial amount of text processing work.",
        "We, instead, concentrated on an easier method, namely, PPM, a statistical model used for text compression which almost needs no text preprocessing.",
        "Several approaches that apply compression models to text classification have been presented in Eibe et",
        "al.",
        "(2000); Thaper (1996).",
        "The underlying idea of using compression methods for text classification was their ability to create a language model adapted to particular texts.",
        "It was hypothesized that this model captures individual features of the text being modelled.",
        "Theoretical background to this approach was given in Teahan and Harper (2001)."
      ]
    },
    {
      "heading": "3 System description",
      "text": [
        "Detection of the English text author's native language can be viewed as a type of classification task.",
        "Such tasks are solved using learning methods.",
        "There are different types of text classification.",
        "Authorship attribution, spam filtering, dialect identification are just several of the purposes of text categorization.",
        "It is natural that for different types of categorization different methods are pertinent.",
        "The most common type is the content-based categorization which classifies texts by their topic and requires the most common classification methods based on classical set of features.",
        "More specific methods are necessary in cases when classification criterions are not so obvious, for example, in the case of author identification.",
        "In this paper the application of the PPM (Prediction by Partial Matching) model for automatic text classification is explored.",
        "Prediction by partial matching (PPM) is an adaptive finite-context method for text compression that is a back-off smoothing technique for finite-order Markov models (Bratko et al., 2006).",
        "It obtains all information from the original data, without feature engineering, is easy to implement and relatively fast.",
        "PPM produces a language model and can be used in a probabilistic text classifier.",
        "PPM is based on conditional probabilities of the upcoming symbol given several previous symbols (Cleary and Witten, 1984).",
        "The PPM technique uses character context models to build an overall probability distribution for predicting upcoming characters in the text.",
        "A blending strategy for combining context predictions is to assign a weight to each context model, and then calculate the weighted sum of the probabilities:",
        "For example, the probability of character 'm' in context of the word 'algorithm' is calculated as a sum of conditional probabilities dependent on different context lengths up to the limited maximal length:",
        "bility of an unknown character.",
        "PPM is a special case of the general blending strategy.",
        "The PPM models use an escape mechanism to combine the predictions of all character contexts of length m, where m is the maximum model order; the order 0 model predicts symbols based on their unconditioned probabilities, the default order 1 model ensures that a finite probability (however small) is assigned to all possible symbols.",
        "The PPM escape mechanism is more practical to implement than weighted blending.",
        "There are several versions of the PPM algorithm depending on the way the escape probability is estimated.",
        "In our implementation, we used the escape method C (Bell et al., 1989), named PPMC.",
        "Treating a text as a string of characters, a character-based PPM avoids defining word boundaries; it deals with different types of documents in a uniform way.",
        "It can work with texts in any language and be applied to diverse types of classification; more details can be found in Bobicev (2007).",
        "Our utility function for text classification was cross-entropy of the test document:",
        "where n is the number of symbols in a text d, Hd m ?",
        "entropy of the text d obtained by model m, pm(xi) is a probability of a symbol xi in the text d. Hd m was estimated by the modelling part of the compression algorithm.",
        "Usually, the cross-entropy is greater than the entropy, because the probabilities of symbols in diverse texts are different.",
        "The cross-entropy can be used as a measure for document similarity; the lower cross-entropy for two texts is, the more simi",
        "lar they are.",
        "Hence, if several statistical models had been created using documents that belong to different classes and cross-entropies are calculated for an unknown text on the basis of each model, the lowest value of cross-entropy will indicate the class of the unknown text.",
        "In this way cross-entropy is used for text classification.",
        "On the training step, we created PPM models for each class of documents; on the testing step, we evaluated cross-entropy of previously unseen texts using models for each class.",
        "The lowest value of cross-entropy indicates the class of the unknown text.",
        "The maximal length of a context equal to 5 in PPM model was proven to be optimal for text compression (Teahan, 1998).",
        "In other experiments, length of character n-grams used for text classification varied from 2 (Kukushkina et al., 2001) to 4 (Koppel et al., 2011) or a combination of several lengths (Keselj et al., 2003).",
        "Stamatatos (2009) pointed out that the best length of character n-grams depends on different conditions and varies for different texts.",
        "In all our experiments with character-based PPM model we used maximal length of a context equal to 5; thus our method is PPMC5.",
        "The character-based PPM models were used for spam detection, source-based text classification and classification of multi-modal data streams that included texts.",
        "In Bratko et al. (2006), the character-based PPM models were used for spam detection.",
        "In this task there existed two classes only: spam and legitimate email (ham).",
        "The created models showed strong performance in the Text Retrieval Conference competition, indicating that data-compression models are well suited to the spam filtering problem.",
        "In Teahan (2000), a PPM-based text model and minimum cross-entropy as a text classifier were used for various tasks; one of them was an author detection task for the well known Federalist Papers.",
        "In Bobicev and Sokolova (2008), the PPM algorithm was applied to text categorization in two ways: on the basis of characters and on the basis of words.",
        "Character-based methods performed almost as well as SVM, the best method among several machine learning methods compared in Debole and Sebastiani (2004) for the Reuters-21578 corpus.",
        "Usually, PPM models are character-based.",
        "However, word-based models were also used for various purposes.",
        "For example, if texts are classified by the contents, they are better characterized by words and word combinations than by fragments consisting of five letters.",
        "For some tasks words can be more indicative text features than character sequences.",
        "That's why we decided to use both character-based and word-based models for PPM text classification.",
        "In the case of word-based PPM, the context is only one word and an example for formula (1) looks like the following:",
        "where wordi is the current word; wordi-1 is the previous word.",
        "This model is coded as PPMC1 because of the same C escape method and one length context used for probability estimation.",
        "Training and testing data is distributed quite unevenly in many tasks, for example, in Reuters21578 corpus.",
        "This imbalance drastically affected the results of the classification experiments; the classification was biased towards classes with a larger volume of data for training.",
        "Such imbalance class distribution problems were mentioned in Bobicev and Sokolova (2008), Stamatatos (2009), Narayanan et al. (2012).",
        "Considering the fact that unbalanced data affected classification results in such a substantial way we used a normalization procedure for balancing entropies of the statistical data models.",
        "The first step of our algorithm was training.",
        "In the process of training, statistical models for each class of texts were created.",
        "This meant that probabilities of text elements were estimated.",
        "The next step after training was calculation of entropies of test documents on the basis of each class model.",
        "We obtained a matrix of entropies ?class statistical models x test documents?.",
        "The columns were entropies for the class statistical models and rows were entropies for a given test documents.",
        "After this step the normalization procedure was applied.",
        "The procedure consisted of several steps.",
        "(1) Mean entropy for each class of texts was calculated on the base of the matrix; (2) Each value in the matrix was divided by the mean entropy for this class.",
        "Thereby we obtained more balanced values and classification improved considerably.",
        "Although the application of PPM model to the document classification is not new, PPM was never",
        "applied to the task of English text author's native language detection.",
        "In order to evaluate the PPM classification method for English text author's native language identification a number of experiments were performed.",
        "The aim of the experiments was twofold:",
        "- to evaluate the quality of PPM-based document classification; - to compare letter-based and word-based PPM classification."
      ]
    },
    {
      "heading": "4 Evaluation",
      "text": [
        "Three sets of experiments were carried out during the NLI shared task event.",
        "The first one was performed on the training and development data released in January.",
        "The second set consisted of evaluation runs on test data released in March and the results for these experiments were provided by the organizers.",
        "The third set was 10-fold cross-validation on training + development data requested by the organizers."
      ]
    },
    {
      "heading": "4.1 The First set of experiments",
      "text": [
        "The first set of experiments was carried out on the first set of data released by the organizers: TOEFL essays written by 11 native languages speakers.",
        "9,900 essays of this set were sequestered as the training data and 1,100 were for the development set.",
        "Thus, we trained our model on 900 files for each native language speakers, for each class.",
        "Next, we attributed classes to 1,100 development texts.",
        "We carried out four experiments.",
        "The first two were done on the basis of the character-based PPMC5 method with and without the normalization procedure described earlier.",
        "The second two experiments were done with the word-based PPMC1 method with and without the normalization.",
        "The Precision, Recall and F-measure for these four experiments are presented in Table 1.",
        "Tables 2 and 3 are confusion tables for the worst and for the best cases of the four experiments."
      ]
    },
    {
      "heading": "4.2 The second set of experiments",
      "text": [
        "The second set of experiments was done on the 1,100 test files during the evaluation phase of the challenge.",
        "The results of these experiments were provided by the organizers.",
        "Again, we carried out four experiments: character-based PPMC5 method with and without normalization and word-based PPMC1 method with and without normalization.",
        "Confusion tables 4 and 5 presents the worst and the best results.",
        "The overall accuracies for these experiments are:",
        "The overall accuracy is 31.9%."
      ]
    },
    {
      "heading": "4.3 The third set of experiments",
      "text": [
        "The third set of the experiments was done at the organizers?",
        "request on the basis of training + development data.",
        "10-fold cross-validation was made on this data with exactly the same splitting used in Tetreault et al. (2012).",
        "The results of these experiments are presented in Table 6.",
        "Tables 7 and 8 are confusion tables for the worst and the best cases among all 10 folds and four experiments."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "The task of identifying the native language of a writer based solely on a sample of their English writing is an exiting and intriguing task.",
        "It is a type of text classification task; however it requires task specific features.",
        "The PPM method presented in this paper uses two types of features: (1) character sequences of length from 5 characters and shorter, (2) words and bigrams of words.",
        "This method achieved lower results than methods which used carefully selected and adjusted feature sets.",
        "The advantage of this method is its relative simplicity of use and ability to work with any text.",
        "Two interesting and surprising conclusions we have drawn from these experiments: (1) normalization did not improve the results for this data; (2) word-based method performed much better than character-based.",
        "In most previous experiments with PPM-based classification (Bobicev, 2007; Bobicev and Sokolova, 2008) we obtained inverse results: character-based methods were much better than word-based.",
        "The author recognition experiments showed the same, much better performance of character-based methods.",
        "The possible explanation is that the data for this experiment was cleaned and tokenized whereas the data in other experiments was much noisier which created problems for the word-based method.",
        "The same was with normalization.",
        "The organizers prepared very well balanced data and there was no need of normalization which helped to gain another 20-25% of accuracy on other data."
      ]
    }
  ]
}
