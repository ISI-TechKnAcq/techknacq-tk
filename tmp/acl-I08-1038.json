{
  "info": {
    "authors": [
      "Bo Wang",
      "Houfeng Wang"
    ],
    "book": "Proceedings of the Third International Joint Conference on Natural Language Processing",
    "id": "acl-I08-1038",
    "title": "Bootstrapping Both Product Features and Opinion Words from Chinese Customer Reviews with Cross-Inducing",
    "url": "https://aclweb.org/anthology/I08-1038",
    "year": 2008
  },
  "references": [
    "acl-J93-1003",
    "acl-P02-1053",
    "acl-P89-1010",
    "acl-W03-0404",
    "acl-W03-1014",
    "acl-W05-0408"
  ],
  "sections": [
    {
      "text": [
        "Bo Wang Houfeng Wang",
        "Institute of Computational Linguistics Institute of Computational Linguistics Peking University Peking University",
        "Beijing, 100871, China Beijing, 100871, China",
        "wangbo@pku.edu.cn wanghf@pku.edu.cn",
        "We consider the problem ofl identifying product features and opinion words in a unified process from Chinese customer reviews when only a much small seed set of opinion words is available.",
        "In particular, we consider a problem setting motivated by the task of identifying product features with opinion words and learning opinion words through features alternately and iteratively.",
        "In customer reviews, opinion words usually have a close relationship with product features, and the association between them is measured by a revised formula of mutual information in this paper.",
        "A bootstrapping iterative learning strategy is proposed to alternately both of them.",
        "A linguistic rule is adopted to identify low-frequent features and opinion words.",
        "Furthermore, a mapping function from opinion words to features is proposed to identify implicit features in sentence.",
        "Empirical results on three kinds of product reviews indicate the effectiveness of our method."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "With the rapid expansion of network application, more and more customer reviews are available online, which are beneficial for product merchants to track the viewpoint of old customers and to assist potential customers to purchase products.",
        "However, it's time-consuming to read all reviews in person.",
        "As a result, it's significant to mine customer reviews automatically and to provide users with opinion summary.",
        "In reality, product features and opinion words play the most important role in mining opinions of customers.",
        "One customer review on some cell phone is given as follows:",
        "appearance is beautiful, the screen is big and the photo effect is OK.)",
        "Product features are usually nouns such as \"^b M\" (appearance) and \"MH\" (screen) or noun phrases such as \"^SM^H\" (photo effect) expressing which attributes the customers are mostly concerned.",
        "Opinion words (opword is short for \"opinion word\") are generally adjectives used to express opinions of customers such as \"iUcnS\" (beautiful), \";*v\" (big) and \"£P\" (well).",
        "As the core part of an opinion mining system, this paper is concentrated on identifying both product features and opinion words in Chinese customer reviews.",
        "There is much work on feature extraction and opinion word identification.",
        "Hu and Liu (2004) makes use of association rule mining (Agrawal and Srikant, 1994) to extract frequent features, the surrounding adjectives of any extracted feature are considered as opinion words.",
        "Popescu and Etzioni (2005) has utilized statistic-based point-wise mutual information (PMI) to extract product features.",
        "Based on the association of opinion words with product features, they take the advantage of the syntactic dependencies computed by the MINIPAR parser (Lin, 1998) to identify opinion words.",
        "Turney (2002) applied a specific unsupervised learning technique based on the mutual information between document phrases and two seed words \"excellent\" and \"poor\".",
        "Nevertheless, in previous work, identifying product features and opinion words are always considered two separate tasks.",
        "Actually, most product features are modified by the surrounding opinion words in customer reviews, thus they are highly context dependent on each other, which is referred to as context-dependency property henceforth.",
        "With the co-occurrence characteristic, identifying product features and opinion words could be combined into a unified process.",
        "In particular, it is helpful to identify product features by using identified opinion words and vice versa.",
        "That implies that such two subtasks can be carried out alternately in a unified process.",
        "Since identifying product features are induced by opinion words and vice versa, this is called cross-inducing.",
        "As the most important part of a feature-based opinion summary system, this paper focuses on learning product features and opinion words from Chinese customer reviews.",
        "Two subtasks are involved as follows:",
        "Identifying features and opinion words: Resorting to context-dependency property, a bootstrapping iterative learning strategy is proposed to identify both of them alternately.",
        "Identifying implicit features: Implicit features occur frequently in customer reviews.",
        "An implicit feature is defined as a feature that does not appear in an opinion sentence.",
        "The association between features and opinion words calculated with the revised mutual information is used to identify implicit features.",
        "This paper is sketched as follows: Section 2 describes the approach in detail; Experiment in section 3 indicates the effectiveness of our approach.",
        "Section 4 presents related work and section 5 concludes and presents the future work."
      ]
    },
    {
      "heading": "2. The Approach",
      "text": [
        "Figure 1 illustrates the framework of an opinion summary framework, the principal parts related to this paper are shown in bold.",
        "The first phase \"identifying features and opinion words\", works iteratively to identify features with the opinion words identified and learn opinion words through the product features identified alternately.",
        "Then, one linguistic rule is used to identify low-frequent features and opinion words.",
        "After that, a mapping function is designed to identify implicit features.",
        "Preprocessings Identifying and Opinion Identifying Low Frequent Features and Opinion Wordss",
        "Identifying Implicit Features^ Feature Lexicons Opword Lexicons Opinion Sentences with Featiu'es and Opinion Words*- Seed Opwordss",
        "Polarity Identification*-",
        "Figure 1.",
        "The framework of an opinion summary system",
        "Product features and opinion words are highly context-dependent on each other in customer reviews, i.e., the feature \"ft^\" (body) for digital camera often co-occur with some opinion words such as (big) or \"/JnI5\" (delicate) while the feature \"ttffrtfc\" (the proportion of performance to price) often co-occurs with the opinion word \"iU\" (high).",
        "Product features can be identified resorting to the surrounding opinion words identified before and vice versa.",
        "A bootstrapping method that works iteratively is proposed in algorithm 1.",
        "Algorithm 1 works as follows: given the seed opinion words and all the reviews, all noun phrases (noun phrases in the form \"noun+\") form CandFe-aLex (the set of feature candidates) and all adjectives compose of CandOpLex (the set of the candidates of opinion words).",
        "The sets ResFeaLex and ResOpLex are used to store final features and opinion words.",
        "Initially, ResFeaLex is set empty while ResOpLex is composed of all the seed opinion words.",
        "At each iterative step, each feature candidate in CandFeaLex is scored by its context-dependent association with each opword in ResO-pLex, the candidate whose score is above the pre-specified threshold Thresholdfeature is added to Res",
        "eatures r\"V- – ",
        "Algorithm 1.",
        "Bootstrap learning product features and opinion words with cross-inducing Bootstrap-Learning (ReviewData, SeedOpLex, Thresholdfeature, Thresholdopword)"
      ]
    },
    {
      "heading": "1. Parse(ReviewData);",
      "text": []
    },
    {
      "heading": "2. ResFeaLex = {}, ResOpLex = SeedOpLex;",
      "text": []
    },
    {
      "heading": "3. CandFeaLex = all noun phrases in ReviewData;",
      "text": []
    },
    {
      "heading": "4. CandOpLex = all adjectives in ReviewData;",
      "text": []
    },
    {
      "heading": "6. do for each candfea e CandFeaLex",
      "text": []
    },
    {
      "heading": "7. do for each opword e ResOpLex",
      "text": []
    },
    {
      "heading": "8. do calculate RMI(candfea,opword) with ReviewData;",
      "text": [
        "9 score(canfea)=EopwordeResOpLexRMI(candfea,opword)/|ResOpLex|;"
      ]
    },
    {
      "heading": "10. sort CandFeaLex by score;",
      "text": []
    },
    {
      "heading": "11. for each candfea e CandFeaLex",
      "text": []
    },
    {
      "heading": "12. do if (score(candfea)> Threshold feature )",
      "text": []
    },
    {
      "heading": "13. then ResFeaLex=ResFeaLex+{candfea};",
      "text": []
    },
    {
      "heading": "14. CanFeaLex=CandFeaLex - {candfea};",
      "text": []
    },
    {
      "heading": "15. for each candop e CandOpLex",
      "text": []
    },
    {
      "heading": "16. do for each feature e ResFeaLex",
      "text": []
    },
    {
      "heading": "17. do calculate RMI(candop,feature) with D;",
      "text": [
        "18 score(candop)=EfeatureeResFeaLexRMI(feature,candop)/|ResFeaLex| ;"
      ]
    },
    {
      "heading": "19. sort CandOpLex by score;",
      "text": []
    },
    {
      "heading": "20. for each candop e CandOpLex",
      "text": []
    },
    {
      "heading": "21. do if (score (candop)>Threshold opword)",
      "text": []
    },
    {
      "heading": "22. then ResOpLex=ResOpLex+{candop };",
      "text": []
    },
    {
      "heading": "23. CanOpLex=CandOpLex - {candop};",
      "text": []
    },
    {
      "heading": "24. if (neither candfea and candop is learned) then break;",
      "text": []
    },
    {
      "heading": "25. return ResFeaLex, ResOpLex;",
      "text": [
        "FeaLex and subtracted from CandFeaLex.",
        "Similarly, opinion words are processed in this way, but the scores are related to features in ResFeaLex.",
        "The iterative process continues until neither Res-FeaLex nor ResOpLex is altered.",
        "Any feature candidate and opinion word candidate, whose relative distance in sentence is less than or equal to the specified window size Minimum-Offset, are regarded to co-occur with each other.",
        "The association between them is calculated by the revised mutual information denoted by RMI, which will be described in detail in the following section and employed to identify implicit features in sentences.",
        "In customer reviews, features and opinion words usually co-occur frequently, features are usually modified by the surrounding opinion words.",
        "If the absolute value of the relative distance in a sentence for a feature and an opinion word is less than Minimum-Offset, they are considered context-dependent.",
        "Many methods have been proposed to measure the co-occurrence relation between two words such as x (Church and Mercer,1993) , mutual information (Church and Hanks, 1989; Pantel and Lin, 2002), t-test (Church and Hanks, 1989), and log-likelihood (Dunning,1993).",
        "In this paper a revised formula of mutual information is used to measure the association since mutual information of a low-frequency word pair tends to be very high.",
        "Table 1 gives the contingency table for two words or phrases w1 and w2, where A is the number of reviews where w1 and w2 co-occur; B indicates the number of reviews where w1 occurs but does not co-occur with w2; C denotes the number of reviews where w2 occurs but does not co-occur with w1; D is number of reviews where neither w1 nor w2 occurs; N = A + B + C + D.",
        "With the table, the revised formula of mutual information is designed to calculate the association of w1 with w2 as formula (1).",
        "Table 1: Contingency table RMI(wx,= freq(wx,xlog fvu",
        "In Chinese reviews, one linguistic rule \"noun+ adverb* adjective+\" occurs frequently and most of the instances of the rule are used to express positive or negative opinions on some features, i.e., \"ft JlVnoun tfc$J/adverb ^^/adjective\" (The body is rather delicate) , where each Chinese word and its part-of-speech is separated by the symbol \"/\".",
        "Intuitively, this linguistic rule can be used to improve the output of the iterative learning.",
        "For each instance of the rule, if \"noun+\" exists in Res-FeaLex, the \"adjective\" part would be added to ResOpLex, and if \"adjective+\" exists in ResOpLex, the noun phrase \"noun+\" part will be added to ResFeaLex.",
        "After that, most low-frequent features and opinion words will be recognized.",
        "The context-dependency property indicates the context association between product features and opinion words.",
        "As a result, with the revised mutual information, the implicit features can be deduced from opinion words.",
        "A mapping function f: op-word-}feature is used to deduce the mapping feature for opword , where f(opword) is defined as the feature with the largest association with opinion word.",
        "If an opinion sentence contains opinion words, but it does not have any explicit features, the mapping function f: opword-^ feature is employed to generate the implicit feature for each opinion word and the feature is considered as an implicit feature in the opinion sentence.",
        "Two instances are given in (b) and (c), where the implicit features are inserted in suitable positions and they are separated in parentheses.",
        "Since f (\"il^S\" (beautiful)) = \"£hl\" (appearance) and f (\"B\\blRj\" (fashionable)) = \"^b l\" (appearance), \"^bl\" (appearance) is an implicit feature in (b).",
        "Similarly, the implicit features in (c) are \"fW (performance) and (picture ).",
        "(b) (^l)i^MK^DW^.",
        "It's (appearance) beautiful and (appearance) fashionable.",
        "(c) (ffffi)tii,Ml(Bf)Iifo It's (performance) very stable and (picture) very clear."
      ]
    },
    {
      "heading": "3. Experiment 3.1 Data Collection",
      "text": [
        "We have gathered customer reviews of three kinds of electronic products from http://it168.com: digital camera, cellphone and tablet.",
        "The first 300 reviews for each kind of them are downloaded.",
        "One annotator was asked to label each sentence with product features (including implicit features) and opinion words.",
        "The annotation set for features and opinion words are shown in table 2.",
        "Unlike English, Chinese are not separated by any symbol.",
        "Therefore, the reviews are tokenized and tagged with part-of-speech by a tool ICTCLAS2.One example of the output of this tool is as (d).",
        "(d) JFft/n Sft/n j£/d )f/d f£/a , /w fji^ /n f^M/n feJF/v Sfc/d TO/v iA/v ftg/n «/n , /w Jl^/n j£#/vn « /vn &/d Wd JjWa „ /w The seed opinion words employed in the iterative learning are: \" yfBflr\" (clear), \" '\\k\" (quick), \"fi\" (white), \"MSfr\" (weak).",
        "(good), \"TO\" (good), \" U \" (high), \" <b \" (little), \" £ \" (many), \" ^ \" (long).",
        "Empirically, Thresholdfeature and Thresholdopword in Algorithm 1 is set to 0.2, Minimum-Offset is set to 4.",
        "Product",
        "No.",
        "of Fea-",
        "No.",
        "of Opin-",
        "Name",
        "tures",
        "ion Words",
        "digital camera",
        "135",
        "97",
        "cell-phone",
        "155",
        "125",
        "tablet",
        "96",
        "83",
        "Table 3.",
        "Evaluation of apriori algorithm Type Product Name _On Set_On Sentence_ Precision Recall F-Score Precision Recall F-score linguistic rule (the lower).",
        "As Hu and Liu (2004), the features mined form the result set while the features in the manually annotated corpus construct the answer set.",
        "With the two sets, precision, recall and f-score are used to evaluate the experiment result on set level.",
        "In our work, the evaluation is also conducted on sentence for three factors: Firstly, each feature or opinion word may occur many times in reviews but it just occurs once in the corresponding answer set; Secondly, implicit features should be evaluated on sentence; Besides, to generate an opinion summary, the features and the opinion words should be identified for each opinion sentence.",
        "On sentence, the features and opinion words identified for each opinion sentence are compared with the annotation result in the corresponding sentence.",
        "Precision, recall and f-score are also used to measure the performance.",
        "Hu and Liu (2004) have adopted associate rule mining to mine opinion features from customer reviews in English.",
        "Since the original corpus and source code is not available for us, in order to make comparison with theirs, we have reimplemented their algorithm, which is denoted as apriori method as follows.",
        "To be pointed out is that, the two pruning techniques proposed in Hu and Liu (2004): compactness pruning and redundancy pruning, were included in our experiment.",
        "The evaluation on our test data is listed in table 3.",
        "The row indexed by average denotes the average performance of the corresponding column and each entry in it is bold.",
        "Table 4 shows our testing result on the same data, the upper value in each entry presents the result for iterative learning strategy while the lower values denote that for the combination of iterative learning and the linguistic rule.",
        "The average row shows the average performance for the corresponding columns and each entry in the row is shown in bold.",
        "Product Name",
        "On Set",
        "On Sentence",
        "Precision",
        "Recall",
        "F-Score",
        "Precision",
        "Recall",
        "F-Score",
        "digital camera",
        "64.03%",
        "45.92%",
        "53.49%",
        "46.62%",
        "65.72%",
        "54.55%",
        "cell-phone",
        "54.43%",
        "43.87%",
        "48.58%",
        "34.17%",
        "55.15%",
        "42.19%",
        "tablet",
        "51.45%",
        "59.38%",
        "55.13%",
        "41.39%",
        "60.21%",
        "49.06%",
        "average",
        "56.64%",
        "49.72%",
        "52.40%",
        "40.73%",
        "60.36%",
        "48.60%",
        "digital camera",
        "73.57% 54.81% 62.82% 78.20% 73.33% 75.69%",
        "55.80% 68.69% 61.58%",
        "54.71% 70.80% 63.49%",
        "cell-phone",
        "80.92% 45.81% 58.50% 82.30% 66.46% 73.53%",
        "47.31% 58.59% 52.35% 49.22% 61.63% 54.73%",
        "tablet",
        "72.73% 57.29% 64.09% 77.99% 73.96% 75.92%",
        "49.79% 61.03% 54.84% 52.54% 64.43% 57.88%",
        "average",
        "75.74% 52.64% 61.80% 79.50% 71.25% 75.05%",
        "50.97% 62.77% 56.26% 52.16% 65.62% 58.70%",
        "digital camera",
        "89.02% 38.02% 53.28% 87.31% 60.94% 71.78%",
        "72.35% 50.24% 59.30% 69.40% 85.28% 76.53%",
        "cell-phone",
        "87.95% 30.80% 45.63% 88.49% 51.90% 65.43%",
        "66.44% 42.84% 52.09% 63.14% 79.51% 70.39%",
        "tablet",
        "77.94% 30.64% 43.98% 80.73% 50.87% 62.41%",
        "61.30% 42.69% 50.34% 63.92% 81.02% 71.46%",
        "average",
        "84.97% 33.15% 47.63% 85.51% 54.57% 66.54%",
        "66.70% 45.26% 53.91%",
        "65.49% 81.94% 72.79%",
        "On feature, the average precision, recall and f-score on set or sentence increase according to the order apriori < iterative < ite+rule, where apriori indicates Hu and Liu's method, iterative represents iterative strategy and iterative+rule denotes the combination of iterative strategy and the linguistic rule.",
        "The increase range from apriori to itera-tive+rule of f-score on set gets to 22.65% while on sentence it exceeds 10%.",
        "The main reason for the poor performance on set for apriori is that many common words such as \"NeLK\" (computer), \"t^H\" (China) and \"tftfl]\" (time of use) with high frequency are extracted as features.",
        "Moreover, the poor performance on sentence for apriori method is due to that it can't identify implicit features.",
        "Furthermore, the increase in f-score from iterative to ite+rule on set and on sentence shows the performance can be enhanced by the linguistic rule.",
        "Table 4 also shows that the performance in learning opinion words has been improved after the linguistic rule has been used.",
        "On set, the average precision increases from 84.97% to 85.51% while the average recall from 33.15% to 54.57%.",
        "Accordingly, the average f-score increase significantly by about 18.91%.",
        "On sentence, although there is a slow decrease in the average precision, there is a dramatic increase in the average recall, thus the average f-score has increased from 53.91% to 72.79%.",
        "Furthermore, the best f-score (66.54%) on set and the best f-score (72.79%) on sentence indicate the effectiveness of ite+rule on identifying opinion words."
      ]
    },
    {
      "heading": "4. Related Work",
      "text": [
        "Our work is much related to Hu's system (Hu and Liu,2004), in which association rule mining is used to extract frequent review noun phrase as features.",
        "After that, two pruning techniques: compactness pruning and redundancy pruning, are utilized.",
        "Frequent features are used to find potential opinion words (adjectives) and WordNet synonyms/antonyms in conjunction with a set of seed words are used in order to find actual opinion words.",
        "Finally, opinion words are used to extract associated infrequent features.",
        "The system only extracts explicit features.",
        "Our work differs from hers at two aspects: (1) their method can't identify implicit features which occur frequently in opinion sentences; (2) Product features and opinion words are identified on two separate steps in Hu's system but they are learned in a unified process here and induced by each other in this paper.",
        "Popescu and Etzioni (2005) has used web-based point-wise mutual information (PMI) to extract product features and use the identified features to identify potential opinion phrases with cooccurrence association.",
        "They take advantage of the syntactic dependencies computed by the MINIPAR parser.",
        "If an explicit feature is found in a sentence, 10 extraction rules are applied to find the heads of potential opinion phrases.",
        "Each head word together with its modifier is returned as a potential opinion phrase.",
        "Our work is different from theirs on two aspects: (1) Product features and opinion words are identified separately but they are learned simultaneously and are boosted by each other here.",
        "(2) They have utilized a syntactic parser MINIPAR, but there's no syntactic parser available in Chinese, thus the requirement of our algorithm is only a small seed opinion word lexicon.",
        "Although cooccurrence association is used to derive opinion words from explicit features in their work, the way how co-occurrence association is represented is different.",
        "Besides, the two subtasks are boosted by each other in this paper.",
        "On identifying opinion words, Morinaga et al. (2002)has utilized information gain to extract classification features with a supervised method; Hat-zivassiloglou and Wiebe (1997) used textual junctions such as \"fair and legitimate\" or \"simplistic but well-received\" to separate similarity-and oppositely-connoted words; Other methods are present in (Riloff et al., 2003; Riloff and Wiebe, 2003; Gamon and Aue, 2005; Wilson et al., 2006) The principal difference from previous work is that, they have considered extracting opinion words as a separate work but we have combined identifying features and opinion words in a unified process.",
        "Besides, the opinion words are identified for sentences but in their work they are identified for reviews."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "In this paper, identifying product features and opinion words are induced by each other and are combined in a unified process.",
        "An iterative learning strategy based on context-dependence property is proposed to learn product features and opinion words alternately, where the final feature lexicon and opinion word lexicon are identified with very few knowledge (only ten seed opinion words) and augmented by each other alternately.",
        "A revised formula of mutual information is used to calculate the association between each feature and opinion word.",
        "A linguistic rule is utilized to recall low-frequent features and opinion words.",
        "Besides, a mapping function is designed to identify implicit features in sentence.",
        "In addition to evaluating the result on set, the experiment is evaluated on sentence.",
        "Empirical result indicates that the performance of iterative learning strategy is better than apriori method and that features and opinion words can be identified with cross-inducing effectively.",
        "Furthermore, the evaluation on sentence shows the effectiveness in identifying implicit features.",
        "In future, we will learn the semantic orientation of each opinion word, calculate the polarity of each subjective sentence, and then construct a feature-based summary system."
      ]
    }
  ]
}
