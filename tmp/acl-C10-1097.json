{
  "info": {
    "authors": [
      "Derya Ozkan",
      "Kenji Sagae",
      "Louis-Philippe Morency"
    ],
    "book": "COLING",
    "id": "acl-C10-1097",
    "title": "Latent Mixture of Discriminative Experts for Multimodal Prediction Modeling",
    "url": "https://aclweb.org/anthology/C10-1097",
    "year": 2010
  },
  "references": [
    "acl-D09-1140",
    "acl-E06-1022",
    "acl-H94-1020",
    "acl-N07-1004",
    "acl-P03-1070",
    "acl-P05-1003",
    "acl-P07-1045",
    "acl-P07-2031",
    "acl-P08-1024",
    "acl-P08-1097",
    "acl-W07-1903",
    "acl-W98-0319"
  ],
  "sections": [
    {
      "text": [
        "Derya Ozkan, Kenji Sagae and Louis-Philippe Morency",
        "During face-to-face conversation, people naturally integrate speech, gestures and higher level language interpretations to predict the right time to start talking or to give backchannel feedback.",
        "In this paper we introduce a new model called Latent Mixture of Discriminative Experts which addresses some of the key issues with multimodal language processing: (1) temporal synchrony/asynchrony between modalities, (2) micro dynamics and (3) integration of different levels of interpretation.",
        "We present an empirical evaluation on listener nonverbal feedback prediction (e.g., head nod), based on observable behaviors of the speaker.",
        "We confirm the importance of combining four types of multimodal features: lexical, syntactic structure, eye gaze, and prosody.",
        "We show that our Latent Mixture of Discriminative Experts model outperforms previous approaches based on Conditional Random Fields (CRFs) and Latent-Dynamic CRFs."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Face-to-face communication is highly interactive.",
        "Even when only one person speaks at a time, other participants exchange information continuously amongst themselves and with the speaker through gestures, gaze and prosody.",
        "These different channels contain complementary information essential to interpretation and understanding of human behaviors (Oviatt, 1999).",
        "Psycholinguistic studies also suggest that gesture and speech come from a single underlying mental process, and they",
        "Figure 1: Example of multimodal prediction model: listener nonverbal backchannel prediction based on speaker's speech and eye gaze.",
        "As the speaker says the word her, which is the end of the clause (her is also the object of the verb bothering), and lowers the pitch while looking back at the listener and eventually pausing, the listener is then very likely to head nod (i.e., nonverbal backchannel).",
        "are related both temporally and semantically (McNeill, 1992; Cassell and Stone, 1999; Kendon, 2004).",
        "A good example of such complementarity is how people naturally integrate speech, gestures and higher level language to predict when to give backchannel feedback.",
        "Building computational models of such a predictive process is challenging since it involves micro dynamics and temporal relationship between cues from different modalities (Quek, 2003).",
        "Figure 1 shows an example of backchannel prediction where a listener head nod",
        "Pitch Gaze",
        "Look at listener",
        "Listener",
        "is more likely.",
        "For example, a temporal sequence from the speaker where he/she reaches the end of segment (syntactic feature) with a low pitch and looks at the listener before pausing is a good opportunity for the listener to give nonverbal feedback (e.g., head nod).",
        "These prediction models have broad applicability, including the improvement of nonverbal behavior recognition, the synthesis of natural animations for robots and virtual humans, the training of cultural-specific nonverbal behaviors, and the diagnoses of social disorders (e.g., autism spectrum disorder).",
        "In this paper we introduce a new model called Latent Mixture of Discriminative Experts (LMDE) which addresses some of the key issues with multimodal language processing: (1) temporal synchrony/asynchrony between modalities, (2) micro dynamics and (3) integration of different levels of interpretation.",
        "We present an empirical evaluation on nonverbal feedback prediction (e.g., head nod) confirming the importance of combining different types of multimodal features.",
        "We show that our LMDE model outperforms previous approaches based Conditional Random Fields (CRFs) and Latent-Dynamic CRFs."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Earlier work in multimodal language processing focused on multimodal dialogue systems where the gestures and speech may be constrained (Johnston, 1998; Jurafsky et al., 1998).",
        "Most of the research in multimodal language processing over the past decade fits within two main trends that have emerged: (1) recognition of individual multimodal actions such as speech and gestures (e.g, (Eisenstein et al., 2008; Frampton et al., 2009; Gravano et al., 2007)), and (2) recognition/summarization of the social interaction between more than one participants (e.g., meeting analysis (Heylen and op den Akker, 2007; Moore, 2007; Murray and Carenini, 2009; Jovanovic et al., 2006)).",
        "The work described in this paper can be seen from a third intermediate category where multimodal cues from one person is used to predict the social behavior of another participant.",
        "This type of predictive models has been mostly studied in the context of embodied conversational agents (Nakano et al., 2003; Nakano et al., 2007).",
        "In particular, backchannel feedback (the nods and paraverbals such as \"uh-hu\" and \"mm-hmm\" that listeners produce as someone is speaking) has received considerable interest due to its pervasiveness across languages and conversational contexts and this paper addresses the problem of how to predict and generate this important class of dyadic nonverbal behavior.",
        "Several researchers have developed models to predict when backchannel should happen.",
        "In general, these results are difficult to compare as they utilize different corpora and present varying evaluation metrics.",
        "Ward and Tsukahara (2000) propose a unimodal approach where backchannels are associated with a region of low pitch lasting 110ms during speech.",
        "Models were produced manually through an analysis of English and Japanese conversational data.",
        "Nishimura et al.",
        "(2007) present a unimodal decision-tree approach for producing backchannels based on prosodic features.",
        "Cathcart et al.",
        "(2003) propose a unimodal model based on pause duration and tri-gram part-of-speech frequency.",
        "The model was constructed by identifying, from the HCRC Map Task Corpus (Anderson et al., 1991), trigrams ending with a backchannel.",
        "Fujie et al.",
        "(2004) used Hidden Markov Models to perform head nod recognition.",
        "In their paper, they combined head gesture detection with prosodic low-level features from the same person to determine strongly positive, weak positive and negative responses to yes/no type utterances.",
        "In recent years, great research has shown the strength of latent variable models for natural language processing (Blunsom et al., 2008).",
        "One of the most relevant works is that of Eisenstein and Davis (2007), which presents a latent conditional model for fusion of multiple modalities (speech and gestures).",
        "One of the key difference of our work is that we are explicitly modeling the micro dynamics and temporal relationship between modalities."
      ]
    },
    {
      "heading": "3. Multimodal Prediction Models",
      "text": [
        "Human face-to-face communication is a little like a dance, in that participants continuously adjust their behaviors based on verbal and nonverbal displays and signals.",
        "A topic of central interest in modeling such behaviors is the patterning of interlocutor actions and interactions, moment-by-moment, and one of the key challenges is identifying the patterns that best predict specific actions.",
        "Thus we are interested in developing predictive models of communication dynamics that integrate previous and current actions from all interlocutors to anticipate the most likely next actions of one or all interlocutors.",
        "Humans are good at this: they have an amazing ability to predict, at a micro-level, the actions of an interlocutor (Bave-las et al., 2000); and we know that better predictions can correlate with more empathy and better outcomes (Goldberg, 2005; Fuchs, 1987).",
        "With turn-taking being perhaps the best-known example, we now know a fair amount about some aspects of communication dynamics, but much less about others.",
        "However, recent advances in machine learning and experimental methods, and recent findings from a variety of perspectives, including conversation analysis, social signal processing, adaptation, corpus analysis and modeling, perceptual experiments, and dialog systemsbuilding and experimentation, mean that the time is ripe to start working towards more comprehensive predictive models.",
        "The study of multimodal prediction models bring a new series of research challenges:",
        "Multimodal asynchrony While speech and gestures seem to come from a single underlying mental process (McNeill, 1992), they not always happen at the same time, making it hard for earlier multimodal fusion approaches based on synchrony.",
        "A multimodal prediction model needs to be able to learn automatically the temporal relationship (and relative importance) between modalities.",
        "Micro dynamics The dynamic between multimodal signals should be taken at a micro level since many of the interactions between speech and gesture happen at the sub-gesture level or subword level (Quek, 2003).",
        "Typical word-based sampling may not be sufficient and instead a higher sampling rate should be used.",
        "Limited annotated data Given the time requirement to correctly annotate multimodal data,",
        "Figure 2: Latent Mixture of Discriminative Experts: a new dynamic model for multimodal fusion.",
        "In this graphical model, Xj represents the jth multimodal observation, hj is a hidden state assigned to Xj , and yj the class label of Xj.",
        "Gray circles are latent variables.",
        "The micro dynamics and multimodal temporal relationships are automatically learned by the hidden states hj during the learning phase.",
        "most multimodal datasets contain only a limited number of labeled examples.",
        "Since many machine learning algorithms rely on a large training corpus, effective training of a predictive model on multimodal datasets is challenging."
      ]
    },
    {
      "heading": "4. Latent Mixture of Discriminative Experts",
      "text": [
        "In this paper we present a multimodal fusion algorithm, called Latent Mixture of Discriminative Experts (shown in Figure 2), that addresses the three challenges discussed in the previous section.",
        "The hidden states of LMDE automatically learn the temporal asynchrony between modalities.",
        "By using a constant sample rate of 30Hz in our experiments, we can model the micro dynamics of speech and prosody (e.g., change of intonation in the middle of a word).",
        "And finally, by training separate experts for each modalities, we improve the prediction performance even with limited datasets.",
        "The task of our LMDE model is to learn a mapping between a sequence of multimodal observations x = {x1; x2,xm} and a sequence of labels y = {yi,y2, ...,ym}.",
        "Each yj is a class label for the jth frame of a video sequence and is a member of a set Y of possible class labels, for example, Y = {head-nod, other-gesture}.",
        "Each frame observation x j is represented by a feature vector 0(xj) G Rd, for example, the prosodie features at each sample.",
        "For each sequence, we also assume a vector of \"sub-structure\" variables h = [hi, h2,hm}.",
        "These variables are not observed in the training examples and will therefore form a set of hidden variables in the model.",
        "Following Morency et al.",
        "(2007), we define our LMDE model as follows:",
        "where 6 is the model parameters that is to be estimated from training data.",
        "To keep training and inference tractable, Morency et al.",
        "(2007) restrict the model to have disjoint sets of hidden states associated with each class label.",
        "Each hj is a member of a set Hyjof possible hidden states for the class label yj.",
        "H, the set of all possible hidden states, is defined to be the union of all Hy sets.",
        "Since sequences which have any hj G Hyj will by definition have P (y | h, x, 6) = 0, latent conditional model becomes:",
        "h:Vhj £Hyj",
        "What differentiates our LMDE model from the original work of Morency et al.",
        "is the definition of P (h|x,6):",
        "where Z is the partition function and Pa(y|x) is the conditional distribution of the expert indexed by a.",
        "The expert conditional distributions are defined Pa(y|x, Aa) using the usual conditional random field formulation:",
        "Fa>k is defined as and each feature function /a k(yj-l,yj-, x,j) is either a state function sk (yj, x, j) or a transition function tk(yj-i,yj, x, j).",
        "State functions sk depend on a single hidden variable in the model while transition functions tk can depend on pairs of hidden variables.",
        "Ti (h, x), defined in Equation 3, is a special case, summing only over the transition feature functions ti(hi-i,hl, x, l).",
        "Each expert a contains a different subset of /a,k (yj-i,yj, x,j ).",
        "These feature functions are defined in Section 5.2.",
        "Given a training set consisting of n labeled sequences (xi, Yi) for i = 1...n, training is done in a two step process.",
        "First each expert a is trained following (Kumar and Herbert., 2003; Lafferty et al., 2001) objective function to learn the parameter A*a :",
        "The first term in Eq.",
        "5 is the conditional log-likelihood of the training data.",
        "The second term is the log of a Gaussian prior with variance <r, i.e., P(Aa) ~ exp (2^3 ||Aa||).",
        "Then the marginal probabilities Pa(yj = a | y, x, Ag), are computed using belief propagation and used as input for Equation 3.",
        "The optimal parameter 6 was learned using the log-likelyhood of the conditional probability defined in Equation 2 (i.e., no regularization).",
        "For testing, given a new test sequence x, we want to estimate the most probable sequence of labels y that maximizes our LMDE model:",
        "y h:VhteHyi"
      ]
    },
    {
      "heading": "5. Experimental Setup",
      "text": [
        "We evaluate our Latent Mixture of Discriminative Experts on the multimodal task of predicting listener nonverbal backchannel (i.e., head nods).",
        "Backchannel feedback (the nods and paraverbals such as \"uh-hu\" and \"mm-hmm\" that listeners produce as some is speaking) has received considerable interest due to its pervasiveness across languages and conversational contexts.",
        "We are using the RAPPORT dataset from (Maat-man et al., 2005), which contains 47 dyadic interactions between a speaker and a listener.",
        "Data is drawn from a study of face-to-face narrative discourse (\"quasi-monologic\" storytelling).",
        "In this dataset, participants in groups of two were told they were participating in a study to evaluate a communicative technology.",
        "Subjects were randomly assigned the role of speaker and listener.",
        "The speaker viewed a short segment of a video clip taken from the Edge Training Systems, Inc.",
        "Sexual Harassment Awareness video.",
        "After the speaker finished viewing the video, the listener was led back into the computer room, where the speaker was instructed to retell the stories portrayed in the clips to the listener.",
        "The listener was asked to not talk during the story retelling.",
        "Elicited stories were approximately two minutes in length on average.",
        "Participants sat approximately 8 feet apart.",
        "Video sequences were manually annotated to determine the ground truth head nod labels.",
        "A total of 587 head nods occured over all video sequences.",
        "This section describes the different multimodal features used to create our five experts.",
        "Prosody Prosody refers to the rhythm, pitch and intonation of speech.",
        "Several studies have demonstrated that listener feedback is correlated with a speaker's prosody (Nishimura et al., 2007; Ward and Tsukahara, 2000; Cathcart et al., 2003).",
        "For example, Ward and Tsukahara (2000) show that short listener backchannels (listener utterances like \"ok\" or \"uh-huh\" given during a speaker's utterance) are associated with a lowering of pitch over some interval.",
        "Listener feedback often follows speaker pauses or filled pauses such as \"um\" (see (Cathcart et al., 2003)).",
        "Using openS-MILE (Eyben et al., 2009) toolbox, we extract the following prosodic features, including standard linguistic annotations and the prosodic features suggested by Ward and Tsukhara: downslopes in pitch continuing for at least 40ms, regions of pitch lower than the 26th percentile continuing for at least 110ms (i.e., lowness), drop or rise in energy of speech (i.e., energy edge), Fast drop or rise in energy of speech (i.e., energy fast edge), vowel volume (i.e., vowels are usually spoken softer) and Pause in speech (i.e., no speech).",
        "Visual gestures Gestures performed by the speaker are often correlated with listener feedback (Burgoon et al., 1995).",
        "Eye gaze, in particular, has often been implicated as eliciting listener feedback.",
        "Thus, we manually annotate the following contextual feature: speaker looking at the listener.",
        "Lexical Some studies have suggested an association between lexical features and listener feedback (Cathcart et al., 2003).",
        "Using the transcriptions, we included all individual words (i.e., uni-grams) spoken by the speaker during the interactions.",
        "Syntactic structure Finally, we attempt to capture syntactic information that may provide relevant cues by extracting four types of features from a syntactic dependency structure corresponding to the utterance.",
        "The syntactic structure is produced automatically using a CRF part-of-speech (POS) tagger and a data-driven left-to-right shift-reduce dependency parser (Sagae and Tsujii, 2007), both trained on POS tags and dependency trees extracted from the Switchboard section of the Penn Treebank (Marcus et al., 1994), converted to dependency trees using the Penn2Malt tool.",
        "The four syntactic features are:",
        "• Part-of-speech tags for each word (e.g. noun, verb, etc.",
        "), taken from the output of the POS",
        "• Grammatical function for each word (e.g. subject, object, etc.",
        "), taken directly from the dependency labels produced by the parser",
        "• Part-of-speech of the syntactic head of each word, taken from the dependency links produced by the parser",
        "• Distance and direction from each word to its syntactic head, computed from the dependency links produced by the parser",
        "Figure 3: Baseline Models: a) Conditional Random Fields (CRF), b) Latent Dynamic Conditional Random Fields(LDCRF), c) CRF Mixture of Experts (no latent variable)",
        "Although our current method for extracting these features requires that the entire utterance be available for processing, this provides us with a first step towards integrating information about syntactic structure in multimodal prediction models.",
        "Many of these features could in principle be computed incrementally with only a slight degradation in accuracy, with the exception of features that require dependency links where a word's syntactic head is to the right of the word itself.",
        "We leave an investigation that examines only syntactic features that can be produced incrementally in real time as future work.",
        "Individual experts Our first baseline model consists of a set of CRF chain models, each trained with different set of multimodel features (as described in the previous section).",
        "In other words, only visual, prosodic, lexical or syntactic features are used to train a single CRF expert.",
        "In one CRF chain model, each gesture class corresponds to a state label.",
        "(See Figure 3a).",
        "Multimodal classifiers (early fusion)",
        "Our second baseline consists of two models: CRF and LDCRF (Morency et al., 2007).",
        "To train these models, we concatenate all multimodal features (lexical, syntactic, prosodic and visual) in one input vector.",
        "Graphical representation of these baseline models are given in Figure 3.",
        "CRF Mixture of experts To show the importance of latent variable in our LMDE model, we trained a CRF-based mixture of discriminative experts.",
        "This model is similar to the Logarithmic Opinion Pool (LOP) CRF suggested by Smith et al.",
        "(2005).",
        "The training is performed in two steps.",
        "A graphical representation of a CRF Mixture of experts is given in the last graph of Figure 3.",
        "5.4 Methodology",
        "We performed held-out testing by randomly selecting a subset of 11 interactions (out of 47) for the test set.",
        "The training set contains the remaining 36 dyadic interactions.",
        "All models in this paper were evaluated with the same training and test sets.",
        "Validation of all model parameters (regular-ization term and number of hidden states) was performed using a 3-fold cross-validation strategy on the training set.",
        "The regularization term was validated with values 10k, k = – 1..3.",
        "Three different number of hidden states were tested for the LMDE models: 2, 3 and 4.",
        "The performance is measured by using the F-measure.",
        "This is the weighted harmonic mean of precision and recall.",
        "Precision is the probability that predicted backchannels correspond to actual listener behavior.",
        "Recall is the probability that a backchannel produced by a listener in our test set was predicted by the model.",
        "We use the same weight for both precision and recall, so-called Fi.",
        "During validation we find all the peaks (i.e., local maxima) from the marginal probabilities.",
        "These backchannel hypotheses are filtered using the optimal threshold from the validation set.",
        "A backchannel (i.e., head nod) is predicted correctly if a peak happens during an actual listener backchannel with high enough probability.",
        "The same evaluation measurement is applied to all models.",
        "The training of all CRFs and LDCRFs were done using the hCRF library.",
        "The LMDE model was implemented in Matlab based on the hCRF",
        "Latent Mixture",
        "library."
      ]
    },
    {
      "heading": "6. Results and Discussion",
      "text": [
        "In this section we present the results of our empirical evaluation designed to test the three main characteristics of the LMDE model: (1) integration of multiple sources of information, (2) late fusion approach and (3) latent variable which models the hidden dynamic between experts.",
        "We also present an analysis of the output probabilities from the LMDE model and individual experts.",
        "Individual Experts We trained one individual expert for each feature types: visual, prosodic, lexical and syntactic features (both part-of speech and syntactic structure).",
        "Precision, recall and Fi values for each individual expert and our LMDE model are shown in Table 1 and Figure 4.",
        "Pairwise two-tailed t-test comparison between our LMDE model and individual experts shows a fusion technique (CRF vs LDCRF) and the CRF",
        "Mixture of Experts (Smith et al., 2005).",
        "significant difference for Lexical, Prosody, Syntactic and Eye gaze, with respective p-values of some experts may not perform well individually (e.g., eye gaze), they can bring important information once merged with others.",
        "Table 1 shows that our LMDE model was able to take advantage of the complementary information from each expert.",
        "Late Fusion We compare our approach with two early fusion models: CRF and Latent-dynamic CRF (see Figure 3).",
        "Table 2 summarizes the results.",
        "The CRF model learns direct weights between input features and the gesture labels.",
        "The LDCRF is able to model more complex dynamics between input features with the latent variable.",
        "We can see that our LMDE model outperforms both early fusion approaches because of its late fusion approach.",
        "Pairwise two-tailed t-test analysis gives p-values of 0.0481 and 0.0748, for CRF and LDCRF respectively.",
        "Latent Variable The CRF Mixture of Experts (2005) directly merges the expert outputs while our model uses a latent variable to model the hidden dynamic between experts (see Figure 3).",
        "Table 2 summarizes the results.",
        "Pairwise two-tailed t-test comparison between these two models shows a significant difference with a p-value of 0.0062.",
        "This result is important since it shows that our LMDE model does learn the hidden interaction between experts.",
        "Model analysis To understand the multimodal integration which happens at the latent variable level in our LMDE model, Figure 5 shows the output probabilities for all five individual experts as well as our model.",
        "The strength of the latent variable is to enable different weigting",
        "Expert",
        "Precision",
        "Recall",
        "f1",
        "Lexical",
        "0.1647",
        "0.3305",
        "0.2198",
        "Prosody",
        "0.1396",
        "0.9112",
        "0.2421",
        "Syntactic",
        "0.1833",
        "0.4663",
        "0.2632",
        "POS",
        "0.1935",
        "0.4514",
        "0.2709",
        "Eye Gaze",
        "0.1573",
        "0.1741",
        "0.1653",
        "LMDE",
        "0.2295",
        "0.5677",
        "0.3268",
        "model",
        "Precision",
        "Recall",
        "f1",
        "LMDE",
        "0.2295",
        "0.5677",
        "0.3268",
        "Early CRF",
        "0.13958",
        "0.9245",
        "0.2425",
        "Early LDCRF",
        "0.1826",
        "0.2484",
        "0.2105",
        "Mixture CRF",
        "0.1502",
        "0.2712",
        "0.1934",
        "Figure 5: Output probabilities from LMDE and individual experts for two different sub-sequences.",
        "The gray areas in the graph corresponds to ground truth backchannel feedbacks of the listener.",
        "of the experts at different point in time.",
        "By analyzing the sequence (a), we observe that both the POS and Syntactic experts learned that when no words are present (i.e., pause) there is a high likelihood of backchennel feedback from the listener (shown at 5.6s and 10.3s).",
        "These two experts are highly weighted (by one of the hidden state) during this part of the sequence.",
        "Also, both the Lexical and POS experts learned that the word \"'that\"' (and its part-of-speech) are important but since the speaker is not looking at the listener when saying it, the output from LMDE model is low (see Figure 5, Sequence (a), 7.7s).",
        "By analyzing sequence (b), we see that the Lexical and POS experts learned the importance of the \"'and\"' at 15.6s and 20.5s.",
        "More importantly, we can see at 17.0s and 18.7s that the influence of the POS and Syntactic experts have been reduced in the LMDE output probability.",
        "This difference of weighting shows that a different hidden state is active during Sequence (b)."
      ]
    },
    {
      "heading": "7. Conclusion",
      "text": [
        "In this paper we introduced a new model called Latent Mixture of Discriminative Experts (LMDE) for learning predictive models of human communication behaviors.",
        "Many of the interactions between speech and gesture happen at the sub-gesture or sub-word level.",
        "LMDE learns automatically the temporal relationship between different modalities.",
        "Since, we train separate experts for each modality, LMDE is capable of improving the prediction performance even with limited datasets.",
        "We evaluated our model on the task of nonverbal feedback prediction (e.g., head nod).",
        "Our experiments confirm the importance of combining the four types of multimodal features: lexical, syntactic structure, eye gaze, and prosody.",
        "LMDE is a generic model that can be applied to a wide range of problems.",
        "As future work, we are planning to test our model on dialog act classification and multimodal behavior recognition tasks."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This material is based upon work supported by the National Science Foundation under Grant No.",
        "0917321 and the U.S. Army Research, Development, and Engineering Command (RDECOM).",
        "The content does not necessarily reflect the position or the policy of the Government, and no official endorsement should be inferred.",
        "\\ h",
        "J Ground truth labels ^",
        "a",
        "jv -"
      ]
    }
  ]
}
