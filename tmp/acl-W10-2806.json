{
  "info": {
    "authors": [
      "Daoud Clarke",
      "Rudi Lutz",
      "David Weir"
    ],
    "book": "Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics",
    "id": "acl-W10-2806",
    "title": "Semantic Composition with Quotient Algebras",
    "url": "https://aclweb.org/anthology/W10-2806",
    "year": 2010
  },
  "references": [
    "acl-J98-1004",
    "acl-P04-1036",
    "acl-P08-1028",
    "acl-P98-2127",
    "acl-W01-0514",
    "acl-W09-0208"
  ],
  "sections": [
    {
      "text": [
        "Daoud Clarke Rudi Lutz David Weir",
        "Hatfield, UK Brighton, UK Brighton, UK",
        "We describe an algebraic approach for computing with vector based semantics.",
        "The tensor product has been proposed as a method of composition, but has the undesirable property that strings of different length are incomparable.",
        "We consider how a quotient algebra of the tensor algebra can allow such comparisons to be made, offering the possibility of data-driven models of semantic composition."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Vector based techniques have been exploited in a wide array of natural language processing applications (Schütze, 1998; McCarthy et al., 2004; Grefenstette, 1994; Lin, 1998; Bellegarda, 2000; Choi et al., 2001).",
        "Techniques such as latent semantic analysis and distributional similarity analyse contexts in which terms occur, building up a vector of features which incorporate aspects of the meaning of the term.",
        "This idea has its origins in the distributional hypothesis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa.",
        "However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors.",
        "While these techniques work well at the word level, for longer strings, data becomes extremely sparse.",
        "This has led to various proposals exploring methods for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001;",
        "Lapata, 2008; Erk and Pado, 2009; Preller and Sadrzadeh, 2009).",
        "Many of these approaches use a predefined composition operation such as addition (Landauer and Dumais, 1997; Foltz et al., contrasts with the data-driven definition of composition developed here."
      ]
    },
    {
      "heading": "2. Tensor Algebras",
      "text": [
        "Following the context-theoretic semantics of Clarke (2007), we take the meaning of strings as being described by a multiplication on a vector space that is bilinear with respect to the addition of the vector space, i.e.",
        "It is assumed that the multiplication is associative, but not commutative.",
        "The resulting structure is an associative algebra over a field – or simply an algebra when there is no ambiguity.",
        "One commonly used bilinear multiplication operator on vector spaces is the tensor product (denoted (g), whose use as a method of combining meaning was first proposed by Smolensky (1990), and has been considered more recently by Clark and Pulman (2007) and Widdows (2008), who also looked at the direct sum (which Widdows calls the direct product, denoted ©).",
        "We give a very brief account of the tensor product and direct sum in the finite-dimensional case; see (Halmos, 1974) for formal and complete definitions.",
        "Roughly speaking, if u\\, u2,... un form an orthonormal basis for a vector space U and v\\,v2,.. .vm form an orthonormal basis for vector space V, then the space U g V has dimensionality nm with an orthonormal basis formed by the set of all ordered pairs (ui,Vj), denoted by ui g Vj, of the individual basis elements.",
        "For arbitrary elements u = 7=i aiui and v = jm=l ßjVj the tensor product of u and v is then given by",
        "u g v = ^2 X] aißuv",
        "For two finite dimensional vector spaces U and V (over a field F) of dimensionality n and m respectively, the direct sum U © V is defined as the cartesian product U x V together with the operations (ui,vi) + (u2,v2) = (ui + u2,vi + v2), and a(ui,vi) = (aui,avi), for ui,u2 G U, vi,v2 G V and a G F .In this case the vectors ui,u2,... un, vi,v2,.. .vm form an orthonormal set of basis vectors in U © V, which is thus of dimensionality n + m. In this case one normally identifies U with the set of vectors in U © V of the form (u, 0), and V with the set of vectors of the form (0, v).",
        "This construction makes U © V isomorphic to V © U, and thus the direct sum is often treated as commutative, as we do in this paper.",
        "The motivation behind using the tensor product to combine meanings is that it is very fine-grained.",
        "So, if, for example, red is represented by a vector u consisting of a feature for each noun that is modified by red, and apple is represented by a vector v consisting of a feature for each verb that occurs with apple as a direct object, then red apple will be represented by u g v with a non-zero component for every pair of non-zero features (one from u and one from v).",
        "So, there is a non-zero element for each composite feature, something that has been described as red, and something that has been done with an apple, for example, sky and eat.",
        "Both © and g are intuitively appealing as semantic composition operators, since u and v are reconstructible from each of u g v and u © v, and thus no information is lost in composing u and v. Conversely, this is not possible with ordinary vector addition, which also suffers from the fact that it is strictly commutative (not simply up to isomorphism like ©), whereas natural language composition is in general manifestly non-commutative.",
        "We make use of a construction called the tensor algebra on a vector space V (where V is a space of context features), defined as:",
        "Any element of T (V ) can be described as a sum of components with each in a different tensor power of V. Multiplication is defined as the tensor product on these components, and extended linearly to the whole of T (V ).",
        "We define the degree of a vector u in T( V) to be the tensor power of its highest dimensional non-zero component, and denote it deg(u); so for example, both vgv and u©(vgv) have degree two, for 0 = u, v G V. We restrict T(V) to only contain vectors of finite degree.",
        "A standard way to compare elements of a vector space is to make use of an inner product, which provides a measure of semantic distance on that space.",
        "Assuming we have an inner product (•, •) on V, T(V) can be given an inner product by defining (a, ß) = aß for a,ß G R, and for xi , yi , x2, y2 G V, and then extending this inductively (and by linearity) to the whole of T(V).",
        "We assume that words are associated with vectors in V, and that the higher tensor powers represent strings of words.",
        "The problem with the tensor product as a method of composition, given the inner product as we have defined it, is that strings of different lengths will have orthogonal vectors, clearly a serious problem, since strings of different lengths can have similar meanings.",
        "In our previous example, the vector corresponding to the concept red apple lives in the vector space U g V, and so we have no way to compare it to the space V of nouns, even though red apple should clearly be related to apple.",
        "Previous work has not made full use of the tensor product space; only tensor products are used, not sums of tensor products, giving us the equivalent of the product states of quantum mechanics.",
        "Our approach imposes relations on the vectors of the tensor product space that causes some product states to become equivalent to entangled states, containing sums of tensor products of different degrees.",
        "This allows strings of different lengths to share components.",
        "We achieve this by constructing a quotient algebra."
      ]
    },
    {
      "heading": "3. Quotient Algebras",
      "text": [
        "An ideal I of an algebra A is a sub-vector space of A such that xa G I and ax G I for all a G A and all x G I.",
        "An ideal introduces a congruence = on A defined by x = y if and only if x – y G I.",
        "For any set of elements A ç A there is a unique minimal ideal Ia containing all elements of A; this is called the ideal generated by A.",
        "The quotient algebra A/I is the set of all equivalence classes defined by this congruence.",
        "Multiplication is defined on A/I by the multiplication on A, since = is a congruence.",
        "By adding an element x – y to the generating set A of an ideal, we are saying that we want to set x – y to zero in the quotient algebra, which has the effect of setting x equal to y.",
        "Thus, if we have a set of pairs of vectors that we wish to make equal in the quotient algebra, we put their differences in the generating set of the ideal.",
        "Note that putting a single vector v in the generating set can have knock-on effects, since all products ofv with elements of A will also end up in the ideal.",
        "Although we have an inner product defined on T (V ), we are not aware of any satisfactory method for defining an inner product on T(V)/I, a consequence of the fact that both T(V) and I are not complete.",
        "Instead, we define an inner product on a space which contains the quotient algebra, T(V)/I.",
        "Rather than considering all elements of the ideal when computing the quotient, we consider a sub-vector space of the ideal, limiting ourselves to the space Gk generated from A by only allowing multiplication by elements up to a certain degree, k.",
        "Let us denote the vector subspace generated by linearity alone (no multiplications) from a subset A of T(V) by G(A).",
        "Also suppose B = {ei,..., eN} is a basis for V. We then define the spaces Gk as follows.",
        "Define sets Ak (k = 0,1,2,...) inductively as follows:",
        "Afc = Afc_i U {(a g Afc_i)|ei G B} U {(Afc_i g ei)lei G B}",
        "We note that form an increasing sequence of linear vector subspaces of T(V), and that",
        "This means that for any x G I there exists a smallest k such that for all k' > k we have that x G Gk.",
        "Lemma.",
        "Let x G I,x = 0 and let deg(x) = d. Then for all k > d – mindeg (A) we have that x G Gk, where mindeg (A) is defined to be the minimum degree ofthe non-zero components occurring in the elements ofA.",
        "Proof.",
        "We first note that for x G I it must be the case that deg(x) > mindeg(A) since I is generated from A.",
        "Therefore we know d – mindeg(A) > 0.",
        "We only need to show that x G Gd_mindeg(a).",
        "Let k' be the smallest integer such that x G Gki.",
        "Since x G G y _i it must be the case that the highest degree term of x comes from V g Gki_i U Gki_i g V. Therefore k' + mindeg (A) < d < k' + maxdeg (A).",
        "From this it follows that the smallest k' for which x G Gk satisfies k' < d – mindeg (A), and we know x G Gk for all k > k'.",
        "In particular x G Gk for k > d – mindeg (A).",
        "□",
        "We show that T(V)/Gk (for an appropriate choice of k) captures the essential features of T(V)/I in terms ofequivalence:",
        "Proposition.",
        "Let deg(a – b) = d and let k > d – mindeg (A).",
        "Then a = b in T (V )/Gk if and only ifa = b in T(V)/I.",
        "Proof.",
        "Since Gk ç I, the equivalence class of an element a in T(V)/I is a superset of the equivalence class of a in T(V)/Gk, which gives the forward implication.",
        "The reverse follows from the lemma above.",
        "□",
        "In order to define an inner product on T(V)/Gk, we make use of the result of Berbe-rian (1961) that if M is a finite-dimensional linear subspace of a pre-Hilbert space P, then P = M © M±, where M± is the orthogonal complement of M in P. In our case this implies T(V) = Gk © and that every element x G T(V) has a unique decomposition as x = y + x'k where y G Gk and x'k G Gk.",
        "This implies that T(V)/Gk is isomorphic to G^~, and that for each equivalence class [x]k in T(V)/Gkthere is a unique corresponding element x'k G Gesuch that x'k G [x]k. This element x'k can be thought of as the canonical representation of all elements of [x]k in T(V)/Gk, and can be found by projecting any element in an equivalence class onto Gk.",
        "This enables us to define an inner product on T(V)/Gk by ([x]k, [y]k)k = (x'k,y'k).",
        "The idea behind working in the quotient algebra T(V)/I rather than in T(V) is that the elements of the ideal capture differences that we wish to ignore, or alternatively, equivalences that we wish to impose.",
        "The equivalence classes in T(V)/I represent this imposition, and the canonical representatives in I± are elements which ignore the distinctions between elements of the equivalence classes.",
        "However, by using Gk, for some k, instead of the full ideal I, we do not capture some of the equivalences implied by I.",
        "We would, therefore, like to choose k so that no equivalences of importance to the sentences we are considering are ignored.",
        "While we have not precisely established a minimal value for k that achieves this, in the discussion that follows, we set k heuristically as k = I – mindeg (A) where I is the maximum length of the sentences currently under consideration, and A is the generating set for the ideal I.",
        "The intuition behind this is that we wish all vectors occurring in A to have some component in common with the vector representation of our sentences.",
        "Since components in the ideal are generated by multiplication (and linearity), in order to allow the elements of A containing the lowest degree components to potentially interact with our sentences, we will have to allow multiplication of those elements (and all others) by components of degree up to I – mindeg (A).",
        "Given a finite set A ç T(V) of elements generating the ideal I, to compute canonical representations, we first compute a generating set Akfor Gk following the inductive definition given earlier, and removing any elements that are not linearly independent using a standard algorithm.",
        "Using the Gram-Schmidt process (Trefethen and Bau, 1997), we then calculate an orthonormal basis A' for Gk, and, by a simple extension of Gram-Schmidt, compute the projection of a vector u onto Gk using the basis A'.",
        "We now show how A, the set of vectors generating the ideal, can be constructed on the basis of a treebank, ensuring that the vectors for any two strings of the same grammatical type are comparable."
      ]
    },
    {
      "heading": "4. Data-driven Composition",
      "text": [
        "Suppose we have a treebank, its associated treebank grammar G, and a way of associating a context vector with every occurrence of a subtree in the treebank (where the vectors indicate the presence of features occurring in that particular context).",
        "The context vector associated with a specific occurrence of a subtree in the treebank is an individual context vector.",
        "We assume that for every rule, there is a distinguished non-terminal on the right hand side which we call the head.",
        "We also assume that for every production n there is a linear function <n from the space generated by the individual context vectors of the head to the space generated by the individual context vectors of the left hand side.",
        "When there is no ambiguity, we simply denote this function < .",
        "Let X be the sum over all individual vectors of subtrees rooted with X in the treebank.",
        "Similarly, for each Xj in the right-hand-side of the rule ni : X – Xi... Xr(n.), where r(n) is the rank of n, let TTijj be the sum over the individual vectors of those subtrees rooted with Xj where the subtree occurs as the j th daughter of a local tree involving the production ni in the treebank.",
        "For each rule n : X – Xi.. .Xr with head Xhwe add vectors for each basis element ei of VXh to the generating set.",
        "The reasoning behind this is to ensure that the meaning corresponding to a vector associated with the head of a rule is maintained as it is mapped to the vector space associated with the left hand side of the rule.",
        "It is often natural to assume that the individual context vector of a non-terminal is the same as the individual context vector of its head.",
        "In this case, we can take < to be the identity map.",
        "In particular, for a rule of the form n : X – Xi, then X7T}i is zero.",
        "It is important to note at this point that we have presented only one of many ways in which a grammar could be used to generate an ideal.",
        "In particular, it is possible to add more vectors to the ideal, allowing more fine-grained distinctions, for example through the use of a lexicalised grammar.",
        "For each sentence w, we compute the tensor product W = âi g â2 g • • • g an where the string of words ai ...an form w, and each âi is a vector in V. For a sentence w we find an element Woof the orthogonal complement of Gk in T(V) such that wâo G [ wâ], where [wâ] denotes the equivalence class of wâ given the subspace Gk."
      ]
    },
    {
      "heading": "5. Example",
      "text": [
        "We show how our formalism applies in a simple example.",
        "Assume we have a corpus which consists of the following sentences:",
        "see red apple see big city buy apple visit big apple read big book modernise city throw old small red book see modern city buy large new book together with the following productions.",
        "where N and Adj are terminals representing nouns and adjectives, along with rules for the terminals.",
        "We consider the space of adjective/noun phrases, generated by N', and define the individual context of a noun to be the verb it occurs with, and the individual context of an adjective to be the noun it modifies.",
        "For each rule, we take 0 to be the identity map, so the vector spaces associated with N and N', and the vector space generated by individual contexts of the nouns are all the same.",
        "In this case, the only non-zero vectors which we add to the ideal are those for the second rule (ignoring the first rule, since we do not consider verbs in this example except as contexts), which has the set of vectors where i ranges over the basis vectors for contexts of nouns: see, buy, visit, read, modernise, and",
        "2eapple + 2ebook + ecity",
        "In order to compute canonical representations of vectors, we take k = 1.",
        "Figure 1 shows the similarity between the noun phrases in our sample corpus.",
        "Note that the vectors we have put in the generating set describe only compositionality of meaning – thus for example the similarity of the non-compositional phrase big apple to city is purely due to the distributional similarity between apple and city and composition with the adjective big.",
        "Our preliminary investigations indicate that the cosine similarity values are very sensitive to the particular corpus and features chosen; we are currently investigating other ways of measuring and computing similarity.",
        "One interesting feature in the results is how adjectives alter the similarity between nouns.",
        "For example, red apple and red city have the same similarity as apple and city, which is what we would expect from a pure tensor product.",
        "This also explains why all phrases containing book are disjoint to those containing city, since the original vector for book is disjoint to city.",
        "The contribution that the quotient algebra gives is in comparing the vectors for nouns with those for noun-adjective phrases.",
        "For example, red apple has components in common with apple, as we would expect, which would not be the case with just the tensor product."
      ]
    },
    {
      "heading": "6. Conclusion and Further Work",
      "text": [
        "We have presented the outline of a novel approach to semantic composition that uses quotient algebras to compare vector representations of strings of different lengths.",
        "apple",
        "big apple",
        "red apple",
        "city",
        "big city",
        "red city",
        "book",
        "big book",
        "red book",
        "apple",
        "1.0",
        "0.26",
        "0.24",
        "0.52",
        "0.13",
        "0.12",
        "0.33",
        "0.086",
        "0.080",
        "big apple",
        "1.0",
        "0.33",
        "0.13",
        "0.52",
        "0.17",
        "0.086",
        "0.33",
        "0.11",
        "red apple",
        "1.0",
        "0.12",
        "0.17",
        "0.52",
        "0.080",
        "0.11",
        "0.33",
        "city",
        "1.0",
        "0.26",
        "0.24",
        "0.0",
        "0.0",
        "0.0",
        "big city",
        "1.0",
        "0.33",
        "0.0",
        "0.0",
        "0.0",
        "red city",
        "1.0",
        "0.0",
        "0.0",
        "0.0",
        "book",
        "1.0",
        "0.26",
        "0.24",
        "big book",
        "1.0",
        "0.33",
        "red book",
        "1.0",
        "The dimensionality of the construction we use increases exponentially in the length of the sentence; this is a result of our use of the tensor product.",
        "This causes a problem for computation using longer phrases; we hope to address this in future work by looking at the representations we use.",
        "For example, product states can be represented in much lower dimensions by representing them as products of lower dimensional vectors.",
        "The example we have given would seem to indicate that we intend putting abstract (syntactic) information about meaning into the set of generating elements of the ideal.",
        "However, there is no reason that more fine-grained aspects of meaning cannot be incorporated, even to the extent of putting in vectors for every pair of words.",
        "This would automatically incorporate information about non-compositionality of meaning.",
        "For example, by including the vector big apple – big <g) apple, we would expect to capture the fact that the term big apple is non-compositional, and more similar to city than we would otherwise expect.",
        "Future work will also include establishing the implications of varying the constant k and exploring different methods for choosing the set A that generates the ideal.",
        "We are currently preparing an experimental evaluation of our approach, using vectors obtained from large corpora."
      ]
    },
    {
      "heading": "7. Acknowledgments",
      "text": [
        "We are grateful to Peter Hines, Stephen Clark, Peter Lane and Paul Hender for useful discussions.",
        "The first author also wishes to thank Metrica for supporting this research."
      ]
    }
  ]
}
