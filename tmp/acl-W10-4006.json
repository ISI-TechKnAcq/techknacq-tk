{
  "info": {
    "authors": [
      "Tsuyoshi Okita",
      "Alfredo Maldonado Guerra",
      "Yvette Graham",
      "Andy Way"
    ],
    "book": "Proceedings of the 4th Workshop on Cross Lingual Information Access",
    "id": "acl-W10-4006",
    "title": "Multi-Word Expression-Sensitive Word Alignment",
    "url": "https://aclweb.org/anthology/W10-4006",
    "year": 2010
  },
  "references": [
    "acl-A97-1050",
    "acl-C96-2141",
    "acl-E03-1035",
    "acl-J03-1002",
    "acl-J93-2003",
    "acl-N03-1017",
    "acl-P03-1010",
    "acl-P04-1023",
    "acl-P07-2045",
    "acl-P91-1023",
    "acl-P93-1003",
    "acl-W04-3243",
    "acl-W06-2402"
  ],
  "sections": [
    {
      "text": [
        "Tsuyoshi Okita, Alfredo Maldonado Guerra, Yvette Graham, Andy Way",
        "{CNGL, NCLT} / School of Computing / Dublin City University, CNGL / School of Computer Science and Statistics / Trinity College Dublin{tokita,ygraham,away}@computing.dcu.ie, maldonaa@scss.tcd.ie",
        "This paper presents a new word alignment method which incorporates knowledge about Bilingual Multi-Word Expressions (BMWEs).",
        "Our method of word alignment first extracts such BMWEs in a bidirectional way for a given corpus and then starts conventional word alignment, considering the properties of BMWEs in their grouping as well as their alignment links.",
        "We give partial annotation of alignment links as prior knowledge to the word alignment process; by replacing the maximum likelihood estimate in the M-step of the IBM Models with the Maximum A Posteriori (MAP) estimate, prior knowledge about BMWEs is embedded in the prior in this MAP estimate.",
        "In our experiments, we saw an improvement of 0.77 Bleu points absolute in JP-EN.",
        "Except for one case, our method gave better results than the method using only BMWEs grouping.",
        "Even though this paper does not directly address the issues in Cross-Lingual Information Retrieval (CLIR), it discusses an approach of direct relevance to the field.",
        "This approach could be viewed as the opposite of current trends in CLIR on semantic space that incorporate a notion of order in the bag-of-words model (e.g. co-occurences)."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "2007) remains key to providing high-quality translations as all subsequent training stages rely on its performance.",
        "It alone does not effectively capture many-to-many word correspondences, but instead relies on the ability of subsequent heuristic phrase extraction algorithms, such as grow-diag-final (Koehn et al., 2003), to resolve them.",
        "Some aligned corpora include implicit partial alignment annotation, while for other corpora a partial alignment can be extracted by state-of-the-art techniques.",
        "For example, implicit tags such as reference number within the patent corpus of Fujii et al.",
        "(2010) provide (often many-to-many) correspondences between source and target words, while statistical methods for extracting a partial annotation, like Kupiec et al.",
        "(1993), extract terminology pairs using linguistically predefined POS patterns.",
        "Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information.",
        "Resnik and Melamed (1997) automatically extract domain-specific lexica.",
        "Moore (2003) extracts named-entities.",
        "In Machine Translation, Lambert and Banchs (2006) extract BMWEs from a phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process.",
        "This paper introduces a new method ofincorpo-rating previously known many-to-many word correspondences into word alignment.",
        "A well-known method of incorporating such prior knowledge in Machine Learning is to replace the likelihood maximization in the M-step of the EM algorithm with either the MAP estimate or the Maximum Penalized Likelihood (MPL) estimate (McLach-",
        "MAP estimate allows us to incorporate the prior, a probability used to reflect the degree of prior belief about the occurrences of the events.",
        "A small number of studies have been carried out that use partial alignment annotation for word alignment.",
        "Firstly, Graca et al.",
        "(2007) introduce a posterior regularization to employ the prior that cannot be easily expressed over model parameters such as stochastic constraints and agreement constraints.",
        "These constraints are set in the E-step to discard intractable alignments contradicting these constraints.",
        "This mechanism in the E-step is in a similar spirit to that in GIZA++ for IBM Model 3 and 4 which only searches around neighbouring alignments around the Viterbi alignment.",
        "For this reason, this algorithm is not intended to be used combined with IBM Models 3 and 4.",
        "Although theoretically it is possible to incorporate partial annotation with a small change in its code, Graca et al.",
        "do not mention it.",
        "Secondly, Talbot (2005) introduces a constrained EM method which constrains the E-step to incorporate partial alignment into word alignment, which is in a similar manner to Graca et al.",
        "(2007).",
        "He conducted experiments using partial alignment annotation based on cognate relations, a bilingual dictionary, domain-specific bilingual semantic annotation, and numerical pattern matching.",
        "He did not incorporate BMWEs.",
        "Thirdly, Callison-Burch et al.",
        "(2004) replace the likelihood maximization in the M-step with mixed likelihood maximization, which is a convex combination of negative log likelihood of known links and unknown links.",
        "The remainder of this paper is organized as follows: in Section 2 we define the anchor word alignment problem.",
        "In Section 3 we include a review of the EM algorithm with IBM Models 1-5, and the HMM Model.",
        "Section 4 describes our own algorithm based on the combination of BMWE extraction and the modified word alignment which incorporates the groupings of BMWEs and enforces their alignment links; we explain the EM algorithm with MAP estimation with three kinds of priors.",
        "In Section 5 our experimental results are presented, and we conclude in Section 6."
      ]
    },
    {
      "heading": "2. Anchor Word Alignment Problem",
      "text": [
        "The input to standard methods of word alignment is simply the sentence-aligned corpus, whereas our alignment method takes in additionally a partial alignment.",
        "We assume, therefore, the availability of a partial alignment, for example via a MWE extraction tool.",
        "Let ë denote an English sentence, and e denote an English word, throughout this paper.",
        "The anchor word alignment problem is defined as follows:",
        "Definition 1 (Anchor Word Alignment Problem)",
        "Let (ë, f) = {(ëi, fi),..., (ën, fn)} be a parallel corpus.",
        "By prior knowledge we additionally have knowledge of anchor words (ê, /) = {(senti ,tei ,tf1, posei, pos f1, lengthe, length f ), ..., (sentk, ten, tfn, posen, posfn, lengthe, lengthf)} where senti denotes sentence ID, posei denotes the position of tei in a sentence ëi, and lengthe (and lengthf) denotes the sentence length of the original sentence which includes ei.",
        "Under a given (e , / ) and (eê, /ê), our objective is to obtain word alignments.",
        "It is noted that an anchor word may include a phrase pair which forms n-to-m mapping objects.",
        "Table 1 shows two example phrase pairs for French to English c 'est la vie and that is life, and la vie en rose and rosy life with the initial value for the EM algorithm, the prior value and the fi-",
        "pair",
        "GIZA++(no prior)",
        "Ours(with prior)",
        "EN-FR",
        "fin",
        "ini",
        "prior",
        "fin",
        "ini",
        "prior",
        "is NULL",
        ".25",
        "0",
        "0",
        ".25",
        ".25",
        "rosy en",
        ".5",
        "0",
        "0",
        ".5",
        ".2",
        "that.",
        ".25",
        "0",
        "0",
        ".25",
        ".25",
        "life la",
        ".25",
        "0",
        "0",
        ".25",
        "0",
        ".",
        "c'",
        ".25",
        "0",
        "0",
        ".25",
        ".25",
        "that c'",
        "0",
        ".25",
        "0",
        "1",
        ".25",
        ".25",
        "is est",
        "0",
        ".25",
        "0",
        "1",
        ".25",
        ".25",
        "life vie",
        "0",
        ".5",
        "0",
        "1",
        ".5",
        "1",
        "rosy rose",
        "0",
        ".25",
        "0",
        "1",
        ".25",
        ".2",
        "There are 5 columns for each term: sentence number, source term, target term, source position, and target position.",
        "The number appended to each term from the patent corpus (lower half) is a reference number.",
        "In this corpus, all the important technical terms have been identified and annotated with reference numbers.",
        "nal lexical translation probability for Giza++ IBM Model 4 and that of our modified Giza++.",
        "Our modified Giza++ achieves the correct result when anchor words 'life' and 'vie' are used to assign a value to the prior in our model."
      ]
    },
    {
      "heading": "3. Word Alignment",
      "text": [
        "We review two models which address the problem of word alignment.",
        "The aim of word alignment is to obtain the model parameter t among English and French words, ei and fj respectively.",
        "We search for this model parameter under some model M where M is chosen by IBM Models 15 and the HMM model.",
        "We introduce the latent variable a, which is an alignment function with the hypothesis that each e and / correspond to this latent variable.",
        "(e, /, a) is a complete data set, and ( e, / ) is an incomplete data set.",
        "We follow the description of the EM algorithm for IBM Models of Brown et al.",
        "(1993) but introduce the parameter t explicitly.",
        "In this model, the parameter t represents the lexical translation probabilities t(ei|fj).",
        "It is noted that we use e|f rather than f Ie following the notation of Koehn (2010).",
        "One important remark is that the Viterbi alignment of the sentence pair (ë , f ) = (e\\, f( ), which is obtained as in (1):",
        "provides the best alignment for a given log-likelihood distribution p^(f, a|e).",
        "Instead of summing, this step simplifies the E-step.",
        "However, under our modification of maximum likelihood estimate with MAP estimate, this simplification is not a correct approximation of the summation since our surface in the E-step is greatly perturbed by the prior.",
        "There is no guarantee that the Viterbi alignment is within the proximity of the target alignment (cf. Table 1).",
        "Let z be the latent variable, t be the parameters, and x be the observations.",
        "The EM algorithm is an iterative procedure repeating the E-step and the M-step as in (2):",
        "In the E-step, our knowledge of the values of the latent variables in a is given only by the posterior distribution p(a|e, f, t).",
        "Hence, the (negative log)-likelihood of complete data (e,f,a), which we denote by – log p(t|e, f, a), is obtained over all possible alignments a.",
        "We use the current parameter values told to find the posterior distribution of the latent variables given by p(a|e, f, told).",
        "We then use this posterior distribution to find the expectation of the complete data log-likelihood evaluated for parameter value t. This expectation is given by £ap^e, f, told) logp(e, f, a|t).",
        "In the M-step, we use a maximal likelihood estimation to minimize negative log-likelihood in order to determine the parameter t; note that t is a lexical translation probability.",
        "Instead of using the log-likelihood log p(a,e, f |t), we use the expected complete data log-likelihood over all the possible alignments a that we obtained in the E-"
      ]
    },
    {
      "heading": "4. Our Approach",
      "text": [
        "where an auxiliary function c(e|f ; e, f ) for IBM Model 1 introduced by Brown et al.",
        "is defined as and where the Kronecker-Delta function ö(x,y) is 1 if x = y and 0 otherwise.",
        "This auxiliary function is convenient since the normalization factor of this count is also required.",
        "We note that if we use the MAP estimate, the E-step remains the same as in the maximum likelihood case, whereas in the M-step the quantity to be minimized is given by Q(t,told) + logp(t).",
        "Hence, we search for the value of t which maximizes the following equation:",
        "A first-order Hidden Markov Model (Vogel et al., 1996) uses the sentence length probability p( J the mixture alignment probability p(i|j, I), and the translation probability, as in (4):",
        "Algorithm 1 Overall Algorithm",
        "Given: a parallel corpus, 1.",
        "Extract MWEs by Algorithm 2.",
        "2.",
        "Based on the results of Step 1, specify a set of anchor word alignment links in the format of anchor word alignment problem (cf.",
        "Definition 1 and Table 2).",
        "3.",
        "Group MWEs in source and target text.",
        "4.",
        "Calculate the prior in order to embed knowledge about anchor words.",
        "5.",
        "Calculate lexical translation probabilities with the prior.",
        "6.",
        "Obtain alignment probabilities.",
        "7.",
        "Ungroup of MWEs in source and target text.",
        "Algorithm 1 consists of seven steps.",
        "We use the Model I prior for the case where our prior knowledge is sparse and evenly distributed throughout the corpus, whereas we use the Model II prior when our prior knowledge is dense in a partial corpus.",
        "A typical example of the former case is when we use partial alignment annotation extracted throughout a corpus for bilingual terminology.",
        "A typical example of the latter case is when a sample of only a few hundred lines from the corpus have been hand-annotated.",
        "Suppose we have a training set of R observation sequences Xr, where r = 1, • • • ,R, each of which is labelled according to its class m, where m = 1, ••• ,M, as in (5):",
        "The HMM alignment probabilities p(i|i', I) depend only on the jump width (i – i').",
        "Using a set of non-negative parameters s(i – i'), we have (6):",
        "Our algorithm of extracting MWEs is a statistical method which is a bidirectional version ofKu-piec (1993).",
        "Firstly, Kupiec presents a method to extract bilingual MWE pairs in a unidirectional manner based on the knowledge about typical POS patterns of noun phrases, which is language-dependent but can be written down with some ease by a linguistic expert.",
        "For example in French they are N N, N prep N, and N Adj.",
        "Secondly, we take the intersection (or union) of extracted bilingual",
        "MWE pairs.",
        "Algorithm 2 MWE Extraction Algorithm",
        "Given: a parallel corpus and a set of anchor word alignment links:",
        "1.",
        "We use a POS tagger (Part-Of-Speech Tagger) to tag a sentence on the SL side.",
        "2.",
        "Based on the typical POS patterns for the SL, extract noun phrases on the SL side.",
        "3.",
        "Count n-gram statistics (typically n = 1, • • •, 5 are used) on the TL side which jointly occur with each source noun phrase extracted in Step 2.",
        "4.",
        "Obtain the maximum likelihood counts of joint phrases, i.e. noun phrases on the SL side and n-gram phrases on the TL side.",
        "5.",
        "Repeat the same procedure from Step 1 to 4 reversing the SL and TL.",
        "6.",
        "Intersect (or union) the results in both directions.",
        "Let SL be the source language side and TL be the target language side.",
        "The procedure is shown in Algorithm 2.",
        "We informally evaluated the MWE extraction tool following Kupiec (1993) by manually inspecting the mapping of the 100 most frequent terms.",
        "For example, we found that 93 of the 100 most frequent English terms in the patent corpus were correctly mapped to their Japanese translation.",
        "Depending on the corpus, we can use more prior knowledge about implicit alignment links.",
        "For example in some categories of patent and technical documents corpora, we can use heuristics to extract the \"noun phrase\" + \"reference number\" from both sides.",
        "This is due to the fact that terminology is often labelled with a unique reference number, which is labelled on both the SL and TL sides.",
        "Prior for Exhaustive Alignment Space IBM",
        "Models 1 and 2 implement a prior for all possible Algorithm 3 Prior Model I for IBM Model 1",
        "Given: parallel corpus e , f ,",
        "anchor words biTerm initialize t(e|f ) uniformly do until convergence set count(e| f) to 0 for all e,f for all sentence pairs (ës,f s)",
        "prior(e|f )s = getPriorModelI(ë, f , biTerm) for all words e in ë stotals(e) = 0 for all words f in f s",
        "for all words e in e s for all words f in f s",
        "for all f for all e alignments exhaustively.",
        "Such a prior requires the following two conditions.",
        "Firstly, partial knowledge about the prior that we use in our context is defined as follows.",
        "Let us denote a bilingual term list T = {(s1 ,h),..., (sm ,tm )}.",
        "For example with IBM Model 1: Let us define the following prior p(e| f, e, f; T) from Equation (4):",
        "uniform (ei – Si, fj – tj)",
        "Secondly, this prior should be proper for the exhaustive case and non-proper for the sampled alignment space where by proper we mean that the probability is normalized to 1.",
        "Algorithm 3 shows the pseudo-code for Prior Model I.",
        "Note that if the prior is uniform in the MAP estimation, this is equivalent to maximum likelihood estimation.",
        "Prior for Sampled Alignment (Function) Space",
        "Due to the exponential costs introduced by fertility, null token insertion, and distortion probability, IBM Models 3 and 4 do not consider all (I + 1)Jalignments exhaustively, but rather a small subset in the E-step.",
        "Each iteration only uses the subset of all the alignment functions: this sampling is not uniform, as it only includes the best possible alignment with all its neighbouring alignments which differ from the best alignment by one word (this can be corrected by a move operation) or two words (this can be corrected by a swap operation).",
        "If we consider the neighbouring alignment via a move or a swap operation, two issues arise.",
        "Firstly, the fact that these two neighbouring alignments are drawn from different underlying distributions needs to be taken into account, and secondly, that the application of a move and a swap operation alters a row or column of a prior matrix (or indices of the prior) since either operation involves the manipulation of links.",
        "Algorithm 4 Pseudo-code for Prior Model II Exhaustive Alignment Space def getPriorModelII(e ,f ,biTerm):",
        "for i in sentence:",
        "allWordsi = length of sentence e for fin fi:",
        "if (e, f) in biTerm:",
        "uni(e|/)i expSum(e|f) else:",
        "allWordsi",
        "countSum(e|f)i += n countSum(e|f ) += count(e|f )ifor e in alle: for f in allf:",
        "return prior(e| f)",
        "When neither ai nor aj is determined, this probability is expressed as in (9):"
      ]
    },
    {
      "heading": "0. (condition 1)",
      "text": [
        "(uniform distribution) (Pascal's triangle distribution)",
        "Prior Model II assumes that we have prior knowledge only in some part of the training corpus.",
        "A typical example is when a small part of the corpus has a hand-crafted 'gold standard' annotation.",
        "Prior for Exhaustive Alignment Space Prior Model II is used to obtain the prior probability p(e|f ) over all possible combinations of e and f. In contrast to Prior Model I, which computes the prior probability p(e|f ) for each sentence, Prior Model II computes the prior probability globally for all sentences in the corpus.",
        "Algorithm 4 shows the pseudo-code for Prior Model II Exhaustive Alignment Space.",
        "((ei – Si, fj – tj for ai) and (ei ((ei – Si, fj – tj for ai) and (ei tj for aj)) or tj for aj)) or",
        "Prior for Jump Width i One implementation of HMM is to use the forward-backward algorithm.",
        "A prior should be embedded within the forward-backward algorithm.",
        "From Equation (6), there are three cases which depend on whether ai and its neighbouring alignment ai-1 are determined by our prior knowledge about anchor words or not.",
        "When both ai and aj are determined, this probability is expressed as in (7):",
        "'condition 2' is as follows:",
        "S i , fj S i , fj",
        "tj for aj)) or tj for aj)) or tj for aj)) or",
        "((ei – Si, fj – tj for ai) and (ei – Si, fj – tj for aj))",
        "'condition 4' is as follows:",
        "((ei – Si, fj – tj for ai) and (ei – Si, fj – tj for aj)) or",
        "si, fj = tj for ai) and ((ei – si> fj – tj for ai) and (ei – si- fj – % for aj)) = si, fj = tj for aj)",
        "n= num of anchor words in i tj for ai) and (ei – Si, fj tj for ai) and (ei – Si, fj – tj for aj))",
        "Prior for Sampled Alignment (Function) Space",
        "This is identical to that of the Prior Model II exhaustive alignment space with only a difference in the normalization process.",
        "Prior for Jump Width i This categorization of Prior Model II is the same as that of Prior Model I for for Jump Width i (see Section 4.2).",
        "Note that Prior Model II requires more memory compared to the Prior Model I."
      ]
    },
    {
      "heading": "5. Experimental Settings",
      "text": [
        "The baseline in our experiments is a standard log-linear phrase-based MT system based on Moses.",
        "The GIZA++ implementation (Och and Ney, 2003 a) of IBM Model 4 is used as the baseline for word alignment, which we compare to our modified GIZA++.",
        "Model 4 is incrementally trained by performing 5 iterations of Model 1, 5 iterations of HMM, 5 iterations of Model 3, and 5 iterations of Model 4.",
        "For phrase extraction the grow-diag-final heuristics are used to derive the refined alignment from bidirectional alignments.",
        "We then perform MERT while a 5-gram language model is trained with SRILM.",
        "Our implementation is based on a modified version of GIZA++ (Och and Ney, 2003a).",
        "This modification is on the function that reads a bilingual terminology file, the function that calculates priors, the M-step in IBM Models 1-5, and the forward-backward algorithm in the HMM Model.",
        "Other related software tools are written in Python and Perl: terminology concatenation, terminology numbering, and so forth."
      ]
    },
    {
      "heading": "6. Experimental Results",
      "text": [
        "We conduct an experimental evaluation on the NTCIR-8 corpus (Fujii et al., 2010) and on Eu-roparl (Koehn, 2005).",
        "Firstly, MWEs are extracted from both corpora, as shown in Table 3.",
        "In the second step, we apply our modified version of GIZA++ in which we incorporate the results of",
        "Table 3: Statistics ofourMWE extraction method.",
        "The numbers of MWEs are from 0.08 to 0.6 MWE / sentence pair in our statistical MWE extraction methods.",
        "MWE extraction.",
        "Secondly, in order to incorporate the extracted MWEs, they are reformatted as shown in Table 2.",
        "Thirdly, we convert all MWEs into a single token, i.e. we concatenate them with an underscore character.",
        "We then run the modified version of GIZA++ and obtain a phrase and reordering table.",
        "In the fourth step, we split the concatenated MWEs embedded in the third step.",
        "Finally, in the fifth step, we run MERT, and proceed with decoding before automatically evaluating the translations.",
        "Table 4 shows the results where 'baseline' indicates no BMWE grouping nor prior, and 'base-line2' represents a BMWE grouping but without the prior.",
        "Although 'baseline2' (BMWE grouping) shows a drop in performance in the JP-EN / EN-JP 50k sentence pair setting, Prior Model I results in an increase in performance in the same setting.",
        "Except for EN-ES 200k, our Prior Model I was better than 'baseline2'.",
        "For EN-JP NT-CIR using 200k sentence pairs, we obtained an absolute improvement of 0.77 Bleu points compared to the 'baseline'; for EN-JP using 50k sentence pairs, 0.75 Bleu points; and for ES-EN Eu-roparl corpus using 200k sentence pairs, 0.63 Bleu points.",
        "In contrast, Prior Model II did not work well.",
        "The possible reason for this is the misspecification, i.e. the modelling by IBM Model 4 was wrong in terms of the given data.",
        "One piece ofev-idence for this is that most of the enforced alignments were found correct in a manual inspection.",
        "For EN-JP NTCIR using the same corpus of 200k, although the number of unique MWEs ex-",
        "corpus",
        "language",
        "size",
        "#unique",
        "#all",
        "MWEs",
        "MWEs",
        "statistical method",
        "NTCIR",
        "EN-JP",
        "200k",
        "1,121",
        "120,070",
        "europarl",
        "EN-FR",
        "200k",
        "312",
        "22,001",
        "europarl",
        "EN-ES",
        "200k",
        "406",
        "16,350",
        "heuristic method",
        "NTCIR",
        "EN-JP",
        "200k",
        "50,613",
        "114,373",
        "Table 4: Results.",
        "Baseline is plain GIZA++ / Moses (without BMWE grouping / prior), base-line2 is with BMWE grouping, prior I / II are with BMWE grouping and prior.",
        "tracted by the statistical method and the heuristic method varies significantly, the total number of MWEs by each method becomes comparable.",
        "The resulting Bleu score for the heuristic method (24.24 / 22.48 Blue points for 200k EN-JP / JP-EN) is slightly better than that of the statistical method.",
        "The possible reason for this is related to the way the heuristic method groups terms including reference numbers, while the statistical method does not.",
        "As a result, the complexity of the alignment model simplifies slightly in the case of the heuristic method."
      ]
    },
    {
      "heading": "7. Conclusion",
      "text": [
        "This paper presents a new method of incorporating BMWEs into word alignment.",
        "We first detect BMWEs in a bidirectional way and then use this information to do groupings and to enforce already known alignment links.",
        "For the latter process, we replace the maximum likelihood estimate in the M-step of the EM algorithm with the MAP estimate; this replacement allows the incorporation of the prior in the M-step of the EM algorithm.",
        "We include an experimental investigation into incorporating extracted BMWEs into a word aligner.",
        "Although there is some work which incorporates BMWEs in groupings, they do not enforce alignment links.",
        "There are several ways in which this work can be extended.",
        "Firstly, although we assume that our a priori partial annotation is reliable, if we extract such MWEs automatically, we cannot avoid erroneous pairs.",
        "Secondly, we assume that the reason why our Prior Model II did not work was due to the misspecification (or wrong modelling).",
        "We would like to check this by discriminative modelling.",
        "Thirdly, although here we extract BMWEs, we can extend this to extract paraphrases and non-literal expressions."
      ]
    },
    {
      "heading": "8. Acknowledgments",
      "text": [
        "This research is supported by the Science Foundation Ireland (Grant 07/CE/I1142) as part of the Centre for Next Generation Localisation (http : / / www.",
        "cngl .ie) at Dublin City University and Trinity College Dublin.",
        "We would also like to thank the Irish Centre for High-End Computing.",
        "size",
        "EN-JP",
        "Bleu",
        "JP-EN",
        "Bleu",
        "50k",
        "baseline",
        "16.33",
        "baseline",
        "22.01",
        "50k",
        "baseline2",
        "16.10",
        "baseline2",
        "21.71",
        "50k",
        "prior I",
        "17.08",
        "prior I",
        "22.11",
        "50k",
        "prior II",
        "16.02",
        "prior II",
        "20.02",
        "200k",
        "baseline",
        "23.42",
        "baseline",
        "21.68",
        "200k",
        "baseline2",
        "24.10",
        "baseline2",
        "22.32",
        "200k",
        "prior I",
        "24.22",
        "prior I",
        "22.45",
        "200k",
        "prior II",
        "23.22",
        "prior II",
        "21.00",
        "size",
        "FR-EN",
        "Bleu",
        "EN-FR",
        "Bleu",
        "50k",
        "baseline",
        "17.68",
        "baseline",
        "17.80",
        "50k",
        "baseline2",
        "17.76",
        "baseline2",
        "18.00",
        "50k",
        "prior I",
        "17.81",
        "prior I",
        "18.02",
        "50k",
        "prior II",
        "17.01",
        "prior II",
        "17.30",
        "200k",
        "baseline",
        "18.40",
        "baseline",
        "18.20",
        "200k",
        "baseline2",
        "18.80",
        "baseline2",
        "18.50",
        "200k",
        "prior I",
        "18.99",
        "prior I",
        "18.60",
        "200k",
        "prior II",
        "18.20",
        "prior II",
        "17.50",
        "size",
        "ES-EN",
        "Bleu",
        "EN-ES",
        "Bleu",
        "50k",
        "baseline",
        "16.21",
        "baseline",
        "15.17",
        "50k",
        "baseline2",
        "16.61",
        "baseline2",
        "15.60",
        "50k",
        "prior I",
        "16.91",
        "prior I",
        "15.87",
        "50k",
        "prior II",
        "16.15",
        "prior II",
        "14.60",
        "200k",
        "baseline",
        "16.87",
        "baseline",
        "17.62",
        "200k",
        "baseline2",
        "17.40",
        "baseline2",
        "18.21",
        "200k",
        "prior I",
        "17.50",
        "prior I",
        "18.20",
        "200k",
        "prior II",
        "16.50",
        "prior II",
        "17.10"
      ]
    }
  ]
}
