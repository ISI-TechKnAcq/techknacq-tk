{
  "info": {
    "authors": [
      "Alexandre Gil",
      "Gaël Dias"
    ],
    "book": "Workshop on Multiword Expressions: Analysis, Acquisition and Treatment",
    "id": "acl-W03-1804",
    "title": "Using Masks, Suffix Array-Based Data Structures and Multidimensional Arrays to Compute Positional Ngram Statistics from Corpora",
    "url": "https://aclweb.org/anthology/W03-1804",
    "year": 2003
  },
  "references": [
    "acl-J01-1001"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper describes an implementation to compute positional ngram statistics (i.e.",
        "Frequency and Mutual Expectation) based on masks, suffix array-based data structures and multidimensional arrays.",
        "Positional ngrams are ordered sequences of words that represent continuous or discontinuous substrings of a corpus.",
        "In particular, the positional ngram model has shown successful results for the extraction of discontinuous collocations from large corpora.",
        "However, its computation is heavy.",
        "For instance, 4.299.742 positional ngrams (n=1..7) can be generated from a 100.000-word size corpus in a seven-word size window context.",
        "In comparison, only 700.000 ngrams would be computed for the classical ngram model.",
        "It is clear that huge efforts need to be made to process positional ngram statistics in reasonable time and space.",
        "Our solution shows O(h(F) N log N) time complexity where N is the corpus size and h(F) a function of the window context."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Many models have been proposed to evaluate word dependencies.",
        "One of the most successful statistical models is certainly the ngram model (Jelinek, 1990).",
        "However, in order to overcome its conceptual rigidity, T. Kuhn et al.",
        "(1994) have defined the polygram model that estimates the probability of an ngram by interpolating the relative frequencies of all its kgrams (k < n).",
        "Another way to account for variable length dependencies is the n-multigram model designed by Deligne and Bimbot (1995).",
        "All these models have in common the fact that they need to compute continuous string frequencies.",
        "This task can be colossal when gigabytes of data need to be processed.",
        "Indeed, Yamamoto and Church (2000) show that there exist N(N+1)/2 substrings in a N-size corpus.",
        "That is the reason why low order ngrams have been commonly used in Natural Language Processing applications.",
        "In the specific field of multiword unit extraction, Dias (2002) has introduced the positional ngram model that has evidenced successful results for the extraction of discontinuous collocations from large corpora.",
        "Unlikely previous models, positional ngrams are ordered sequences of tokens that represent continuous or discontinuous substrings of a corpus computed in a (2.F+1)-word size window context (F represents the context in terms of words on the right and on the left of any word in the corpus).",
        "As a consequence, the number of generated substrings rapidly explodes and reaches astronomic figures.",
        "Dias (2002) shows that A (Equation 1) positional ngrams can be computed for an N-size corpus in a (2.F+1)-size window context.",
        "In order to illustrate this equation, 4.299.742 positional ngrams (n=1..7) would be generated from a 100.000- word size corpus in a seven-word size window context.",
        "In comparison, only 700.000 ngrams would be com* The authors want to thank Professor José Gabriel Pereira Lopes from the New University of Lisbon for his advices.",
        "puted for the classical ngram model.",
        "It is clear that huge efforts need to be made to process positional ngram statistics in reasonable time and space.",
        "In this paper, we describe an implementation that computes the Frequency and the Mutual Expectation (Dias et al.",
        "1999) of any positional ngram with time complexity O(h(F) N log N).",
        "The global architecture is based on the definition of masks that allow virtually representing any positional ngram in the corpus.",
        "Thus, we follow the Virtual Corpus approach introduced by Kit and Wilks (1998) and apply a suffix-array-like method, coupled to the Multikey Quicksort algorithm (Bentley and Sedge-wick, 1997), to compute positional ngram frequencies.",
        "Finally, a multidimensional array is built to easily process the Mutual Expectation, an association measure for collocation extraction.",
        "The evaluation of our C++ implementation has been realized over the CETEMPúblico2 corpus and shows satisfactory results.",
        "For example, it takes 8.59 minutes to compute both frequency and Mutual Expectation for a 1.092.7233-word corpus on an Intel Pentium III 900 MHz Personal Computer for a seven-word size window context.",
        "This article is divided into four sections: (1) we explain the basic principles of positional ngrams and the mask representation to build the Virtual Corpus; (2) we present the suffix-array-based data structure that allows counting occurrences of positional ngrams; (3) we show how a multidimensional array eases the efficient computation of the Mutual Expectation; (4) we present results over different size sub-corpora of the CETEMPúblico corpus."
      ]
    },
    {
      "heading": "2 Positional Ngrams",
      "text": [
        "In the specific field of multiword unit extraction, Dias (2002) has introduced the positional ngram model that has evidenced successful results for the extraction of discontinuous collocations from large corpora.",
        "words to the right of the same pivot word and the pivot word itself).",
        "In general terms, a collocation can be defined as a specific4 continuous or discontinuous sequence of words in a (2.F+1)-word size window context (i.e. F words to the left of a pivot word, F words to the right of the same pivot word and the pivot word itself).",
        "This situation is illustrated in Figure 1 for the collocation Ngram Statistics that fits in the window context.",
        "Thus, as computation is involved, we need to process all possible substrings (continuous or discontinuous) that fit inside the window context and contain the pivot word.",
        "Any of these substrings is called a positional ngram.",
        "For instance, [Ngram Statistics] is a positional ngram as is the discontinuous sequence [Ngram ___ from] where the gap represented by the underline stands for any word occurring between Ngram and from (in this case, Statistics).",
        "More examples are given in Table 1.",
        "In order to compute all the positional ngrams of a corpus, we need to take into account all the words as possible pivot words.",
        "• B C D E F G H I J K L M N .... X Y Z .... • B C D E F G H I J K L M N .... X Y Z .... • B C D E F G H I J K L M N .... X Y Z .... • B C D E F G H I J K L M N .... X Y Z .... F=3 2.1 Principles .... • B C D E F G H I J K L M N .... X Y Z ....",
        "....",
        "The original idea of the positional ngram model comes from the lexicographic evidence that most lexical relations associate words separated by at most five other words (Sinclair, 1974).",
        "As a consequence, lexical relations such as collocations can be continuous or discontinuous sequences of words in a context of at most eleven words (i.e. 5 words to the left of a pivot word, 5",
        "A simple way would be to shift the two-window context to the right so that each word would sequentially be processed.",
        "However, this would inevitably lead to duplications of positional ngrams.",
        "Instead, we propose a",
        "one-window context that shifts to the right along the corpus as illustrated in Figure 2.",
        "It is clear that the size of the new window should be 2.F+1.",
        "to adapt the suffix representation (Manber and Myers, 1990) to the positional ngram case.",
        "This new representation implies new restrictions.",
        "While all combinations of words were valid positional ngrams in the two-window context, this is not true for a one-window context.",
        "Indeed, two restrictions must be observed.",
        "Following the suffix representation, any continuous corpus substring is virtually represented by a single position of the corpus as illustrated in Figure 3.",
        "In fact, the substring is the sequence of words that goes from the word referred by the position till the end of the corpus.",
        "Restriction 2: For any continuous or discontinuous substring in the window context, by shifting the substring from left to right, excluding gaps and words on the right and inserting gaps on the left, so that there always exists a word in the central position cpos (Equation 2) of the window, there should be at least one shift that contains all the words of the substring in the context window.",
        "For example, from the first case of Figure 2, the discontinuous sequence [A B _ _ E _ G] is not a positional ngram although it is a possible substring as it does not follow the second restriction.",
        "Indeed, whenever we try to align the sequence to the central position, at least one word is lost as shown in Table 2:",
        "In contrast, the sequence [A _ C _ E F _] is a positional ngram as the shift [_ A _ C _ E F], with C in the central position, includes all the words of the substring.",
        "Basically, the first restriction aims at avoiding duplications and the second restriction simply guarantees that no substring that would not be computed in a two-window context is processed."
      ]
    },
    {
      "heading": "2.2 Virtual Representation",
      "text": [
        "The representation of positional ngrams is an essential step towards efficient computation.",
        "For that, purpose, we propose a reference representation rather than an explicit structure of each positional ngram.",
        "The idea is Unfortunately, the suffix representation can not directly be extended to the specific case of positional ngrams.",
        "One main reason aims at this situation: a positional ngram may represent a discontinuous sequence of words.",
        "In order to overcome this situation, we propose a representation of positional ngrams based on masks.",
        "As we saw in the previous section, the computation of all the positional ngrams is a repetitive process.",
        "For each word in the corpus, there exists an algorithmic pattern that identifies all the possible positional ngrams in a 2.F+1-word size window context.",
        "So, what we need is a way to represent this pattern in an elegant and efficient way.",
        "One way is to use a set of masks that identify all the valid sequences of words in a given window context.",
        "Thus, each mask is nothing more than a sequence of 1 and 0 (where 1 stands for a word and 0 for a gap) that represents a specific positional ngram in the window context.",
        "An example is illustrated in Figure 4.",
        "Computing all the masks is an easy and quick process.",
        "In our implementation, the generation of masks is done recursively and is negligible in terms of space and time.",
        "In table 3, we give the number of masks h(F) for different values of F.",
        "From these structures, the virtual representation of any positional ngram is straightforward.",
        "Indeed, any positional ngram can be identified by a position in the corpus and a given mask.",
        "Taking into account that a corpus is a set of documents, any positional ngram can be represented by the tuple {{iddoc, posdoc}, idmask} where iddoc stands for the document id of the corpus, posdoc for a given position in the document and idmask for a specific mask.",
        "An example is illustrated in Figure 6.",
        "As we will see in the following section, this reference representation will allow us to follow the Virtual Corpus approach introduced by Kit and Wilks (1998) to compute ngram frequencies.",
        "With the Virtual Corpus approach, counting continuous substrings can easily and efficiently be achieved.",
        "After sorting the suffix-array data structure presented in Figure 3, the count of an ngram consisting of any n words in the corpus is simply the count of the number of adjacent indices that take the n words as prefix.",
        "We illustrate the Virtual Corpus approach in Figure 6.",
        "Counting positional ngrams can be computed exactly in the same way.",
        "The suffix-array structure is sorted using lexicographic ordering for each mask in the array of masks.",
        "After sorting, the count of a positional ngram in the corpus is simply the count of adjacent indices that stand for the same sequence.",
        "We illustrate the Virtual Corpus approach for positional ngrams in Figure 7.",
        "Different reasons have lead to use the Multikey Quicksort algorithm.",
        "First, it performs independently from the vocabulary size.",
        "Second, it shows O(N log N) time complexity in our specific case.",
        "Third, Anderson and Nilsson (1998) show that it performs better than the MSD radixsort and proves comparable results to the newly introduced Forward radixsort.",
        "The basic idea of the Normalized Expectation is to evaluate the cost, in terms of cohesiveness, of the loss of one word in a positional ngram.",
        "Thus, the Normalized Expectation measure is defined in Equation 3 where the function k(.)",
        "returns the frequency of any positional ngram7.",
        "By statement, any pii is equal to zero.",
        "The efficiency of the counting mainly resides in the use of an adapted sort algorithm.",
        "Kit and Wilks (1998) propose to use a bucket-radixsort although they acknowledge that the classical quicksort performs faster for large-vocabulary corpora.",
        "Around the same perspective, Yamamoto and Church (2000) use the Manber and Myers’s algorithm (1990), an elegant radixsort-based algorithm that takes at most O(N log N) time and shows improved results when long repeated substrings are common in the corpus.",
        "For the specific case of positional ngrams, we have chosen to implement the Multikey Quicksort algorithm (Bentley and Sedgewick, 1997) that can be seen as a mixture of the Ternary-Split Quicksort (Bentley and McIlroy, 1993) and the MSD6 radixsort (Anderson and Nilsson, 1998).",
        "The algorithm processes as follows: (1) the array of string is partitioned into three parts based on the first symbol of each string.",
        "In order to process the split a pivot element is chosen just as in the classical quicksort giving rise to: one part with elements smaller than the pivot, one part with elements equal to the pivot and one part with elements larger than the pivot; (2) the smaller and the larger parts are recursively processed in exactly the same manner as the whole array; (3) the equal part is also sorted recursively but with partitioning starting from the second symbol of each string; (4) the process goes on recursively: each time an equal part is being processed, the considered position in each string is moved forward by one symbol.",
        "In Figure 8, we propose an illustration of the Multikey Quicksort taken from the paper (Bentley and Sedgewick, 1997).",
        "The pivot is chosen using the median method.",
        "However, to understand the Matrix itself, we first need to show how the sub-ngrams of any positional ngram can be represented."
      ]
    },
    {
      "heading": "Representing sub-ngrams",
      "text": [
        "A sub-ngram is obtained by extracting one word at a time from its related positional ngram as shown in Figure 9."
      ]
    },
    {
      "heading": "Mutual Expectation",
      "text": [
        "One effective criterion for multiword lexical unit identification is frequency.",
        "From this assumption, Dias et al.",
        "(1999) pose that between two positional ngrams with the same Normalized Expectation, the most frequent positional ngram is more likely to be a collocation.",
        "So, the Mutual Expectation of any positional ngram is defined in Equation 5 based on its Normalized Expectation and its relative frequency.",
        "Equation 5: Mutual Expectation In order to compute the Mutual Expectation of any positional ngram, it is necessary to build a data structure that allows rapid and efficient search over the space of all positional ngrams.",
        "For that purpose, we propose a multidimensional array structure called Matrix9."
      ]
    },
    {
      "heading": "4.2 Matrix",
      "text": [
        "The attentive reader will have noticed that the denominator of the Normalized Expectation formula is the average frequency of all the positional (n-1)grams included in a given positional ngram.",
        "These specific positional ngrams are called positional sub-ngrams of order n-110.",
        "So, in order to compute the Normalized Expectation and a fortiori the Mutual Expectation, it is necessary to access efficiently to the sub-ngrams frequencies.",
        "This operation is done through the Matrix.",
        "By representing a sub-ngram, we mean calculating its virtual representation that identifies its related substring.",
        "The previous figure shows that representing the first three sub-ngrams of the positional ngram {{0,0),14) is straightforward as they all contain the first word of the window context.",
        "The only difficulty is to know the mask they are associated to.",
        "Knowing this, the first three sub-ngrams would respectively be represented as: {{0,0),15), {{0,0),16), {{0,0),13).",
        "For the last sub-ngram, the situation is different.",
        "The first word of the window context is omitted.",
        "As a consequence, in order to calculate its virtual representation, we need to know the position of the first word of the substring as well as its corresponding mask.",
        "In this case, the position in the document of the positional sub-ngram is simply the position of its related positional ngram plus the distance that separates the first word of the window context from the first word of the substring.",
        "We call delta this distance.",
        "The obvious representation of the fourth sub-ngram is then {{0,2),18) where the position is calculated as 0+(delta=2)=2.",
        "In order to represent the sub-ngrams of any positional ngram, all we need is to keep track of the masks related",
        "to the mask of the positional ngram and the respective deltas.",
        "Thus, it is clear that for each mask, there exists a set of pairs {idmask, delta} that allows identifying all the sub-ngrams of any given positional ngram.",
        "Each pair is called a submask and is associated to its upper mask11 as illustrated in Figure 10. based data structure, calculating all these positions is straightforward.",
        "Calculating the Mutual Expectation is also straightforward and fast as accessing to any positional ngram can be done in O(1) time complexity.",
        "We will illustrate this reality in the next section.",
        "Now that all necessary virtual representations are well-established, in order to calculate the Mutual Expectation, we need to build a structure that allows efficiently accessing any positional ngram frequency.",
        "This is the objective of the Matrix, a 2-dimension array structure."
      ]
    },
    {
      "heading": "2-dimension Array Structure",
      "text": [
        "Searching for specific positional ngrams in a huge sample space can be overwhelming.",
        "To overcome this computation problem, two solutions are possible: (1) keep the suffix array-based data structure and design optimized search algorithms or (2) design a new data structure to ease the searching process.",
        "We chose the second solution as our complete system heavily depends on searching through the entire space of positional ngrams 12 and, as a consequence, we hardly believe that improved results may be reached following the second solution.",
        "This new structure is a 2-dimension array where lines stand for the masks ids and the columns for the positions in the corpus.",
        "Thus, each cell of the 2-dimension array represents a given positional ngram as shown in Figure 11.",
        "This structure is called the Matrix.",
        "The frequency of each positional ngram can easily be represented by all its positions in the corpus.",
        "Indeed, a given positional ngram is a substring that can appear in different positions of the corpus being the count of these positions its frequency.",
        "From the previous suffix array",
        "The illustration of our architecture is now complete.",
        "We now need to test our assumptions.",
        "For that purpose, we present results of our implementation over the CETEMPilblico corpus."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "We have conducted a number of experiments of our C++ implementation on the CETEMPilblico Portuguese corpus to derive positional ngram statistics (Frequency and Mutual Expectation).",
        "The experiments have been realized on an Intel Pentium 900 MHz PC with 390MB of RAM.",
        "From the original corpus, we have randomly defined 5 different size sub-corpora that we present in",
        "For each sub-corpus we have calculated the execution time of different stages of the process: (1) the tokenization that transforms the corpus into a set of integers; (2) the preparation of the mask structure and the construction of the suffix-array data structure; (3) the sorting of the suffix-array data structure and the creation of the Matrix; (4) the calculation of the ME.",
        "The results are given in Table 5.",
        "The results clearly show that the construction of the Matrix and the sort operation over the suffix-array data structure are the most time consuming procedures.",
        "On the contrary, the computation of the Mutual Expectation is quick due to the direct access to sub-ngrams frequencies enabled by the Matrix.",
        "In order to understand the evolution of the results, we present, in Figure 12, a graphical representation of the results.",
        "The graphical representation illustrates a linear time complexity.",
        "In fact, Alexandre Gil (2002) has proved that, mainly due to the implementation of the Multikey Quicksort algorithm, our implementation evidences a time complexity of O(h(F) N log N) where N is the size of the corpus and h(F) a function of the window context."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "In this paper, we have described an implementation to compute positional ngram statistics based on masks, suffix array-based data structure and multidimensional arrays.",
        "Our C++ solution shows that it takes 8.59 minutes to compute both frequency and Mutual Expectation for a 1.092.723-word corpus on an Intel Pentium III 900 MHz for a seven-word size window context.",
        "In fact, our architecture evidences O(h(F) N log N) time complexity.",
        "To some extent, this work proposes a response to the conclusion of (Kit and Wilks, 1998) that claims that"
      ]
    }
  ]
}
