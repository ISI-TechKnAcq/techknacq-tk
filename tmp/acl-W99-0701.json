{
  "info": {
    "authors": [
      "Chunyu Kitt",
      "Yorick Wilks"
    ],
    "book": "Workshop on Computational Natural Language Learning CoNLL",
    "id": "acl-W99-0701",
    "title": "Unsupervised Learning of Word Boundary With Description Length Gain",
    "url": "https://aclweb.org/anthology/W99-0701",
    "year": 1999
  },
  "references": [
    "acl-J93-2004"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents an unsupervised approach to lexical acquisition with the goodness measure description length gain (DLG) formulated following classic information theory within the minimum description length (MDL) paradigm.",
        "The learning algorithm seeks for an optimal segmentation of an utterance that maximises the description length gain from the individual segments.",
        "The resultant segments show a nice correspondence to lexical items (in particular, words) in a natural language like English.",
        "Learning experiments on large-scale corpora (e.g., the Brown corpus) have shown the effectiveness of both the learning algorithm and the goodness measure that guides that learning."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Detecting and handling unknown words properly has become a crucial issue in today's practical natural language processing (NLP) technology.",
        "No matter how large the dictionary that is used in a NLP system, there can be many new words in running/real texts, e.g., in scientific articles, newspapers and Web pages, that the dictionary does not include.",
        "Many such words are proper names and special terminology that provide critical information.",
        "It is unreliable to rest on delimiters such as white spaces to detect new lexical units, because many basic lexical items contain one or more spaces, e.g., as in \"New York\", \"Hong Kong\" and \"hot dog\".",
        "It appears that unsupervised learning techniques are necessary in order to alleviate the problem of unknown words in the NLP domain.",
        "There have been a number of studies on lexical acquisition from language data of different types.",
        "Wolff attempts to infer word boundaries from artificially-generated natural language sentences, heavily relying on the co-occurrence frequency of adjacent characters [Wolff 1975, Wolff 1977].",
        "Nevi11-Manning's text compression - program Sequitur can also identify word boundaries and gives a binary tree structure for an identified word [Nevi1I-Manning 1996].",
        "de Mar-cken explores unsupervised lexical acquisition from English spoken and written corpora and from a Chinese written corpus [de Marken 1995, de Marken 19961.",
        "In this paper, we present an unsupervised approach to lexical acquisition within the minimum description length (MDL) paradigm [Rissanen 1978, Rissanen 1982] [Rissanen 19891, with a goodness measure, namely, the description length gain (DLG), which is formulated in [Kit 1998] following classic information theory [Shannon 1948, Cover and Thomas 1991].",
        "This measure is used, following the MDL principle, to evaluate the goodness of identifying a (sub)sequence of characters in a corpus as a lexical item.",
        "In order to rigorously evaluate the effectiveness of this unsupervised learning approach, we do not limit ourselves to the detection of unknown words with respect to any given dictionary.",
        "Rather, we use it to perform unsupervised lexical acquisition from large-scale English text corpora.",
        "Since it is a learning-via-compression approach, the algorithm can be further extended to deal with text compression and, very likely, other data sequencing problems.",
        "The rest of the paper is organised as follows: Section 2 presents the formulation of the DLG mea",
        "sure in terms of classic information theory; Section 3 formulates the learning algorithm within the MDL framework, which aims to achieve an optimal segmentation of the given corpus into lexical items with regard to the DLG measure; Section 4 presents experiments and discusses experimental results with respect to previous studies; and finally, the conclusions of the paper are given in Section 5."
      ]
    },
    {
      "heading": "2. Description Length Gain",
      "text": [
        "Kit defines the description length of a corpus X = x1x2 • • • a sequence of linguistic tokens (e.g., characters, words, POS tags), as the Shannon-Fano code length for the corpus [Kit 1998].",
        "Following classic information theory [Shannon 1948, Cover and Thomas 1991], it can be formulated in terms of token counts in the corpus as below for empirical calculation:",
        "xor IXI where V is the set of distinct tokens (i.e., the vocabulary) in X and c(x) is the count of x in X.",
        "Accordingly, the description length gain (DLG) from identifying a (sub)sequence s = s182• • -.5k in the corpus X as a segment or chunk, which is expected to have a nice correspondence to a linguistically significant unit (e.g., a lexical item such as a word, or a syntactic phrase), is formulated as",
        "where r is an index, X[r .5] represents the resultant corpus by the operation of replacing all occurrences of s with r through out X (in other words, we extract a rule r s from X) and e represents the concatenation of two strings (e.g., X[r • – * .5] and s) with a delimiter inserted in between.",
        "It is straightforward that the average DLG for extracting an individual s from X is",
        "This average DLG is an estimation of the coinpres-sion effect of extracting an individual instance of s from X.",
        "As an extracted s is supposed to be appended to the modified corpus by a string concatenation, as shown in (2), the original corpus can be easily recovered by a transformation that reverses the extraction, i.e., replacing all r's in X[r a] with the string s. It is worth noting that we can achieve the purpose of calculating DL(X[r s) .5) without carrying out the string substitution operations throughout the original corpus.",
        "The calculation can be based on the token count change involved in the substitution operations to derive the new corpus X[r a] S, as follows:",
        "where ci(x) is the new count of x in the new corpus and n' is the new corpus length.",
        "The new counts and the new length are, straightforwardly,",
        "(5) where c(x) and c8(x) are the counts of x in the original corpus X and in the string s, respectively.",
        "A key problem in this straightforward calculation is that we need to derive the count c(s) for all possible string s's in the original corpus X, because during the lexical learning process it is necessary to consider all fragments (i.e., all n-grams) in the corpus in order to select a set of good candidates for lexical items.",
        "Kit and Wilks provide an efficient method for deriving n-grams of any length and their counts from large-scale corpora [Kit and Wilks 1998].",
        "It has been adopted as the operational basis for the implementation of the unsupervised lexical acquisition algorithm that is to be reported in the next sections."
      ]
    },
    {
      "heading": "3. Learning Algorithm",
      "text": [
        "Given an utterance U = toti- • tn as a string of some linguistic tokens (e.g., characters, words, POS tags), the unsupervised lexical acquisition algorithm seeks for an optimal segmentation OS(U) over the string U such that the sum of the compression effect over the segments is maximal.",
        "Formally",
        "where 0 < k < n, + represents a string concatenation and aDLG(s,) is the average DLG for each instance of the string s, in the original corpus, as defined in (3) above.",
        "Based on this description length gain calculation, a Viterbi algorithm is formulated to search for the optimal segmentation over an utterance U that fulfils (6).",
        "It is presented in Figure 1 with an illustration.",
        "The algorithm uses a list of intermediate variables OS[0], OS[1], • • - , 0 S[n], each OS s[i] stores the optimal segmentation over toti - ti (for i = 0,1.",
        "2, - • • , n).",
        "A segmentation is an ordered set (or list) of adjacent segments.",
        "The sign represents an ordered set union operation.",
        "The DLG over a list of segments, e.g., DLG(OS[j]), is defined as the sum of all segments' DLGs in the set:",
        "Notice that the algorithm has a bias against the extraction of a single token as a rule, due to the fact that a single token rule bears a negative DLG.",
        "When j = k – 1, OS[j] W [t3+1 • • tic] becomes OS[k – 1] W fitkil, which is less preferable than OS[k – 1] t-±) {tk}.",
        "The difference between the denotations [tk] and tk is that the former indicates that the string tk is extracted from the corpus as the right-hand side of a rule (a deterministic CFG rule), which results in a negative DLG; whereas the latter treats tk as an individual token instead of a segment, which has a zero DLG.",
        "It is worth noting that the breaking condition c([ t3 • • • tk]) < 2 in the inner loop in the algorithm is an empirical condition.",
        "Its main purpose is to speed up the algorithm by avoiding fruitless iterations on strings of count 1.",
        "According to our observation in experiments, learning without this breaking condition leads to exactly the same result on large-scale corpora but the speed is many times slower.",
        "Strings with a count c = 1 can be skipped in the learning, because they are all long strings with a negative DLGland none of them can become a good segment that contributes a positive compression effect to the entire segmentation of the 'Since extracting a string [t, • • 4] of count 1 as a rule does not change any token's count in the new corpus C[r t, • • tie] 0 t, • tk), except the new non-terminal r and the delimiter 10, whose counts become 1 (i.e., c(r) = cat, • tkp = 1 and c(0) = 1) after the extraction: Thus, DLGat, • • • tkp = DL(C) – DL(C[r ---+ t, • • • tkl=b t, • tk)",
        "utterance.",
        "Rather, they can be broken into shorter segments with a positive DLG.",
        "Time complexity analysis also shows that this breaking condition can speed up the algorithm significantly.",
        "Without this condition, the time complexity of the algorithm is 0(n2).",
        "With it, the complexity is bounded by 0(7-171), where m is the maximal common prefix length of sub-strings (i.e., n-grams) in the corpus.",
        "Accordingly, the average time complexity of the algorithm is (Nan), where a is the average common prefix length in the corpus, which is much smaller than m."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We have conducted a series of lexical acquisition experiments with the above algorithm on large-scale English corpora, e.g., the Brown corpus [Francis and Kucera 1982] and the PTB WSJ corpus [Marcus et a/.",
        "1993].",
        "Below is the segmentation result on the first few sentences in the Brown corpus:",
        "where uppercase letters are converted to lowercase ones, the spaces are visualised by an underscore and the fullstops are all replaced by ('s. Although a space is not distinguished from any other characters for the learner, we have to rely on the spaces to judge the correctness of a word boundary prediction: a predicted word boundary immediately before or after a space is judged as correct.",
        "But we also have observed that this criterion overlooks many meaningful predictions like \"- -charge) [d_by• • •\", \"-- -are_outmode] (c1_•-•\" and \".",
        "-govenunent) [s• • •\".",
        "If this is taken into account, the learning performance is evidently better than the precision and recall figures reported in Table 1 below.",
        "Interestingly, it is observed that n-gram counts derived from a larger volume of data can significantly improve the precision but decrease the recall of the word boundary prediction.",
        "The correlation between the volume of data used for deriving n-gram counts and the change of precision and recall is shown in Table 1.",
        "The effectiveness of the unsupervised learning is evidenced by the fact that its precision and recall are, respectively, all three times as high as the precision and recall by random guessing.",
        "The best learning performance, in terms of both precision and recall, in the experiments is the one with 79.33% precision and 63.01% recall, obtained from the experiment on the entire Brown corpus.",
        "It is straightforwardly understandable that the increase of data volume leads to a significant increase of precision in the learning, because prediction based on more data is more reliable.",
        "The reason for the drop of recall is that when the volume of data increases, more multi-word strings have a higher compression effect (than individual words) and, consequently, they are learned by the",
        "learner as lexical items, e.g., [fulton_county] (grand_jury] and [..took_place] .",
        "If the credit in such multi-word lexical items is counted, the recall must be much better than the one in Table 1.",
        "Of course, this also reflects a limitation of the learning algorithm: it only conducts an optimal segmentation instead of a hierarchical chunking on an utterance.",
        "The precision and recall reported above is not a big surprise.",
        "To our knowledge, however, it is the first time that the performance of unsupervised learning of word boundaries is examined with the criteria of both precision and recall.",
        "Unfortunately, this performance can't be compared with any previous studies, for several reasons.",
        "One is that the learning results of previous studies are not presented in a comparable manner, for example, [Wolff 1975, Wolff 1977] and [Nevill-Manning 1996], as noted by [de Marken 1996] as well.",
        "Another is that the learning outcomes are different.",
        "For example, the output of lexical learning from an utterance (as a character sequence) in [Nevill-Manning 1996] and [de Marken 1995, de Marken 1996] is a hierarchical chunking of the utterance.",
        "The chance to hit the correct words in such chunking is obviously many times higher than that in a flat segmentation.",
        "The hierarchical chunking leads to a recall above 90% in de Marken's work.",
        "Interestingly, however, de Marken does not report the precision, which seems too low, therefore meaningless, to report, because the learner produces so many chunks."
      ]
    },
    {
      "heading": "5. Conclusions and Future Work",
      "text": [
        "We have presented an unsupervised learning algorithm for lexical acquisition based on the goodness measure description length gain formulated following information theory.",
        "The learning algorithm follows the essence of the MDL principle to search for the optimal segmentation of an utterance that has the maximal description length gain (and therefore approaches the minimum description length of the utterance).",
        "Experiments on word boundary prediction with large-scale corpora have shown the effectiveness of the learning algorithm.",
        "For the time being, however, we are unable to compare the learning performance with other re-searchers' previous work, simply because they do not present the performance of their learning algorithms in terms of the criteria of both precision and recall.",
        "Also, our algorithm is significantly simpler, in that it rests on n-gram counts only, instead of any more complicated statistical data or a more sophisticated training algorithm.",
        "Our future work will focus on the investigation into two aspects of the lexical learning with the DLG measure.",
        "First, we will incorporate the expectation-maximization (EM) algorithm [Dempster et al.",
        "1977] into our lexical learning to see how much performance can be improved.",
        "Usually, a more sophisticated learning algorithm leads to a better learning result.",
        "Second, we will explore the hierarchical chunking with the DLG measure.",
        "We are particularly interested to know how much more compression effect can be further squeezed out by hierarchical chunking from' a text corpus (e.g., the Brown corpus) and how much improvement in the recall can be achieved."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "The first author gratefully acknowledges a University of Sheffield Research Scholarship awarded to him that enables him to undertake this work.",
        "We wish to thank two anonymous reviewers for their invaluable comments, and thank Hamish Cunningham, Ted Dunning, Rob Gaizauskas, Randy LaPolla, Steve Renals, Jonathon Webster and many other colleagues for various kinds of help and useful discussions."
      ]
    }
  ]
}
