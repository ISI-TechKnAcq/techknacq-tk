{
  "info": {
    "authors": [
      "Nigel Collier",
      "Chikashi Nobata",
      "Jun'ichi Tsujii"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1030",
    "title": "Extracting the Names of Genes and Gene Products With a Hidden Markov Model",
    "url": "https://aclweb.org/anthology/C00-1030",
    "year": 2000
  },
  "references": [
    "acl-A97-1029",
    "acl-E99-1043",
    "acl-M93-1007",
    "acl-P96-1041",
    "acl-W98-1118"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We report; the results of a study into the use of a linear interpolating hidden Markov model (IIMM) for the task of extracting technical terminology from MEDUNE abstracts and texts in the molecular-biology domain.",
        "This is the first stage in a system that will extract event information for automatically updating biology databases.",
        "We trained the IIMM entirely with ingrains based on lexical and character features in a relatively small corpus of 1.00 MED-LINE abstracts that were marked-up by domain experts with term classes such as proteins and DNA.",
        "Using cross-validation methods we achieved an 1.-score of 0.73 and we examine the contribution made by each part of the interpolation model to overcoming data sparseness."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In the last few years there has been a. great investment in molecular-biology research.",
        "This has yielded many results that, together witli a migration of archival material to the Internet, has resulted in an explosion in the number of research publications available in online databases.",
        "The results in these papers however are not available in a structured format and have to be extracted and synthesized manually.",
        "Updating databases such as SwissProt (13airoch and Apweiler, 1997) this way is tune consuming and means that the results are not accessible so c.:onveniently to help researchers in their work.",
        "Our research is aimed at automatically ex-traciting facts from scientific abstracts and full papers in the molecular-biology domain and using these to update databases.",
        "A.s the first stage in achieving this goal we have explored the use of a generalisable, supervised training method based on hidden Markov models (1.-B4Ms) (R,a-biner and Juang, 1986) for the identification and classification of technical expressions in these texts.",
        "This task can be considered to be similar to the named entity task in the MUC evaluation exercises (MIX, 1995).",
        "In our current work we are using abstracts available from PubMed's MEDLINE (MEDLINE, 1999).",
        "The MEDLINE database is an online collection of abstracts for published journal articles in biology and medicine and contains more than nine million articles.",
        "With the rapid growth ill the number of published papers in the field of molecular-biology there has been growing interest in the application of information extraction, (Sekimizu al., 1998)(Collier et al., 1999) (Thomas et al., 1999)(Craven and Kundien, 1999), to help solve some of the problems that are associated with information overload.",
        "In the remainder of this paper we will first of all outline the background to the task and then describe Hie basics of IIMMs and the formal model we are using.",
        "The following sections give an outline of a new tagged corpus (Olita et al., 1.999) that our team has developed using abstracts taken from a sub-domain of MAXINE and the results of our experiments on this corpus."
      ]
    },
    {
      "heading": "2 Background",
      "text": [
        "Recent studies into the use of supervised learning-based models for the named entity task in the microbiology domain have shown that models based on 111\\4Ms and decision trees such as (Nobata et al., 1999) are much more genet.-alisable and adaptable to new classes of words than systems based on traditional hand-built patterns and domain specific heuristic rules such as (Fukuda et al., 1.998), overcoming the problems associated with data sparseness with the help of sophisticated smoothing algorithms",
        "(Chen and Goodman, 1996).",
        "HIVIMs can be considered to be stochastic finite state machines and have enjoyed success in a number of fields including speech recognition and part-of-speech tagging (Kupiec, 1992).",
        "It has been natural therefore that these models have been adapted for use in other word-class prediction tasks such as the named-entity task in IE.",
        "Such models are often based on ii-grams.",
        "Although the assumption that a word's part-of-speech or name class can be predicted by the previous n-i words and their classes is counter-intuitive to our understanding of linguistic structures and long distance dependencies, tins simple method does seem to be highly effective in practice.",
        "Nymble (Bikel et al., 1997), a system which uses HMMs is one of the most successful such systems and trains on a corpus of marked-up text, using only character features in addition to word bigrams.",
        "Although it is still early days for the use of HMMs for IE, we can see a number of trends in the research.",
        "Systems can be divided into those winch use one state per class such as Nymble (at the top level of their backoff model) and those winch automatically learn about the model's structure such as (Seymore et al., 1999).",
        "Additionally, there is a distinction to be made in the source of the knowledge for estimating transition probabilities between models which are built by hand such as (Freitag and McCal-lum, 1999) and those which learn from tagged corpora in the same domain such as the model presented in this paper, word lists and corpora in different domains - so-called distantly-labeled data (Seymore et al., 1999)."
      ]
    },
    {
      "heading": "2.1 Challenges of name finding in molecular-biology texts",
      "text": [
        "The names that we are trying to extract fall into a number of categories that are often wider than the definitions used for the traditional named-entity task used in MUG and may be considered to share many characteristics of term recognition.",
        "The particular difficulties with identifying and classifying terms in the molecular-biology domain are an open vocabulary and irrgeular naming conventions as well as extensive crossover in vocabulary between classes.",
        "The irregular naming arises in part because of the number of researchers from different fields who are",
        "working on the same knowledge discovery area as well as the large number of substances that need to be named.",
        "Despite the best efforts of major journals to standardise the terminology, there is also a significant problem with synonymy so that often an entity has more than one name that is widely used.",
        "The class crossover of terms arises because many proteins are named after DNA or RNA with winch they react.",
        "All of the names winch we mark up must belong to only one of the name classes listed in Table 1.",
        "We determined that all of these name classes were of interest to domain experts and were essential to our domain model for event extraction.",
        "Example sentences from a marked up abstract are given in Figure 1.",
        "We decided not to use separate states for pre and post-class words as had been used in some other systems, e.g. (Freitag and McCal-lum, 1999).",
        "Contrary to our expectations, we observed that our training data provided very poor maximum-likelihood probabilities for these words as class predictors.",
        "We found that protein predictor words had the only significant evidence and even this was quite weak, except in the case of post-class words which included a number of head nouns such as \"molecules\" or \"heterodimers\".",
        "In our",
        "abstrat:ts.",
        "early experiments using 11A4A4s that incorporated pre and post-class states we found that performance was significantly worSe than without such states and so WV formulated the model as given in section 3."
      ]
    },
    {
      "heading": "3 Method",
      "text": [
        "The purpose of our model is to find the most likely sequence of name classes (C) for a given sequence of words (W).",
        "The set of name classes includes the Unk' name class which we use for background words not belonging to any of the interesting name classes given in Table I and the given sequence of words which we use spans a single sentence.",
        "The task is therefore to maximize Pr (C1W).",
        "We implement a HAIM to es-thnate this using the 1\\4arkov assumption that Pr (CITY) can be found from bigrams of name classes.",
        "In the following model we consider words to be ordered pairs consisting of a surface word, W, and a word feature, F, given as < IV; F >.",
        "The word features themselves are discussed in Section 3.1.",
        "As is common practice, we need to calculate the probabilities for a word sequence for the first word's name class and every other word differently since we have no initial name-class to make a transition from.",
        "Accordingly we use the following equation to calculate the initial name class probability,",
        "and for all other words and their name classes as follows:",
        "where 1(1) is calculated with maximum-likelihood estimates 11..oin counts on training data, so that for example,",
        "Where TO has been found from counting the events in the training corpus.",
        "In our current system we set the constants Ai and ai by hand and let X; = r, -= ao > > a2, Ao > > A5.",
        "The current name-class Ci; is conditioned on the current word and feature, the previous name-class, Ci_1, and previous word and feature.",
        "Equations 1 and 2 implement a linear-intopolating HAIM that incorporates a number",
        "of sub-models (referred to from now by their A coefficients) designed to reduce the effects of data sparseness.",
        "While we hope to have enough training data to provide estimates for all model parameters, in reality we expect to encounter highly fragmented probability distributions.",
        "In the worst case, when even a name class pair has not been observed before in training, the model defaults at A5 to an estimate of name class unigrams.",
        "We note here that the bigram language model has a non-zero probability associated with each bigram over the entire vocabulary.",
        "Our model differs to a backoff formulation because we found that this model tended to suffer from the data sparseness problem on our small training set.",
        "Bikel et al. for example considers each backoff model to be separate models, starting at the top level (corresponding approximately to our Ao model) and then falling back to a lower level model when there not enough evidence.",
        "In contrast, we have combined these within a single probability calculation for state (class) transitions.",
        "Moreover, we consider that where direct bigram counts of 6 or more occur in the training set, we can use these directly to estimate the state transition probability and we use just the Ao model in tins case.",
        "For counts of less than 6 we smooth using Equation 2; this can be thought of as a simple form of 'bucketing'.",
        "The I-11VIM models one state per name class as well as two special states for the start and end of a sentence.",
        "Once the state transition probabilities have been calculated according to Equations 1 and 2, the Viterbi algorithm (Viterbi, 1967) is used to search the state space of possible name class assignments.",
        "This is done in linear time, 0(M/V2) for M the number of words to be classified and N the number of states, to find the highest probability path, i.e. to maximise Pr(W, C).",
        "In our experiments M is the length of a test sentence.",
        "The final stage of our algorithm that is used after name-class tagging is complete is to use a clean-up module called Unity.",
        "This creates a frequency list of words and name-classes for a document and then re-tags the document using the most frequently used name class assigned by the IIMM.",
        "We have generally found that this improves F-score performance by about 2.3%, both for re-tagging spuriously tagged words and",
        "for finding untagged words in unknown contexts that had been correctly tagged elsewhere in the text."
      ]
    },
    {
      "heading": "3.1 Word features",
      "text": [
        "Table 2 shows the character features that we used which are based on those given for Nymble and extended to give high performance in both molecular-biology and newswire domains.",
        "The intuition is that such features provide evidence that helps to distinguish name classes of words.",
        "Moreover we hypothesize that such features will help the model to find similarities between known words that were found in the training set and unknown words (of zero frequency in the training set) and so overcome the unknown word problem.",
        "To give a simple example: if we know that LMP - 1 is a member of PROTEIN and we encounter AP - I for the first time in testing, we can make a fairly good guess about the category of the unknown word `LIVIP' based on its sharing the same feature Two Caps with the known word 'AP' and 'AP's known relationship with 1'.",
        "Such unknown word evidence is captured in submodels Al through A3 in Equation 2.",
        "We",
        "consider that character information provides more meaningful distinctions between name classes than for example part-of-speech (POS), since POS will predominantly be noun for all name-class words.",
        "The features were chosen to be as domain independent as possible, with the exception of Hyphon and Greeliletter which have particular significance for the terminology in this domain."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": []
    },
    {
      "heading": "4.1 Training and testing set",
      "text": [
        "The training set we used in our experiments consisted of 100 MEDLINE abstracts, marked up in XML by a domain expert for the name classes given in Table 1.",
        "The number of NEs that were marked up by class are also given in Table 1 and the total number of words in the corpus is 29940.",
        "The abstracts were chosen from a subdomain of molecular-biology that we formulated by searching under the terms human, blood cell, tramwription factor in the PubMed database.",
        "This yielded approximately 3300 abstracts.",
        "4.2 li.esults r-.1'he results are given as F-scores, a connnon measurement; for accuracy in the NIUE; conferences that combines recall and precision.",
        "These are calculated using a standard MEC tool (Chinchor, 1995).",
        "F-score is defined as",
        "The first set of experiments we did shows the effectiveness of the model for all name classes and is summarized in Table 3.",
        "We see that data sparseness does have an effect, with proteins - the most numerous class in training - getting the best result and RNA - the smallest; training class - getting the worst result.",
        "'rhe table also shows the effectiveness of the character feature set, which in general adds 10.6% to the F-score.",
        "This is mainly due to a positive effect on words in the PROTEIN and DNA Oases, but we also see that members of all SOURCE subclasses suffer from featurization.",
        "We have attempted to incorporate generalisation through character features and linear interpolation, which has generally been quite successful.",
        "Nevertheless we were curious to see just",
        "models during testing as a fraction of:total number of state transitions in the Viterbi lattice.",
        "# Texts indicates the munber of abstracts used in training.",
        "which parts of the model were contributing to the bigram scores.",
        "Table 4 shows the percentage of bigrams which could be matched against training bigrams.",
        "The result indicate that a high percentage of direct bigrams in the test corpus never appear in the training corpus and shows that our 111\\41\\4 model is highly dependent on smoothing through inodels AI and A.",
        "We can take another view of the training data by salami-slicing' the model so that only evidence from part of the model is used.",
        "Results are shown in Table 5 and support the conclusion that models A.",
        "], A2 and A3 are crucial at this size of training data, although we would expect their relative importance to fall as we have more direct observations of bigrams with linger training data sets.",
        "Table 6 shows the robustness of the model",
        "training corpus (in number of MEDLINE abstracts).",
        "for data sparseness, so that even with only 10 training texts the model can still make sensible decisions about term identification and classification.",
        "As we would expect, the table also clearly shows that more training data is better, and we have not yet reached a peak in performance."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "HIVIMs are proving their worth for various tasks in information extraction and the results here show that this good performance can be achieved across domains, i.e. in molecular-biology as well as using news paper reports.",
        "The task itself, while being similar to named entity in MUC, is we believe more challenging due to the large number of terms which are not proper nouns, such as those in the source subclasses as well as the large lexical overlap between classes such as PROTEIN and DNA.",
        "A useful line of work in the future would be to find empirical methods for comparing difficulties of domains.",
        "Unlike traditional dictionary-based methods, the method we have shown has the advantage of being portable and no handmade patterns were used.",
        "Additionally, since the character features are quite powerful, yet very general, there is little need for intervention to create domain specific features, although other types of features could be added within the interpolation framework.",
        "Indeed the only thing that is required is a quite small corpus of text containing entities tagged by a domain expert.",
        "Currently we have optimized the A constants by hand but clearly a better way would be to do this automatically.",
        "An obvious strategy to use would be to use some iterative learning method such as Expectation Maximization (Dempster et al., 1977).",
        "The model still has limitations, most obviously when it needs to identify term boundaries for phrases containing potentially ambiguous local structures such as coordination and parentheses.",
        "For such cases we will need to add post-processing rules.",
        "There are of course many NE models that are not based on HMMs that have had success in the NE task at the MUC conferences.",
        "Our main requirement in implementing a model for the domain of molecular-biology has been ease of development, accuracy and portability to other sub-domains since molecular-biology itself is a wide field.",
        "HM1VIs seemed to be the most fiwourable option at this time.",
        "Alternatives that have also had considerable success are decision trees, e.g. (Nobata et al., 1999) and maximum-entropy.",
        "The maximum entropy model shown in (Borthwick et al., 1998) in particular seems a promising approach because of' its ability to handle overlapping and large feature sets within a well founded mathematical framework.",
        "However this implementation of the method seems to incorporate a number of hand-coded domain specific lexical features and dictionary lists that reduce portability.",
        "Undoubtedly we could incorporate richer features into our model and based on the evidence of others we would like to add head nouns as one type of feature in the future."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We would like to express our gratitude to Yuka Tateishi and Tomoko Ohta of the Tsujii laboratory for their efforts to produce the tagged corpus used in these experiments and to Sang-Zoo Lee also of the Tsujii laboratory for his comments regarding HMMs.",
        "We would also like to thank the anonymous referees for their helpful continents."
      ]
    }
  ]
}
