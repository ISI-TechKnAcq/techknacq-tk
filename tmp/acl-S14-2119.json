{
  "info": {
    "authors": [
      "Arodami Chorianopoulou",
      "Georgia Athanasopoulou",
      "Elias Iosif",
      "Ioannis Klasinas",
      "Alexandros Potamianos"
    ],
    "book": "*SEM",
    "id": "acl-S14-2119",
    "title": "tucSage: Grammar Rule Induction for Spoken Dialogue Systems via Probabilistic Candidate Selection",
    "url": "https://aclweb.org/anthology/S14-2119",
    "year": 2014
  },
  "references": [
    "acl-H92-1022",
    "acl-P09-1009",
    "acl-P95-1031",
    "acl-P99-1010",
    "acl-S12-1051",
    "acl-S12-1061",
    "acl-W01-0713",
    "acl-W04-3242"
  ],
  "sections": [
    {
      "text": [
        ", Georgia Athanasopoulou ?",
        ", Elias Iosif ?",
        "?",
        ", Ioannis Klasinas ?",
        ", Alexandros Potamianos ?",
        "?",
        "School of ECE, Technical University of Crete, Chania 73100, Greece ?",
        "School of ECE, National Technical University of Athens, Zografou 15780, Greece ?",
        "?Athena?",
        "Abstract",
        "We describe the grammar induction system for Spoken Dialogue Systems (SDS) submitted to SemEval?14: Task 2.",
        "A statistical model is trained with a rich feature set and used for the selection of candidate rule fragments.",
        "Posterior probabilities produced by the fragment selection model are fused with estimates of phrase-level similarity based on lexical and contextual information.",
        "Domain and language portability are among the advantages of the proposed system that was experimentally validated for three thematically different domains in two languages."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "A critical task for Spoken Dialogue Systems (SDS) is the understanding of the transcribed user input, that utilizes an underlying domain grammar.",
        "An obstacle to the rapid deployment of SDS to new domains and languages is the time-consuming development of grammars that require human expertise.",
        "Machine-assisted grammar induction has been an open research area for decades (K. Lari and S. Young, 1990; S. F. Chen, 1995) aiming to lower this barrier.",
        "Induction algorithms can be broadly distinguished into resource-based, e.g., (A. Ranta, 2004), and data-driven, e.g., (H. Meng and K.-C. Siu, 2002).",
        "The main drawback of the resource-based paradigm is the requirement of pre-existing knowledge bases.",
        "This is addressed by the data-driven paradigm that relies (mostly) on plain corpora.",
        "SDS grammars are built by utilizing low- and high-level rules.",
        "Low-level rules This work is licenced under a Creative Commons Attribution 4.0 International License.",
        "License de-tails: http://creativecommons.org/licenses/ by/4.0/ are similar to gazetteers consisting of terminal en-tries, e.g., list of city names.",
        "High-level rules can be lexicalized as textual fragments (or chunks), which are semantically defined on top of low-level rules, e.g., ?depart from <City>?.",
        "The data-driven induction of low-level rules is a well-researched area enabled by various technologies including web harvesting for corpora creation (Klasinas et al., 2013), term extraction (K. Frantzi and S. Ananiadou, 1997), word-level similarity computation (Pargellis et al., 2004) and clustering (E. Iosif and A. Potamianos, 2007).",
        "High-level rule induction is a less researched area that poses two main challenges: 1) the extraction and selection of salient candidate fragments from a corpus that convey semantics relevant to the domain of interests and 2) the organization of such fragments (e.g., via clustering) according to their semantic similarity.",
        "Despite the recent interest on phrase (J. Mitchell and M. Lapata, 2010) and sentence simi-larity, each respective problem remains open.",
        "Next, our submission 1 for the Se-mEval?14: Task2 is briefly described, which constitutes a data-driven approach for inducing high-level SDS grammar rules.",
        "At the system's core lies a statistical model for the selection of textual fragments based on a rich set of features.",
        "This set includes various lexical features, augmented with statistics from n-gram language models, as well as with heuristic features.",
        "The candidate selection model posterior is fused with a phrase-level semantic similarity metric.",
        "Two different approaches are used for similarity computation relying on the overlap of character bigrams or context-based similarity according to the distributional hypothesis of meaning.",
        "The domain and language portability of the proposed system is demonstrated by its successful application across three different domains and 1 Please note that the last three authors of this submission are among the organizers of this task.",
        "668 two languages.",
        "All the four subtasks defined by the organizers were completed with very good performance that exceeds the baseline.",
        "2 System Description The basic functionality of the proposed system is the mapping (assignment) of unknown textual fragments into known high-level grammar rules.",
        "Let E be the set of unknown fragments, while the set of known rules is denoted byR.",
        "Each unknown fragment f ?E is allowed to be mapped to a single high-level rule r s ?R, where 1?",
        "s?",
        "m and m is the total number of rules in the grammar.",
        "Figure 1: Overview of system architecture.",
        "The system consists of three major components as shown at the system architecture diagram in Fig. 1, specifically: 1) candidate selection: a set of classifiers is built, one for each r s to select whether f ?",
        "E is a candidate member of the specific rule 2 , 2) similarity computation between f and r s , and 3) mapping f to a high-level rule r s (denoted as f 7?",
        "r s ) according to the following model: argmax s {p(r s |f) w S(f, r s )} : f 7?",
        "r s (1) where p(r s |f) stands for the probability of f belonging to rule r s and it is estimated via the respective classifier.",
        "The similarity between f and r s is denoted by S(f |r s ), while w is a fixed weight taking values in the interval [0 ?).",
        "The fusion weight w controls the relative importance of the candidate selection and semantic similarity modules, e.g., for w = 0 only the similarity metric S(f, r s ) is used in the decision.",
        "For example, consider the fragment f ?leaving <City>?.",
        "Also, assume two high-level rules, namely, <ArrCity>={?arrive 2 The requirement for building a classifier for each grammar rule is realistic for the case of SDS, especially for the typical iterative human-in-the-loop grammar development scenario.",
        "at <City>?,...}",
        "and <DepCity>= {?depart <City>?,...}.",
        "According to (1) f is mapped to the <DepCity> rule.",
        "2.1 Candidate Selection In this section, the features used for building the candidate selection module for each r s ?",
        "R are briefly described.",
        "Given a pair (f ,r s ) a two-class statistical classification model that corresponds to r s is used for estimating p(r s |f) in (1).",
        "Definitions.",
        "A high-level rule r s can be considered as a set of fragments, e.g.,?depart <City>?, ?leaving <City>?.",
        "For each fragment there are two types of constituents, namely, lexical (e.g., ?depart?,?leaving?)",
        "and low-level rules (e.g., ?<City>?).",
        "The following features are extracted for r s considering its respective fragments, as well as for f .",
        "Shallow features.",
        "1) the number of constituents (i.e., tokens), 2) the count of lexical constituents to the number of tokens, 3) the count of low-level rules to the number of tokens, 4) the count of lexical constituents that follow the right-most low-level rule of the fragment, and 5) the count of low-level rules that appear twice in a fragment.",
        "Perplexity-based features.",
        "A fragment ?",
        "f can be represented as a sequence of tokens as w 1 w 2 ... w z .",
        "The perplexity of ?",
        "f is defined as PP ( ?",
        "f)=2 H( ?",
        "f) , where H( ?",
        "f)= 1 z log(p( ?",
        "f)).",
        "p( ?",
        "f) stands for the probability of ?",
        "f estimated using an n-gram language model.",
        "Two PP values were used as features computed for n=2, 3.",
        "Features of lexical similarity.",
        "Four scores of lexical similarity computed between f and r s were used as features.",
        "Let N s denote the set of fragments that are included in the training set of each rule r s .",
        "The following metrics were employed for computing the similarity between the unknown fragment f and a fragment f s ?",
        "N s : 1) the normalized longest common subsequence (Stoilos et al., 2005) denoted as S C , 2) the normalized overlap in character bigrams that is denoted as S B and it is defined in (2), 3) a proposed variation of the Levenshtein distance, S L , defined as S L (f, f s ) = l 1 ?L(f,f s ) l 1 +d , where l 1 and l 2 are the lengths (in char-acters) of the lengthiest and the shortest fragment between f and f s , respectively, while d= l 1 ?",
        "l 2 .",
        "L(.)",
        "stands for the Levenshtein distance (V. I. Lev-enshtein, 1966; R. A. Wagner and M. J. Fischer, 1974).",
        "4) if f and f s differ by one token exactly S L is applied, otherwise their similarity is set to 0.",
        "Regarding S C and S B , the similarity between 669 f and r s was estimated as the maximum similarity yielded when computing the similarities between f and each f s ?N s .",
        "For the rest metrics, the similarity between f and r s was estimated by averaging the |N s | similarities computed between f and each f s ?N s .",
        "Heuristic features.",
        "Considering an unknown fragment f and the set of training fragments N s corresponding to rule r s , in total nine features were used: 1) the difference between the average length (in tokens) of fragments in N s and the length of f , 2) the difference between the average number of low-level rules in N s and the number of low-level rules in f , 3) as 2) but considering the lexical constituents instead of low-level rules, 4) the number of low-level rules shared between N s and f , 5) as 4) but considering the lexical constituents instead of low-level rules, 6) a boolean function that equals 1 if f is a substring of at least one f s ?",
        "N s , 7) a boolean function that equals 1 if f shares the same lexical constituents at least one f s ?",
        "N s , 8) a boolean function that equals 1 if f is shorter by one token compared to any f s ?",
        "N s , 9) a boolean function that equals 1 if f is lengthier by one token compared to any f s ?",
        "N s .",
        "Selection.",
        "The aforementioned features are used for building a binary classifier for each r s ?",
        "R, where 1 ?",
        "s ?",
        "m, for deciding whether f can be regarded as a candidate member of r s or not.",
        "Given an unknown fragment f these classifiers are employed for estimating in total m probabilities p(r s |f).",
        "2.2 Similarity Metrics Here, two types of similarity metrics are defined, which are used for estimating S(f, r s ) in (1).",
        "String-based similarity.",
        "Consider two fragments f i and f j whose sets of character bigrams are denoted as M i and M j , respectively.",
        "Also, M min = min(|M i |, |M j |) and M max = max(|M i |, |M j | ).",
        "The similarity between f i and f j is based on the overlap of their respective character bigrams defined as (Jimenez et al., 2012): S B (f i , f j ) = |M i ?M j | ?M max + (1?",
        "?",
        ")M min , (2) where 0???",
        "1, while, here we use ?=0.5.",
        "The similarity between a fragment f and a rule r s is computed by averaging the similarities computed between f and each f s ?N s .",
        "Context-based similarity.",
        "This is a corpus-based metric relying on the distributional hypothesis of meaning suggesting that similarity of context implies similarity of meaning (Z. Harris, 1954).",
        "A contextual window of size 2K+1 words is cen-tered on the fragment of interest f i and lexical features are extracted.",
        "For every instance of f i in the corpus the K words left and right of f i formulate a feature vector v i .",
        "For a given value of K the context-based semantic similarity between two fragments, f i and f j , is computed as the cosine of their feature vectors: S K (f i , f j ) = v i .v j ||v i || ||v j || .",
        "The elements of feature vectors can be weighted according various schemes (E. Iosif and A. Potami-anos, 2010), while, here we use a binary scheme.",
        "The similarity between a fragment f and a rule r s is computed by averaging the similarities computed between f and each f s ?N s .",
        "2.3 Mapping of Unknown Fragments The output of the described system is the mapping of a fragment f to a single (i.e., one-to-one assign-ment) high-level rule r s ?",
        "R, where 1 ?",
        "s ?",
        "m. This is achieved by applying (1).",
        "The p(r s |f) probabilities were estimated as described in Section 2.1.",
        "The S(f, r s ) similarities were estimated using either S K or S B defined in Section 2.2.",
        "3 Datasets and Experiments Datasets.",
        "The data was organized with respect to three different domains: 1) air travel (flight book-ing, car rental etc.",
        "), 2) tourism (information for city guide), and 3) finance (currency exchange).",
        "In total, there are four separate datasets: two datasets for the air travel domain in English (EN) and Greek (GR), one dataset for the tourism domain in English, and one dataset for the finance domain in English.",
        "The number of high-level rules for each dataset Domain #rules #train frag.",
        "#test frag.",
        "Travel:EN 32 982 284 Travel:GR 35 956 324 Tourism:EN 24 1004 285 Finance:EN 9 136 37 Table 1: Number of rules and train/test fragments.",
        "are shown in Table 1, along with the number of fragments included in training and test data.",
        "Experiments.",
        "Regarding the computation of perplexity-based features (defined in Section 2.1) the SRILM toolkit (A. Stolcke, 2002) was used.",
        "The n-gram probabilities were estimated over a corpus that was created by aggregating all the 670 valid fragments included in the training data.",
        "For the computation of the context-based similarity metric S K (defined in Section 2.2) a corpus of web-harvested data was created for each do-main/language.",
        "The context window size K was Domain # sentences Travel:EN 5721 Travel:GR 6359 Tourism:EN 829516 Finance:EN 168380 Table 2: Size of corpora used in S K metric.",
        "set to 1.",
        "The size of the used corpora are presented Table 2, while the process of corpus creation is detailed in (Klasinas et al., 2013).",
        "The classifiers used for the candidate selection module, described in Section 2.1 were random forests with 50 trees (L. Breiman, 2001).",
        "4 Evaluation Metrics and Results The proposed model defined by (1) was evaluated in terms of weighted F-measure, (FM ).",
        "Initially, we run our system using the training and development set provided by the task organizers, in order to tune the w and K parameters.",
        "The tuning was conducted on the Travel English domain, while the respective evaluation results are shown in Table 3 in terms of FM .",
        "We observe that the best re-Weight w 0 1 50 500 FM 0.68 0.72 0.70 0.72 Table 3: Results for the tuning of w. sults are achieved for w = 1 and w = 500.",
        "In the case where w = 0 the rule mapping relies only on the similarity metric.",
        "In addition, we experimented with various values the context window size K of the context-based similarity metric S K : K = 1, 3, 7.",
        "For all values of K similar performance was obtained (0.70).",
        "Given the aforemen-Domains Baseline Run 1 Run 2 Run 3 Travel:EN 0.51 0.66 0.65 0.68 Travel:GR 0.26 0.52 0.49 0.49 Tourism:EN 0.87 0.86 0.85 0.86 Finance:EN 0.60 0.70 0.63 0.58 UA 0.56 0.69 0.66 0.65 WA 0.52 0.66 0.64 0.65 Table 4: Official results.",
        "tioned tuning the following values were selected for the official runs: w = 1, w = 500 and K = 1.",
        "In total, three system runs were submitted: Run 1.",
        "The character bigram similarity metric was used, while w was set to 1.",
        "Run 2.",
        "The context-based similarity metrics was used with K = 1, while w was set to 1.",
        "Run 3.",
        "The character bigram similarity metric was used, while w was set to 500.",
        "The results for the aforementioned runs, along with the baseline performance are shown in Table 4.",
        "An overview of the participating systems suggests that our submission achieved the highest performance for almost all domains and languages.",
        "The weighted (WA) and unweighted (UA) average across the 4 datasets are also presented, where the weight depends on the number of rules in the dataset.",
        "Using these measures, our main run (Run 1) obtained the best results.",
        "We observe that the performance is consistently worse for Runs 2 and 3, with the exception of the Travel English dataset.",
        "Comparing the performance of Runs 1 and 2, we observe that the character bigram metric consistently outperforms the context-based one.",
        "For individual datasets, our system underper-forms for the Finance (in Run 3) and the Tourism domain (in all Runs).",
        "For the case of the Finance domain this may be attributed to the relatively limited training data.",
        "5 Conclusions We proposed a supervised grammar induction system using the fusion of a grammar fragment selection and similarity estimation modules.",
        "The best configuration of our system was Run 1 which achieved the highest performance compared to other submissions, in almost all domains.",
        "To sum-marize, 1) the selection module boost the sys-tem's performance significanlty, 2) the high performance in different domains is a promising indicator for domain and language portability.",
        "Future work should involve the implementation of more complex features for the candidate selection algorithm and further investigation of phrase level similarity metrics.",
        "Acknowledgements This work has been partially funded by the projects: 1) SpeDial, and 2) PortDial, supported by the EU Seventh Framework Programme (FP7), with grant number 611396 and 296170, respectively.",
        "671 References"
      ]
    }
  ]
}
