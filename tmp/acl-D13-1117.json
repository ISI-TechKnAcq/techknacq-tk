{
  "info": {
    "authors": [
      "Sida Wang",
      "Mengqiu Wang",
      "Stefan Wager",
      "Percy Liang",
      "Christopher D. Manning"
    ],
    "book": "EMNLP",
    "id": "acl-D13-1117",
    "title": "Feature Noising for Log-Linear Structured Prediction",
    "url": "https://aclweb.org/anthology/D13-1117",
    "year": 2013
  },
  "references": [
    "acl-P05-1003",
    "acl-P05-1045",
    "acl-P06-1027",
    "acl-W03-0419"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1170?1179, Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational Linguistics Feature Noising for Log-linear Structured Prediction Sida I.",
        "Wang?, Mengqiu Wang?, Stefan Wager?,"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "NLP models have many and sparse features, and regularization is key for balancing model overfitting versus underfitting.",
        "A recently re-popularized form of regularization is to generate fake training data by repeatedly adding noise to real data.",
        "We reinterpret this noising as an explicit regularizer, and approximate it with a second-order formula that can be used during training without actually generating fake data.",
        "We show how to apply this method to structured prediction using multinomial logistic regression and linear-chain CRFs.",
        "We tackle the key challenge of developing a dynamic program to compute the gradient of the regularizer efficiently.",
        "The regularizer is a sum over inputs, so we can estimate it more accurately via a semi-supervised or transduc-tive extension.",
        "Applied to text classification and NER, our method provides a >1% absolute performance gain over use of standard L2 regularization."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "NLP models often have millions of mainly sparsely attested features.",
        "As a result, balancing overfitting versus underfitting through good weight regularization remains a key issue for achieving optimal performance.",
        "Traditionally, L2 or L1 regularization is employed, but these simple types of regularization penalize all features in a uniform way without taking into account the properties of the actual model.",
        "An alternative approach to regularization is to generate fake training data by adding random noise to the input features of the original training data.",
        "Intuitively, this can be thought of as simulating miss-?Both authors contributed equally to the papering features, whether due to typos or use of a previously unseen synonym.",
        "The effectiveness of this technique is well-known in machine learning (Abu-Mostafa, 1990; Burges and Scho?lkopf, 1997; Simard et al., 2000; Rifai et al., 2011a; van der Maaten et al., 2013), but working directly with many corrupted copies of a dataset can be computationally prohibitive.",
        "Fortunately, feature noising ideas often lead to tractable deterministic objectives that can be optimized directly.",
        "Sometimes, training with corrupted features reduces to a special form of regularization (Matsuoka, 1992; Bishop, 1995; Rifai et al., 2011b; Wager et al., 2013).",
        "For example, Bishop (1995) showed that training with features that have been corrupted with additive Gaussian noise is equivalent to a form of L2 regularization in the low noise limit.",
        "In other cases it is possible to develop a new objective function by marginalizing over the artificial noise (Wang and Manning, 2013; van der Maaten et al., 2013).",
        "The central contribution of this paper is to show how to efficiently simulate training with artificially noised features in the context of log-linear structured prediction, without actually having to generate noised data.",
        "We focus on dropout noise (Hinton et al., 2012), a recently popularized form of artificial feature noise where a random subset of features is omitted independently for each training example.",
        "Dropout and its variants have been shown to outperform L2 regularization on various tasks (Hinton et al., 2012; Wang and Manning, 2013; Wan et al., 2013).",
        "Dropout is is similar in spirit to feature bagging in the deliberate removal of features, but performs the removal in a preset way rather than randomly (Bryll et al., 2003; Sutton et al., 2005; Smith et al., 2005).",
        "Our approach is based on a second-order approximation to feature noising developed among others by Bishop (1995) and Wager et al. (2013), which allows us to convert dropout noise into a form of adaptive regularization.",
        "This method is suitable for structured prediction in log-linear models where second derivatives are computable.",
        "In particular, it can be used for multiclass classification with maximum entropy models (a.k.a., softmax or multinomial logistic regression) and for the sequence models that are ubiquitous in NLP, via linear chain Conditional Random Fields (CRFs).",
        "For linear chain CRFs, we additionally show how we can use a noising scheme that takes advantage of the clique structure so that the resulting noising regularizer can be computed in terms of the pairwise marginals.",
        "A simple forward-backward-type dynamic program can then be used to compute the gradient tractably.",
        "For ease of implementation and scalability to semi-supervised learning, we also outline an even faster approximation to the regularizer.",
        "The general approach also works in other clique structures in addition to the linear chain when the clique marginals can be computed efficiently.",
        "Finally, we extend feature noising for structured prediction to a transductive or semi-supervised setting.",
        "The regularizer induced by feature noising is label-independent for log-linear models, and so we can use unlabeled data to learn a better regularizer.",
        "NLP sequence labeling tasks are especially well suited to a semi-supervised approach, as input features are numerous but sparse, and labeled data is expensive to obtain but unlabeled data is abundant (Li and McCallum, 2005; Jiao et al., 2006).",
        "Wager et al. (2013) showed that semi-supervised dropout training for logistic regression captures a similar intuition to techniques such as entropy regularization (Grandvalet and Bengio, 2005) and transductive SVMs (Joachims, 1999), which encourage confident predictions on the unlabeled data.",
        "Semi-supervised dropout has the advantage of only using the predicted label probabilities on the unlabeled data to modulate an L2 regularizer, rather than requiring more heavy-handed modeling of the unlabeled data as in entropy regularization or expectation regularization (Mann and McCallum, 2007).",
        "In experimental results, we show that simulated feature noising gives more than a 1% absolute boost yt yt+1yt?1f (yt, xt )f (yt?1, yt ) f (yt, yt+1)yt yt+1yt?1f (yt, xt )f (yt?1, yt ) f (yt, yt+1)",
        "in linear-chain CRFs with only transition features and node features.",
        "The green squares are node features f(yt, xt), and the orange squares are edge features f(yt?1, yt).",
        "Conceptually, given a training example, we sample some features to ignore (generate fake data) and make a parameter update.",
        "Our goal is to train with a roughly equivalent objective, without actually sampling.",
        "in performance over L2 regularization, on both text classification and an NER sequence labeling task."
      ]
    },
    {
      "heading": "2 Feature Noising Log-linear Models",
      "text": [
        "Consider the standard structured prediction problem of mapping some input x ?",
        "X (e.g., a sentence) to an output y ?",
        "Y (e.g., a tag sequence).",
        "Let f(y, x) ?",
        "Rd be the feature vector, ?",
        "?",
        "Rd be the weight vector, and s = (s1, .",
        ".",
        ".",
        ", s|Y|) be a vector of scores for each output, with sy = f(y, x) ?",
        "?.",
        "Now define a log-linear model:",
        "function.",
        "Given an example (x,y), parameter estimation corresponds to choosing ?",
        "to maximize p(y | x; ?).",
        "The key idea behind feature noising is to artificially corrupt the feature vector f(y, x) randomly",
        "into some f?",
        "(y, x) and then maximize the average log-likelihood of y given these corrupted features?",
        "the motivation is to choose predictors ?",
        "that are robust to noise (missing words for example).",
        "Let s?, p?",
        "(y |x; ?)",
        "be the randomly perturbed versions corresponding to f?",
        "(y, x).",
        "We will also assume the feature noising preserves the mean: E[f?",
        "(y, x)] = f(y, x), so that E[s?]",
        "= s. This can always be done by scaling the noised features as described in the list of noising schemes.",
        "It is useful to view feature noising as a form of regularization.",
        "Since feature noising preserves the mean, the feature noising objective can be written as the sum of the original log-likelihood plus the difference in log-normalization constants:",
        "Since A(?)",
        "is convex, R(?, x) is always positive by Jensen's inequality and can therefore be interpreted as a regularizer.",
        "Note that R(?, x) is in general non-convex.",
        "Computing the regularizer (4) requires summing over all possible noised feature vectors, which can imply exponential effort in the number of features.",
        "This is intractable even for flat classification.",
        "Following Bishop (1995) and Wager et al. (2013), we approximate R(?, x) using a second-order Taylor expansion, which will allow us to work with only means and covariances of the noised features.",
        "We take a quadratic approximation of the log-partition function A(?)",
        "of the noised score vector s?",
        "around the the unnoised score vector s:",
        "(s??",
        "s)>?2A(s)(s??",
        "s).",
        "Plugging (5) into (4), we obtain a new regularizer Rq(?, x), which we will use as an approximation to",
        "This expression still has two sources of potential intractability, a sum over an exponential number of noised score vectors s?",
        "and a sum over the |Y |components of s?.",
        "Multiclass classification If we assume that the components of s?",
        "are independent, then Cov(s?)",
        "?",
        "R|Y|?|Y |is diagonal, and we have",
        "ability, the variance ?y(1?",
        "?y) measures model uncertainty, and",
        "measures the uncertainty caused by feature noising.",
        "The regularizerRq(?, x) involves the product of two variance terms, the first is non-convex in ?",
        "and the second is quadratic in ?.",
        "Note that to reduce the regularization, we will favor models that (i) predict confidently and (ii) have stable scores in the presence of feature noise.",
        "For multiclass classification, we can explicitly sum over each y ?",
        "Y to compute the regularizer, but this will be intractable for structured prediction.",
        "To specialize to multiclass classification for the moment, let us assume that we have a separate weight vector for each output y applied to the same feature vector g(x); that is, the score sy = ?y ?",
        "g(x).",
        "Further, assume that the components of the noised feature vector g?",
        "(x) are independent.",
        "Then we can simplify (9) to the following:",
        "Noising schemes We now give some examples of possible noise schemes for generating f?",
        "(y, x) given the original features f(y, x).",
        "This distribution affects the regularization through the variance term",
        "1Here, we are using the fact that first and second derivatives of the log-partition function are the mean and variance.",
        "In this case, the contribution to the regularizer from noising is Var[s?y] =",
        "f?",
        "(y, x) = f(y, x) z, where takes the el-ementwise product of two vectors.",
        "Here, z is a vector with independent components which has zi = 0 with probability ?, zi = 11??",
        "with probability 1 ?",
        "?.",
        "In this case, Var[s?y] =",
        "order approximation Rq(?, x), the multiplicative Gaussian and dropout schemes are equivalent, but they differ under the original regularizer R(?, x)."
      ]
    },
    {
      "heading": "2.1 Semi-supervised learning",
      "text": [
        "A key observation (Wager et al., 2013) is that the noising regularizer R (8), while involving a sum over examples, is independent of the output y.",
        "This suggests estimating R using unlabeled data.",
        "Specifically, if we have n labeled examples D = {x1, x2, .",
        ".",
        ".",
        ", xn} and m unlabeled examples Dunlabeled = {u1, u2, .",
        ".",
        ".",
        ", un}, then we can define a regularizer that is a linear combination the regularizer estimated on both datasets, with ?",
        "tuning the tradeoff between the two:"
      ]
    },
    {
      "heading": "3 Feature Noising in Linear-Chain CRFs",
      "text": [
        "So far, we have developed a regularizer that works for all log-linear models, but?in its current form?",
        "is only practical for multiclass classification.",
        "We now exploit the decomposable structure in CRFs to define a new noising scheme which does not require us to explicitly sum over all possible outputs y ?",
        "Y .",
        "The key idea will be to noise each local feature vector (which implicitly affects many y) rather than noise each y independently.",
        "Assume that the output y = (y1, .",
        ".",
        ".",
        ", yT ) is a sequence of T tags.",
        "In linear chain CRFs, the feature vector f decomposes into a sum of local feature vec",
        "where gt(a, b, x) is defined on a pair of consecutive tags a, b for positions t?",
        "1 and t. Rather than working with a score sy for each y ?",
        "Y , we define a collection of local scores s = {sa,b,t}, for each tag pair (a, b) and position t = 1, .",
        ".",
        ".",
        ", T .",
        "We consider noising schemes which independently set g?t(a, b, x) for each a, b, t. Let s?",
        "= {s?a,b,t} be the corresponding collection of noised scores.",
        "We can write the log-partition function of these local scores as follows:",
        "The first derivative yields the edge marginals under the model, ?a,b,t = p?",
        "(yt?1 = a, yt = b |x), and the diagonal elements of the Hessian ?2A(s) yield the marginal variances.",
        "Now, following (7) and (8), we obtain the follow",
        "where ?a,b,t(1?",
        "?a,b,t) measures model uncertainty about edge marginals, and Var[s?a,b,t] is simply the uncertainty due to noising.",
        "Again, minimizing the regularizer means making confident predictions and having stable scores under feature noise.",
        "Computing partial derivatives So far, we have defined the regularizer Rq(?, x) based on feature noising.",
        "In order to minimize Rq(?, x), we need to take its derivative.",
        "First, note that log?a,b,t is the difference of a restricted log-partition function and the log-partition function.",
        "So again by properties of its first derivative, we have:",
        "Using the fact that ?",
        "?a,b,t = ?a,b,t?",
        "log?a,b,t and the fact that Var[s?a,b,t] is a quadratic function in ?, we can simply apply the product rule to derive the final gradient?Rq(?, x)."
      ]
    },
    {
      "heading": "3.1 A Dynamic Program for the Conditional Expectation",
      "text": [
        "A naive computation of the gradient ?Rq(?, x) requires a full forward-backward pass to compute Ep?",
        "(y|yt?1=a,yt=b,x)[f(y, x)] for each tag pair (a, b) and position t, resulting in a O(K4T 2) time algorithm.",
        "In this section, we reduce the running time to O(K2T ) using a more intricate dynamic program.",
        "By the Markov property of the CRF, y1:t?2 only depends on (yt?1, yt) through yt?1 and yt+1:T only depends on (yt?1, yt) through yt.",
        "First, it will be convenient to define the partial sum of the local feature vector from positions i to j as follows:",
        "Consider the task of computing the feature expectation Ep?",
        "(y|yt?1=a,yt=b)[f(y, x)] for a fixed (a, b, t).",
        "We can expand this quantity into",
        "are the expected feature vectors summed over the prefix and suffix of the tag sequence, respectively.",
        "Note that F at and B b t are analogous to the forward and backward messages of standard CRF inference, with the exception that they are vectors rather than scalars.",
        "We can compute these messages recursively in the standard way.",
        "The forward recurrence is",
        "and a similar recurrence holds for the backward messages Bbt .",
        "Running the resulting dynamic program takes O(K2Tq) time and requires O(KTq) storage, where K is the number of tags, T is the sequence length and q is the number of active features.",
        "Note that this is the same order of dependence as normal CRF training, but there is an additional dependence on the number of active features q, which makes training slower."
      ]
    },
    {
      "heading": "4 Fast Gradient Computations",
      "text": [
        "In this section, we provide two ways to further improve the efficiency of the gradient calculation based on ignoring long-range interactions and based on exploiting feature sparsity."
      ]
    },
    {
      "heading": "4.1 Exploiting Feature Sparsity and Co-occurrence",
      "text": [
        "In each forward-backward pass over a training example, we need to compute the conditional expectations for all features active in that example.",
        "Naively applying the dynamic program in Section 3 is O(K2T ) for each active feature.",
        "The total complexity has to factor in the number of active features, q.",
        "Although q only scales linearly with sentence length, in practice this number could get large pretty quickly.",
        "For example, in the NER tagging experiments (cf.",
        "Section 5), the average number of active features per token is about 20, which means q ' 20T ; this term quickly dominates the computational costs.",
        "Fortunately, in sequence tagging and other NLP tasks, the majority of features are sparse and they often co-occur.",
        "That is, some of the active features would fire and only fire at the same locations in a given sequence.",
        "This happens when a particular token triggers multiple rare features.",
        "We observe that all indicator features that only fired once at position t have the same conditional expectations (and model expectations).",
        "As a result, we can collapse such a group of features into a single",
        "feature as a preprocessing step to avoid computing identical expectations for each of the features.",
        "Doing so on the same NER tagging experiments cuts down q/T from 20 to less than 5, and gives us a 4 times speed up at no loss of accuracy.",
        "The exact same trick is applicable to the general CRF gradient computation as well and gives similar speedup."
      ]
    },
    {
      "heading": "4.2 Short-range interactions",
      "text": [
        "It is also possible to speed up the method by resorting to approximate gradients.",
        "In our case, the dynamic program from Section 3 together with the trick described above ran in a manageable amount of time.",
        "The techniques developed here, however, could prove to be useful on larger tasks.",
        "Let us rewrite the quantity we want to compute slightly differently (again, for all a, b, t):",
        "The intuition is that conditioned on yt?1, yt, the terms gi(yi?1, yi, x) where i is far from t will be close to Ep?",
        "(y|x)[gi(yi?1, yi, x)].",
        "This motivates replacing the former with the latter whenever |i?",
        "k |?",
        "r where r is some window size.",
        "This approximation results in an expression which only has to consider the sum of the local feature vectors from i?r to i+r, which is captured byGi?r:i+r:",
        "?",
        "Ep?(y|x)[Gt?r:t+r].",
        "We can further approximate this last expression by",
        "The second expectation can be computed from the edge marginals.",
        "The accuracy of this approximation hinges on the lack of long range dependencies.",
        "Equation (21) shows the case of r = 0; this takes almost no additional effort to compute.",
        "However, for some of our experiments, we observed a 20% difference with the real derivative.",
        "For r > 0, the computational savings are more limited, but the bounded-window method is easier to implement.",
        "Dataset q d K Ntrain Ntest",
        "of non-zero features per example, d: total number of features, K: number of classes to predict, Ntrain: number of training examples, Ntest: number of test examples."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "We show experimental results on the CoNLL-2003 Named Entity Recognition (NER) task, the SANCL Part-of-speech (POS) tagging task, and several document classification tasks.",
        "The datasets used are described in Table 1.",
        "We used standard splits whenever available; otherwise we split the data at random into a test set and a train set of equal sizes (RCV14, TDT2).",
        "CoNLL has a development set of size 51578, which we used to tune regularization parameters.",
        "The SANCL test set is divided into 3 genres, namely answers, newsgroups, and reviews, each of which has a corresponding development set.3"
      ]
    },
    {
      "heading": "5.1 Multiclass Classification",
      "text": [
        "We begin by testing our regularizer in the simple case of classification where Y = {1, 2, .",
        ".",
        ".",
        ",K} for K classes.",
        "We examine the performance of the noising regularizer in both the fully supervised setting as well as the transductive learning setting.",
        "In the transductive learning setting, the learner is allowed to inspect the test features at train time (without the labels).",
        "We used the method described in Section 2.1 for transductive dropout.",
        "tive learning results on some standard datasets.",
        "None: use no regularization, Drop: quadratic approximation to the dropout noise (8), +Test: also use the test set to estimate the noising regularizer (11)."
      ]
    },
    {
      "heading": "Noising",
      "text": [
        "In the transductive setting, we used test data (without labels) to learn a better regularizer.",
        "As an alternative, we could also use unlabeled data in place of the test data to accomplish a similar goal; this leads to a semi-supervised setting.",
        "To test the semi-supervised idea, we use the same datasets as above.",
        "We split each dataset evenly into 3 thirds that we use as a training set, a test set and an unlabeled dataset.",
        "Results are given in Table 3.",
        "In most cases, our semi-supervised accuracies are lower than the transductive accuracies given in Table 2; this is normal in our setup, because we used less labeled data to train the semi-supervised classifier than the transductive one.4",
        "The results reported above all rely on the approximate dropout regularizer (8) that is based on a second-order Taylor expansion.",
        "To test the validity of this approximation we compare it to the Gaussian method developed by Wang and Manning (2013) on a two-class classification task.",
        "We use the 20-newsgroups alt.atheism vs soc.religion.christian classification task; results are shown in Figure 2.",
        "There are 1427 exam",
        "supervised results are better than the transductive ones.",
        "The reason for this is that the original CoNLL test set came from a different distributions than the training set, and this made the task more difficult.",
        "Meanwhile, in our semi-supervised experiment, the test and train sets are drawn from the same distribution and so our semi-supervised task is actually easier than the original one.",
        "mance.",
        "Plotted is the test set accuracy with logistic regression as a function of ?",
        "for the L2 regularizer, Gaussian dropout (Wang and Manning, 2013) + additional L2, and quadratic dropout (8) + L2 described in this paper.",
        "The default noising regularizer is quite good, and additional L2 does not help.",
        "Notice that no choice of ?",
        "in L2 can help us combat overfitting as effectively as (8) without underfitting.",
        "ples with 22178 features, split evenly and randomly into a training set and a test set.",
        "Over a broad range of ?",
        "values, we find that dropout plus L2 regularization performs far better than using just L2 regularization for any value of ?.",
        "We see that Gaussian dropout appears to perform slightly better than the quadratic approximation discussed in this paper.",
        "However, our quadratic approximation extends easily to the multiclass case and to structured prediction in general, while Gaussian dropout does not.",
        "Thus, it appears that our approximation presents a reasonable trade-off between",
        "computational efficiency and prediction accuracy."
      ]
    },
    {
      "heading": "5.2 CRF Experiments",
      "text": [
        "We evaluate the quadratic dropout regularizer in linear-chain CRFs on two sequence tagging tasks: the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and the SANCL 2012 POS tagging task (Petrov and McDonald, 2012) .",
        "The standard CoNLL-2003 English shared task benchmark dataset (Tjong Kim Sang and De Meulder, 2003) is a collection of documents from Reuters newswire articles, annotated with four entity types: Person, Location, Organization, and Miscellaneous.",
        "We predicted the label sequence Y = {LOC, MISC, ORG, PER, O}T without considering the BIO tags.",
        "For training the CRF model, we used a comprehensive set of features from Finkel et al. (2005) that gives state-of-the-art results on this task.",
        "A total number of 437906 features were generated on the CoNLL-2003 training dataset.",
        "The most important",
        "features are: ?",
        "The word, word shape, and letter n-grams (up to 6gram) at current position ?",
        "The prediction, word, and word shape of the previous and next position ?",
        "Previous word shape in conjunction with current word shape ?",
        "Disjunctive word set of the previous and next 4 positions ?",
        "Capitalization pattern in a 3 word window ?",
        "Previous two words in conjunction with the word",
        "shape of the previous word ?",
        "The current word matched against a list of name titles (e.g., Mr., Mrs.) The F?=1 results are summarized in Table 4.",
        "We obtain a 1.6% and 1.1% absolute gain on the test and dev set, respectively.",
        "Detailed results are broken down by precision and recall for each tag and are shown in Table 6.",
        "These improvements are significant at the 0.1% level according to the paired bootstrap resampling method of 2000 iterations (Efron and Tibshirani, 1993).",
        "For the SANCL (Petrov and McDonald, 2012) POS tagging task, we used the same CRF framework with a much simpler set of features",
        "We obtained a small but consistent improvement using the quadratic dropout regularizer in (14) over the L2-regularized CRFs baseline.",
        "Although the difference on SANCL is small, the performance differences on the test sets of reviews and newsgroups are statistically significant at the 0.1% level.",
        "This is also interesting because here is a situation where the features are extremely sparse, L2 regularization gave no improvement, and where regularization overall matters less."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We have presented a new regularizer for learning log-linear models such as multiclass logistic regression and conditional random fields.",
        "This regularizer is based on a second-order approximation of feature noising schemes, and attempts to favor models that predict confidently and are robust to noise in the data.",
        "In order to apply our method to CRFs, we tackle the key challenge of dealing with feature correlations that arise in the structured prediction setting in several ways.",
        "In addition, we show that the regularizer can be applied naturally in the semi-supervised setting.",
        "Finally, we applied our method to a range of different datasets and demonstrate consistent gains over standard L2 regularization.",
        "Inves",
        "tigating how to better optimize this non-convex regularizer online and convincingly scale it to the semi-supervised setting seem to be promising future directions."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "The authors would like to thank the anonymous reviewers for their comments.",
        "We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) Broad Operational Language Translation (BOLT) program through IBM.",
        "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the view of the DARPA, or the US government.",
        "S. Wager is supported by a BC and EJ Eaves SGF Fellowship."
      ]
    }
  ]
}
