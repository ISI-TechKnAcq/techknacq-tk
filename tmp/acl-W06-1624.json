{
  "info": {
    "authors": [
      "Weilin Wu",
      "Ruzhan Lu",
      "Jianyong Duan",
      "Hui Liu",
      "Feng Gao",
      "Yu-Quan Chen"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W06-1624",
    "title": "A Weakly Supervised Learning Approach for Spoken Language Understanding",
    "url": "https://aclweb.org/anthology/W06-1624",
    "year": 2006
  },
  "references": [
    "acl-H94-1039",
    "acl-J92-1004",
    "acl-P02-1016",
    "acl-P02-1046",
    "acl-P93-1008",
    "acl-P94-1004",
    "acl-P94-1013",
    "acl-W02-1028",
    "acl-W95-0104",
    "acl-W99-0613"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we present a weakly supervised learning approach for spoken language understanding in domain-specific dialogue systems.",
        "We model the task of spoken language understanding as a successive classification problem.",
        "The first classifier (topic classifier) is used to identify the topic of an input utterance.",
        "With the restriction of the recognized target topic, the second classifier (semantic classifier) is trained to extract the corresponding slot-value pairs.",
        "It is mainly data-driven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for spoken language.",
        "Most importantly, it allows the employment of weakly supervised strategies for training the two classifiers.",
        "We first apply the training strategy of combining active learning and self-training (Tur et al., 2005) for topic classifier.",
        "Also, we propose a practical method for bootstrapping the topic-dependent semantic classifiers from a small amount of labeled sentences.",
        "Experiments have been conducted in the context of Chinese public transportation information inquiry domain.",
        "The experimental results demonstrate the effectiveness of our proposed SLU framework and show the possibility to reduce human labeling efforts significantly."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Spoken Language Understanding (SLU) is one of the key components in spoken dialogue systems.",
        "Its task is to identify the user’s goal and extract from the input utterance the information needed to complete the query.",
        "Traditionally, there are mainly two mainstreams in the SLU researches: knowledge-based approaches, which are based on robust parsing or template matching techniques (Sneff, 1992; Dowding et al., 1993; Ward and Issar, 1994); and data-driven approaches, which are generally based on stochastic models (Pieraccini and Levin, 1993; Miller et al., 1995).",
        "Both approaches have their drawbacks, however.",
        "The former approach is cost-expensive to develop since its grammar development is time-consuming, laboursome and requires linguistic skills.",
        "It is also strictly domain-dependent and hence difficult to be adapted to new domains.",
        "On the other hand, although addressing such drawbacks associated with knowledge-based approaches, the latter approach often suffers the data sparseness problem and hence needs a fully annotated corpus in order to reliably estimate an accurate model.",
        "More recently, some new variation methods are proposed through certain trade-offs, such as the semi-automatically grammar learning approach (Wang and Acero, 2001) and Hidden Vector State (HVS) model (He and Young, 2005).",
        "The two methods require only minimally annotated data (only the semantic frames are annotated).",
        "This paper proposes a novel weakly supervised spoken language understanding approach.",
        "Our SLU framework mainly includes two successive classifiers: topic classifier and semantic classifier.",
        "The main advantage of the proposed approach is that it is mainly data-driven and requires only minimally annotated corpus for training whilst retaining the understanding robustness and deepness for spoken language.",
        "In particular, the two classifiers are trained using weakly supervised strategies: the former one is trained through the combination of active learning and self-training (Tur et al., 2005), and the latter one",
        "is trained using a practical bootstrapping technique."
      ]
    },
    {
      "heading": "2 The System Architecture",
      "text": [
        "The semantic representation of an application domain is usually defined in terms of the semantic frame, which contains a frame type representing the topic of the input sentence, and some slots representing the constraints the query goal has to satisfy.",
        "Then, the goal of the SLU system is to translate an input utterance into a semantic frame.",
        "Besides the two key components, i.e., topic classifier and semantic classifier, our system also contains a preprocessor and a slot-value merger.",
        "Figure 1 illustrates the overall system architecture.",
        "It also describes the whole SLU procedure using an example sentence."
      ]
    },
    {
      "heading": "2.1 The Preprocessor",
      "text": [
        "Usually, the preprocessor is to look for the substrings in a sentence that correspond to a semantic class or matching a regular expression and to replace them with the class label, e.g., “Huashan Road” and “1954” are replaced with two class labels [road_name] and [number] respectively.",
        "In our system, the preprocessor can recognize more complex word sequences, e.g., “1954 Huashan Road” can be recognized as [address] through matching a rule like “[address] 4 [number] [road_name]”.",
        "The preprocessor is implemented with a local chart parser, which is a variation of the robust parser introduced in (Wang, 1999).",
        "The robust local parser can skip noise words in the sentence, which ensures that the system has the low level robustness.",
        "For example, “1954 of the Huashan Road)” can also be recognized as",
        "[address] by skipping the words “of the”.",
        "However, the robust local parser possibly skips the words in the sentence by mistake and produces an incorrect class label.",
        "To avoid this side-effect, this local parser exploits an embedded decision tree for pruning, of which the details can be seen in (Wu et al., 2005).",
        "According to our experience, it is fairly easy for a general developer with good understanding of the application to author the small grammar used by the local chart parser and annotate the training cases for the embedded decision tree.",
        "The work can be finished in several hours."
      ]
    },
    {
      "heading": "2.2 Topic Classification",
      "text": [
        "Given the representation of semantic frame, topic classification can be regarded as identifying the frame type.",
        "It is suited to be dealt using pattern recognition techniques.",
        "The application of statistical pattern techniques to topic classification can improve the robustness of the whole understanding system.",
        "Also, in our system, topic classification can greatly reduce the search space and hence improve the performance of subsequent semantic classification.",
        "For example, the total number of slots into which the concept [location] can be filled in all topics is 33 and the corresponding maximum number of slots in a single topic is decreased to 10.",
        "Many statistical pattern recognition techniques have been applied to similar tasks, such as Naïve Bayes, N-Gram and Support Vector Machines (SVMs) (Wang et al., 2002).",
        "According to the literature (Wang et al., 2002) and our experiments, the SVMs showed the best performance among many other statistical classifiers.",
        "Also, it has been showed that active learning can be effectively applied to the SVMs (Schohn and Cohn, 2000; Tong and Koller, 2000).",
        "Therefore, we choose the SVMs as the topic classifier.",
        "We resorted to the LIBSVM toolkit (Chang and Lin, 2001) to construct the SVMs for our experiments.",
        "Following the practice in (Wang et al., 2002), the SVMs use a binary valued features vector.",
        "If the simplest feature (Chinese character) is used, each query is converted into a feature vector J J K J J K ch =< ch1 , ... , ch – > ( |ch |is the total number of Chinese characters occur in the corpus) with binary valued elements: 1 if a given Chinese character is in this input sentence or 0 otherwise.",
        "Due to the existence of the preprocessor, we can also include semantic class labels (e.g., [location]) as features for topic classification.",
        "Intuitively, the class label features are more informative than the Please tell me how can I FRAME: ShowRoute go from the people's SLOTS: [routeJ.",
        "[originJ = the people's square square to the bund by bus [routeJ.",
        "[destinationJ = the bund",
        "Chinese character features.",
        "At the same time, including class labels as features can also relieve the data sparseness problem."
      ]
    },
    {
      "heading": "2.3 Topic-dependent Semantic Classification",
      "text": [
        "The job of semantic classification is to assign the concepts with the most likely slots.",
        "It can also be modeled as a classification problem since the number of possible slot names for each concept is limited.",
        "Let’s consider the example sentence in Figure 1.",
        "After the preprocessing and topic classification, we get the preprocessed result “Please tell me how can I go from [location]1 to [location]2 by [bus]?” and the topic ShowRoute.",
        "We have to work out which slots are to be filled with the values such as [location]2.",
        "The first clue is the surrounding literal context.",
        "Intuitively, we can infer that it is a [destination] since a [destination] indicator “to” is before it.",
        "If [location]1 has already been recognized as a [origin], it is another clue to imply that [location]2 is a [destina tion].",
        "Since initially the slot context is not available, the slot context is only employed for the semantic re-classification, which will be described in latter section.",
        "To learn the topic-dependent semantic classifiers, the training sentences need to be annotated against the semantic frame.",
        "Our annotating scenario is relatively simple and can be performed by general developers.",
        "For example, for the sentence “Please tell me how can I go from the people’s square to the bund by bus?”, the annotated results are like the following: FRAME: ShowRoute Slots: [route].",
        "[origin].",
        "[location].",
        "( the people’s square) [route].",
        "[destination].",
        "[location].",
        "(the bund) [route].[transport_type].[by_bus].",
        "(bus) The corresponding slot names can be automatically extracted from the domain model.",
        "A domain model is usually a hierarchical structure of the relevant concepts in the application domain.",
        "For every occurrence of a concept in the domain model graph, we list all the concept names along the path from the root to its occurrence position and regard their concatenation as a slot name.",
        "Thus, the slot name is not flat since it inherits the hierarchy from the domain model.",
        "With provision of the annotated data, we can collect all the literal and slot context features related to each concept.",
        "The examples of features for the concept [location] are illustrated as follows:",
        "(1) to within the –3 windows (2) from _ to (3) ShowRoute.",
        "[route].",
        "[origin] within the ±2 windows",
        "The former two are literal context features.",
        "Feature (1) is a context-word that tends to indicate ShowRoute.",
        "[route].",
        "[destination].",
        "Feature (2) is a collocation that checks for the pattern “from” and “to” immediately before and after the concept [location] respectively, and tends to indicate ShowRoute.",
        "[route].",
        "[origin].",
        "The third one is a slot context feature, which tends to imply the target concept [location] is of type ShowRoute.",
        "[route].",
        "[destination].",
        "In nature, these features are equivalent to the rules in the semantic grammar used by the robust rule-based parser.",
        "For example, the feature (2) has the same function as the semantic rule “[origin] 4 from [location] to”.",
        "The advantage of our approach is that we can automatically learn the semantic “rules” from the training data rather than manually authoring them.",
        "Also, the learned “rules” are intrinsically robust since they may involves gaps, for example, feature (1) allows skipping some noise words between “to” and [location].",
        "The next problem is how to apply these features when predicting a new case since the active features for a new case may make opposite predictions.",
        "One simple and effective strategy is employed by the decision list (Rivest, 1987), i.e., always applying the strongest features.",
        "In a decision list, all the features are sorted in order of descending confidence.",
        "When a new target concept is classified, the classifier runs down the list and compares the features against the contexts of the target concept.",
        "The first matched feature is applied to make a predication.",
        "Obviously, how to measure the confidence of features is a very important issue for the decision list.",
        "We use the metric described in (Yarowsky, 1994; Golding, 1995).",
        "Provided that P(s1 |f) > 0 for all i : confidence(f) = max P(si |f) (1) i This value measures the extent to which the context is unambiguously correlated with one particular slot si ."
      ]
    },
    {
      "heading": "2.4 Slot-value merging and semantic reclassification",
      "text": [
        "The slot-value merger is to combine the slots assigned to the concepts in an input sentence.",
        "Another simultaneous task of the slot-value merger is to check the consistency among the identified slot-values.",
        "Since the topic-dependent classifiers corresponding to different concepts",
        "are training and running independently, it possibly results in inconsistent predictions.",
        "Considering the preprocessed word sequence “Please tell me how can I go from [location]1 to [location]2 by [bus]” , they are semantically clashed if [location]1 and [location]2 are both classified as ShowRoute.",
        "[route].",
        "[origin].",
        "To relieve this problem, we can use the semantic classifier based on the slot context feature.",
        "We apply the context features like, for example, “ShowRoute.",
        "[route].",
        "[origin] within the ±k windows”, which tends to imply ShowRoute.",
        "[route].",
        "[destination].",
        "The literal contexts reflect the local lexical semantic dependency.",
        "The slot contexts, however, are good at capturing the long distance dependency.",
        "Therefore, when the slot-value merger finds that two or more slot-value pairs clash, it first anchors the one with the highest confidence.",
        "Then, it extracts the slot contexts for the other concepts and passes them to the semantic classification module for reclassification.",
        "If the re classification results still clash, the dialog system can involve the user in an interactive dialog for clarity.",
        "The idea of semantic classification and reclassification can be understood as follows: it first finds the concept or slot islands (like partial parsing) and then links them together.",
        "This mechanism is well-suited for SLU since the spoken utterance usually consists of several phrases and noises (restart, repeats and filled pauses, etc) are most often between them (Ward and Issar, 1994).",
        "Especially, this phenomena and the out-of-order structures are very frequent in the spoken Chinese utterances."
      ]
    },
    {
      "heading": "3 Weakly Supervised Training of the Topic Classifier and Topic-dependent",
      "text": []
    },
    {
      "heading": "Semantic Classifiers",
      "text": [
        "As stated before, to train the classifiers for topic identification and slot-filling, we need to label each sentence in the training set against the semantic frame.",
        "Although this annotating scenario is relatively minimal, the labeling process is still time-consuming and costly.",
        "Meanwhile unlabeled sentences are relatively easy to collect.",
        "Therefore, to reduce the cost of labeling training utterances, we employ weakly supervised techniques for training the topic and semantic classifiers.",
        "The weakly supervised training of the two classifiers is successive.",
        "Assume that a small amount of seed sentences are manually labeled against the semantic frame.",
        "We first exploit the labeled frame types (e.g. ShowRoute) of the seed sentences to train a topic classifier through the combination of active learning and self-training.",
        "The resulting topic classifier is used to label the remaining training sentences with the corresponding topic, which are not selected by active learning.",
        "Then, we use all the sentences annotated against the semantic frame (including the seed sentences and sentences labeled by active learning) and the remaining training sentences labeled the topic to train the semantic classifiers using a practical bootstrapping technique."
      ]
    },
    {
      "heading": "3.1 Combining Active Learning and Self-training for Topic Classification",
      "text": [
        "We employ the strategy of combining active learning and self-training for training the topic classifier, which was firstly proposed in (Tur et al., 2005) and applied to a similar task.",
        "One way to reduce the number of labeling examples is active learning, which have been applied in many domains (McCallum and Nigam, 1998; Tang et al., 2002; Tur et al., 2005).",
        "Usually, the classifier is trained by randomly sampling the training examples.",
        "However, in active learning, the classifier is trained by selectively sampling the training examples (Cohn et al., 1994).",
        "The basic idea is that the most informative ones are selected from the unlabeled examples for a human to label.",
        "That is to say, this strategy tries to always select the examples, which will have the largest improvement on performance, and hence minimizes the human labeling effort whilst keeping performance (Tur et al., 2005).",
        "According to the strategy of determining the informative level of an example, the active learning approaches can be divided into two categories: uncertainty-based and committee-based.",
        "Here, we employ the uncertainty-based strategy for selective sampling.",
        "It is assumed that a small amount of labeled examples is initially available, which is used to train a basic classifier.",
        "Then the classifier is applied to the unannotated examples.",
        "Typically the most unconfident examples are selected for a human to label and then added to the training set.",
        "The classifier is retrained and the procedure is repeated until the system performance converges.",
        "Another alternative for reducing human labeling effort is self-training.",
        "In self-training, an initial classifier is built using a small amount of annotated examples.",
        "The classifier is then used to label the unannotated training examples.",
        "The examples with classification confidence scores",
        "over a certain threshold, together with their predicted labels, are added to the training set to retrain the classifier.",
        "This procedure repeated until the system performance converges.",
        "These two strategies are complementary and hence can be combined.",
        "The combination strategy is quite straightforward for pool-based training.",
        "At each iteration, the current classifier is applied to the examples in the current pool.",
        "The most unconfident examples in the pool are selected by active learning and labeled by a human.",
        "The remaining examples in the pool are automatically labeled by the current classifier.",
        "Then, these two parts of labeled examples are both added into the training set and used for retraining the classifier.",
        "Since the LIBSVM toolkit provides the class probability, we directly use the class probability as the confidence score.",
        "Our dynamic pool-based (the pool size is n ) algorithm of combining active learning and self-training for training the topic classifier is as follows:",
        "1.",
        "Given a small amount of human-labeled training set St ( n sentences) and a larger amount of unlabeled set Su, build the initial classifier using St .",
        "2.",
        "While labelers/ sentences are available (a) Get n unlabeled sentences from Su (b) Apply the current classifier to n unlabeled sentences (c) Select m examples which are most informative to the current classifier and manually label the selected m examples (d) Add the m human-labeled examples and the remaining n − m machine-labeled examples to the training set St (e) Train a new classifier on all labeled examples"
      ]
    },
    {
      "heading": "3.2 Bootstrapping the Topic-dependent Semantic Classifiers",
      "text": [
        "Bootstrapping refers to a problem of inducing a classifier given a small set of labeled data and a large set of unlabeled data (Abney, 2002).",
        "It has been applied to problems such as word-sense disambiguation (Yarowsky, 1995), web-page classification (Blum and Mitchell, 1998), named-entity recognition (Collins and Singer, 1999) and automatic construction of semantic lexicon (Thelen and Riloff, 2003).",
        "The key to the bootstrapping methods is to exploit the redundancy in the unlabeled data (Collins and Singer, 1999).",
        "Thus, many language processing problems can be dealt using the bootstrapping methods since language is highly redundant (Yarowsky, 1995).",
        "The semantic classification problem here also exhibits the redundancy.",
        "In the example “Please tell me how can I go from [locat.on]1 to [loca-t.on]2 by [bus]?”, there are multiple literal context features which all indicate that [location]1 is of type ShowRoute.",
        "[route].",
        "[origin], such as:",
        "(1) from within the –1 windows; (2) from _ to ; (3) to within the +1 windows.",
        "If the [location]2 has already be recognized as ShowRoute.",
        "[route].",
        "[destination], thus the slot context feature “ShowRoute.",
        "[route].",
        "[origin] within the ±2 windows” is also a strong evidence that [location]1 is of type ShowRoute.",
        "[route].",
        "[origin].",
        "That is to say, the literal context and slot context features above effectively overdetermine the slot of a concept in the input sentence.",
        "Especially, the literal and slot context features can be seen as two natural “views” of an example from the respective of “Co-Training” (Blum and Mitchell, 1998).",
        "Our bootstrapping algorithm exploits the property of redundancy to incrementally identify the features for assigning slots of a concept, given a few annotated seed sentences.",
        "The bootstrapping algorithm is performed on each topic T. (1 ≤ .",
        "≤ n , n is the number of",
        "topic) as follows: 1.",
        "For each concept Cj in T. (1≤ j≤ m , m is",
        "the number of concepts appears in the sentences of topic T. ): (1.1) Build the two initial classifiers based on literal and slot context features respectively using a small amount of labeled seed sentences.",
        "(1.2) Apply the current classifier based on the literal context feature to the remaining unlabeled concepts in the training sentences belong to topic T. .",
        "Keep those classified slots with confidence score above a certain threshold (In this paper, the threshold is fixed on 0.5).",
        "2.",
        "Check the consistency of the classified slots in each sentence.",
        "If some slots in a sentence clashed, take the one with the highest confidence score among them and leave the others unlabeled.",
        "3.",
        "For each concept Cj in T., apply the current",
        "classifier based on the slot context to the residual unlabeled concepts.",
        "Keep those classi",
        "fied slots with confidence score above a certain threshold.",
        "Repeat Step 3.",
        "4.",
        "Augment the new classified cases into the training set and retrain the two classifiers based on literal and slot context features respectively.",
        "5.",
        "If new slots are classified from the training data, return to step 2.",
        "Otherwise, repeat 2-5 to label training data and keep all new classified slots regardless of the confidence score.",
        "Train the two final semantic classifiers based on the literal and context features respectively using the new labeled training data."
      ]
    },
    {
      "heading": "4 Experiments and Results",
      "text": []
    },
    {
      "heading": "4.1 Data Collection and Experimental Setting",
      "text": [
        "Our experiments were carried out in the context of Chinese public transportation information inquiry domain.",
        "We collected two kinds of corpus for our domain using different ways.",
        "Firstly, a natural language corpus was collected through a specific website which simulated a dialog system.",
        "The user can conduct some mixed-initiative conversational dialogues with it by typing Chinese queries.",
        "Then we collected 2,286 natural language utterances through this way.",
        "It was divided into two parts: the training set contained 1,800 sentences (TR), and the test set contained 486 sentences (TS 1).",
        "Also, a spoken language corpus was collected through the deployment of a preliminary version of telephone-based dialog system, of which the speech recognizer is based on the speaker-independent Chinese dictation system of IBM ViaVoice Telephony and the SLU component is a robust rule-based parser.",
        "The spoken utterances corpus contained 363 spoken utterances.",
        "Then we obtained two test set from this corpus: one consisted of the recognized text (TS2); the other consisted of the corresponding transcription (TS3).",
        "The Chinese character error rate and concept error rate of TS2 are 35.6% and 41.1% respectively.",
        "We defined ten types of topic for our domain: ListStop, ShowFare, ShowRoute, ShowRouteTime, etc.",
        "The first corpus covers all the ten topic types and the second corpus only covers four topic types.",
        "The total number of Chinese characters appear in the data set is 923.",
        "All the sentences were annotated against the semantic frame.",
        "In our experiments, the topic classifier and semantic classifiers were trained on the natural language training set (TR) and tested on three test sets (TS 1, TS2 and TS3).",
        "The performance of topic classification and semantic classification are measured in terms of topic error rate and slot error rate respectively.",
        "Topic performance is measured by comparing the topic of a sentence predicated by the topic classifier with the reference topic.",
        "The slot error rate is measured by counting the insertion, deletion and substitution errors between the slots generated by our system and these in the reference annotation."
      ]
    },
    {
      "heading": "4.2 Supervised Training Experiments",
      "text": [
        "Firstly, in order to validate the effectiveness of our proposed SLU system using successive learners, we compared our system with a rule-based robust semantic parser.",
        "The parsing algorithm of this parser is same as the local chart parser used by the preprocessor.",
        "The handcrafted grammar for this semantic parser took a linguistic expert one month to develop, which consists of 798 rules (except the lexical rules for named entities such as [loc_name]).",
        "In our SLU system, we first use the SVMs to identify the topic and then apply the semantic classifier (decision list) related to the identified topic to assign the slots to the concepts.",
        "The SVMs used the augmented binary features (923 Chinese characters and 20 semantic class labels).",
        "A general developer independently annotated the TR set against the semantic frame, which took only four days.",
        "Through feature extraction from the TR set and feature pruning, we obtained 2,259 literal context features and 369 slot context features for 20 kinds of concepts in our domain.",
        "Table 1 Shows that our SLU method has better performance than the rule-based robust parser in both topic classification and slot identification.",
        "Due to the high concept error rate of recognized utterances, the performance of semantic classification on the TS2 is relatively poor.",
        "However, if considering only the correctly identified concepts on TS2, the slot error rate is 9.2%.",
        "Note that, since the TS2 (recognized speech) covers only four types of topic but TS 1 (typed utterance) covers ten topics, the topic error on the TS2 (recognized speech) is lower than that on TS 1.",
        "Table 1 also compares our system with the two-stage classification with the reversed order.",
        "Another alternative for our system is to reverse the two main processing stages, i.e., finding the roles for the concepts prior to identifying the topic.",
        "For instance, in the example sentence in Fig.1, the concept (e.g., [location]) in the preprocessed sequence is first recognized as slots (e.g., [route].",
        "[origin]) before topic classification.",
        "Therefore, the slots like [route].",
        "[origin] can be included as features for topic classification, which is deeper than the concepts like [location] and potential to achieve improvement on performance of topic classification.",
        "This strategy was adopted in some previous works (He and Young, 2003; Wutiwiwatchai and Furui, 2003).",
        "However, the results indicate that, at least in our two-stage classification formwork, the strategy of identifying the topic before assigning the slots to the concepts is more optimal.",
        "According to our error analysis, the unsatisfied performance of the reversed two-stage classification system can be explained as follows: (1) Since the semantic classification is performed on all topics, the search space is much bigger and the ambiguities increase.",
        "This deteriorates the performance of semantic classification.",
        "(2) In the case that the slots and Chinese characters are included as features, the topic classifier relies heavily on the slot features.",
        "Then, the errors of semantic classification have serious negative effect on the topic classification."
      ]
    },
    {
      "heading": "4.3 Weakly Supervised Training Experiments 4.3.1 Active Learning and Self-training Experiments for Topic Classification",
      "text": [
        "In order to evaluate the performance of active learning and self-training, we compared three sampling strategies: random sampling, active learning only, active learning and self-training.",
        "At each iteration of pool-based active learning and self-training, we get 200 sentences (i.e., the pool size is set as 200) and select 50 most unconfident sentences from them for manually labeling and exploit the remaining sentences using self-training.",
        "All the experiments were repeated ten times with different randomly selected seed sentences and the results were averaged.",
        "Figure 1 plots the learning curves of three strategies trained on TR and tested on the TS1 set.",
        "It is evident that active learning significantly reduces the need for labeled data.",
        "For instance, it requires 1600 examples if they are randomly chosen to achieve a topic error rate of 3.2% on TS1, but only 600 actively selected examples, a saving of 62.5%.",
        "The strategy of combing active learning and self-training can further improve the performance of topic classification compared with active learning only with the same amount of labeled data.",
        "We also evaluated the performance of topic classification using active learning and self-training with the pool size of 200 on the three test sets.",
        "Table 2 shows that active learning and self-training with the pool size of 200 achieves almost the same performance on three test sets as random sampling, but requires only 33.3% data."
      ]
    },
    {
      "heading": "4.3.2 Bootstrapping Experiments for Semantic Classification",
      "text": [
        "As stated before, the bootstrapping procedure begins with a small amount of sentences annotated against the semantic frame, which is the initial seed sentence or annotated by active learning, and the remaining training sentences, the topics of which are machine-labeled by the resulting topic classifier.",
        "For example, in the",
        "weakly supervised training scenario with the pool size of 200, the active learning and self-training procedure ran 8 iterations.",
        "At each iteration, 50 sentences were selected by active learning.",
        "So the total number of labeled sentences is 600.",
        "We compared our bootstrapping methods with supervised training for semantic classification.",
        "We tried two bootstrapping methods: using only the literal context features (Bootstrapping 1) and using the literal and slot context features (Bootstrapping 2).",
        "If the step 4 of the bootstrapping algorithm in Section 3.2 is canceled, the new bootstrapping variation corresponds to Bootstrapping 2.",
        "Also, we repeated the experiments ten times with different labeled sentences and the results were averaged.",
        "Figure 3 plots the learning curves of bootstrapping and supervised training with different number of labeled sentences on the TS 1 set.",
        "The results indicate that bootstrapping methods can effectively make use of the unlabeled data to improve the semantic classification performance.",
        "In particular, the learning curve of bootstrapping 1 achieves more significant improvement than the curve of bootstrapping 2.",
        "It can be explained as follows: including the slot context features further increases the redundancy of data and hence corrects the initial misclassified cases by the semantic classifier using only literal context features or provides new cases.",
        "Finally, we compared two SLU systems through weakly supervised and supervised training respectively.",
        "The supervised one was trained using all the annotated sentences in TR (1800 sentences).",
        "In the weakly supervised training scenario (the pool size is still 200), The topic classifier and semantic classifiers were both trained using only 600 labeled sentences.",
        "Table 3 shows that the weakly supervised scenario achieves comparable performance to the supervised one, but requires only 33.3% labeled data."
      ]
    },
    {
      "heading": "5 Conclusion and Future work",
      "text": [
        "We have presented a new SLU framework using two successive classifiers.",
        "The proposed framework exhibits the advantages as follows.",
        "• It has good robustness on processing spoken language: (1) The preprocessor provides the low level robustness.",
        "(2) It inherits the robustness of topic classification using statistical pattern recognition techniques.",
        "It can also make use of topic classification to guide slot filling.",
        "(3) The strategy of first finding the concepts or slot islands and then linking them is suited for processing spoken language.",
        "• It also keeps the understanding deepness: (1)",
        "The class of semantic classification is the slot name, which inherits the hierarchy from the domain model.",
        "(2) The semantic reclassification mechanism ensures the consistency among the identified slot-value pairs.",
        "• It is mainly data-driven and requires only",
        "minimally annotated corpus for training.",
        "Most importantly, our proposed SLU framework allows the employment of weakly supervised strategies for training the two classifiers, which can reduce the cost of annotating labeled sentences.",
        "The future work includes further evaluation of our approach in other application domains and languages.",
        "We also plan to integrate this understanding system into a whole dialog system.",
        "Then, high level knowledges, such as the dialog context, can also be included as the features of topic and semantic classifiers.",
        "Moreover, currently, the topics are manually defined through examination of the example sentences by human.",
        "Then, it is worthwhile to investigate how to appropriately define topics and the probability of",
        "exploiting the sentence clustering techniques to facilitate the topic (frame) designment."
      ]
    },
    {
      "heading": "6 Acknowledgements",
      "text": [
        "The authors would like to thank three anonymous reviewers for their careful reading and helpful suggestions.",
        "This work is supported by National Natural Science Foundation of China (NSFC, No.",
        "60496326) and 863 project of China (No.",
        "2001 AA 114210-11)."
      ]
    }
  ]
}
