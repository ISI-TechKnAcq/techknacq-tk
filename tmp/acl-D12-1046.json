{
  "info": {
    "authors": [
      "Xian Qian",
      "Yang Liu"
    ],
    "book": "EMNLP",
    "id": "acl-D12-1046",
    "title": "Joint Chinese Word Segmentation, POS Tagging and Parsing",
    "url": "https://aclweb.org/anthology/D12-1046",
    "year": 2012
  },
  "references": [
    "acl-C08-1049",
    "acl-C10-1045",
    "acl-D10-1019",
    "acl-D10-1082",
    "acl-D11-1109",
    "acl-I08-4010",
    "acl-I08-4017",
    "acl-J11-1005",
    "acl-N07-1051",
    "acl-P06-2123",
    "acl-P08-1043",
    "acl-P08-1101",
    "acl-P08-1102",
    "acl-P08-1109",
    "acl-P09-1058",
    "acl-P09-1059",
    "acl-P11-1089",
    "acl-P11-1139",
    "acl-W02-1001",
    "acl-W09-1207"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we propose a novel decoding algorithm for discriminative joint Chinese word segmentation, part-of-speech (POS) tagging, and parsing.",
        "Previous work often used a pipeline method ?",
        "Chinese word segmentation followed by POS tagging and parsing, which suffers from error propagation and is unable to leverage information in later modules for earlier components.",
        "In our approach, we train the three individual models separately during training, and incorporate them together in a unified framework during decoding.",
        "We extend the CYK parsing algorithm so that it can deal with word segmentation and POS tagging features.",
        "As far as we know, this is the first work on joint Chinese word segmentation, POS tagging and parsing.",
        "Our experimental results on Chinese Tree Bank 5 corpus show that our approach outperforms the state-of-the-art pipeline system."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "For Asian languages such as Japanese and Chinese that do not contain explicitly marked word boundaries, word segmentation is an important first step for many subsequent language processing tasks, such as POS tagging, parsing, semantic role label-ing, and various applications.",
        "Previous studies for POS tagging and syntax parsing on these languages sometimes assume that gold standard word segmentation information is provided, which is not the real scenario.",
        "In a fully automatic system, a pipeline approach is often adopted, where raw sentences are first segmented into word sequences, then POS tagging and parsing are performed.",
        "This kind of approach suffers from error propagation.",
        "For example, word segmentation errors will result in tagging and parsing errors.",
        "Additionally, early modules cannot use information from subsequent modules.",
        "Intuitively a joint model that performs the three tasks together should help the system make the best decisions.",
        "In this paper, we propose a unified model for joint Chinese word segmentation, POS tagging, and parsing.",
        "Three sub-models are independently trained using the state-of-the-art methods.",
        "We do not use the joint inference algorithm for training because of the high complexity caused by the large amount of parameters.",
        "We use linear chain Conditional Random Fields (CRFs) (Lafferty et al2001) to train the word segmentation model and POS tagging model, and averaged perceptron (Collins, 2002) to learn the parsing model.",
        "During decoding, parameters of each sub-model are scaled to represent its importance in the joint model.",
        "Our decoding algorithm is an extension of CYK parsing.",
        "Initially, weights of all possible words together with their POS tags are calculated.",
        "When searching the parse tree, the word and POS tagging features are dynamically generated and the transition information of POS tagging is considered in the span merge operation.",
        "Experiments are conducted on Chinese Tree Bank (CTB) 5 dataset, which is widely used for Chinese word segmentation, POS tagging and parsing.",
        "We compare our proposed joint model with the pipeline system, both built using the state-of-the-art sub-models.",
        "We also propose an evaluation metric to",
        "calculate the bracket scores for parsing in the face of word segmentation errors.",
        "Our experimental results show that the joint model significantly outperforms the pipeline method based on the state-of-the-art sub-models."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "There is very limited previous work on joint Chinese word segmentation, POS tagging, and parsing.",
        "Previous joint models mainly focus on word segmentation and POS tagging task, such as the virtual nodes method (Qian et al2010), cascaded linear model (Jiang et al2008a), perceptron (Zhang and Clark, 2008), sub-word based stacked learning (Sun, 2011), reranking (Jiang et al2008b).",
        "These joint models showed about 0.2 ?",
        "1% F-score improvement over the pipeline method.",
        "Recently, joint tagging and dependency parsing has been studied as well (Li et al. 2011; Lee et al2011).",
        "Previous research has showed that word segmentation has a great impact on parsing accuracy in the pipeline method (Harper and Huang, 2009).",
        "In (Jiang et al2009), additional data was used to improve Chinese word segmentation, which resulted in significant improvement on the parsing task using the pipeline framework.",
        "Joint segmentation and parsing was also investigated for Arabic (Green and Manning, 2010).",
        "A study that is closely related to ours is (Goldberg and Tsarfaty, 2008), where a single generative model was proposed for joint morphological segmentation and syntactic parsing for Hebrew.",
        "Different from that work, we use a discriminative model, which benefits from large amounts of features and is easier to deal with unknown words.",
        "Another main difference is that, besides segmentation and parsing, we also incorporate the POS tagging model into the CYK parsing framework."
      ]
    },
    {
      "heading": "3 Methods",
      "text": [
        "For a given Chinese sentence, our task is to generate the word sequence, its POS tag sequence, and the parse tree (constituent parsing).",
        "A joint model is expected to make more optimal decisions than a pipeline approach; however, such a model will be too complex and it is difficult to estimate model parameters.",
        "Therefore we do not perform joint inference for training.",
        "Instead, we develop three individual models independently during training and perform joint decoding using them.",
        "In this section, we first describe the three sub-models and then the joint decoding algorithm."
      ]
    },
    {
      "heading": "3.1 Word Segmentation Model",
      "text": [
        "Methods for Chinese word segmentation can be broadly categorized into character based and word based models.",
        "Previous studies showed that character-based models are more effective to detect out-of-vocabulary words while word-based models are more accurate to predict in-vocabulary words (Zhang et al2006).",
        "Here, we use order-0 semi-Markov model (Sarawagi and Cohen, 2004) to take advantages of both approaches.",
        "More specifically, given a sentence x = c1, c2, .",
        ".",
        ".",
        ", cl (where ci is the ith Chinese character, l is the sentence length), the character-based model assigns each character with a word boundary tag.",
        "Here we use the BCDIES tag set, which achieved the best official performance (Zhao and Kit, 2008): B, C, D, E denote the first, second, third, and last character of a multi-character word respectively, I denotes the other characters, and S denotes the single character word.",
        "We use the same character-based feature templates as in the best official system, shown in Table 1 (1.1-1.3), including character un-igram and bigram features, and transition features.",
        "Linear chain CRFs are used for training.",
        "Feature templates in the word-based model are shown in Table 1 (1.4-1.6), including word features, sub-word features, and character bigrams within words.",
        "The word feature is activated if a predicted word w is in the vocabulary (i.e., appears in training data).",
        "Subword(w) is the longest in-vocabulary word within w. To use word features, we adopt a K-best reranking approach.",
        "The top K candidate segmentation results for each training sample are generated using the character-based model, and the gold segmentation is added if it is not in the candidate set.",
        "We use the Maximum Entropy (ME) model to learn the weights of word features such that the probability of the gold candidate is maximal.",
        "A problem arises when combining the two models and using it in joint segmentation and parsing, since the linear chain used in the character-based model is incompatible with CYK parsing model and the word-based model due to the transition informa",
        "Character Level Feature Templates (1.1) ci?2yi, ci?1yi, ciyi, ci+1yi, ci+2yi (1.2) ci?1ciyi, cici+1yi, ci?1ci+1yi (1.3) yi?1yi Word Level Feature Templates (1.4) word w (1.5) subword(w) (1.6) character bigrams within w",
        "the ith character in the sentence, yi is its label, w is a predicted word.",
        "tion.",
        "Thus, we slightly modify the linear chain CRFs by fixing the weights of transition features during training and testing.",
        "That is, weights of impossible transition features (e.g., B?B) are set to ?",
        "?, and weights of the other transition features (e.g., E?B) are set to 0.",
        "In this way, the transition feature could be neglected in testing for two reasons.",
        "First, all illegal label assignments are prohibited in prediction, since their weights are ??",
        "; second, because weights of legal transition features are 0, they do not affect the prediction at all.",
        "In the following, transition features are excluded.",
        "Now we can use order-0 semi Markov model as the hybrid model.",
        "We define the score of a word as the sum of the weights of all the features within the word.",
        "Formally, the score of a multi-character word",
        "where fCRF and fME are the feature vectors in the character and word based models respectively, and ?CRF , ?ME are their corresponding weight vectors.",
        "For simplicity, we denote ?seg = ?CRF?ME , fseg = fCRF?ME , where ?CRF?ME means the concatenation of ?CRF and ?ME .",
        "Scores for single character words are defined similarly.",
        "These word scores will be used in the joint segmentation and parsing task Section 3.4."
      ]
    },
    {
      "heading": "3.2 POS Tagging Model",
      "text": [
        "Though syntax parsing model can directly predict the POS tag itself, we choose not to use this, but use an independent POS tagger for two reasons.",
        "First, there is a large amount of data with labeled POS tags but no syntax annotations, such as the People's Daily corpus and SIGHAN bakeoff corpora (Jin and Chen, 2008).",
        "Such data can only be used to train POS tag-gers, but not for training the parsing model.",
        "Often using a larger training set will result in a better POS tagger.",
        "Second, the state-of-the-art POS tagging systems are often trained by sequence labeling models, not parsing models.",
        "ith word in the sentence, ti is its POS tag.",
        "For a word w, cj(w) is its jth character, c?j(w) is the last jth character, and l(w) is its length.",
        "The POS tagging problem is to assign a POS tag t ?",
        "T to each word in a sentence.",
        "We also use linear chain CRFs for POS tagging.",
        "Feature templates shown in Table 2 are the same as those in (Qian et al2010), which have been shown effective on CTB corpus.",
        "Three feature sets are considered: (i) word level features, including surrounding word uni-grams, bigrams, and word length; (ii) character level features, such as the first and last characters in the words; (iii) transition features."
      ]
    },
    {
      "heading": "3.3 Parsing Model",
      "text": [
        "We choose discriminative models for parsing since it is easy to handle unknown words by simply adding character level features.",
        "Online structured learning algorithms were demonstrated to be effective for training, such as stochastic optimization (Finkel et al., 2008).",
        "In this study, we use averaged perceptron algorithm for parameter estimation since it is easier to implement and has competitive performance.",
        "A Context Free Grammar (CFG) consists of (i) a set of terminals; (ii) a set of nonterminals {Nk}; (iii) a designated start symbol ROOT; and (iv) a set of rules, {r = N i ?",
        "?j}, where ?j is a sequence of terminals and nonterminals.",
        "In the parsing task, ter",
        "minals are the words, and nonterminals are the POS tags and phrase types.",
        "In this paper, nonterminal is named state for short.",
        "A parse tree T of sentence x can be factorized into several one-level subtrees, each corresponding to a rule r. In practice, binarization of rules is necessary to obtain cubic parsing time.",
        "That is, the right hand side of each rule should contain no more than 2 states.",
        "We used right branching binarization, as illustrated in Figure 1.",
        "We did not use parent annotation, since we found it degraded the performance in our experiments (shown in Section 4).",
        "We used the same preprocessing step as (Harper and Huang, 2009), collapsing all the allowed nonterminal-yield unary chains to single unary rules.",
        "Therefore, all spans in the binarized trees contain no more than one unary rules.",
        "To facilitate decoding, we unify the form of spans so that each span contains exactly one unary rule.",
        "This is done by adding identity unary rules",
        "identity unary rules will be removed in evaluation.",
        "Hence, there are two states of a span: the top state N and the bottom state N that correspond to the left and right hand of the unary rule runary = N ?",
        "N respectively, as shown in Figure 2.",
        "Table 3 lists the feature templates we use for parsing.",
        "There are 4 feature sets: (i) bottom state features fbottom(i, j,x, N i,j), which depend on the bot",
        "unary chains are collapsed to single unary rules.",
        "Identity unary rules are added to spans that have no unary rule.",
        "tom states; (ii) top state features ftop(i, j,x, N i,j); (iii) unary rule features funary(i, j,x, runaryi,j ), which extract the transition information from bottom states to top states; (iv) binary rule features fbinary(i, j, k,x, rbinaryi,j,k = N i,j ?",
        "N i,k?1 +Nk,r), where N i,k?1, Nk,r are the top states of the left and right children.",
        "The score function for a sentence xwith parse tree",
        "where ?bottom, ?top, ?unary, ?binary are the weight vectors of the four feature sets.",
        "Given the training corpus {(xi, T?i)}, the learning task is to estimate the weight vectors so that for each sentence xi, the gold standard tree T?i achieves the maximal score among all the possible trees.",
        "The perceptron algorithm is guaranteed to find the solution if it exists."
      ]
    },
    {
      "heading": "3.4 Joint Decoding",
      "text": [
        "The three models described above are separately trained to make parameter estimation feasible as well as optimize each individual component.",
        "In test",
        "bigram of word, POS tag.",
        "Xl+a/Xr?a denotes the first/last ath X in the span, while Xl?a/Xr+a denotes the ath X left/right to span.",
        "Xm is the first X of right child, and Xm?1 is the last X of the left child.",
        "len, lenl, lenr denote the length of the span, left child and right child respectively.",
        "wl is the length of word.",
        "ROOT/LEAF means the template can only generate the features for the root/initial span.",
        "ing, we perform joint decoding to combine information from the three models.",
        "Parameters of word segmentation (?seg), POS tagging (?pos), and parsing models (?parse = ?bottom?top?",
        "unary?bianry) are scaled by three positive hyper-parameters ?, ?, and ?",
        "respectively, which control their contribution in the joint model.",
        "If ?",
        ">> ?",
        ">> ?, then the joint model is equivalent to a pipeline model, in which there is no feedback from downstream models to upstream ones.",
        "For well tuned hyper-parameters, we expect that segmentation and POS tagging results can be improved by parsing information.",
        "The hyper-parameters are tuned on development data.",
        "In the following sections, for simplicity we drop ?, ?, ?, and just use ?seg, ?pos, ?parse to represent the scaled parameters.",
        "The basic idea of our decoding algorithm is to extend the CYK parsing algorithm so that it can deal with transition features in POS tagging and segmentation scores in word segmentation.",
        "The joint decoding algorithm is shown in Algorithm 1.",
        "Given a sentence x = c1, .",
        ".",
        ".",
        ", cl, Line 0 calculates the scores of all possible words in the sentence using Eq(1).",
        "There are l(l + 1)/2 word candidates in total.",
        "Surrounding words are important features for POS tagging and parsing; however, they are unavailable because segmentation is incomplete before parsing.",
        "Therefore, we adopt pseudo surrounding features by simply fixing the context words as the single most likely ones.",
        "Given a word candidate wi,j from ci to cj , its previous word s?",
        "is the rightmost one in the best word sequence of c1, .",
        ".",
        ".",
        ", ci?1, which can be obtained by dynamic programming.",
        "Recursively, the second word left to wi,j is the previous word of s?.",
        "The next word of wi,j is defined similarly.",
        "In Line 1, we use bidirectional Viterbi decoding to obtain all the surrounding words.",
        "In the forward direction, the algorithm starts from the first character boundary to the last, and finds the best previous word for the ith character boundary bi.",
        "In the backward direction, the algorithm starts from right to left, and finds the best next word of each bi.",
        "In Line 2, for each word candidate, we can calculate the score of each POS tag using state features in the POS tagging model, since the context words are available now.",
        "The score function of word wi,j with",
        "In Line 3, POS tags of surrounding words can be obtained similarly using bidirectional decoding.",
        "Algorithm 1 Joint Word Segmentation, POS tagging, and Parsing Algorithm Input: Sentence x = c1, .",
        ".",
        ".",
        ", cl, beam size B, scaled word segmentation model, POS tagging model and parsing model.",
        "Output: Word sequence, POS tag sequence, and parse tree 0: ?0 ?",
        "i ?",
        "j ?",
        "l ?",
        "1, calculate scoreseg(x, i, j) using Equation (1) 1: For each character boundary bi, 0 ?",
        "i ?",
        "l, get the best previous and next words of bi using bidirectional Viterbi decoding",
        "2: ?0 ?",
        "i ?",
        "j ?",
        "l ?",
        "1, t ?",
        "T , calculate scoreseg?pos(x, i, j, t) using Equation (2) 3: ?bi, 0 ?",
        "i ?",
        "l, t ?",
        "T , get the best POS tags of words left/right to bi using bidirectional viterbi decoding.",
        "4: For each word candidate wi,j , 0 ?",
        "i ?",
        "j ?",
        "l ?",
        "1 5: For each bottom state N , POS tag t ?",
        "T step 1 (Line 5-7): get bottom states 6: scorebottom(x, i, j, wi,j , t, N) = scoreseg?pos(x, i, j, t) + ?bottom ?",
        "fbottom(x, i, j, wi,j , t, N) 7: Keep B best scorebottom.",
        "8: For each top state N step 2 (Line 8-9): get top states 9: scoretop(x, i, j, wi,j , t, N) = maxN {scorebottom(x, i, j, wi,j , t, N) + ?top ?",
        "ftop(x, i, j, wi,j , t, N) +?unary ?",
        "funary(x, i, j, wi,j , t, N ?",
        "N) } 10: for i = 0, .",
        ".",
        ".",
        ", l ?",
        "1 do 11: for width = 1, .",
        ".",
        ".",
        ", l ?",
        "1 do 12: j = i + width 13: for k = i + 1, .",
        ".",
        ".",
        ", j do 14: scorebottom(x, i, j,w, t, N) = maxl,r { scoretop(x, i, k ?",
        "1,wl, tl, N l) + scoretop(x, k, j,wr, tr, Nr) +?binary ?",
        "fbinary(x, i, j, k,w, t, N ?",
        "Nr + Nr) + ?pos ?",
        "fpos(tlastl ?",
        "tfirstr ) +?bottomfbottom(x, i, j,w, t, N)} 15: Keep B best scorebottom step 1 (Line 14-15): get bottom states 16: For each top state N step 2 (Line 16-17): get top states 17: scoretop(x, i, j,w, t, N) = maxN {scorebottom(x, i, j,w, t, N) +?unary ?",
        "funary(x, i, j,w, t, N ?",
        "N) } 18: end for 19: end for 20: end for",
        "That is, for wi,j with POS tag t, we use Viterbi algorithm to search the optimal POS tags of its left and right words.",
        "In Lines 4-9, each word was initialized as a basic span.",
        "A span structure in the joint model is a 6-tuple: S(i, j,w, t, N,N), where i, j are the boundary in-dices,w, t are the word sequence and POS sequence within the span respectively, and N,N are the bottom and top states.",
        "There are two types of surrounding n-grams: one is inside the span, for example, the first word of a span, which can be obtained from w; the other is outside the span, for example, the previous word of a span, which is obtained from the pseudo context information.",
        "The score of a basic span depends on its corresponding word and POS pair score, and the weights of the active state and unary features.",
        "To avoid enumerating the combination of the bottom and top states, initialization for each span is divided into 2 steps.",
        "In the first step, the score of every bottom state is calculated using bottom state features, and only the B best states are maintained (see Line 6-7).",
        "In the second step, top state features and unary rule features are used to get the score of each top state (Line 9), and only the top B states are preserved.",
        "Similarly, there are two steps in the merge operation: S(i, j,w, t, N,N) = Sl(i, k,wl, tl, Nl, Nl)+ Sr(k + 1, j,wr, tr, Nr, Nr).",
        "The score of the bottom state N is calculated using binary features fbinary(x, i, j, k,w, t, N ?",
        "N r+N r), bottom state features fbottom(x, i, j,w, t, N), and POS tag transition features that depend on the boundary POS tags of Sl and Sr. See Line 14 of Algorithm 1, where tlastl and t first r are the POS tags of the last word in the left child span and the first word in the right child span respectively.",
        "Given a sentence of length l, the complexity for each line of Algorithm 1 is listed in Table 4, where |T |is the size of POS tag set, M is the number of states, and B is the beam size."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": []
    },
    {
      "heading": "4.1 Data",
      "text": [
        "For comparison with other systems, we use the CTB5 corpus, which has been studied for Chinese word segmentation, POS tagging and parsing.",
        "We use the standard train/develop/test split of the data.",
        "Details are shown in Table 5."
      ]
    },
    {
      "heading": "4.2 Evaluation Metric",
      "text": [
        "We evaluate system performance on the individual tasks, as well as the joint tasks.1 For word segmentation, three metrics are used for evaluation: precision (P), recall (R), and F-score (F) defined by 2PR/(P+R).",
        "Precision is the percentage of correct words in the system output.",
        "Recall is the percentage of words in gold standard annotations that are correctly predicted.",
        "For parsing, we use the standard parseval evaluation metrics: bracketing precision, recall and F-score.",
        "1Note that the joint task refers to automatic segmentation and tagging/parsing.",
        "It can be achieved using a pipeline system or our joint decoding method.",
        "For joint word segmentation and POS tagging, a word is correctly predicted if both the boundaries and the POS tag are correctly identified.",
        "For joint segmentation, POS tagging, and parsing task, when calculating the bracket scores using existing parseval tools, we need to consider possible word segmentation errors.",
        "To do this, we add the word boundary information in states ?",
        "a bracket is correct only if its boundaries, label and word segmentation are all correct.",
        "One example is shown in Figure 3.",
        "Notice that identity unary rules are removed during evaluation.",
        "The basic spans are characters, not words, because the number of words in reference and prediction may be different.",
        "POS tags are removed since they do not affect the bracket scores.",
        "If the segmentation is perfect, then the bracket scores of the modified tree are exactly the same as the original tree.",
        "This is similar to evaluating parsing performance on speech transcripts with automatic sentence segmentation (Roark et al2006).",
        "culate the bracket scores in the face of word segmentation errors.",
        "Left: the original parse tree, Right: the converted parse tree.",
        "The numbers in the brackets are the indices of the character boundaries based on word segmentation."
      ]
    },
    {
      "heading": "4.3 Parameter Estimation",
      "text": [
        "We train three submodels using the gold features, that is, POS tagger is trained using the perfect segmentation, and parser is trained using perfect segmentation and POS tags.",
        "Some studies reported that better performance may be achieved by training subsequent models using representative output of the preceding models (Che et al2009).",
        "Hence for comparison we trained another parser using automatically generated POS tags obtained from 10-fold cross validation, but did not find significant difference between these two parsers when testing on the perfectly segmented development dataset.",
        "Therefore",
        "we use the parser trained with perfect POS tags for the joint task.",
        "Three hyper-parameters, ?, ?, and ?, are tuned on development data using a heuristic search.",
        "Parameters that achieved the best joint parsing result are selected.",
        "In the search, we fixed ?",
        "= 1 and varied ?, ?.",
        "First, we set ?",
        "= 1, and enumerate",
        "select the best ??.",
        "Table 6 lists the parameters we used for training the submodels, as well as the hyper-parameters for joint decoding."
      ]
    },
    {
      "heading": "4.4 Experimental Results",
      "text": [
        "In this section we first show that our sub-models are better than or comparable to state-of-the-art systems, and then the joint model is superior to the pipeline approach.",
        "Table 7 shows word segmentation results using our word segmentation submodel, in comparison to a few state-of-the-art systems.",
        "For our segmentor, we show results for two variants: one removes transition features as described in Section 3.1, the other uses CRFs to learn the weights of transition features.",
        "We can see that our system is competitive with all the others except Sun's that used additional idiom resources.",
        "Our two word segmentors have similar performance.",
        "Since the one without transition features can be naturally integrated into the joint system, we use it in the following joint tasks.",
        "For the POS tagging only task that takes gold standard word segmentation as input, we have two systems.",
        "One uses the linear chain CRFs as described in Section 3.2, the other is obtained using the parser described in Section 3.3 ?",
        "the parser generates POS tag hypotheses when POS tag features are not used.",
        "The POS tagging accuracy is 95.53% and 95.10% using these two methods respectively.",
        "The better performance from the former system may be because the local label dependency is more helpful for POS tagging than the long distance dependencies that might be noisy.",
        "This result also confirms our choice of using an independent POS tagger for the sub-model, rather than relying on a parser for POS tagging.",
        "However, since there are no reported results for this setup, we demonstrate the competence of our POS tagger using the joint word segmentation and POS tagging task.",
        "Table 8 shows the performance of a few systems along with ours, all using the pipeline approach where automatic segmentation is followed by POS tagging.",
        "We can see that our POS tagger is comparable to the others.",
        "tagging task.",
        "For parsing, Table 9 presents the parsing result on gold standard segmented sentence.",
        "Notice that the result of (Harper and Huang, 2009; Zhang and",
        "Clark, 2011) are not directly comparable to ours, as they used a different data split.",
        "The best published system result on CTB5 is Petrov and Klein?s, which used PCFG with latent Variables.",
        "Our system performs better mainly because it benefits from a large",
        "For our parser, besides the model described in Section 3.3, we tried two variations: one does not use the automatic POS tag features, the other one is learned on the parent annotated training data.",
        "The results in Table 9 show that there is a performance degradation when using parent annotation.",
        "This may be due to the introduction of a large number of states, resulting in sparse features.",
        "We also notice that with the help of the POS tag information, even automatically generated, the parser gained 0.9% improvement in F-score.",
        "This demonstrates the advantage of using a better independent POS tagger and incorporating it in parsing.",
        "Finally Table 10 shows the results for the three tasks using our joint decoding method in comparison to the pipeline method.",
        "We can see that the joint model outperforms the pipeline one.",
        "This is mainly because of a better parsing module as well as joint decoding.",
        "In the table we also include results of (Jiang et al2009), which is the only reported joint parsing result we found using the same data split on CTB5.",
        "They achieved 80.28% parsing F-score using automatic word segmentation.",
        "Their adapted system Jiang09+ leveraged additional corpus to improve Chinese word segmentation, resulting in an F-score of 81.07%.",
        "Our system has better performance than these.",
        "parsing task using pipeline and joint models."
      ]
    },
    {
      "heading": "4.5 Error Analysis",
      "text": [
        "We compared the results from the pipeline and our joint decoding systems in order to understand the impact of the joint model on word segmentation and POS tagging.",
        "We notice that the joint model tend to generate more words than the pipeline model.",
        "For example, ??????",
        "is one word in the pipeline model, but correctly segmented as two words ??",
        "?/???",
        "in the joint model.",
        "This tendency of segmentation also makes it fail to recognize some long words, especially OOV words.",
        "For example, ??",
        "???",
        "is segmented as ???/??.",
        "In the data set, we find that, the joint model corrected 10 missing boundaries over the pipeline method, and introduced 3 false positive segmentation errors.",
        "For the analysis of POS tags, we only examined the words that are correctly segmented by both the pipeline and the joint models.",
        "Table 11 shows the increase and decrease of error patterns of the joint model over the pipeline POS tagger.",
        "An error pattern ?X ?",
        "Y?",
        "means that the word whose true tag is ?X?",
        "is assigned a tag ?Y?.",
        "All the patterns are ranked in descending order of the reduction/increase of the error number.",
        "We can see that the joint model has a clear advantage in the disambiguation of {VV, NN} and {DEG, DEC}, which results in the overall improved performance.",
        "In contrast, the joint method performs worse on ambiguous POS pairs such as {NN,NR}.",
        "This observation is similar to those reported by (Li et al2011; Hatori et al2011)."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "In this paper, we proposed a new algorithm for joint Chinese word segmentation, POS tagging, and parsing.",
        "Our algorithm is an extension of the CYK",
        "number of the corresponding pattern made by the pipeline tagging model.",
        "?",
        "and ?",
        "mean the error number reduced or increased by the joint model.",
        "parsing method.",
        "The sub-models are independently trained for the three tasks to reduce model complexity and optimize individual sub-models.",
        "Our experiments demonstrate the advantage of the joint models.",
        "In the future work, we will compare this joint model to the pipeline approach that uses multiple candidates or soft decisions in the early modules.",
        "We will also investigate methods for joint learning as well as ways to speed up the joint decoding algorithm.",
        "Acknowledgments The authors thank Zhongqiang Huang for his help with experiments.",
        "This work is partly supported by DARPA under Contract No.",
        "HR0011-12-C-0016.",
        "Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA."
      ]
    }
  ]
}
