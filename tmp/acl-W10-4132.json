{
  "info": {
    "authors": [
      "Wenjun Gao",
      "Xipeng Qiu",
      "Xuanjing Huang"
    ],
    "book": "Proceedings of the Joint Conference on Chinese Language Processing",
    "id": "acl-W10-4132",
    "title": "Adaptive Chinese Word Segmentation with Online Passive-Aggressive Algorithm",
    "url": "https://aclweb.org/anthology/W10-4132",
    "year": 2010
  },
  "references": [
    "acl-W06-1615"
  ],
  "sections": [
    {
      "text": [
        "xpqiu@fudan.edu.cn",
        "xjhuang@fudan.edu.cn",
        "In this paper, we describe our systemfor CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation, which focused on the cross-domain performance of Chinese word segmentation algorithms.",
        "We use the online passive-aggressive algorithm with domain invariant information for cross-domain Chinese word segmentation."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In recent years, Chinese word segmentation (CWS) has undergone great development (Xue, 2003; Peng et al., 2004).",
        "The popular method is to regard word segmentation as a sequence labeling problems.",
        "The goal of sequence labeling is to assign labels to all elements of a sequence.",
        "Due to the exponential size of the output space, sequence labeling problems tend to be more challenging than the conventional classification problems.",
        "Many algorithms have been proposed and the progress has been encouraging, such as SVMstruct (Tsochantaridis et al., 2004), conditional random fields (CRF) (Lafferty et al., 2001), maximum margin Markov networks (M3N) (Taskar et al., 2003) and so on.",
        "After years of intensive researches, Chinese word segmentation achieves a quite high precision.",
        "However, the performance of segmentation is not so satisfying for out-of-domain text.",
        "There are two domains in domain adaption problem, a source domain and a target domain.",
        "When we use the machine learning methods for",
        "Chinese word segmentation, we assume that training and test data are drawn from the same distribution.",
        "This assumption underlies both theoretical analysis and experimental evaluations oflearn-ing algorithms.",
        "However, the assumption does not hold for domain adaptation(Ben-David et al., 2007; Blitzer et al., 2006).",
        "The challenge is the difference of distribution between the source and target domains.",
        "In this paper, we use online margin maximization algorithm and domain invariant features for domain adaptive CWS.",
        "The online learning algorithm is Passive-Aggressive (PA) algo-rithm(Crammer et al., 2006), which passively accepts a solution whose loss is zero, while it aggressively forces the new prototype vector to stay as close as possible to the one previously learned.",
        "The rest of the paper is organized as follows.",
        "Section 2 introduces the related works.",
        "Then we describe our algorithm in section 3 and 4.",
        "The feature templates are described in section 5.",
        "Section 6 gives the experimental analysis.",
        "Section 7 concludes the paper."
      ]
    },
    {
      "heading": "2. Related Works",
      "text": [
        "There are several approaches to deal with the domain adaption problem.",
        "The first approach is to use semi-supervised learning (Zhu, 2005).",
        "The second approach is to incorporate supervised learning with domain invariant information.",
        "The third approach is to improve the present model with a few labeled domain data.",
        "Altun et al.",
        "(2006) investigated structured classification in a semi-supervised setting.",
        "They presented a discriminative approach that utilizes the intrinsic geometry of inputs revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables.",
        "Self-training (Zhu, 2005) is also a popular technology.",
        "In self-training a classifier is first trained with the small amount of labeled data.",
        "The classifier is then used to classify the unlabeled data.",
        "Typically the most confident unlabeled points, together with their predicted labels, are added to the training set.",
        "The classifier is retrained and the procedure repeated.",
        "Note the classifier uses its own predictions to teach itself.",
        "Yarowsky (1995) uses self-training for word sense disambiguation, e.g. deciding whether the word plant means a living organism or a factory in a given context.",
        "Zhao and Kit (2008) integrated unsupervised segmentation and CRF learning for Chinese word segmentation and named entity recognition.",
        "They found word accessory variance (Feng et al., 2004) is useful to CWS."
      ]
    },
    {
      "heading": "3. Online Passive-Aggressive Algorithm",
      "text": [
        "Sequence labeling, the task of assigning labels y = yi,...,yL to an input sequence x =",
        "Give a sample (x, y), we define the feature is $(x, y).",
        "Thus, we can label x with a score function, where w is the parameter of function F(•).",
        "The score function of our algorithm is linear function.",
        "Given an example (x, y), y is denoted as the incorrect label with the highest score,",
        "7 (w; (x, y)) = wT $(x, y) - wT $(x, y).",
        "(3) Thus, we calculate the hinge loss.",
        "We use the online PA learning algorithm to learn the weights of features.",
        "In round t, we find new weight vector wt+1 by where C is a positive parameter which controls the influence of the slack term on the objective function.",
        "The algorithms goal is to achieve a margin at least 1 as often as possible, thus the Hamming loss is also reduced indirectly.",
        "On rounds where the algorithm attains a margin less than 1 it suffers an instantaneous loss.",
        "We abbreviate ^(wt; (x,y)) to lt.",
        "If lt = 0 then wt itself satisfies the constraint in Eq.",
        "(5) and is clearly the optimal solution.",
        "We therefore concentrate on the case where lt > 0.",
        "First, we define the Lagrangian of the optimization problem in Eq.",
        "(5) to be where a,ß is a Lagrange multiplier.",
        "Setting the partial derivatives of L with respect to the elements of £ to zero gives",
        "The gradient of w should be zero,",
        "Differentiate with a, and set it to zero, we get",
        "Finally, we get update strategy,",
        "Our final algorithm is shown in Algorithm 1.",
        "In order to avoiding overfitting, the averaging technology is employed.",
        "Algorithm 1: Labelwise Margin Maximization Algorithm"
      ]
    },
    {
      "heading": "4. Inference",
      "text": [
        "The PA algorithm is used to learn the weights of features in training procedure.",
        "In inference procedure, we use Viterbi algorithm to calculate the maximum score label.",
        "Let be the best score of the partial label sequence ending with yn.",
        "The idea of the Viterbi algorithm is to use dynamic programming to compute",
        "Using this recursive definition, we can evaluate u)(N) for all yN, where N is the input length.",
        "This results in the identification of the best label sequence.",
        "The computational cost of the Viterbi algorithm is O(NL), where L is the number of labels."
      ]
    },
    {
      "heading": "5. Feature Templates",
      "text": [
        "All feature templates used in this paper are shown in Table 1.",
        "C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character, whose subscript is 0.",
        "T represents the character-based tag: \"B\", \"B2\", \"B3\", \"M\", \"E\" and \"S\", which represent the beginning, second, third, middle, end or single character of a word respectively.",
        "The type of character includes: digital, letter, punctuation and other.",
        "We also use the word accessor variance for domain adaption.",
        "Word accessor variance (AV) was proposed by (Feng et al., 2004) and was used to evaluate how independently a string is used, and thus how likely it is that the string can be a word.",
        "The accessor variety of a string s of more than one character is defined as",
        "Lav (s) is called the left accessor variety and is defined as the number of distinct characters (predecessors) except \"S\" that precede s plus the number of distinct sentences of which s appears at the beginning.",
        "Similarly, the right accessor variety Rav (s) is defined as the number of distinct characters (successors) except \"E\" that succeed s plus the number of distinct sentences in which s appears at the end.",
        "The characters \"S\" and \"E\" are defined as the begin and end of a sentence.",
        "The word accessor variance was found effective for CWS with unsegmented text (Zhao and Kit,",
        "2008).",
        "Tc: Type of Character AV : word accessor variance"
      ]
    },
    {
      "heading": "6. CIPS-SIGHAN-2010 Bakeoff",
      "text": [
        "CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation focused on the cross-domain performance of Chinese word segmentation algorithms.",
        "There are two subtasks for this evaluation:",
        "(1) Word Segmentation for Simplified Chinese Text; (2) Word Segmentation for Traditional Chinese",
        "Text.",
        "The test corpus of each subtask covers four domains: literature, computer science, medicine and finance.",
        "We participate in closed training evaluation of both subtasks.",
        "Firstly, we calculate the word accessor variance AVL(s)of the continuous string s from labeled corpus.",
        "Here, we set the largest length of string s to be 4.",
        "Secondly, we train our model with feature temples and AVL(s).",
        "Thirdly, when we process the different domain unlabeled corpus, we recalculate the word accessory variance AVu (s) from the corresponding corpus.",
        "Fourthly, we segment the domain corpus with new word accessory variance AVu(s) instead of AVl(s).",
        "The results are shown in Table 2 and 3.",
        "The results show our method has a poor performance in OOV ( Out-Of-Vocabulary) word.",
        "The running environment is shown in Table 4.",
        "We set the max iterative number is 20.",
        "Our running time is shown in Table 5.",
        "\"s\" represents second, \"chars\" is the number of Chinese character, and \"MB\" is the megabyte.",
        "In practice, we found the system can achieve the same performance after 7 loops.",
        "Therefore, we just need less half the time in Table 5 actually."
      ]
    },
    {
      "heading": "7. Conclusion",
      "text": [
        "In this paper, we describe our system in CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation.",
        "Although our method just achieve a consequence of being average and not outstanding, it has an advantage of faster training than other batch learning algorithm, such as CRF and",
        "M3N.",
        "In the future, we wish to improve our method in the following aspects.",
        "Firstly, we will investigate more effective domain invariant feature representation.",
        "Secondly, we will integrate our algorithm with self-training and other semi-supervised learning methods."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was (partially) funded by 863 Program (No.",
        "2009AA01A346), 973 Program (No.",
        "2010CB327906), and Shanghai Science and Technology Development Funds (No.",
        "08511500302).",
        "OS",
        "Win 2003",
        "CPU",
        "Intel Xeon 2.0G",
        "Memory",
        "4G",
        "R",
        "P",
        "F1",
        "OOVRR",
        "IV RR",
        "Literature",
        "Best",
        "0.945",
        "0.946",
        "0.946",
        "0.816",
        "0.954",
        "Our",
        "0.915",
        "0.925",
        "0.92",
        "0.577",
        "0.94",
        "Computer",
        "Best",
        "0.953",
        "0.95",
        "0.951",
        "0.827",
        "0.975",
        "Our",
        "0.934",
        "0.919",
        "0.926",
        "0.739",
        "0.969",
        "Medicine",
        "Best",
        "0.942",
        "0.936",
        "0.939",
        "0.75",
        "0.965",
        "Our",
        "0.927",
        "0.924",
        "0.925",
        "0.714",
        "0.953",
        "Finance",
        "Best",
        "0.959",
        "0.96",
        "0.959",
        "0.827",
        "0.972",
        "Our",
        "0.94",
        "0.942",
        "0.941",
        "0.719",
        "0.961",
        "R",
        "P",
        "F1",
        "OOVRR",
        "IV RR",
        "Literature",
        "Best",
        "0.942",
        "0.942",
        "0.942",
        "0.788",
        "0.958",
        "Our",
        "0.869",
        "0.91",
        "0.889",
        "0.698",
        "0.887",
        "Computer",
        "Best",
        "0.948",
        "0.957",
        "0.952",
        "0.666",
        "0.977",
        "Our",
        "0.933",
        "0.949",
        "0.941",
        "0.791",
        "0.948",
        "Medicine",
        "Best",
        "0.953",
        "0.957",
        "0.955",
        "0.798",
        "0.966",
        "Our",
        "0.908",
        "0.932",
        "0.92",
        "0.771",
        "0.919",
        "Finance",
        "Best",
        "0.964",
        "0.962",
        "0.963",
        "0.812",
        "0.975",
        "Our",
        "00.925",
        "0.939",
        "0.932",
        "0.793",
        "0.935",
        "Training",
        "Task",
        "A",
        "B",
        "C",
        "D",
        "Simp",
        "817.2s",
        "795.6s",
        "774.0s",
        "792.0s",
        "Trad",
        "903.6s",
        "889.2s",
        "885.6s",
        "874.8s",
        "Test",
        "20327 chars/s, or 17.97 s/MB"
      ]
    }
  ]
}
