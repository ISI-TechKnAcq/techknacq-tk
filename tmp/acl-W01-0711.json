{
  "info": {
    "authors": [
      "Remi Zajac"
    ],
    "book": "Workshop on Computational Natural Language Learning CoNLL",
    "id": "acl-W01-0711",
    "title": "Morpholog: Constrained and Supervised Learning of Morphology",
    "url": "https://aclweb.org/anthology/W01-0711",
    "year": 2001
  },
  "references": [
    "acl-C86-1069",
    "acl-C90-2074",
    "acl-C96-1018",
    "acl-C96-2198",
    "acl-E89-1020",
    "acl-E91-1019",
    "acl-J01-1003",
    "acl-P84-1070",
    "acl-P85-1009",
    "acl-P97-1057",
    "acl-W98-1310"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Many corpus-based natural language processing systems rely on using large quantities of annotated text as their training examples.",
        "Building this kind of resource is an expensive and labor-intensive project.",
        "To minimize effort spent on annotating examples that are not helpful the training process, recent research efforts have begun to apply active learning techniques to selectively choose data to be annotated.",
        "In this work, we consider selecting training examples with the tree-entropy metric.",
        "Our goal is to assess how well this selection technique can be applied for training different types of parsers.",
        "We find that tree-entropy can significantly reduce the amount of training annotation for both a history-based parser and an EM-based parser.",
        "Moreover, the examples selected for the history-based parser are also good for training the EM-based parser, suggesting that the technique is parser independent."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In recent years, large collections of text in machine readable format have become readily This material is based upon work supported by the National Science Foundation under Grant No.",
        "IRI 9712068 and DARPA contract N6600197C8540.",
        "We thank Michael Collins for the use of his parser; and Ric Crabbe and the anonymous reviewers for their comments on the paper.",
        "available.",
        "These ought be valuable resources for training natural language processing system.",
        "Unfortunately, most systems cannot take advantage of the data in their raw text form; typically, the data must be annotated by a human to become effective training examples.",
        "For instance, consider the task of inducing a grammar to parse English sentences.",
        "Studies have shown that a grammar trained on sentences annotated with their constituent trees produces much better parses than one trained on just the sentences alone (Pereira and Schabes, 1992).",
        "Recent state-of-the-art parsers developed by Collins (1997) and Charniak (1999) are all trained from hand-annotated corpora such as those from the Penn Treebank Project (Marcus et al., 1993).",
        "However, building an annotated corpus is a human labor-intensive project; therefore, it is important to find ways to minimize the size of the corpus.",
        "Out of a large pool of raw text, what subset should be annotated and added to the training set?",
        "Recent studies have begun to address this question using sample selection, in which training process is seen as interactive session between the learning system and the human annotator (Lewis and Gale, 1994), (Engelson and Dagan, 1996), (Fujii et al., 1995), (Thompson et al., 1999), and (Ngai and Yarowsky, 2000).",
        "The system actively influences its learning progress by evaluating potential candidates from the pool of raw text and selecting those with high Training Utility Values (TUV) for humans to annotate.",
        "As the learning process continues, the system should become better at identifying good training candidates so that the annotators would not need to waste time on processing uninformative examples.",
        "This work considers the problem of applying sample selection techniques to the task of training statistical parsers.",
        "Our primary challenge is in designing a function that can accurately estimate an unlabeled candidate's potential utility for training a parser.",
        "In a previous study (Hwa, 2000b), we have applied sample selection to an induction algorithm based on the expectation-maximization (EM) principle that induces Probabilistic Lexicalized Tree Insertion Grammars (PLTIGs).",
        "In that work, we proposed an uncertainty-based evaluation function to estimate the TLTV of unlabeled candidates called tree entropy.",
        "We have empirically shown that sample selection with tree entropy can reduce the size of the training corpus significantly.",
        "However, because only an EM-based learner was used, it is unknown whether the evaluation function would be general enough to be applicable to other types of learners.",
        "The goal of this work is to assess the robustness of the tree-entropy evaluation function.",
        "We have performed experiments to evaluate how well the metric selects training examples for different types of parsers and to determine whether examples selected for one type of parser might be good for training a different type of parser.",
        "Our experimental results show that the tree-entropy metric can reduce the amount of training annotation by 23% for a history-based lexical statistical parser, the Model 2 parser described by Collins (1997).",
        "Moreover, we found that the data selected for training the Collins Parser also make good training examples for inducing the EM-based PLTIG parser, suggesting that the tree-entropy evaluation function is parser independent."
      ]
    },
    {
      "heading": "2 The Learning Framework",
      "text": [
        "There are two types of sample selection algorithms committee based or single learner.",
        "A committee-based selection algorithm works with multiple learners, each maintaining a different hypothesis (perhaps pertaining to different aspects of the problem).",
        "The candidate examples that lead to the most disagreements U is a set of unlabeled candidates.",
        "L is a set of labeled training examples.",
        "ll is the current model.",
        "among the different learners are considered to have the highest TLTV.",
        "(Cohn et al., 1994; Fre-und et al., 1997).",
        "For computationally intensive problems, keeping multiple learners may be impractical.",
        "In this work, we focus on sample selection algorithms that use only a single learner that keeps just one working hypothesis.",
        "Without access to multiple hypotheses, the selection algorithm can nonetheless estimate the TLTV of an example.",
        "We categorize some possible ranking criteria into the following three classes: Problem-space: Knowledge about the problem-space may help to locate good training canidates.",
        "For example, knowing the distribution of the pool, we might select the most frequently occuring instances.",
        "Performance of the hypothesis: Testing the candidates on the current hypothesis may show the type of data on which the hypothesis performs weakly (Lewis and Catlett, 1994).",
        "Parameters of the hypothesis: Estimating the potential impact of the candidates will have on the parameters of the current working hypothesis locates those examples that will change the current hypothesis the most.",
        "Figure 1 outlines the single-learner sample selection training loop in pseudo-code.",
        "Initially, the training set, L, consists of a small number of labeled examples.",
        "The learner uses L to train an initial model ll.",
        "Also available to the learner is a large pool of unlabeled training candidates, U.",
        "In each iteration, the selection algorithm, Select(n, U, ll, f), uses an evaluation function f to compute the expected TUV of each candidate in U and returns the n candidates with the highest values.",
        "The set of the n chosen candidates are then labeled by human experts and added to the existing training set.",
        "Training on the updated set L, the system modifies the model so that it is consistent with all the examples seen thus far.",
        "The loop continues until one of the stopping conditions is met: the model is considered to be good enough, all candidates are labeled, or all human resources are used up."
      ]
    },
    {
      "heading": "2.1 The Evaluation Function",
      "text": [
        "At the heart of the sample selection algorithm is the evaluation function that predicts each unlabeled candidate's training utility.",
        "Our proposed function ranks candidates based on the \"performance of hypothesis.\" In other words, we wish to find the set of sentences that the current parsing model is the most uncertain about.",
        "One way to measure the parser's uncertainty is to compute the tree entropy over the distribution of parsing probabilities of the set of trees produced by the parser.",
        "More specifically, the tree entropy for a sentence u is:",
        "where T is the set of possible trees that ll generated for u.",
        "Details of computing tree entropy have been discussed previously (Hwa, 2000b).",
        "Our proposed function evaluates each candidate by measuring the similarity between the tree entropy of the candidate and the uniform distribution for the same number of trees.",
        "That is,"
      ]
    },
    {
      "heading": "2.2 Parsing Models",
      "text": [
        "To test the robustness of the tree-entropy evaluation function, we use it to select training examples for the Collins Parser and the PLTIG parser.",
        "Although both are lexical-ized and statistical parsers, their learning algorithms are different.",
        "The Collins Parser is a fully-supervised, history-based learner that models the parameters of the parser by taking statistics directly from the training data.",
        "In contrast, PLTIG's EM-based induction algorithm (Hwa, 2000a) is partially-supervised; the model's parameters are estimated indirectly from the training data.",
        "Our goal for this study is to determine whether the success of the tree-entropy metric is learner dependent."
      ]
    },
    {
      "heading": "3 Experimental Setup and Results",
      "text": [
        "Two experiments are performed.",
        "The first experiment assesses whether the tree-entropy evaluation function can select good examples for a history-based learner.",
        "The second experiment is a preliminary study on whether the examples selected for a history-based learner are also good training examples for a EM-based learner."
      ]
    },
    {
      "heading": "3.1 Experiment 1",
      "text": [
        "We use the Collins Parser as the basic learning model ll in the sample selection framework described in Figure 1.",
        "To simulate the interactive process, we create a large unlabeled candidate pool U by stripping all annotated information from sections 02 through 21 of the Wall Street Journal corpus.",
        "Initially, L, the set of labeled training data, consists of 500 parsed sentences.",
        "In each iteration, n = 1000 new sentences are picked from U to be added to L. Then, a new parser is trained from the updated L and tested on section 00 to chart the learning progress.",
        "We compare the learning rate of the parser trained on examples selected by the tree entropy evaluation function, ft, with a baseline in which the model was trained with examples sequentially selected.",
        "The experimental results are graphed in Figure 2(a).",
        "The"
      ]
    }
  ]
}
