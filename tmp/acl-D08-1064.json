{
  "info": {
    "authors": [
      "David Chiang",
      "Steve DeNeefe",
      "Yee Seng Chan",
      "Hwee Tou Ng"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-D08-1064",
    "title": "Decomposability of Translation Metrics for Improved Evaluation and Efficient Algorithms",
    "url": "https://aclweb.org/anthology/D08-1064",
    "year": 2008
  },
  "references": [
    "acl-E06-1032",
    "acl-N03-2021",
    "acl-N04-1022",
    "acl-N04-1035",
    "acl-P05-1066",
    "acl-P06-1096",
    "acl-P06-1121",
    "acl-P08-1007",
    "acl-W04-3250",
    "acl-W05-0909",
    "acl-W07-0414",
    "acl-W07-0718",
    "acl-W07-0734",
    "acl-W07-0738",
    "acl-W08-0301"
  ],
  "sections": [
    {
      "text": [
        "David Chiang and Steve DeNeefe",
        "Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 USA",
        "Yee Seng Chan and Hwee Tou Ng",
        "Bleu is the de facto standard for evaluation and development of statistical machine translation systems.",
        "We describe three real-world situations involving comparisons between different versions of the same systems where one can obtain improvements in Bleu scores that are questionable or even absurd.",
        "These situations arise because Bleu lacks the property of decomposability, a property which is also computationally convenient for various applications.",
        "We propose a very conservative modification to Bleu and a cross between Bleu and word error rate that address these issues while improving correlation with human judgments."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Bleu (Papineni et al., 2002) was one of the first automatic evaluation metrics for machine translation (MT), and despite being challenged by a number of alternative metrics (Melamed et al., 2003; Baner-jee and Lavie, 2005; Snover et al., 2006; Chan and Ng, 2008), it remains the standard in the statistical MT literature.",
        "Callison-Burch et al.",
        "(2006) have subjected Bleu to a searching criticism, with two real-world case studies of significant failures of correlation between Bleu and human adequacy/fluency judgments.",
        "Both cases involve comparisons between statistical MT systems and other translation methods (human post-editing and a rule-based MT system), and they recommend that the use of Bleu be restricted to comparisons between related systems or different versions of the same systems.",
        "In Bleu's defense, comparisons between different versions of the same system were exactly what Bleu was designed for.",
        "However, we show that even in such situations, difficulties with Bleu can arise.",
        "We illustrate three ways that properties of Bleu can be exploited to yield improvements that are questionable or even absurd.",
        "All of these scenarios arose in actual practice and involve comparisons between different versions of the same statistical MT systems.",
        "They can be traced to the fact that Bleu is not decomposable at the sentence level: that is, it lacks the property that improving a sentence in a test set leads to an increase in overall score, and degrading a sentence leads to a decrease in the overall score.",
        "This property is not only intuitive, but also computationally convenient for various applications such as translation reranking and discriminative training.",
        "We propose a minimal modification to Bleu that reduces its nondecomposability, as well as a cross between Bleu and word error rate (WER) that is decomposable down to the subsentential level (in a sense to be made more precise below).",
        "Both metrics correct the observed problems and correlate with human judgments better than Bleu."
      ]
    },
    {
      "heading": "2. The Bleu metric",
      "text": [
        "Let gk(w) be the multiset of all k-grams of a sentence w. We are given a sequence of candidate translations c to be scored against a set of sequences of reference translations, {rj} = r,..., rR:",
        "rR = rR rR rR rR",
        "Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 610-619, Honolulu, October 2008.",
        "©2008 Association for Computational Linguistics",
        "Then the Bleu score of c is defined to be is the k-gram precision of c with respect to {rj}, and bp(c, r), known as the brevity penalty, is defined as follows.",
        "Let <p(x) = exp(1 - 1/x).",
        "In the case of a single reference r,",
        "In the multiple-reference case, the length |ri| is replaced with an effective reference length, which can be calculated in several ways.",
        "• In the original definition (Papineni et al., 2002), it is the length of the reference sentence whose length is closest to the test sentence.",
        "• In the NIST definition, it is the length of the shortest reference sentence.",
        "• A third possibility would be to take the average length of the reference sentences.",
        "The purpose of the brevity penalty is to prevent a system from generating very short but precise translations, and the definition of effective reference length impacts how strong the penalty is.",
        "The NIST definition is the most tolerant of short translations and becomes more tolerant with more reference sentences.",
        "The original definition is less tolerant but has the counterintuitive property that decreasing the length of a test sentence can eliminate the brevity penalty.",
        "Using the average reference length seems attractive but has the counterintuitive property that an exact match with one of the references may not get a 100% score.",
        "Throughout this paper we use the NIST definition, as it is currently the definition most used in the literature and in evaluations.",
        "The brevity penalty can also be seen as a standin for recall.",
        "The fraction #7-7 in the definition of the brevity penalty (3) indeed resembles a weak recall score in which every guessed item counts as a match.",
        "However, with recall, the per-sentence score would never exceed unity, but with the brevity penalty, it can.",
        "This means that if a system generates a long translation for one sentence, it can generate a short translation for another sentence without facing a penalty.",
        "This is a serious weakness in the Bleu metric, as we demonstrate below using three scenarios, encountered in actual practice."
      ]
    },
    {
      "heading": "3. Exploiting the Bleu metric 3.1 The sign test",
      "text": [
        "We are aware of two methods that have been proposed for significance testing with Bleu: bootstrap resampling (Koehn, 2004b; Zhang et al., 2004) and the sign test (Collins et al., 2005).",
        "In bootstrap resampling, we sample with replacement from the test set to synthesize a large number of test sets, and then we compare the performance of two systems on those synthetic test sets to see whether one is better 95% (or 99%) of the time.",
        "But Collins et al.",
        "(2005) note that it is not clear whether the conditions required by bootstrap resampling are met in the case of Bleu, and recommend the sign test instead.",
        "Suppose we want to determine whether a set of outputs c from a test system is better or worse than a set of baseline outputs b.",
        "The sign test requires a function f(bi, ci) that indicates whether ci is a better, worse, or same-quality translation relative to bi.",
        "However, because Bleu is not defined on single sentences, Collins et al.",
        "use an approximation: for each i, form a composite set of outputs b' = {b1,..., bi-1, ci, bi+1,..., bN}, and compare the Bleu scores of b and b'.",
        "The goodness of this approximation depends on to what extent the comparison between b and b' is dependent only on bi and ci, and independent of the other sentences.",
        "However, Bleu scores are highly context-dependent: for example, if the sentences in b are on average e words longer than the reference sentences, then ci can be as short as (N - 1)e words shorter than ri without incurring the brevity penalty.",
        "Moreover, since the ci are substituted in one at a time, we can do this for all of the ci.",
        "Hence, c could have a disastrously low Bleu score (because of the brevity penalty) yet be found by the sign test to be significantly better than the baseline.",
        "We have encountered this situation in practice: two versions of the same system with Bleu scores of 29.6 (length ratio 1.02) and 29.3 (length ratio 0.97), where the sign test finds the second system to be significantly better than the first (and the first system significantly better than the second).",
        "Clearly, in order for a significance test to be sensible, it should not contradict the observed scores, and should certainly not contradict itself.",
        "In the rest of this paper, except where indicated, all significance tests are performed using bootstrap resampling.",
        "For several years, much statistical MT research has focused on translating newswire documents.",
        "One likely reason is that the DARPA TIDES program used newswire documents for evaluation for several years.",
        "But more recent evaluations have included other genres such as weblogs and conversation.",
        "The conventional wisdom has been that if one uses a single statistical translation system to translate text from several different genres, it may perform poorly, and it is better to use several systems optimized separately for each genre.",
        "However, if our task is to translate documents from multiple known genres, but they are evaluated together, the Bleu metric allows us to use that fact to our advantage.",
        "To understand how, notice that our system has an optimal number of words that it should generate for the entire corpus: too few and it will be penalized by Bleu's brevity penalty, and too many increases the risk of additional non-matching k-grams.",
        "But these words can be distributed among the sentences (and genres) in any way we like.",
        "Instead of translating sentences from each genre with the best genre-specific systems possible, we can generate longer outputs for the genre we have more confidence in, while generating shorter outputs for the harder genre.",
        "This strategy will have mediocre performance on each individual genre (according to both intuition and Bleu), yet will receive a higher Bleu score on the combined test set than the combined systems optimized for each genre.",
        "In fact, knowing which sentence is in which genre is not even always necessary.",
        "In one recent task, we translated documents from two different genres, without knowing the genre of any given sentence.",
        "The easier genre, newswire, also tended to have shorter reference sentences (relative to the source sentences) than the harder genre, weblogs.",
        "For example, in one dataset, the newswire reference sets had between 1.3 and 1.37 English words per Arabic word, but the weblog reference set had 1.52 English words per Arabic word.",
        "Thus, a system that is uniformly verbose across both genres will apportion more of its output to newswire than to weblogs, serendipitously leading to a higher score.",
        "This phenomenon has subsequently been observed by Och (2008) as well.",
        "We trained three Arabic-English syntax-based statistical MT systems (Galley et al., 2004; Galley et al., 2006) using max-Bleu training (Och, 2003): one on a newswire development set, one on a weblog development set, and one on a combined development set containing documents from both genres.",
        "We then translated a new mixed-genre test set in two ways: (1) each document with its appropriate genre-specific system, and (2) all documents with the system trained on the combined (mixed-genre) development set.",
        "In Table 3, we report the results of both approaches on the entire test dataset as well as the portion of the test dataset in each genre, for both the genre-specific and mixed-genre trainings.",
        "The genre-specific systems each outperform the mixed system on their own genre as expected, but when the same results are combined, the mixed system's output is a full Bleu point higher than the combination of the genre-specific systems.",
        "This is because the mixed system produces outputs that have about 1.35 English words per Arabic word on average: longer than the shortest newswire references, but shorter than the weblog references.",
        "The mixed system does worse on each genre but better on the combined test set, whereas, according to intuition, a system that does worse on the two subsets should also do worse on the combined test set.",
        "A third way to take advantage of the Bleu metric is to permit an MT system to delete arbitrary words in the input sentence.",
        "We can do this by introducing new phrases or rules into the system that match words in the input sentence but generate no output; to these rules we attach a feature whose weight is tuned during max-Bleu training.",
        "Such rules have been in use for some time but were only recently discussed by Li et al.",
        "(2008).",
        "When we add word-deletion rules to our MT system, we find that the Bleu increases significantly (Table 6, line 2).",
        "Figure 1 shows some examples of deletion in Chinese-English translation.",
        "The first sentence has a proper name, jt«[[/maigesaisai 'Magsaysay', which has been mistokenized into four tokens.",
        "The baseline system attempts to translate the first two phonetic characters as \"wheat Georgia,\" whereas the other system simply deletes them.",
        "On the other hand, the second sentence shows how word deletion can sacrifice adequacy for the sake of fluency, and the third sentence shows that sometimes word deletion removes words that could have been translated well (as seen in the baseline translation).",
        "Does Bleu reward word deletion fairly?",
        "We note two reasons why word deletion might be desirable.",
        "First, some function words should truly be deleted: for example, the Chinese particle „/de and Chinese measure words often have no counterpart in English (Li et al., 2008).",
        "Second, even content word deletion might be helpful if it allows a more fluent translation to be assembled from the remnants.",
        "We observe that in the above experiment, word deletion caused the absolute number of k-gram matches, and not just k-gram precision, to increase for all 1 < k < 4.",
        "Human evaluation is needed to conclusively determine whether Bleu rewards deletion fairly.",
        "But to control for these potentially positive effects of deletion, we tested a sentence-deletion system, which is the same as the word-deletion system but constrained to delete all of the words in a sentence or none of them.",
        "This system (Table 6, line 3) deleted 8-10% of its input and yielded a Bleu score with no significant decrease (p > 0.05) from the baseline system s. Given that our model treats sentences independently, so that it cannot move information from one sentence to another, we claim that deletion of nearly 10% of the input is a grave translation deficiency, yet Bleu is insensitive to it.",
        "What does this tell us about word deletion?",
        "While acknowledging that some word deletions can improve translation quality, we suggest in addition that because word deletion provides a way for the system to translate the test set selectively, a behavior which we have shown that Bleu is insensitive to, part of the score increase due to word deletion is likely an artifact of Bleu."
      ]
    },
    {
      "heading": "4. Other metrics",
      "text": [
        "Are other metrics susceptible to the same problems as the Bleu metric?",
        "In this section we examine several other popular metrics for these problems, propose two of our own, and discuss some desirable characteristics for any new MT evaluation metric.",
        "We ran a suite of other metrics on the above problem cases to see whether they were affected.",
        "In none of these cases did we repeat minimum-error-rate training; all these systems were trained using max-BLEU.",
        "The metrics we tested were:",
        "• METEOR (Banerjee and Lavie, 2005), version 0.6, using the exact, Porter-stemmer, and WordNet synonmy stages, and the optimized parameters a = 0.81, ß = 0.83, y = 0.28 as reported in (Lavie and Agarwal, 2007).",
        "WMT 2007 shared task (Callison-Burch et al., 2007).",
        "• MaxSim (Chan and Ng, 2008), more specifically MAxSiMn, which skips the dependency relations.",
        "On the sign test (Table 2), all metrics found significant differences consistent with the difference in score between the two systems.",
        "The problem related to genre-specific training does not seem to affect the other metrics (see Table 4), but they still manifest the unintuitive result that genre-specific training is sometimes worse than mixed-genre training.",
        "Finally, all metrics but GTM disfavored both word deletion and sentence deletion (Table 7).",
        "A very conservative way of modifying the Bleu metric to combat the effects described above is to imbaseline fei xiaotong was awarded the wheat georgia xaixai prize delete fei xiaotong was awarded xaixai award reference the center of the yuhua stone bears an image which very much resembles the territory",
        "of the people s republic of china .",
        "baseline rain huashi center is a big clear images of chinese territory .",
        "delete rain is a clear picture of the people s republic of china .",
        "reference urban construction becomes new hotspot for foreign investment in qinghai baseline urban construction become new hotspot for foreign investment qinghai delete become new foreign investment hotspot",
        "Figure 1: Examples of word deletion.",
        "Underlined Chinese words were deleted in the word-deletion system; underlined English words correspond to deleted Chinese words.",
        "pose a stricter brevity penalty.",
        "In Section 2, we presented the brevity penalty as a stand-in for recall, but noted that unlike recall, the per-sentence score",
        "can exceed unity.",
        "This suggests the simple fix of clipping the per-sentence recall scores in a similar fashion to the clipping of precision scores:",
        "Then if a translation system produces overlong translations for some sentences, it cannot use those translations to license short translations for other sentences.",
        "Call this revised metric Bleu-sbp (for Bleu with strict brevity penalty).",
        "We can test this revised definition on the problem cases described above.",
        "Table 2 shows that Bleu-sbp resolves the inconsistency observed between Bleu and the sign test, using the example test sets from Section 3.1 (no max-Bleu-sbp training was performed).",
        "Table 5 shows the new scores of the mixed-genre example from Section 3.2 after max-Bleu-sbp training.",
        "These results fall in line with intuition – tuning separately for each genre leads to slightly better scores in all cases.",
        "Finally, Table 8 shows the Bleu-sbp scores for the word-deletion example from Section 3.3, using both max-Bleu training and max-Bleu-sbp training.",
        "We see that Bleu-sbp reduces the benefit of word deletion to an insignificant level on the test set, and severely punishes sentence deletion.",
        "When we retrain using max-Bleu-sbp, the rate of word deletion is reduced and sentence deletion is all but eliminated, and there are no significant differences on the test set.",
        "All of the problems we have examined – except for word deletion – are traceable to the fact that Bleu is not a sentence-level metric.",
        "Any metric which is defined as a weighted average of sentence-level scores, where the weights are system-independent, will be immune to these problems.",
        "Note that any metric involving micro-averaged precision (in which the sentence-level counts of matches and guesses are summed separately before forming their ratio) cannot have this property.",
        "Of the metrics surveyed in the WMT 2007 evaluation-evaluation (Callison-Burch et al., 2007), at least the following metrics have this property: WER (Nießen et al., 2000), TER (Snover et al., 2006), and ParaEval-Recall (Zhou et al., 2006).",
        "Moreover, this evaluation concern dovetails with a frequent engineering concern, that sentence-level scores are useful at various points in the MT pipeline: for example, minimum Bayes risk decoding (Kumar and Byrne, 2004), selecting oracle translations for discriminative reranking (Liang et al., 2006; Watanabe et al., 2007), and sentence-by-sentence comparisons of outputs during error analysis.",
        "A variation on Bleu is often used for these purposes, in which the k-gram precisions are \"smoothed\" by adding one to the numerator and denominator (Lin and Och, 2004); this addresses the problem of a zero k-gram match canceling out the entire score, but it does not address the problems illustrated above.",
        "The remaining issue, word deletion, is more difficult to assess.",
        "It could be argued that part of the gain due to word deletion is caused by Bleu allowing a system to selectively translate those parts of a sentence on which higher precision can be obtained.",
        "It would be difficult indeed to argue that an evaluation metric, in order to be fair, must be decomposable into subsentential scores, and we make no such claim.",
        "However, there is again a dovetailing engineering concern which is quite legitimate.",
        "If one wants to select the minimum-Bayes-risk translation from a lattice (or shared forest) instead of an n-best list (Tromble et al., 2008), or to select an oracle translation from a lattice (Tillmann and Zhang, 2006; Dreyer et al., 2007; Leusch et al., 2008), or to perform discriminative training on all the examples contained in a lattice (Taskar et al., 2004), one would need a metric that can be calculated on the edges of the lattice.",
        "Of the metrics surveyed in the WMT 2007 evaluation-evaluation, only one metric, to our knowledge, has this property: word error rate (Nießen et al., 2000).",
        "Here, we deal with the related word recognition rate (McCowan et al., 2005), where I is the number of insertions, D of deletions, S of substitutions, and M = |r| - D - S the number of matches.",
        "The dynamic program for WRR can be formulated as a Viterbi search through a finite-state automaton: given a candidate sentence c and a reference sentence r, find the highest-scoring path matching c through the automaton with states 0,..., |r|, initial state 0, final state |r|, and the following transitions (a * matches any symbol):",
        "i-> i insertion",
        "This automaton can be intersected with a typical stack-based phrase-based decoder lattice (Koehn, 2004a) or CKY-style shared forest (Chiang, 2007) in much the same way that a language model can, yielding a polynomial-time algorithm for extracting the best-scoring translation from a lattice or forest (Wagner, 1974).",
        "Intuitively, the reason for this is that WRR, like most metrics, implicitly constructs a word alignment between c and r and only counts matches between aligned words; but unlike other metrics, this alignment is constrained to be monotone.",
        "We can combine WRR with the idea of k-gram matching in Bleu to yield a new metric, the 4-gram recognition rate:",
        "where Mk is the number of k-gram matches, a and ß control the penalty for insertions and deletions, and gk is as defined in Section 2.",
        "We presently set a = 1,ß = 0 by analogy with WRR, but explore other settings below.",
        "To calculate 4-GRR on a whole test set, we sum the numerators and denominators as in micro-averaged recall.",
        "The 4-GRR can also be formulated as a finite-state automaton, with states {(i, m) | 0 < i < |r|, 0 < m < 3}, initial state (0,0), final states (|r|, m), and the following transitions:",
        "Table 1: Our new metrics correlate with human judgments better than Bleu (case-sensitive).",
        "Adq = Adequacy, Flu = Fluency, Con = Constituent, Avg = Average.",
        "Therefore 4-GRR can also be calculated efficiently on lattices or shared forests.",
        "We did not attempt max-4-GRR training, but we evaluated the word-deletion test sets obtained by max-Bleu and max-Bleu-sbp training using 4-GRR.",
        "The results are shown in Table 7.",
        "In general, the results are very similar to Bleu-sbp except that 4-GRR sometimes scores word deletion slightly lower than baseline."
      ]
    },
    {
      "heading": "5. Correlation with human judgments",
      "text": [
        "The shared task of the 2007 Workshop on Statistical Machine Translation (Callison-Burch et al., 2007) was conducted with several aims, one of which was to measure the correlation of several automatic MT evaluation metrics (including Bleu) against human judgments.",
        "The task included two datasets (one drawn from the Europarl corpus and the other from the News Commentary corpus) and across three language pairs (from German, Spanish, and French to English, and back).",
        "In our experiments, we focus on the tasks where the target language is English.",
        "For human evaluations of the MT submissions, four different criteria were used:",
        "• Adequacy: how much of the meaning expressed in the reference translation is also expressed in the hypothesis translation.",
        "• Fluency: how well the translation reads in the target language.",
        "• Rank: each translation is ranked from best to worst, relative to the other translations of the same sentence.",
        "• Constituent: constituents are selected from source-side parse-trees, and human judges are asked to rank their translations.",
        "We scored the workshop shared task submissions with Bleu-sbp and 4-GRR, then converted the raw scores to rankings and calculated the Spearman correlations with the human judgments.",
        "Table 1 shows the results along with Bleu and the three metrics that achieved higher correlations than Bleu: semantic role overlap (Gimenez and Marquez, 2007), ParaE-val recall (Zhou et al., 2006), and METEOR (Baner-jee and Lavie, 2005).",
        "We find that both our proposed metrics correlate with human judgments better than Bleu does.",
        "However, recall the parameters a and ßin the definition of 4-GRR that control the penalty for inserted and deleted words.",
        "Experimenting with this parameter reveals that a = -0.9,ß = 1 yields a correlation of 78.9%.",
        "In other words, a metric that unboundedly rewards spuriously inserted words correlates better with human judgments than a metric that punishes them.",
        "We assume this is because there are not enough data points (systems) in the sample and ask that all these figures be taken with a grain of salt.",
        "As a general remark, it may be beneficial for human-correlation datasets to include a few straw-man systems that have very short or very long translations."
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "We have described three real-world scenarios involving comparisons between different versions of the same statistical MT systems where Bleu gives counterintuitive results.",
        "All these issues center around the issue of decomposability: the sign test fails because substituting translations one sentence at a time can improve the overall score yet substituting them all at once can decrease it; genre-specific training fails because improving the score of two halves of a test set can decrease the overall score; and sentence deletion is not harmful because generating empty translations for selected sentences does not necessarily decrease the overall score.",
        "We proposed a minimal modification to Bleu, called Bleu-sbp, and showed that it ameliorates these problems.",
        "We also proposed a metric, 4-GRR, that is decomposable at the sentence level and is therefore guaranteed to solve the sign test, genre-specific tuning, and sentence deletion problems; moreoever, it is decomposable at the subsentential level, which has potential implications for evaluating word deletion and promising applications to translation reranking and discriminative training.",
        "Sem.",
        "role overlap",
        "77.4",
        "83.9",
        "80.3",
        "74.1",
        "78.9",
        "ParaEval recall",
        "71.2",
        "74.2",
        "76.8",
        "79.8",
        "75.5",
        "METEOR",
        "70.1",
        "71.9",
        "74.5",
        "66.9",
        "70.9",
        "Bleu",
        "68.9",
        "72.1",
        "67.2",
        "60.2",
        "67.1",
        "WER",
        "51.0",
        "54.2",
        "34.5",
        "52.4",
        "48.0",
        "Bleu-sbp",
        "73.9",
        "76.7",
        "73.5",
        "63.4",
        "71.9",
        "4-GRR",
        "72.3",
        "75.5",
        "74.3",
        "64.2",
        "71.6"
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Our thanks go to Daniel Marcu for suggesting modifying the Bleu brevity penalty, and to Jonathan May and Kevin Knight for their insightful comments.",
        "This research was supported in part by DARPA grant HR0011-06-C-0022 under BBN Technologies subcontract 9500008412.",
        "none",
        "0",
        "35.8",
        "0",
        "37.1",
        "word",
        "5.3",
        "36.3+",
        "5.0",
        "37.3",
        "sentence",
        "0.02",
        "35.9",
        "0",
        "37.5"
      ]
    }
  ]
}
