{
  "info": {
    "authors": [
      "Xiaohua Liu",
      "Yitong Li",
      "Haocheng Wu",
      "Ming Zhou",
      "Furu Wei",
      "Yi Lu"
    ],
    "book": "ACL",
    "id": "acl-P13-1128",
    "title": "Entity Linking for Tweets",
    "url": "https://aclweb.org/anthology/P13-1128",
    "year": 2013
  },
  "references": [
    "acl-P10-1006",
    "acl-P11-1080",
    "acl-P12-1055"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We study the task of entity linking for tweets, which tries to associate each mention in a tweet with a knowledge base entry.",
        "Two main challenges of this task are the dearth of information in a single tweet and the rich entity mention variations.",
        "To address these challenges, we propose a collective inference method that simultaneously resolves a set of mentions.",
        "Particularly, our model integrates three kinds of similarities, i.e., mention-entry similarity, entry-entry similarity, and mention-mention similarity, to enrich the context for entity linking, and to address irregular mentions that are not covered by the entity-variation dictionary.",
        "We evaluate our method on a publicly available data set and demonstrate the effectiveness of our method."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Twitter is a widely used social networking service.",
        "With millions of active users and hundreds of millions of new published tweets every day1, it has become a popular platform to capture and transmit the human experiences of the moment.",
        "Many tweet related researches are inspired, from named entity recognition (Liu et al., 2012), topic detection (Mathioudakis and Koudas, 2010), clustering (Rosa et al., 2010), to event extraction (Grinev et al., 2009).",
        "In this work, we study the entity linking task for tweets, which maps each entity mention in a tweet to a unique entity, i.e., an entry ID of a knowledge base like Wikipedia.",
        "Entity",
        "linking task is generally considered as a bridge between unstructured text and structured machine-readable knowledge base, and represents a critical role in machine reading program (Singh et al., 2011).",
        "Entity linking for tweets is particularly meaningful, considering that tweets are often hard to read owing to its informal written style and length limitation of 140 characters.",
        "Current entity linking methods are built on top of a large scale knowledge base such asWikipedia.",
        "A knowledge base consists of a set of entities, and each entity can have a variation list2.",
        "To decide which entity should be mapped, they may compute: 1) the similarity between the context of a mention, e.g., a text window around the mention, and the content of an entity, e.g., the entity page of Wikipedia (Mihalcea and Csomai, 2007; Han and Zhao, 2009); 2) the coherence among the mapped entities for a set of related mentions, e.g, multiple mentions in a document (Milne and Witten, 2008; Kulkarni et al., 2009; Han and Zhao, 2010; Han et al., 2011).",
        "Tweets pose special challenges to entity linking.",
        "First, a tweet is often too concise and too noisy to provide enough information for similarity computing, owing to its short and grass root nature.",
        "Second, tweets have rich variations of named entities3, and many of them fall out of the scope of the existing dictionaries mined from Wikipedia (called OOV mentions hereafter).",
        "On 2Entity variation lists can be extracted from the entity resolution pages of Wikipedia.",
        "For example, the link ?http://en.wikipedia.org/wiki/Svm?",
        "will lead us to a resolution page, where ?Svm?",
        "are linked to entities like ?Space vector modulation?",
        "and ?Support vector machine?.",
        "As a result, ?Svm?",
        "will be added into the variation lists of ?Space vector modulation?",
        "and ?Support vector machine?",
        ", respectively.",
        "3According to Liu et al. (2012), on average a named entity has 3.3 different surface forms in tweets.",
        "the other hand, the huge redundancy in tweets offers opportunities.",
        "That means, an entity mention often occurs in many tweets, which allows us to aggregate all related tweets to compute mention-mention similarity and mention-entity similarity.",
        "We propose a collective inference method that leverages tweet redundancy to address those two challenges.",
        "Given a set of mentions, our model tries to ensure that similar mentions are linked to similar entities while pursuing the high total similarity between matched mention-entity pairs.",
        "More specifically, we define local features, including context similarity and edit distance, to model the similarity between a mention and an entity.",
        "We adopt in-link based similarity (Milne and Witten, 2008), to measure the similarity between entities.",
        "Finally, we introduce a set of features to compute the similarity between mentions, including how similar the tweets containing the mentions are, whether they come from the tweets of the same account, and their edit distance.",
        "Notably, our model can resolve OOV mentions with the help of their similar mentions.",
        "For example, for the OOVmention ?LukeBryanOnline?, our model can find similar mentions like ?TheLukeBryan?",
        "and ?LukeBryan?.",
        "Considering that most of its similar mentions are mapped to the American country singer ?Luke Bryan?, our model tends to link ?LukeBryanOnline?",
        "to the same entity.",
        "We evaluate our method on the public available data set shared by Meij et al. (2012)4.",
        "Experimental results show that our method outperforms two baselines, i.e., Wikify!",
        "(Mihalcea and Csomai, 2007) and system proposed by Meij et al. (2012).",
        "We also study the effectiveness of features related to each kind of similarity, and demonstrate the advantage of our method for OOV mention linkage.",
        "We summarize our contributions as follows.",
        "1.",
        "We introduce a novel collective inference method that integrates three kinds of similarities, i.e., mention-entity similarity, entity-entity similarity, and mention-mention similarity, to simultaneously map a set of tweet mentions to their proper entities.",
        "set, and show our method compares favorably with the baselines.",
        "Our paper is organized as follows.",
        "In the next section, we introduce related work.",
        "In Section 3, we give the formal definition of the task.",
        "In Section 4, we present our solution, including the framework, features related to different kinds of similarities, and the training and decoding procedures.",
        "We evaluate our method in Section 5.",
        "Finally in Section 6, we conclude with suggestions of future work."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Existing entity linking work can roughly be divided into two categories.",
        "Methods of the first category resolve one mention at each time, and mainly consider the similarity between a mention-entity pair.",
        "In contrast, methods of the second category take a set of related mentions (e.g., mentions in the same document) as input, and figure out their corresponding entities simultaneously.",
        "Examples of the first category include the first Web-scale entity linking system SemTag (Dill et al., 2003), Wikify!",
        "(Mihalcea and Csomai, 2007), and the recent work of Milne and Witten (2008).",
        "SemTag uses the TAP knowledge base5, and employs the cosine similarity with TF-IDF weighting scheme to compute the match degree between a mention and an entity, achieving an accuracy of around 82%.",
        "Wikify!",
        "identifies the important concepts in the text and automatically links these concepts to the corresponding Wikipedia pages.",
        "It introduces two approaches to define mention-entity similarity, i.e., the contextual overlap between the paragraph where the mention occurs and the corresponding Wikipedia pages, and a Naive Bayes classifier that predicts whether a mention should be linked to an entity.",
        "It achieves 80.69% F1 when two approaches are combined.",
        "Milne and Witten work on the same task of Wikify!, and also train a classifier.",
        "However, they cleverly use the",
        "knowledge base that contains a broad range of lexical and taxonomic information about popular objects like music, movies, authors, sports, autos, health, etc.",
        "links found within Wikipedia articles for training, exploiting the fact that for every link, aWikipedian has manually selected the correct destination to represent the intended sense of the anchor.",
        "Their method achieves an F1 score of 75.0%.",
        "Representative studies of the second category include the work of Kulkarni et al. (2009), Han et al. (2011), and Shen et al. (2012).",
        "One common feature of these studies is that they leverage the global coherence between entities.",
        "Kulkarni et al. (2009) propose a graphical model that explicitly models the combination of evidence from local mention-entity compatibility and global document-level topical coherence of the entities, and show that considering global coherence between entities significantly improves the performance.",
        "Han et al.",
        "(2011) introduce a graph-based representation, called Referent Graph, to model the global interdependence between different entity linking decisions, and jointly infer the referent entities of all name mentions in a document by exploiting the interdependence captured in Referent Graph.",
        "Shen et al. (2012) propose LIEGE, a framework to link the entities in web lists with the knowledge base, with the assumption that entities mentioned in a Web list tend to be a collection of entities of the same conceptual type.",
        "Most work of entity linking focuses on web pages.",
        "Recently, Meij et al. (2012) study this task for tweets.",
        "They propose a machine learning based approach using n-gram features, concept features, and tweet features, to identify concepts semantically related to a tweet, and for every entity mention to generate links to its corresponding Wikipedia article.",
        "Their method belongs to the first category, in the sense that they only consider the similarity between mention (tweet) and entity (Wikipedia article).",
        "Our method belongs to the second category.",
        "However, in contrast with existing collective approaches, our method works on tweets which are short and often noisy.",
        "Furthermore, our method is based on the ?similar mention with similar entity?",
        "assumption, and explicitly models and integrates the mention similarity into the optimization framework.",
        "Compared with Meij et al.",
        "(2012), our method is collective, and integrates more features."
      ]
    },
    {
      "heading": "3 Task Definition",
      "text": [
        "Given a sequence of mentions, denoted by M?",
        "= (m1,m2, ?",
        "?",
        "?",
        ",mn), our task is to output a sequence of entities, denoted by E?",
        "= (e1, e2, ?",
        "?",
        "?",
        ", en), where ei is the entity corresponding to mi.",
        "Here, an entity refers to an item of a knowledge base.",
        "Following most existing work, we use Wikipedia as the knowledge base, and an entity is a definition page in Wikipedia; a mention denotes a sequence of tokens in a tweet that can be potentially linked to an entity.",
        "Several notes should be made.",
        "First, we assume that mentions are given, e.g., identified by some named entity recognition system.",
        "Second, mentions may come from multiple tweets.",
        "Third, mentions with the same token sequence may refer to different entities, depending on mention context.",
        "Finally, we assume each entity e has a variation list6, and a unique ID through which all related information about that entity can be accessed.",
        "Here is an example to illustrate the task.",
        "Given mentions ?nbcbightlynews?, ?Santiago?, ?WH?",
        "and ?Libya?",
        "from the following tweet ?Chuck Todd: Prepping for @nbcnightlynews here in Santiago, reporting on WH handling of Libya situation.",
        "?, the expected output is ?NBC Nightly News(194735)?, ?Santiago Chile(51572)?, ?White House(33057)?",
        "and ?Libya(17633)?, where the numbers in the parentheses are the IDs of the corresponding entities."
      ]
    },
    {
      "heading": "4 Our Method",
      "text": [
        "In this section, we first present the framework of our entity linking method.",
        "Then we introduce features related to different kinds of similarities, followed by a detailed discussion of the training and decoding procedures."
      ]
    },
    {
      "heading": "4.1 Framework",
      "text": [
        "Given the input mention sequence M?",
        "=",
        "sequences for the mention sequence M?",
        "; ?",
        "E?",
        "denotes an entity sequence instance, consisting of e1, e2, ?",
        "?",
        "?",
        ", en; ?",
        "f?",
        "(ei,mi) is the feature vector that models the similarity between mention mi and its linked entity ei; ?",
        "w?",
        "is the feature weight vector related to f?",
        ", which is trained on the training data set; w?",
        "?",
        "f?",
        "(ei,mi) is the similarity between mention mi and entity ei; ?",
        "r(ei, ej) is the function that returns the similarity between two entities ei and ej ; ?",
        "s(mi,mj) is the function that returns the similarity between two mentions mi and mj ; ?",
        "?",
        "?",
        "(0, 1) is a systematic parameter, which is determined on the development data set; it is used to adjust the tradeoff between local compatibility and global consistence.",
        "It is experimentally set to 0.8 in our work.",
        "From Formula 1, we can see that: 1) our method considers the mention-entity similarly, entity-entity similarity and mention-mention similarity.",
        "Mention-entity similarly is used to model local compatibility, while entity-entity similarity and mention-mention similarity combined are to model global consistence; and 2) our method prefers configurations where similar mentions have similar entities and with high local compatibility.",
        "C(M?)",
        "is worth of more discussion here.",
        "It represents the search space, which can be generated using the entity variation list.",
        "To achieve this, we first build an inverted index of all entity variation lists, with each unique variation as an entry pointing to a list of entities.",
        "Then for any mention m, we look up the index, and get al possible entities, denoted by C(m).",
        "In this way, given a mention sequence M?",
        "= (m1,m2, ?",
        "?",
        "?",
        ",mn), we can enumerate all possible entity sequence E?",
        "= (e1, e2, ?",
        "?",
        "?",
        ", en), where ei ?",
        "C(m).",
        "This means |C(M?)",
        "|= ?m?M |C(m) |, which is often large.",
        "There is one special case: if m is an OOV mention, i.e., |C(m) |= 0, then |C(M?)",
        "|= 0, and we get no solution.",
        "To address this problem, we can generate a list of candidates for an OOV mention using its similar mentions.",
        "Let S(m) denote OOV mention m's similar mentions, we define C(m) = ?m?",
        "?S(m) C(m ?).",
        "If still C(m) = 0, we remove m from M?",
        ", and report we cannot map it to any entity.",
        "Here is an example to illustrate our framework.",
        "Suppose we have the following tweets:",
        "framework.",
        "Ovals in orange and in blue represent mentions and entities, respectively.",
        "Each mention pair, entity pair, and mention entity pair have a similarity score represented by s, r and f , respectively.",
        "We need find out the best entity sequence E??",
        "for mentions M?",
        "= { ?Liverpool1?,"
      ]
    },
    {
      "heading": "4.2 Features",
      "text": [
        "We group features into three categories: local features related to mention-entity similarity (f?",
        "(e,m)), features related to entity-entity similarity (r(ei, ej)) , and features related to mention-mention similarity (s(mi,mj)).",
        "where: coccurence number is the the number of the words that occur in both the tweet containing mi and the Wikipedia page of ei; tweet length denotes the number of tokens of the tweet containing mention mi.",
        "computes the character level edit distance.",
        "This feature helps to detect whether a mention is an abbreviation of its corresponding entity7.",
        "?",
        "Mention Contains Title: If the mention contains the entity title, namely the title of the Wikipedia page introducing the entity ei, f4(mi, ei) = 1, else 0.",
        "?ms?",
        "is 2, and the edit distance between them is 7.",
        "2 plus 7 equals to 9, which is the length of ?Microsoft?.",
        "There are two representative definitions of entity similarity: in-link based similarity (Milne and Witten, 2008) and category based similarity (Shen et al., 2012).",
        "Considering that the Wikipedia categories are often noisy (Milne and Witten, 2008), we adopt in-link based similarity, as defined in Formula 4:",
        "?",
        "g(e) is the number of Wikipedia definition pages that have a link to entity e.",
        "We define 5 features to model the similarity between two mentions mi and mj , as listed below, where t(m) denotes the tweet that contains",
        "mention m: ?",
        "s1(mi,mj): The cosine similarity of t(mi) and t(mj); and tweets are represented as TF-IDF vectors; ?",
        "s2(mi,mj): The cosine similarity of t(mi) and t(mj); and tweets are represented as topic distribution vectors; ?",
        "s3(mi,mj): Whether t(mi) and t(mj) are published by the same account; ?",
        "s4(mi,mj): Whether t(mi) and t(mj) contain any common hash tag; ?",
        "s5(mi,mj): Edit distance related similarity between mi and mj , as defined in Formula 5.",
        "Note that: 1) before computing TF-IDF vectors, stop words are removed; 2) we use the Stanford Topic Modeling Toolbox8 to compute the topic model, and experimentally set the number of topics to 50.",
        "Finally, Formula 6 is used to integrate all the features.",
        "a?",
        "= (a1, a2, a3, a4, a5) is the feature weight vector for mention similarity, where ak ?"
      ]
    },
    {
      "heading": "4.3 Training and Decoding",
      "text": [
        "Given n mentions m1,m2, ?",
        "?",
        "?",
        ",mn and their corresponding entities e1, e2, ?",
        "?",
        "?",
        ", en, the goal of training is to determine: w?",
        "?, the weights of local features, and a?",
        "?, the weights of the features related to mention similarity, according to Formula 7 9.",
        "?",
        "?1 is the weight of regularization, which is experimentally set to 1.0; ?",
        "?2 is the weight of L2 loss, which is experimentally set to 0.2.",
        "Since the decoding problem defined by Formula 1 is NP hard (Kulkarni et al., 2009), we develop a greedy hill-climbing approach to tackle this challenge, as demonstrated in Algorithm 1.",
        "In Algorithm 1, it is the number of iterations;",
        "replacing ei with ej ?",
        "C(mi) for current E?",
        "; scij is the score of E?ij , i.e., Score(E?ij , M?).",
        "In each iteration, this rounding solution iteratively substitute entry ei in E?",
        "to increase the total score cur.",
        "If the score cannot be further improved, it stops and returns current E?.",
        "9This optimization problem is non-convex.",
        "We use coordinate descent to get a local optimal solution.",
        "Algorithm 1 Decoding Algorithm.",
        "Input: Mention Set M?",
        "= (m1,m2, ?",
        "?",
        "?",
        ",mn) Output: Entity Set E?",
        "= (e1, e2, ?",
        "?",
        "?",
        ", en)",
        "1: for i = 1 to n do 2: Initialize e(0)i as the entity with the largest prior probability given mention mi.",
        "3: end for 4: cur = Score(E?",
        "(0), M?)",
        "5: it = 1 6: while true do 7: for i = 1 to n do 8: for ej ?",
        "C(mi) do 9: if ej ?= e(it?1)i then 10: E?",
        "(it)ij = E?",
        "(it?1) ?",
        "{e(it?1)i } + {ej}.",
        "11: end if 12: scij = Score(E?",
        "(it)ij , M?).",
        "13: end for 14: end for 15: (l,m) = argmax(i,j)scij .",
        "16: sc?",
        "= sclm 17: if sc?",
        "> cur then 18: cur = sc?.",
        "19: E?",
        "(it) = E?",
        "(it?1) ?",
        "{e(it?1)l } + {em}.",
        "20: it = it + 1.",
        "21: else 22: break 23: end if 24: end while 25: return E?",
        "(it)."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": [
        "In this section, we introduce the data set and experimental settings, and present results."
      ]
    },
    {
      "heading": "5.1 Data Preparation",
      "text": [
        "Following most existing studies, we choose Wikipedia as our knowledge base10.",
        "We index the Wikipedia definition pages, and prepare all required prior knowledge, such as count(e), g(e), and entity variation lists.",
        "We also build an inverted index with about 60 million entries for the entity variation lists.",
        "For tweets, we use the data set shared by Meij et al.",
        "(2012)11.",
        "This data set is annotated manually by two volunteers.",
        "We get 502 annotated tweets from this data set.",
        "We keep 55 of them for",
        "development, and the remaining for 5 fold cross-validation."
      ]
    },
    {
      "heading": "5.2 Settings",
      "text": [
        "We consider following settings to evaluate our method.",
        "?",
        "Comparing our method with two baselines, i.e., Wikify!",
        "(Mihalcea and Csomai, 2007) and the system proposed byMeij et al. (2012)"
      ]
    },
    {
      "heading": "5.3 Results",
      "text": [
        "Table 1 reports the comparison results.",
        "Our method outperforms both systems in terms of all metrics.",
        "Since the main difference between our method and the baselines is that our method considers not only local features, but also global features related to entity similarity and mention similarity, these results indicate the effectiveness of collective inference and global features.",
        "For example, we find two baselines incorrectly link ?Nickelodeon?",
        "in the tweet ?BOH will make a special appearance on Nickelodeon's ?Yo Gabba Gabba?",
        "tomorrow?",
        "to the theater instead of a TV channel.",
        "In contrast, our method notices that ?Yo Gabba Gabba?",
        "in the same tweet can be linked to ?Yo Gabba Gabba (TV show)?, and thus it correctly maps ?Nickelodeon?",
        "to ?Nickelodeon",
        "are incrementally added.",
        "It can be seen that: 1) using only Prior Probability feature already yields a reasonable F1; and 2) Context Similarity and Edit Distance Similarity feature have little contribution to the F1, while Mention and Entity Title Similarity feature greatly boosts the F1.",
        "12We reimplement Wikify!",
        "since we use a new evaluation data set.",
        "and M.E.T.S.",
        "denote Prior Probability, Context Similarity, Edit Distance Similarity, and Mention and Entity Title Similarity, respectively.",
        "The performance of our method with various mention similarity features is reported in Table 3.",
        "First, we can see that with this kind of features, the F1 can be significantly improved from 0.680 to 0.704.",
        "Second, we notice that TF-IDF (s1) and Topic Model (s2) features perform equally well, and combining all mention similarity features yields the best performance.",
        "For any OOV mention, we use the strategy of guessing its possible entity candidates using similar mentions, as discussed in Section 4.1.",
        "Table 4 shows the performance of our system for OOV mentions.",
        "It can be seen that with our OOV strategy, the recall is improved from 0.653 to 0.675 (with p < 0.05) while the Precision is slightly dropped and the overall F1 still gets better.",
        "A further study reveals that among all the 125 OOV mentions, there are 48 for which our method cannot find any entity; and nearly half of these 48 OOV mentions do have corresponding entities 13.",
        "This suggests that we may need enlarge the size of variation lists or develop some mention normalization techniques."
      ]
    },
    {
      "heading": "6 Conclusions and Future work",
      "text": [
        "We have presented a collective inference method that jointly links a set of tweet mentions to their corresponding entities.",
        "One distinguished characteristic of our method is that it integrates mention-entity similarity, entity-entity similarity, and mention-mention similarity, to address the information lack in a tweet and rich OOV mentions.",
        "We evaluate our method on a public data set.",
        "Experimental results show our method outperforms two baselines, and suggests the effectiveness of modeling mention-mention similarity, particularly for OOV mention linking.",
        "In the future, we plan to explore two directions.",
        "First, we are going to enlarge the size of entity variation lists.",
        "Second, we want to integrate the entity mention normalization techniques as introduced by Liu et al. (2012)."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank the anonymous reviewers for their valuable comments.",
        "We also thank all the QuickView team members for the helpful discussions."
      ]
    }
  ]
}
