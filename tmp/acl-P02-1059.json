{
  "info": {
    "authors": [
      "Tadashi Nomoto",
      "Yuji Matsumoto"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P02-1059",
    "title": "Supervised Ranking in Open-Domain Text Summarization",
    "url": "https://aclweb.org/anthology/P02-1059",
    "year": 2002
  },
  "references": [
    "acl-C96-2166"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The paper proposes and empirically motivates an integration of supervised learning with unsupervised learning to deal with human biases in summarization.",
        "In particular, we explore the use of probabilistic decision tree within the clustering framework to account for the variation as well as regularity in human created summaries.",
        "The corpus of human created extracts is created from a newspaper corpus and used as a test set.",
        "We build probabilistic decision trees of different flavors and integrate each of them with the clustering framework.",
        "Experiments with the corpus demonstrate that the mixture of the two paradigms generally gives a significant boost in performance compared to cases where either of the two is considered alone."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Nomoto and Matsumoto (2001b) have recently made an interesting observation that an unsupervised method based on clustering sometimes better approximates human created extracts than a supervised approach.",
        "That appears somewhat contradictory given that a supervised approach should be able to exploit human supplied information about which sentence to include in an extract and which not to, whereas an unsupervised approach blindly chooses sentences according to some selection scheme.",
        "An interesting question is, why this should be the case.",
        "The reason may have to do with the variation in human judgments on sentence selection for a summary.",
        "In a study to be described later, we asked students to select 10% of a text which they find most important for making a summary.",
        "If they agree perfectly on their judgments, then we will have only 10% of a text selected as most important.",
        "However, what we found was that about half of a text were marked as important, indicating that judgments can vary widely among humans.",
        "Curiously, however, Nomoto and Matsumoto (2001a) also found that a supervised system fares much better when tested on data exhibiting high agreement among humans than an unsupervised system.",
        "Their finding suggests that there are indeed some regularities (or biases) to be found.",
        "So we might conclude that there are two aspects to human judgments in summarization; they can vary but may exhibit some biases which could be usefully exploited.",
        "The issue is then how we might model them in some coherent framework.",
        "The goal of the paper is to explore a possible integration of supervised and unsupervised paradigms as a way of responding to the issue.",
        "Taking a decision tree and clustering as representing the respective paradigm, we will show how coupling them provides a summarizer that better approximates human judgments than either of the two considered alone.",
        "To our knowledge, none of the prior work on summarization (e.g., Kupiec et al.",
        "(1995)) explicitly addressed the issue of the variability inherent in human judgments in summarization tasks."
      ]
    },
    {
      "heading": "2 Supervised Ranking with Probabilistic Decision Tree",
      "text": [
        "One technical problem associated with the use of a decision tree as a summarizer is that it is not able to rank sentences, which it must be able do, to allow for the generation of a variable-length summary.",
        "In response to the problem, we explore the use of a probabilistic decision tree as a ranking model.",
        "First, let us review some general features of probabilistic decision tree (ProbDT, henceforth) (Yamanishi, 1997; Rissanen, 1997).",
        "ProbDT works like a usual decision tree except that rather than assigning each instance to a single class, it distributes each instance among classes.",
        "For each instance xi, the strength of its membership to each of the classes is determined by P(ck I xi) for each class ck.",
        "Consider a binary decision tree in Fig 1.",
        "Let X1 and X2 represent non-terminal nodes, and Y1 and Y2 leaf nodes.",
        "‘1’ and ‘0’ on arcs denote values of some attribute at X1 and X2.",
        "θi y and θi n represent the probability that a given instance assigned to the node i is labeled as yes and no, repectively.",
        "Abusing the terms slightly, let us assume that X1 and X2 represent splitting attributes as well at respective nodes.",
        "Then the probability that a given instance with X1 = 1 and X2 = 0 is labeled as yes (no) is",
        "Now to rank sentences with ProbDT simply involves finding the probability that each sentence is assigned to a particular class designating sentences worthy of inclusion in a summary (call it ‘Select’ class) and ranking them accordingly.",
        "(Hereafter and throughout the rest of the paper, we say that a sentence is wis if it is worthy of inclusion in a summary: thus a wis sentence is a sentence worthy of inclusion in a summary.)",
        "The probabiliy that a sentence u is labeled as wis is expressed as in Table 1, where ui is a vector representation of u, consisting of a set of values for features of u; α is a smoothing function, e.g., Laplace’s law; t(u) is some leaf node assigned to u; and DT represents some decision tree used to classify u."
      ]
    },
    {
      "heading": "3 Diversity Based Summarization",
      "text": [
        "As an unsupervised summarizer, we use diversity based summarization (DBS) (Nomoto and Matsumoto, 2001 c).",
        "It takes a cluster-and-rank approach to generating summaries.",
        "The idea is to form a summary by collecting sentences representative of diverse topics discussed in the text.",
        "A nice feature about their approach is that by creating a summary covering potential topics, which could be marginal to the main thread of the text, they are in fact able to accommodate the variability in sentence selection: some people may pick up subjects (sentences) as important which others consider irrelevant or only marginal for summarization.",
        "DBS accomodates this situation by picking them all, however marginal they might be.",
        "More specifically, DBS is a tripartite process consisting of the following:",
        "1.",
        "Find-Diversity: find clusters of lexically similar sentences in text.",
        "(In particular, we represent a sentence here a vector of tfidf weights of index terms it contains.)",
        "2.",
        "Reduce-Redundancy: for each cluster found, choose a sentence that best represents that cluster.",
        "3.",
        "Generate-Summary: collect the representative sentences, put them in some order, and return them to the user.",
        "Find-Diversity is based on the K-means clustering algorithm, which they extended with Minimum Description Length Principle (MDL) (Li, 1998; Ya-manishi, 1997; Rissanen, 1997) as a way of optimizing K-means.",
        "Reduce-Redundancy is a tfidf based ranking model, which assigns weights to sentences in the cluster and returns a sentence that ranks highest.",
        "The weight of a sentence is given as the sum of tfidf scores of terms in the sentence.",
        "P(Select |u, DT) = α the number of “Select” sentences at t(d) the total number of sentences at t(ii) )"
      ]
    },
    {
      "heading": "4 Combining ProbDT and DBS",
      "text": [
        "Combining ProbDT and DBS is done quite straightforwardly by replacing Reduce-Redundacy with ProbDT.",
        "Thus instead of picking up a sentence with the highest tfdif based weight, DBS/ProbDT attempts to find a sentences with the highest score for P(Select I u, DT)."
      ]
    },
    {
      "heading": "4.1 Features",
      "text": [
        "The following lists a set of features used for encoding a sentence in ProbDT.",
        "Most of them are either length or location-related features.1 <LocSen> The location ofa sentence X defined by:",
        "‘#S(X)’ denotes an ordinal number indicating the position of X in a text, i.e. #S(kthsentence) = k. ‘LastSentence’ refers to the last sentence in a text.",
        "LocSen takes values between 0 and N – 1 N. Nis the number of sentences in the text.",
        "< L o c P a r> The location of a paragraph in which a sentence X occurs given by:",
        "‘#Par(X)’ denotes an ordinal number indicating the position of a paragraph containing X.",
        "‘#LastParagraph’ is the position of the last paragraph in a text, represented by the ordinal number.",
        "<LocWithinPar> The location of a sentence X within a paragraph in which it appears.",
        "‘ParInitSen’ refers to the initial sentence of a paragraph in which X occurs, ‘Length(Par(X))’ denotes the number of sentences that occur in that paragraph.",
        "LocWithinPar takes continuous values ranging from 0 toll i , where l is the length of a paragraph: a paragraph initial sentence would have 0 and a paragraph final sentence l−1 l .",
        "<LenText> The text length in Japanese character i.e. kana, kanji.",
        "<LenSen> The sentence length in kana/kanji.",
        "Some work in Japanese linguistics found that a particular grammatical class a sentence final element belongs to could serve as a cue to identifying summary sentences.",
        "These include categories like PAST/NON-PAST, INTERROGATIVE, and NOUN and QUESTION-MARKER.",
        "Along with Ichikawa (1990), we identified a set of sentence-ending cues and marked a sentence as to whether it contains a cue from the set.2 Included in the set are inflectional classes PAST/NON-PAST (for the verb and verbal adjective), COPULA, and NOUN, parentheses, and QUESTION-MARKER -ka.",
        "We use the following attribute to encode a sentence-ending form.",
        "<EndCue> The feature encodes one of sentence2 Word tokens are extracted by using CHASEN, a Japanese morphological analyzer which is reported to achieve the accuracy rate of over 98% (Matsumoto et al., 1999).",
        "ending forms described above.",
        "It is a discrete valued feature.",
        "The value ranges from 0 to 6.",
        "(See Table 2 for details.)",
        "Finally, one of two class labels, ‘Select’ and ‘Don’t Select’, is assigned to a sentence, depending on whether it is wis or not.",
        "The ‘Select’ label is for wis sentences, and the ‘Don’t Select‘ label for non-wis sentences."
      ]
    },
    {
      "heading": "5 Decision Tree Algorithms",
      "text": [
        "To examine the generality of our approach, we consider, in addition to C4.5 (Quinlan, 1993), the following decision tree algorithms.",
        "C4.5 is used with default options, e.g., CF=25%."
      ]
    },
    {
      "heading": "5.1 MDL-DT",
      "text": [
        "MDL-DT stands for a decision tree with MDL based pruning.",
        "It strives to optimize the decision tree by pruning the tree in such a way as to produce the shortest (minimum) description length for the tree.",
        "The description length refers to the number of bits required for encoding information about the decision tree.",
        "MDL ranks, along with Akaike Information Criterion (AIC) and Bayes Information Criterion (BIC), as a standard criterion in machine learning and statistics for choosing among possible (statistical) models.",
        "As shown empirically in Nomoto and Matsumoto (2000) for discourse domain, pruning DT with MDL significantly reduces the size of tree, while not compromising performance."
      ]
    },
    {
      "heading": "5.2 SSDT",
      "text": [
        "SSDT or Subspace Splitting Decision Tree represents another form of decision tree algorithm.",
        "(Wang and Yu, 2001) The goal of SSDT is to discover patterns in highly biased data, where a target class, i.e., the class one likes to discover something about, accounts for a tiny fraction of the whole data.",
        "Note that the issue of biased data distribution is particularly relevant for summarization, as a set of sentences to be identified as wis usually account for a very small portion of the data.",
        "SSDT begins by searching the entire data space for a cluster of positive cases and grows the cluster by adding points that fall within some distance to the center of the cluster.",
        "If the splitting based on the cluster offers a better Gini index than simply using Figure 2: SSDT in action.",
        "Filled circles represent positive class, white circles represent negative class.",
        "SSDT starts with a small spherical cluster of positive points (solid circle) and grows the cluster by ‘absorbing’ positive points around it (dashed circle).",
        "one of the attributes to split the data, SSDT splits the data space based on the cluster, that is, forms one region outside of the cluster and one inside.3 It repeats the process recursively on each subregions spawned until termination conditions are met.",
        "Figure 2 gives a snapshot of SSDT at work.",
        "SSDT locates some clusters of positive points, develops spherical clusters around them.",
        "With its particular focus on positive cases, SSDT is able to provide a more precise characterization of them, compared, for instance, to C4.5."
      ]
    },
    {
      "heading": "6 Test Data and Procedure",
      "text": [
        "We asked 112 Japanese subjects (students at graduate and undergraduate level) to extract 10% sentences in a text which they consider most important in making a summary.",
        "The number of sentences to extract varied from two to four, depending on the length of a text.",
        "The age of subjects varied from 18 to 45.",
        "We used 75 texts from three different categories (25 for each category); column, editorial and news report.",
        "Texts were of about the same size in terms of character counts and the number of paragraphs, and were selected randomly from articles that appeared in a Japanese financial daily (Nihon-Keizai-Shimbun-Sha, 1995).",
        "There were, on average, 19.98 sentences per text.",
        "The kappa agreement among subjects was 0.25.",
        "The result is in a way consistent with Salton et al.",
        "(1999), who report a low inter-subject agreement on paragraph extracts from encyclopedias and also with Gong and Liu (2001) on a sentence selection task in the cable news domain.",
        "While there are some work (Marcu, 1999; Jing et al., 1998) which do report high agreement rates, their success may be attributed to particularities of texts used, as suggested by Jing et al.",
        "(1998).",
        "Thus, the question of whether it is possible to establish an ideal summary based on agreement is far from settled, if ever.",
        "In the face of this, it would be interesting and perhaps more fruitful to explore another view on summary, that the variability of a summary is the norm rather than the exception.",
        "In the experiments that follow, we decided not to rely on a particular level of inter-coder agreement to determine whether or not a given sentence is wis.",
        "Instead, we used agreement threshold to distinguish between wis and non-wis sentences: for a given threshold K, a sentence is considered wis (or positive) if it has at least K votes in favor of its inclusion in a summary, and non-wis (negative) if not.",
        "Thus if a sentence is labeled as positive at K > 1, it means that there are one or more judges taking that sentence as wis. We examined K from 1 to 5.",
        "(On average, seven people are assigned to one article.",
        "However, one would rarely see all of them unanimously agree on their judgments.)",
        "Table 3 shows how many positive/negative instances one would get at a given agreement threshold.",
        "At K > 1, out of 1424 instances, i.e., sentences, 707 of them are marked positive and 717 are marked negative, so positive and negative instances are evenly spread across the data.",
        "On the other hand, at K > 5, there are only 72 positive instances.",
        "This means that there is less than one occurrence of wis case per article.",
        "In the experiments below, each probabilistic rendering of the DTs, namely, C4.5, MDL-DT, and SSDT is trained on the corpus, and tested with and without the diversity extension (Find-Diversity).",
        "When used without the diversity component, each ProbDT works on a test article in its entirety, producing the ranked list of sentences.",
        "A summary with compression rate γ is obtained by selecting top γ percent of the list.",
        "When coupled with Find-Diversity, on the other hand, each ProbDT is set to work on each cluster discovered by the diversity component, producing multiple lists of sentences, each corresponding to one of the clusters identified.",
        "A summary is formed by collecting top ranking sentences from each list.",
        "Evaluation was done by 10-fold cross validation.",
        "For the purpose of comparison, we also ran the diversity based model as given in Nomoto and Matsumoto (2001 c) and a tfidf based ranking model (Zechner, 1996) (call it Z model), which simply ranks sentences according to the tfidf score and selects those which rank highest.",
        "Recall that the diversity based model (DBS) (Nomoto and Matsumoto, 2001c) consists in Find-Diversity and the ranking model by Zechner (1996), which they call Reduce-Redundancy."
      ]
    },
    {
      "heading": "7 Results and Discussion",
      "text": [
        "Tables 4-8 show performance of each ProbDT and its combination with the diversity (clustering) component.",
        "It also shows performance of Z model and DBS.",
        "In the tables, the slashed ‘V’ after the name of a classifier indicates that the relevant classifier is diversity-enabled, meaning that it is coupled with the diversity extension.",
        "Notice that each decision tree here is a ProbDT and should not be confused with its non-probabilistic counterpart.",
        "Also worth noting is that DBS is in fact Z/V, that is, diversity-enabled Z model.",
        "Returning to the tables, we find that for most of the times, the diversity component has clear effects on ProbDTs, significantly improving their performance.",
        "All the figures are in F-measure, i.e.,",
        "ticular choice of ranking model, as performance of Z is also boosted with the diversity component.",
        "Not surprisingly, effects of supervised learning are also evident: diversity-enabled ProbDTs generally outperform DBS (Z/V) by a large margin.",
        "What is surprising, moreover, is that diversity-enabled ProbDTs are superior in performance to their non-diversity counterparts (with a notable exception for SSDT at K > 1), which suggests that selecting marginal sentences is an important part of generating a summary.",
        "Another observation about the results is that as one goes along with a larger K, differences in performance among the systems become ever smaller: at K > 5, Z performs comparably to C4.5, MDL, and SSDT either with or without the diversity component.",
        "The decline of performance of the DTs may be caused by either the absence of recurring patterns in data with a higher K or simply the paucity of positive instances.",
        "At the moment, we do not know which is the case here.",
        "It is curious to note, moreover, that MDL-DT is not performing as well as C4.5 and SSDT at K > 1, K > 2, and K > 3.",
        "The reason may well have to do with the general properties of MDL-DT.",
        "Recall that MDL-DT is designed to produce as small a decision tree as possible.",
        "Therefore, the resulting tree would have a very small number of nodes covering the entire data space.",
        "Consider, for instance, a hypothetical data space in Figure 3.",
        "Assume that MDL-DT bisects the space into region A and B, producing a two-node decision tree.",
        "The problem with the tree is, of course, that point x and y in region B will be assigned to the same probability under the probabilistic tree model, despite the fact that point x is very close to region A and point y is far out.",
        "This problem could happen with C4.5, but in MDL-DT, which covers a large space with a few nodes, points in a region could be far apart, making the problem more acute.",
        "Thus the poor performance of MDL-DT may be attributable to its extensive use of pruning."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "As a way of exploiting human biases towards an increased performance of the summarizer, we have explored approaches to embedding supervised learning within a general unsupervised framework.",
        "In the",
        "paper, we focused on the use of decision tree as a plug-in learner.",
        "We have shown empirically that the idea works for a number of decision trees, including C4.5, MDL-DT and SSDT.",
        "Coupled with the learning component, the unsupervised summarizer based on clustering significantly improved its performance on the corpus of human created summaries.",
        "More importantly, we found that supervised learners perform better when coupled with the clustering than when working alone.",
        "We argued that that has to do with the high variation in human created summaries: the clustering component forces a decision tree to pay more attention to sentences marginally relevant to the main thread of the text.",
        "While ProbDTs appear to work well with ranking, it is also possible to take a different approach: for instance, we may use some distance metric in instead of probability to distinguish among sentences.",
        "It would be interesting to invoke the notion like prototype modeler (Kalton et al., 2001) and see how it might fare when used as a ranking model.",
        "Moreover, it may be worthwhile to explore some non-clustering approaches to representing the diversity of contents of a text, such as Gong and Liu (2001)’s summarizer 1 (GLS1, for short), where a sentence is selected on the basis of its similarity to the text it belongs to, but which excludes terms that appear in previously selected sentences.",
        "While our preliminary study indicates that GLS 1 produces performance comparable and even superior to DBS on some tasks in the document retrieval domain, we have no results available at the moment on the efficacy of combining GLS 1 and ProbDT on sentence extraction tasks.",
        "Finally, we note that the test corpus used for",
        "on C4.5 with the MDL extension.",
        "DBS (=Z/V) denotes the diversity based summarizer.",
        "Z represents the Z-model summarizer.",
        "Performance figures are in F-measure.",
        "‘V’ indicates that the relevant classifier is diversity-enabled.",
        "Note that DBS =Z/V.",
        "evaluation is somewhat artificial in the sense that we elicit judgments from people on the summary-worthiness of a particular sentence in the text.",
        "Perhaps, we should look at naturally occurring abstracts or extracts as a potential source for training/evaluation data for summarization research.",
        "Besides being natural, they usually come in large number, which may alleviate some concern about the lack of sufficient resources for training learning algorithms in summarization."
      ]
    }
  ]
}
