{
  "info": {
    "authors": [
      "Jong-Hoon Oh",
      "Hitoshi Isahara"
    ],
    "book": "Proceedings of the Third International Joint Conference on Natural Language Processing",
    "id": "acl-I08-1031",
    "title": "Hypothesis Selection in Machine Transliteration: A Web Mining Approach",
    "url": "https://aclweb.org/anthology/I08-1031",
    "year": 2008
  },
  "references": [
    "acl-C00-1061",
    "acl-C02-1099",
    "acl-P02-1051",
    "acl-P04-1021",
    "acl-P04-1024"
  ],
  "sections": [
    {
      "text": [
        "Jong-Hoon Oh and Hitoshi Isahara",
        "Computational Linguistics Group National Institute oflnformation and Communications Technology (NICT) 3-5 Hikaridai,Seika-cho, Soraku-gun, Kyoto, 619-0289, Japan {rovellia,isahara}@nict.go.jp",
        "We propose a new method of selecting hypotheses for machine transliteration.",
        "We generate a set of Chinese, Japanese, and Korean transliteration hypotheses for a given English word.",
        "We then use the set of transliteration hypotheses as a guide to finding relevant Web pages and mining contextual information for the transliteration hypotheses from the Web page.",
        "Finally, we use the mined information for machine-learning algorithms including support vector machines and maximum entropy model designed to select the correct transliteration hypothesis.",
        "In our experiments, our proposed method based on Web mining consistently outperformed systems based on simple Web counts used in previous work, regardless ofthe language."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Machine transliteration has been a great challenge for cross-lingual information retrieval and machine translation systems.",
        "Many researchers have developed machine transliteration systems that accept a source language term as input and then output its transliteration in a target language (Al-Onaizan and Knight, 2002; Goto et al., 2003; Grefenstette et al., 2004; Kang and Kim, 2000; Li et al., 2004; Meng et al., 2001; Oh and Choi, 2002; Oh et al., 2006; Qu and Grefenstette, 2004).",
        "Some of these have used the Web to select machine-generated transliteration hypotheses and have obtained promising results (Al-Onaizan and Knight, 2002; Grefenstette et al., 2004;",
        "Oh et al., 2006; Qu and Grefenstette, 2004).",
        "More precisely, they used simple Web counts, estimated as the number of hits (Web pages) retrieved by a Web search engine.",
        "However, there are several limitations imposed on the ability of Web counts to select a correct transliteration hypothesis.",
        "First, the assumption that hit counts approximate the Web frequency of a given query usually introduces noise (Lapata and Keller, 2005).",
        "Moreover, some Web search engines disregard punctuation and capitalization when matching search terms (Lapata and Keller, 2005).",
        "This can cause errors if such Web counts are relied on to select transliteration hypotheses.",
        "Second, it is not easy to consider the contexts of transliteration hypotheses with Web counts because Web counts are estimated based on the number of retrieved Web pages.",
        "However, as our preliminary work showed (Oh et al., 2006), transliteration or translation pairs often appear as parenthetical expressions or tend to be in close proximity in texts; thus context can play an important role in selecting transliteration hypotheses.",
        "For example, there are several Chinese, Japanese, and Korean (CJK) transliterations and their counterparts in a parenthetical expression, as follows.",
        "1) mWM^MiiS£i^2 (Adriennei Clarkson2)",
        "Note that the subscripted numbers in all examples represent the correspondence between the English word and its CJK counterpart.",
        "These parenthetical expressions are very useful in selecting transliteration hypotheses because it is apparent that they are translation pairs or transliteration pairs.",
        "However, we cannot fully use such information with Web counts.",
        "To address these problems, we propose a new method of selecting transliteration hypotheses.",
        "We were interested in how to mine information relevant to the selection of hypotheses and how to select correct transliteration hypotheses using the mined information.",
        "To do this, we generated a set of CJK transliteration hypotheses for a given English word.",
        "We then used the set of transliteration hypotheses as a guide to finding relevant Web page and mining contextual information for the transliteration hypotheses from the Web page.",
        "Finally, we used the mined information for machine-learning algorithms including support vector machines (SVMs) and maximum entropy model designed to select the correct transliteration hypothesis.",
        "This paper is organized as follows.",
        "Section 2 describes previous work based on simple Web counts.",
        "Section 3 describes a way of generating transliteration hypotheses.",
        "Sections 4 and 5 introduce our methods of Web mining and selecting transliteration hypotheses.",
        "Sections 6 and 7 deal with our experiments and the discussion.",
        "Conclusions are drawn and future work is discussed in Section 8."
      ]
    },
    {
      "heading": "2. Related work",
      "text": [
        "Web counts have been used for selecting transliteration hypotheses in several previous work (Al-Onaizan and Knight, 2002; Grefenstette et al., 2004; Oh et al., 2006; Qu and Grefenstette, 2004).",
        "Because the Web counts are estimated as the number of hits by a Web search engine, they greatly depend on queries sent to a search engine.",
        "Previous work has used three types of queries – monolingual queries stette et al., 2004; Oh et al., 2006), bilingual simple queries (BSQs) (Oh et al., 2006; Qu and Grefenstette, 2004), and bilingual bigram queries (BBQs) (Oh et al., 2006).",
        "If we let S be a source language term and H = {h\\, • • • ,hr} be a set of machine-generated transliteration hypotheses of S, the three types of queries can be defined as MQ: hi (e.g., ftftM , ?",
        "'J>h>,and #33).",
        "BSQ: s and hi without quotations (e.g., Clinton j£ #tK , Clinton , and Clinton # 3",
        "3).",
        "BBQ: Quoted bigrams composed of S and hi (e.g., \"Clinton \"Clinton \", and \"Clinton #33\").",
        "MQ is not able to determine whether hi is a counterpart of S, but whether hi is a frequently used target term in target-language texts.",
        "BSQ retrieves Web pages if S and hi are present in the same document but it does not take the distance between S and hi into consideration.",
        "BBQ retrieves Web pages where \"Shi\"or\"hi S\" are present as a bigram.",
        "The relative order of Web counts over H makes it possible to select transliteration hypotheses in the previous work."
      ]
    },
    {
      "heading": "3. Generating Transliteration Hypotheses",
      "text": [
        "Let S be an English word, P be a pronunciation of S, and T be a target language transliteration corresponding to S. We implement English-to-CJK transliteration systems based on three different transliteration models – a grapheme-based model (S – T), a phoneme-based model (S – P and P – T), and a correspondence-based model (S – P and (S, P) – T) – as described in our preliminary work (Oh et al., 2006).",
        "P and T are segmented into a series of sub-strings, each of which corresponds to a source grapheme.",
        "We can thus write S = S1, • • • ,s,n = s?, P = P1, • • • ,pn = Pi, and T = t1,- • • ,tn = tn, where si, pi, and ti represent the ith English grapheme, English phonemes corresponding to si, and target language graphemes corresponding to si, respectively.",
        "Given S, our transliteration systems generate a sequence of ti corresponding to either si (in Eq.",
        "(1)) or pi (in Eq.",
        "(2)) or both of them (in Eq.",
        "(3)).",
        "The maximum entropy model was used to estimate produced the n-best transliteration hypotheses using a stack decoder (Schwartz and Chow, 1990).",
        "We then created a set of transliteration hypotheses comprising the n-best transliteration hypotheses."
      ]
    },
    {
      "heading": "4. Web Mining",
      "text": [
        "Let S be an English word and H = {h1, • • • ,hr } be its machine-generated set of transliteration hypotheses.",
        "We use S and H to generate queries sent to a search engine to retrieve the top-100 snippets.",
        "A correct transliteration and its counterpart tend to be in close proximity on CJK Web pages.",
        "Our goal in Web mining was to find such Web pages and mine information that would help to select transliteration hypotheses from these pages.",
        "To find these Web pages, we used three kinds of queries, Q1=(S and hi), Q2=S, and Q3=hi, where Q1 is the same as BSQ's query and Q3 is the same as MQ's.",
        "The three queries usually result in different sets of Web pages.",
        "We categorize the retrieved Web pages by Q1, Q2, and Q3 into W1, W2, and W3.We extract three kinds of features from Wi as follows, where l = 1, 2, 3.",
        "• Freq(hi,Wl): the number of occurrences of hi",
        "• DFreqk(hi,Wl): Co-occurrence of S and hiwith distance dk £ D in the same snippet of",
        "Wl.",
        "• PFreqk(hi,Wl): Co-occurrence of S and hias parenthetical expressions with distance dk £ D in the same snippet of Wl.",
        "Parenthetical expressions are detected when either S or hi is in parentheses.",
        "We define D = {d1,d2,d3} with three ranges of distances between S and hi, where d1(d< 5), d2(5 < d< 10), and d3(10 < d < 15).",
        "We counted distance d with the total number of characters (or words) between S and hi.",
        "Here, we can take the contexts of transliteration hypotheses into account using DFreq and PFreq; while Freq is counted regardless of the contexts of the transliteration hypotheses.",
        "Freq, DFreqk, and PFreqk, where S = Clinton,",
        "words (for English and Korean) or characters (for Chinese and",
        "Japanese) were chosen to calculate d.",
        "£»(My Life).",
        "###j*^tfl«#«*Ji*M«ftJl«» ^^.Ä*#ÄS^##F3(Hillary Rodham C7intQD^W4l997 ^KtBW ...",
        ": :.£##, (Clinton,) ^JE^fffMifiCKerry):: SjfiWohn Kerry) *aWÄR,ffiff«Kffi*ffÄSWÄR *\"ffÄÄ» £##e (ClintoD^)WSsfi(Kerry) ...",
        "hi=^#® in W1 collected by Q1=(Clinton IK).",
        "The subscripted numbers of Clinton and j£# j were used to indicate how many times they occurred in W1.",
        "In Fig. 1, j£#tK occurs six times thus Freq(hi,W1) = 6.",
        "Table 1 lists the distance between Clinton and j£#tK within each snippet of W1.",
        "We can obtain DFreq1(hi,W1) = 5.",
        "PFreq1(hi,Wl) is calculated by detecting parenthetical expressions between S and hi when DFreq1(hi,Wl) is counted.",
        "Because all S in W1 (Clinton to Clinton5) are in parentheses, PFreq1(hi,W1) is the same as DFreq1(hi,W1).",
        "We ignore Freq, DFreqk, and PFreqk when hiis a substring of other transliteration hypotheses because hi usually has a higher Freq, DFreqk, and PFreqk than hj if hi is a substring of hj.",
        "Let a set of transliteration hypotheses for S = Clinton be H= {h1 = jjiMKK., h2 = iri}.",
        "Here, h2 is a substring of h .",
        "In Fig. 1, h2 appears six times as a substring of h1 and three times independently in Snippet2.",
        "Moreover, independently used h2j£2, and j£3) and S (Clinton3 and Clinton5) are sufficiently close to count DFreqk and PFreqk.",
        "Therefore, the Freq, DFreqk, and PFreqk of h will be lower than those of h2 if we do not take the substring relation between h1 and h2 into account.",
        "Considering the substring relation, we obtain Freq(h2W\\) = 3, DFreq^,W1) = 1, DFreq2(h2,W1) = 2, PFreqi(h2,W{) = 1, and PFreq2(h2,W )=2.",
        "Snippet i",
        "Clintoni",
        "1",
        "41",
        "68",
        "Clinton2",
        "72",
        "29",
        "2",
        "Snippet2",
        "Clintons",
        "0",
        "36",
        "81",
        "Clinton4",
        "40",
        "0",
        "37",
        "Clinton5",
        "85",
        "41",
        "0",
        "Snippet2",
        "^li",
        "Clintons",
        "6",
        "9",
        "85",
        "Clinton4",
        "32",
        "29",
        "42",
        "Clinton5",
        "77",
        "74",
        "1"
      ]
    },
    {
      "heading": "5. Hypothesis Selection",
      "text": [
        "We select transliteration hypotheses by ranking them.",
        "A set of transliteration hypotheses, H = {h1,h2, • • • ,hr}, is ranked to enable a correct hypothesis to be identified.",
        "We devise a rank function, g(hi) in Eq.",
        "(4), that ranks a correct transliteration hypothesis higher and the others lower.",
        "Let xi £X be a feature vector of hi £H, yi = {+1, – 1} be the training label for xi, and TV = {td1 =<X1,y1 >,• • • ,tdz =<xz,yz >} be the training data for g(hi).",
        "We prepare the training data for g(hi) as follows.",
        "1.",
        "Given each English word S in the training-set, generate transliteration hypotheses H.",
        "2.",
        "Given hi £H, assign yi by looking for S and hi in the training-set – yi =+1ifhi is a correct transliteration hypothesis corresponding to S, otherwise yi = – 1."
      ]
    },
    {
      "heading": "3.. For each pair (S, h i ), generate its feature vector",
      "text": [
        "xi."
      ]
    },
    {
      "heading": "4.. Construct a training data set, TV",
      "text": [
        "We used two machine-learning algorithms, support vector machines (SVMs) and maximum entropy model for our implementation of g(hi).",
        "The SVMs assign a value to each transliteration hypothesis (hi) using where w denotes a weight vector.",
        "Here, we use the predicted value of gsvM (hi) rather than the predicted class of hi given by SVMs because our ranking function, as represented by Eq.",
        "(4), determines the relative ordering between hi and hj in H.A ranking function based on the maximum entropy model assigns a probability to hi using",
        "We can finally obtain a ranked list for the given H – the higher the g(hi) value, the better the hi.",
        "We represent the feature vector, xi, with two types of features.",
        "The first is the confidence scores of higiven by Eqs.",
        "(1)-(3) and the second is Web-based features – Freq, DFreqk, and PFreqk.",
        "To normalize Freq, DFreqk, and PFreqk, we use their relative frequency over H as in Eqs.",
        "(7)-(9), where k = 1, 2, 3 and l = 1, 2, 3.",
        "DFreqk(hj ,Wl)",
        "Figure 2 shows how to construct feature vector xi from a given English word, Rachel, and its Chinese hypotheses, H, generated from our transliteration systems.",
        "We can obtain r Chinese transliteration hypotheses and classify them into positive and negative samples according to yi.",
        "Note that yi =+1 if and only if hi is registered as a counterpart of S in the training data.",
        "The bottom of Fig. 2 shows our feature set representing xi.",
        "There are three confidence scores in P(hi\\S) according to transliteration models and the three Web-based features Web(W1), WebW), and WebW).",
        "Freq(hiWl)",
        "hj en Freq(hj W)DFreqk (KWl)",
        "reqk (hi,Wl)",
        "T,hj en PFreqk (hj ,Wl)"
      ]
    },
    {
      "heading": "6. Experiments",
      "text": [
        "We evaluated the effectiveness of our system in selecting CJK transliteration hypotheses.",
        "We used the same test set used in Li et al.",
        "(2004) (ECSet) for Chinese transliterations (Xinhua News Agency, 1992) and those used in Oh et al.",
        "(2006) for Japanese and Korean transliterations – EJSET and EK-SET (Breen, 2003; Nam, 1997).",
        "We divided the test data into training, development, and blind test sets as in Table 2.",
        "The training set was used to train our three transliteration models to generate the n-best transliteration hypotheses.",
        "The development set was used to train hypothesis selection based on support vector machines and maximum entropy model.",
        "We used the blindtestsetforevaluation.",
        "The evaluation was done in terms of word accuracy (WA).",
        "WA is the proportion of correct transliterations in the best hypothesis by a system to correct transliterations in the blind test set.",
        "Table 3: WA of individual transliteration systems (%)",
        "We compared our transliteration system with three previous ones, all of which were based on a grapheme-based model (Goto et al., 2003; Kang and Kim, 2000; Li et al., 2004).",
        "LI04 is an English-to-Chinese transliteration system, which simultaneously takes English and Chinese contexts into consideration (Li et al., 2004).",
        "KANG00 is an English-to-Korean transliteration system and GOTO03 is an English-to-Japanese one - they segment a chunk of English graphemes and identify the most relevant sequence of target graphemes corresponding to the chunk (Goto et al., 2003; Kang and Kim, 2000) .",
        "GM, PM, and CM, which are respectively based on Eqs.",
        "(1)-(3), are the transliteration systems we used for generating transliteration hypotheses.",
        "Our transliteration systems showed comparable or better performance than the previous ones regardless ofthe language.",
        "We compared simple Web counts with our Web mining for hypothesis selection.",
        "We used the same set of transliteration hypotheses H then compared their performance in hypothesis selection with two measures, relative frequency and g(hi).",
        "Tables 4 and 5 list the results.",
        "Here, \"Upper bound\" is a system that always selects the correct transliteration hypothesis if there is a correct one in H. \"Upper bound\" can",
        "data as ours.",
        "1 .",
        "H",
        "h3",
        "h,",
        "h-",
        "h,",
        ") '",
        "sa*",
        "x,",
        "\"3",
        "x,",
        "\"s",
        "xr",
        "Y",
        "y1",
        "Jl",
        "y3",
        "y,",
        "y-",
        "y,",
        "+1",
        "-1",
        "-1",
        "-l",
        "-l",
        "_",
        "-l",
        "System",
        "ECSet",
        "EJSet",
        "EKSet",
        "KANG00",
        "N/A",
        "N/A",
        "54.1",
        "GOTO03",
        "N/A",
        "54.3",
        "N/A",
        "LI04",
        "70.1",
        "N/A",
        "N/A",
        "GM",
        "69.0",
        "61.6",
        "59.0",
        "PM",
        "56.6",
        "54.4",
        "56.7",
        "CM",
        "69.9",
        "65.0",
        "65.1",
        "Pr(hJS)",
        "Web (WJ",
        "Web (WJ",
        "Web (WJ",
        "PrG(hijS)",
        "RF(hpWJ",
        "RF(hi,W2l",
        "RF(W3)",
        "PrPhilS",
        "RDFjhWJ",
        "RDFI(hi,W2l",
        "RDFI(hi,W3)",
        "PvJhJS)",
        "RDF2(hi,WI)",
        "RDF2(hi,W2l",
        "RDF2(hi,W3)",
        "RDF3(hpWJ",
        "RDF3(hi,W2l",
        "RDF3(hi,W3)",
        "RPFjfafWJ",
        "RPFI(hi,W2)",
        "RPFI(h,W3)",
        "RPF2(h,WJ",
        "RPF2(hi,W2)",
        "RPF2(h,W3)",
        "RPF3(h,WJ",
        "RPF3(hi,W2)",
        "RPF3(h,W3)",
        "ECSet",
        "EJSet",
        "EKSet",
        "Training Set",
        "31,299",
        "8,335",
        "5,124",
        "Development Set",
        "3,478",
        "1,041",
        "1,024",
        "Blind Test Set",
        "2,896",
        "1,041",
        "1,024",
        "Total",
        "37,694",
        "10,417",
        "7,172",
        "hypothesis selection by relative frequency (%) hypothesis selection by g(hi) (%) also be regarded as the \"Coverage\" of H generated by our transliteration systems.",
        "MQ, BSQ, and BBQ in the upper section of Table 4, represent hypothesis selection systems based on the relative frequency of Web counts over H, the same measure used in Oh et",
        "WebCountsx(hi) (10) T,hjen WebCountsx(hj) where WebCountsx(hi) is a function returning Web counts retrieved by x £{MQ, BSQ, BBQ} RF (Wl), RDF (Wl), and RPF (Wl) in Table 4 represent hypothesis selection systems with their relative frequency, where RDF(Wl) and RPF(Wl) use TL1 RDFk(hj,Wl) and J2l=1 RPFk(hj,Wi), respectively.",
        "The comparison in Table 4 shows which is best for selecting transliteration hypotheses when each relative frequency is used alone.",
        "Table 5 compares Web counts with features mined from the Web when they are used as features in g(hi) – {Pr(hi\\ S), Web(Wl)} in MEMwm and SVMwm (our proposed method), while {Pr(hi\\S), WebCcountsx(hi)} in MEMwe and SVMwe.",
        "Here, Web(Wl) is a set of mined features from Wl as described in Fig .2.",
        "Snippet, retrieved by BSQ: Aman \"Wjl\" I",
        "|pi|aaBM – iVi(a Man To Call My Own) «S ^fi^TOMMSSSicfnfJS.",
        "ffiiS*±®Mi§S£,-f#il",
        "Snippet, retrieved by MQ: \"Wjp\" (meaning Agard) I",
        "... g.aftWilllttjfc(Academv) &Tg1fgffg7ftSa.",
        "The results in the tables show that our systems consistently outperformed systems based on Web counts, especially for Chinese.",
        "This was due to the difference between languages.",
        "Japanese and Chinese do not use spaces between words.",
        "However, Japanese is written using three different alphabet systems, called Hiragana, Katakana, and Kanji, that assist word segmentation.",
        "Moreover, words written in Katakana are usually Japanese transliterations of foreign words.",
        "This makes it possible for a Web search engine to effectively retrieve Web pages containing given Japanese transliterations.",
        "Like English, Korean has spaces between words (or word phrases).",
        "As the spaces in the languages reduce ambiguity in segmenting words, a Web search engine can correctly identify Web pages containing given Korean transliterations.",
        "In contrast, there is a severe word-segmentation problem with Chinese that causes Chinese Web search engines to incorrectly retrieve Web pages, as shown in Fig. 3.",
        "For example, SniPPet1 is not related to \"Aman\" but to \"a man\".",
        "System",
        "ECSet",
        "EJSet",
        "EKSet",
        "MQ",
        "16.1",
        "40.4",
        "34.7",
        "WC",
        "BSQ",
        "45.S",
        "74.0",
        "72.4",
        "BBQ",
        "34.9",
        "7S.1",
        "79.3",
        "RF (Wl)",
        "62.9",
        "7S.4",
        "77.1",
        "RDF (Wl)",
        "70.S",
        "S0.4",
        "S0.2",
        "RPF (Wl)",
        "73.5",
        "79.7",
        "79.4",
        "RF (W2)",
        "63.5",
        "76.2",
        "74.S",
        "WM",
        "RDF (W2)",
        "67.1",
        "79.2",
        "7S.9",
        "RPF (W2)",
        "69.6",
        "79.1",
        "7S.4",
        "RF (Ws)",
        "37.9",
        "53.9",
        "55.S",
        "RDF (Ws)",
        "76.4",
        "69.0",
        "70.2",
        "RPF (Ws)",
        "76.S",
        "6S.3",
        "6S.7",
        "Upper bound",
        "94.6",
        "93.5",
        "93.2",
        "Snippet, retrieved by MQ: (meaning Rawcliffe) 1",
        "■Sfll*#iftlCliffDe Youngl ^Fl f£nBnl ^JP EOfl^JJMs",
        "i>iC±71t | The Secret Life of Zoey (TV) S^T^ft: 2002 JJS: #SHf$l ,Avery Raskin.",
        "fttf'WS: Larry Carter.",
        "ifih 4.92...",
        "System",
        "ECSet",
        "EJSet",
        "EKSet",
        "WC",
        "MEMwc SVMwc",
        "74.7 74.S",
        "S6.1 S6.9",
        "S5.6 S6.5",
        "WM",
        "MEMwm SVMwm",
        "S2.0 S3.9",
        "SS.2 SS.5",
        "S5.S S6.7",
        "Upper bound",
        "94.6",
        "93.5",
        "93.2",
        "Snippet4 retrieved by MQ: \"jt^SH\" (meaning Aldersey) |",
        "UNESCO.",
        "General Conference; 32nd; Election ofmember",
        "WSffiS^AâSlblSiâEE**.",
        "Ä.19S7--1991.",
        "«flâffiS-S* «*4^KS^iitt**.",
        "ÏÏStfcïÊ.",
        "(1976).",
        "19S7--1991.",
        "J/l^fi^Äffi***.",
        "2001--2005. fitttfcM.",
        "1993--1997....",
        "Snippet2 contains a super-string of a given Chinese query, which corresponds to \"Academy\" rather than to \"Agard\", which is the English counterpart of the Chinese transliteration KJP.",
        "Moreover, Web search engines ignore punctuation marks in Chinese.",
        "In Snippet3 and Snippety, \",\" and \"•\" in the underlined terms are disregarded, so the Web counts based on such Web documents are noisy.",
        "Thus, noise in the Chinese Web counts causes systems based on Web counts to produce more errors than our systems do.",
        "Our proposed method can filter out such noise because our systems take punctuation marks and the contexts of transliterations in Web mining into consideration.",
        "Thus, our systems based on features mined from the Web were able to achieve the best performance.",
        "The results revealed that our systems based on the Web-mining technique can effectively be used to select transliteration hypotheses regardless of the language.",
        "In Web mining, we used W1 , W2, and W3, collected by respective queries Q1=(S and hi), Q2=S, and Q3=hi.",
        "To investigate their contribution, we tested our proposed method with different combinations of Web corpora.",
        "\"Base\" is a baseline system that only uses Pr(hi\\S) as features but does not use features mined from the Web.",
        "We added features mined from different combinations of Web corpora to \"Base\" from W1 to WAll.",
        "In Table 6, we can see that W1 , a set ofWeb pages retrieved by Q1 , tends to give more relevant information than W2 and W3, because Q1 can search more Web pages containing both S and hi in the top100 snippets if S and hi are a correct transliteration pair.",
        "Therefore, its performance tends to be superior in Table 6 if W1 is used, especially for ECSet.",
        "However, as W1 occasionally retrieves few snippets, it is not able to provide sufficient information.",
        "Using W2or W3, we can address the problem.",
        "Thus, combinations of W1 and others (W1+2, W^3, Wau) provided better WAthan W1."
      ]
    },
    {
      "heading": "7. Discussion",
      "text": [
        "Several Web mining techniques for transliteration lexicons have been developed in the last few years (Jiang et al., 2007; Oh and Isahara, 2006).",
        "The main difference between ours and those previous ones is in the way a set of transliteration hypotheses (or candidates) is created.",
        "Jiang et al.",
        "(2007) generated Chinese transliterations for given English words and searched the Web using the transliterations.",
        "They generated only the best transliteration hypothesis and focused on Web mining to select transliteration lexicons rather than selecting transliteration hypotheses.",
        "The best transliteration hypothesis was used to guide Web searches.",
        "Then, transliteration candidates were mined from the retrieved Web pages.",
        "Therefore, their performance greatly depended on their ability to mine transliteration candidates from the Web.",
        "However, this system might create errors if it cannot find a correct transliteration candidate from the retrieved Web pages.",
        "Because of this, their system's coverage and WA were relatively poor than ours.",
        "However, our transliteration process was able to generate a set of transliteration hypotheses with excellent coverage and could thus achieve superior WA.",
        "Oh and Isahara (2006) searched the Web using given source words and mined the retrieved Web pages to find target-language transliteration candidates.",
        "They extracted all possible sequences of target-language characters from the retrieved Web snippets as transliteration candidates for which the beginnings and endings of the given source word and the extracted transliteration candidate were phonetically similar.",
        "However, while this can exponentially increase the number of transliteration candidates, ours used the n-best transliteration hypotheses but still achieved excellent coverage.",
        "ECSet",
        "EJSet",
        "EKSet",
        "SVM",
        "MEM",
        "SVM",
        "MEM",
        "SVM",
        "MEM",
        "Base",
        "73.3",
        "73.S",
        "67.0",
        "66.1",
        "66.0",
        "66.4",
        "Wi",
        "S1.7",
        "79.7",
        "S7.6",
        "S7.3",
        "S6.1",
        "S5.1",
        "W2",
        "S0.S",
        "79.5",
        "S6.9",
        "S6.0",
        "S3.S",
        "S2.1",
        "Ws",
        "77.2",
        "76.7",
        "S3.0",
        "S2.S",
        "79.S",
        "77.3",
        "Wl+2",
        "S3.S",
        "S2.3",
        "SS.5",
        "S7.9",
        "S6.3",
        "S5.9",
        "Wl+3",
        "S1.9",
        "S0.1",
        "S7.6",
        "S7.S",
        "S6.1",
        "S4.7",
        "W2+3",
        "S1.4",
        "79.S",
        "SS.0",
        "S7.7",
        "S5.1",
        "S4.3",
        "WAll",
        "S3.9",
        "S2.0",
        "SS.5",
        "SS.2",
        "S6.7",
        "S5.S"
      ]
    },
    {
      "heading": "8. Conclusion",
      "text": [
        "We have described a novel approach to selecting transliteration hypotheses based on Web mining.",
        "We first generated CJK transliteration hypotheses for a given English word and retrieved Web pages using the transliteration hypotheses and the given English word as queries for a Web search engine.",
        "We then mined features from the retrieved Web pages and trained machine-learning algorithms using the mined features.",
        "Finally, we selected transliteration hypotheses by ranking them.",
        "Our experiments revealed that our proposed method worked well regardless of the language, while simple Web counts were not effective, especially for Chinese.",
        "Because our method was very effective in selecting transliteration pairs, we expect that it will also be useful for selecting translation pairs.",
        "We plan to extend our method in future work to selecting translation pairs."
      ]
    }
  ]
}
