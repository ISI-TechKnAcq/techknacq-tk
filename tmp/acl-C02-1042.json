{
  "info": {
    "authors": [
      "Eduard Hovy",
      "Ulf Hermjakob",
      "Chin-Yew Lin",
      "Deepak Ravichandran"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C02-1042",
    "title": "Using Knowledge to Facilitate Factoid Answer Pinpointing",
    "url": "https://aclweb.org/anthology/C02-1042",
    "year": 2002
  },
  "references": [
    "acl-A00-1023",
    "acl-A00-1041",
    "acl-C02-1026",
    "acl-P02-1006",
    "acl-W01-1203"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In order to answer factoid questions, the Webclopedia QA system employs a range of knowledge resources.",
        "These include a QA Typology with answer patterns, WordNet, information about typical numerical answer ranges, and semantic relations identified by a robust parser, to filter out likely-looking but wrong candidate answers.",
        "This paper describes the knowledge resources and their impact on system performance."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The TREC evaluations of QA systems (Voorhees, 1999) require answers to be drawn from a given source corpus.",
        "Early QA systems used a simple filtering technique, question word density within a fixed n-word window, to pinpoint answers.",
        "Robust though this may be, the window method is not accurate enough.",
        "In response, factoid question answering systems have evolved into two types:",
        "• Use-Knowledge: extract query words from the input question, perform IR against the source corpus, possibly segment resulting documents, identify a set of segments containing likely answers, apply a set of heuristics that each consults a different source of knowledge to score each candidate, rank them, and select the best (Harabagiu et al., 2001; Hovy et al., 2001; Srihari and Li, 2000; Abney et al., 2000).",
        "• Use-the-Web: extract query words from the question, perform IR against the web, extract likely answer-bearing sentences, canonicalize the results, and select the most frequent answer(s).",
        "Then, for justification,",
        "locate examples of the answers in the source corpus (Brill et al., 2001; Buchholz, 2001).",
        "Of course, these techniques can be combined: the popularity ratings from Use-the-Web can also be applied as a filtering criterion (Clarke et al., 2001), or the knowledge resource heuristics can filter the web results.",
        "However, simply going to the web without using further knowledge (Brill et al., 2001) may return the web’s majority opinions on astrology, the killers of JFK, the cancerous effects of microwave ovens, etc. – fun but not altogether trustworthy.",
        "In this paper we describe the range of filtering techniques our system Webclopedia applies, from simplest to most sophisticated, and indicate their impact on the system."
      ]
    },
    {
      "heading": "2. Webclopedia Architecture",
      "text": [
        "As shown in Figure 1, Webclopedia adopts the Use-Knowledge architecture.",
        "Its modules are described in more detail in (Hovy et al., 2001; Hovy et al., 1999):",
        "• Question parsing: Using BBN’s IdentiFinder (Bikel et al., 1999), the CONTEX parser (Hermjakob, 1997) produces a syntactic-semantic analysis of the question and determines the QA type.",
        "• Query formation: Single and multi-word units (content words) are extracted from the analysis, and WordNet synsets (Fellbaum, 1998) are used for query expansion.",
        "A series of Boolean queries of decreasing specificity is formed.",
        "• IR: The publicly available IR engine MG (Witten et al., 1994) returns the top-ranked N documents."
      ]
    },
    {
      "heading": "Question parsing",
      "text": [
        "• Steps: parse question",
        "find desired semantic type",
        "• Engines: IdentiFinder (BBN) CONTEX",
        "• Steps: extract, combine important words expand query words using WordNet create queries, order by specificity • Engines: Query creator IR • Steps: retrieve top 1000 documents • Engines: MG (RMIT Melbourne) Candidate answer parsing • Steps: parse sentences • Engines: CONTEX Select & rank sentences Sentence selection and ranking • Steps: score each sentence in each document rank sentences and pass top 300 along • Engines:Ranker Matching • Steps: match general constraint patterns against parse trees match desired semantic type against parse tree elements assign score to words in sliding window • Engine: Matcher Ranking and answer extraction • Steps: rank candidate answers extract and format them • Engine: Answer ranker/formatter",
        "• Selecting and ranking sentences: For each",
        "document, the most promising K sentences are located and scored using a formula that rewards word and phrase overlap with the question and its expanded query words.",
        "Results are ranked.",
        "• Parsing candidates: CONTEX parses the top-ranked 300 sentences.",
        "• Pinpointing: As described in Section 3, a number of knowledge resources are used to perform filtering/pinpointing operations.",
        "• Ranking of answers: The candidate answers’ scores are compared and the winner(s) are output."
      ]
    },
    {
      "heading": "3. Knowledge Used for Pinpointing",
      "text": []
    },
    {
      "heading": "3.1 Type 1: Question Word Matching",
      "text": [
        "Unlike (Prager et al., 1999), we do not first annotate the source corpus, but perform IR directly on the source text, using MG (Witten et al., 1994).",
        "To determine goodness, we assign an initial base score to each retrieved sentence.",
        "We then compare the sentence to the question and adapt this score as follows:",
        "• exact matches of proper names double the base score.",
        "• matching an upper-cased term adds a 60% bonus of the base score for multi-words terms and 30% for single words (matching “United States” is better than just “United”).",
        "• matching a WordNet synonym of a term discounts by 10% (lower case) and 50% (upper case).",
        "(When “Cage” matches “cage”, the former may be the last name of a person and the latter an object; the case mismatch signals less reliability.)",
        "• lower-case term matches after Porter stemming are discounted 30%; upper-case matches 70% (Porter stemming is more aggressive than WordNet stemming).",
        "• Porter stemmer matches of both question and sentence words with lower case are discounted 60%; with upper case, 80%.",
        "• if CONTEX indicates a term as being qsubsumed (see Section 3.9) the term is discouned 90% (in “Which country manufactures weapons of mass destruction?”, “country” will be marked as qsubsumed).",
        "The top-scoring 300 sentences are passed on for further filtering."
      ]
    },
    {
      "heading": "3.2 Type 2: Qtargets, the QA Typology, and the Semantic Ontology",
      "text": [
        "We classify desired answers by their semantic type, which have been taxonomized in the Webclopedia QA Typology (Hovy et al., 2002), http://www.isi.edu/natural-language/projects/we bclopedia/Taxonomy/taxonomy toplevel.html).",
        "The currently approx.",
        "180 classes, which we call qtargets, were developed after an analysis of over 17,000 questions (downloaded in 1999 from answers.com) and later enhancements to Webclopedia.",
        "They are of several types:",
        "• common semantic classes such as PROPER-PERSON, EMAIL-ADDRESS, LOCATION, PROPER-ORGANIZATION; • classes particular to QA such as YES:NO, ABBREVIATION-EXPANSION, and WHY-FAMOUS; • syntactic classes such as NP and NOUN, when no semnatic type can be determined (e.g., “What does Peugeot manufacture?”); • roles and slots, such as REASON and TITLE-P respectively, to indicate a desired relation with an anchoring concept.",
        "Given a question, the CONTEX parser uses a set of 276 hand-built rules to identify its most likely qtarget(s), and records them in a backoff scheme (allowing more general qtarget nodes to apply when more specific ones fail to find a match).",
        "The generalizations are captured in a typical concept ontology, a 10,000-node extract of WordNet.",
        "The recursive part of pattern matching is driven mostly by interrogative phrases.",
        "For example, the rule that determines the applicability of the qtarget WHY-FAMOUS requires the question word “who”, followed by the copula, followed by a proper name.",
        "When there is no match at the current level, the system examines any interrogative constituent, or words in special relations to it.",
        "For example, the qtarget TEMPERATURE-QUANTITY (as in “What is the melting point of X?” requires as syntactic object something that in the ontology is subordinate to TEMP-QUANTIFIABLE-ABSTRACT with, as well, the word “how” paired with “warm”, “cold”, “hot”, etc., or the phrase “how many degrees” and a TEMPERATURE-UNIT (as defined in the ontology)."
      ]
    },
    {
      "heading": "3.3 Type 3: Surface Pattern Matching",
      "text": [
        "Often qtarget answers are expressed using rather stereotypical words or phrases.",
        "For example, the year of birth of a person is typically expressed using one of these phrases: <name> was born in <birthyear> <name> (<birthyear>–<deathyear>) We have developed a method to learn such patterns automatically from text on the web (Ravichandran and Hovy, 2002).",
        "We have added into the QA Typology the patterns for appropriate qtargets (qtargets with closed-list answers, such as PLANETS, require no patterns).",
        "Where some QA systems use such patterns exclusively (Soubbotin and Soubbotin, 2001) or partially (Wang et al., 2001; Lee et al., 2001), we employ them as an additional source of evidence for the answer.",
        "Preliminary results on for a range of qtargets, using the TREC-10 questions and the TREC corpus, are:"
      ]
    },
    {
      "heading": "3.4 Type 4: Expected Numerical Ranges",
      "text": [
        "Quantity-targeting questions are often underspecified and rely on culturally shared cooperativeness rules and/or world knowledge: Q: How many people live in Chile?",
        "S1: “From our correspondent comes good news about the nine people living in Chile...” A1: nine While certainly nine people do live in Chile, we know what the questioner intends.",
        "We have hand-implemented a rule that provides default range assumptions for POPULATION questions and biases quantity questions accordingly."
      ]
    },
    {
      "heading": "3.5 Type 5: Abbreviation Expansion",
      "text": [
        "Abbreviations often follow a pattern: Q: What does NAFTA stand for?",
        "S1: “This range of topics includes the North American Free Trade Agreement, NAFTA, and the world trade agreement GATT.” S2: “The interview now changed to the subject of trade and pending economic issues, such as the issue of opening the rice market, NAFTA, and the issue of Russia repaying economic cooperation funds.” After Webclopedia identifies the qtarget as ABBREVIATION-EXPANSION, it extracts possible answer candidates, including “North American Free Trade Agreement” from S1 and “the rice market” from S2.",
        "Rules for acronym matching easily prefer the former."
      ]
    },
    {
      "heading": "3.6 Type 6: Semantic Type Matching",
      "text": [
        "Phone numbers, zip codes, email addresses, URLs, and different types of quantities obey lexicographic patterns that can be exploited for matching, as in Q: What is the zip code for Fremont, CA?",
        "S 1: “...from Everex Systems Inc., 48431 Milmont Drive, Fremont, CA 94538.” and Q: How hot is the core of the earth?",
        "S1.",
        "“The temperature of Earth’s inner core may be as high as 9,000 degrees Fahrenheit (5,000 degrees Celsius).” Webclopedia identifies the qtargets respectively as ZIP-CODE and TEMPERATURE-QUANTITY.",
        "Approx.",
        "30 heuristics (cascaded) apply to the input before parsing to mark up numbers and other orthographically recognizable units of all kinds, including (likely) zip codes, quotations, year ranges, phone numbers, dates, times, scores, cardinal and ordinal numbers, etc.",
        "Similar work is reported in (Kwok et al., 2001)."
      ]
    },
    {
      "heading": "3.7 Type 7: Definitions from WordNet",
      "text": [
        "We have found a 10% increase in accuracy in answering definition questions by using external glosses obtained from WordNet.",
        "For Q: What is the Milky Way?",
        "Webclopedia identified two leading answer candidates: A1: outer regions A2: the galaxy that contains the Earth Comparing these with the WordNet gloss: WordNet: “Milky Way – the galaxy containing the solar system” allows Webclopedia to straightforwardly match the candidate with the greater word overlap.",
        "Curiously, the system also needs to use WordNet to answer questions involving common knowledge, as in: Q: What is the capital of the United States?",
        "because authors of the TREC collection do not find it necessary to explain what Washington is: Ex: “Later in the day, the president returned to Washington, the capital of the United States.” While WordNet’s definition Wordnet: “Washington – the capital of the United States” directly provides the answer to the matcher, it also allows the IR module to focus its search on passages containing “Washington”, “capital”, and “United States”, and the matcher to pick a good motivating passage in the source corpus.",
        "Clearly, this capability can be extended to include (definitional and other) information provided by other sources, including encyclopedias and the web (Lin 2002)."
      ]
    },
    {
      "heading": "3.8 Type 8: Semantic Relation Matching",
      "text": [
        "So far, we have considered individual words and groups of words.",
        "But often this is insufficient to accurately score an answer.",
        "As also noted in (Buchholz, 2001), pinpointing can be improved significantly by matching semantic relations among constituents: Q: Who killed Lee Harvey Oswald?",
        "Qtargets: PROPER-PERSON & PROPER-NAME, PROPER-ORGANIZATION S1: “Belli’s clients have included Jack Ruby, who killed John F. Kennedy assassin Lee Harvey Oswald, and Jim and Tammy Bakker.” S2: “On Nov. 22, 1963, the building gained national notoriety when Lee Harvey Oswald allegedly shot and killed President John F. Kennedy from a sixth floor window as the presidential motorcade passed.”",
        "The CONTEX parser (Hermjakob, 1997; 2001) provides the semantic relations.",
        "The parser uses machine learning techniques to build a robust grammar that produces semantically annotated syntax parses of English (and Korean and Chinese) sentences at approx.",
        "90% accuracy (Hermjakob, 1999).",
        "The matcher compares the parse trees of S1 and S2 to that of the question.",
        "Both S1 and S2 receive credit for matching question words “Lee Harvey Oswald” and “kill” (underlined), as well as for finding an answer (bold) of the proper qtarget type (PROPER-PERSON).",
        "However, is the answer “Jack Ruby” or “President John F. Kennedy”?",
        "The only way to determine this is to consider the semantic relationship between these candidates and the verb “kill” (parse trees simplified, and only portions shown here):",
        "as the head of the logical OBJect.",
        "Thus for S1, the matcher awards additional credit to node [2] (Jack Ruby) for being the logical SUBJect of the killing (using anaphora resolution).",
        "In S2, the parse tree correctly records that node [13] (“Joh F. Kennedy” is not the object of the killing.",
        "Thus despite its being closer to “killed” the candidate in S2 receives no extra credit from semantic relation matching.",
        "It is important to note that the matcher awards extra credit for each matching semantic relationship between two constituents, not only when everything matches.",
        "This granularity improves robustness in the case of partial matches.",
        "Semantic relation matching applies not only to logical subjects and objects, but also to all other roles such as location, time, reason, etc.",
        "(for additional examples see http://www.isi.edu/ natural-la guage/projects/webclopedia/sem-relexamples.html).",
        "It also applies at not only the sentential level, but at all levels, such as post-modifyin prepositional and pre-modifying determiner phrase Additionally, Webclopedia uses 10 lists of word variations with a total of 4029 entries for semantically related concepts such as “to invent” “invention and “inventor” and rule for handling them.",
        "For example, via coercing “invention to “invent” the system can give “Joha Vaaler extra credit for being a likely logical subject of",
        "1899 invention, said Per Langaker of the Norwegian School of Management.” while “David” actually loses points for being outside of the clausal scope of the inventing: S2: “‘Like the guy who invented the safety pin, or the guy who invented the paper clip,’ David added.”"
      ]
    },
    {
      "heading": "3.9 Type 9: Word Window Scoring",
      "text": [
        "Webclopedia also includes a typical window-based scoring module that moves a window over the text and assigns a score to each window position depending on a variety of criteria (Hovy et al., 1999).",
        "Unlike (Clarke et al., 2001; Lee et al., 2001; Chen et al., 2001), we have not developed a very sophisticated scoring function, preferring to focus on the modules that employ information deeper than the word level.",
        "This method is applied only when no other method provides a sufficiently high-scoring answer.",
        "The window scoring function is",
        "w: window width (modulated by gaps of various lengths: “white house” “white car and house”), r: rank of qtarget in list returned by CONTEX, I: window word information content (inverse log frequency score of each word), summed, q: # different question words matched, plus specific rewards (bonus q=3.0), e: penalty if word matches one of question word’s WordNet synset items (e=0.8),",
        "b: bonus for matching main verb, proper names, certain target words (b=2.0), u: (value 0 or 1) indicates whether a word has been qsubsumed (“subsumed” by the qtarget) and should not contribute (again) to the score.",
        "For example, “In what year did Columbus discover America?” the qsubsumed words are “what” and “year”."
      ]
    },
    {
      "heading": "4. Performance Evaluation",
      "text": [
        "In TREC-10’s QA track, Webclopedia received an overall Mean Reciprocal Rank (MRR) score of 0.435, which put it among the top 4 performers of the 68 entrants (the average MRR score for the main QA task was about 0.234).",
        "The pinpointing heuristics are fairly accurate: when Webclopedia finds answers, it usually ranks them in the first place (1st place: 35.5%; 2nd: 8.94%; 3rd: 5.69%; 4th: 3.05%; 5th: 5.28%; not found: 41.87%).",
        "We determined the impact of each knowledge source on system performance, using the TREC-10 test corpus using the standard MRR scoring.",
        "We applied the system to the questions of each knowledge type separately, with and without its specific knowledge source/algorithm.",
        "Results are shown in Table 1, columns A (without) and B (with).",
        "To indicate overall effect, we also show (in columns C and D) the percentage of questions in TREC-10 and 9 respecively of each knowledge type."
      ]
    },
    {
      "heading": "5. Conclusions",
      "text": [
        "It is tempting to search for a single technique that will solve the whole problem (for example, Ittycheriah et al.",
        "(2001) focus on the subset of factoid questions answerable by NPs, and train a statistical model to perform NP-oriented answer pinpointing).",
        "Our experience, however, is that even factoid QA is varied enough to require various special-purpose techniques and knowledge.",
        "The theoretical limits of the various techniques are not known, though Light et al.’s (2001) interesting work begins to study this.",
        "Semantic relation scores measured only on questions in which they could logically apply.",
        "We conclude that factoid QA performance can be significantly improved by the use of knowledge attuned to specific question types and specific information characteristics.",
        "Most of the techniques for exploiting this knowledge require learning to ensure robustness.",
        "To improve performance beyond this, we believe a combination of going to the web and turning to deeper world knowledge and automated inference (Harabagiu et al., 2001) to be the answer.",
        "It remains an open question how much work these techniques would require, and what their payoff limits are."
      ]
    }
  ]
}
