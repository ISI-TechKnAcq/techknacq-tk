{
  "info": {
    "authors": [
      "Nitish Aggarwal",
      "Kartik Asooja",
      "Paul Buitelaar"
    ],
    "book": "SemEval",
    "id": "acl-S12-1095",
    "title": "DERI&UPM: Pushing Corpus Based Relatedness to Similarity: Shared Task System Description",
    "url": "https://aclweb.org/anthology/S12-1095",
    "year": 2012
  },
  "references": [
    "acl-P94-1019",
    "acl-S12-1051"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we describe our system submitted for the semantic textual similarity (STS) task at SemEval 2012.",
        "We implemented two approaches to calculate the degree of similarity between two sentences.",
        "First approach combines corpus-based semantic relatedness measure over the whole sentence with the knowledge-based semantic similarity scores obtained for the words falling under the same syntactic roles in both the sentences.",
        "We fed all these scores as features to machine learning models to obtain a single score giving the degree of similarity of the sentences.",
        "Linear Regression and Bagging models were used for this purpose.",
        "We used Explicit Semantic Analysis (ESA) as the corpus-based semantic relatedness measure.",
        "For the knowledge-based semantic similarity between words, a modified WordNet based Lin measure was used.",
        "Second approach uses a bipartite based method over the WordNet based Lin measure, without any modification.",
        "This paper shows a significant improvement in calculating the semantic similarity between sentences by the fusion of the knowledge-based similarity measure and the corpus-based relatedness measure against corpus based measure taken alone."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Similarity between sentences is a central concept of text analysis, however previous studies about semantic similarities have mainly focused either on single word similarity or complete document similarity.",
        "Sentence similarity can be defined by the degree of semantic equivalence of two given sentences, where sentences are typically 10-20 words long.",
        "The role of sentence semantic similarity measures in text-related research is increasing due to potential number of applications such as document summarization, question answering, information extraction & retrieval and machine translation.",
        "One plausible limitation of existing methods for sentence similarity is their adaptation from long text (e.g. documents) similarity methods, where word co-occurrence plays a significant role.",
        "However, sentences are too short, thats why taking syntactic role of each word with its narrow semantic meaning into account, can be highly relevant to reflect the semantic equivalence of two sentences.",
        "These narrow semantics can be reflected from any existing large lexicons [(Wu and Palmer, 1994) and (Lin, 1998)]; nevertheless, these lexicons can not provide the semantics of words which are out of lexicon (e.g. guy) or multiword expressions.",
        "These semantics can be represented by a large distributed semantic space such as Wikipedia and similarity can be reflected by relatedness of these extracted semantics.",
        "However, relatedness covers broader space than similarity, which forced us to tune the Wikipedia based relatedness with lexical structure (e.g. WordNet) based similarities driven by linguistic syntactic structure, in reflecting more sophisticated similarity of two given sentences.",
        "In this work, we present a sentence similarity using ESA and syntactic similarities.",
        "The rest of this paper is organized as follows.",
        "Section 2 explores the related work.",
        "Section 3 describes our approaches",
        "in detail.",
        "Section 4 explains our three different submitted runs for STS task.",
        "Section 5 shows the results and finally we conclude in section 6."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "In recent years, there have been a variety of efforts in improving semantic similarity measures, however most of these approaches address this problem from the viewpoint of large document similarity based on word co-occurrence using string pattern or corpus statistics.",
        "Corpus based approaches such as Latent Semantic Analysis (LSA) [(Landauer et.",
        "al, 1998) and (Foltz et.",
        "al, 1998)] and ESA (Gabrilovich and Markovitch, 2007) use corpus statistics information about all words and reflect their semantics in distributional high semantic space.",
        "However, these approaches perform quite well for long texts as they use word co-occurrence and relying on the principle that words which are used in the same contexts tend to have related meanings.",
        "In case of short text similarities, syntactic role of each word with its meaning plays an important role.",
        "There are several linguistic measures [( Achananu-parp et.",
        "al, 2008) and (Islam and Inkpen, 2008)], which can account for pseudo-syntactic information by analyzing their word order using n-gram.",
        "To do this, Islam and Inkpen defined a syntactic measure, which considers the word order between two strings by computing the maximal ordered word overlapping.",
        "(Oliva et.",
        "al, 2011) present a similarity measure for sentences and short text that takes syntactic information, such as morphology and parsing tree, into account and calculate similarities between words with same syntactic role, by using WordNet.",
        "Our work takes inspiration from existing approaches that exploit a combination of Wikipedia based relatedness with lexical structure based similarities driven by linguistic syntactic structure."
      ]
    },
    {
      "heading": "3 Methodology",
      "text": [
        "We implemented two approaches for the STS task [(Agirre et.",
        "al, 2012)].",
        "First approach is a fusion of corpus-based semantic relatedness and knowledge-based semantic similarity measures.",
        "The core of this combination is the corpus-based measure because the combination includes the corpus-based semantic relatedness score over the whole sentences and the knowledge-based semantic similarity scores for the words falling under the same syntactic roles in both the sentences.",
        "Machine learning models are trained by taking all these scores as different features.",
        "For the submission, we used Linear regression and Bagging models.",
        "Also, the equation obtained after training the linear regression model shows more weightage to the score obtained by the corpus-based relatedness measure as this is the only score (feature), which reflects the semantic relatedness/similarity score over the full sentences, out of all the considered features for the model.",
        "We used ESA as the corpus based semantic relatedness measure and modified WordNet-based Lin measure as the knowledge-based similarity.",
        "The WordNet-based Lin relatedness measure was modified to reflect better the similarity between the words.",
        "For the knowledge-based similarity, currently we considered only the words lying in the three major syntactic role categories i.e. subjects, actions and the objects.",
        "We see the first approach as the corpus-based measure ESA tuned with the knowledge-based measure.",
        "Thus, it is referred as TunedESA later in the paper.",
        "Our second approach is based on the bipartite method over the WordNet based semantic relatedness measures.",
        "WordNet-based Lin measure (without any modification) was used for calculating the relatedness scores for all the possible corresponding pair of words appearing in both the sentences.",
        "Then, the similarity/relatedness score for the sentences is calculated by perceiving the problem as the computation of a maximum total matching weight of a bipartite graph having the words as nodes and the relatedness scores as the weight of the edges between the nodes.",
        "To solve this, we used Hungarian method.",
        "Later, we refer this method as WordNet-Bipartite."
      ]
    },
    {
      "heading": "3.1 TunedESA",
      "text": [
        "In this approach, the ESA based relatedness score for the full sentences is combined with the modified WordNet-based Lin similarity scores calculated for the words falling under the corresponding syntactic role category in both the sentences.",
        "basic steps: ?",
        "Calculate the ESA relatedness score between the sentences.",
        "?",
        "Find the words corresponding to the linguistic syntactical categories like subject, action and object of both the sentences.",
        "?",
        "Calculate the semantic similarity between the words falling in the corresponding subjects, actions and objects in both the sentences using modified WordNet-based measure Lin.",
        "?",
        "Combine these four scores for ESA, Subject, Action and Object to get the final similarity score on the basis of an already learned machine learning model with the training data.",
        "ESA is a promising technique to find the relatedness between documents.",
        "The texts which need to be",
        "compared are represented as high dimensional vectors containing the TF-IDF weight between the term and the Wikipedia article.",
        "The semantic relatedness measure is calculated by taking the cosine measure between these vectors.",
        "In this implementation of ESA 1, the score was calculated by considering the 1ESA?",
        "considering full sentence at a time to make the vector i.e. different from standard ESA full sentence at a time for making the Wikipedia article vector while in the standard ESA, vectors are made for each word of the text followed by the addition of all these vectors to represent the final vector for the text/sentence.",
        "It was done just to reduce the time complexity.",
        "To calculate the lexical similarity between the words, we implemented WordNet-based semantic relatedness measure Lin.",
        "This score was modified to reflect a better similarity between the words.",
        "In the current system, basic linguistic syntactic categories i.e. subjects, actions and objects were used.",
        "For instance, below is a sentences pair from the training MSRvid dataset with the gold standard score and the syntactic roles.",
        "As the modification, the scores given by Lin measure were used only for the cases where sub-sumption relation or hypernymy/hyponymy exists",
        "between the words.",
        "This modification was done only for the words falling under the category of subjects and objects."
      ]
    },
    {
      "heading": "3.2 WordNet Bipartite",
      "text": [
        "WordNet-based semantic relatedness measure was used for the second approach.",
        "Following steps are performed : ?",
        "Each sentence is tokenized to obtain the words.",
        "?",
        "Semantic relatedness between every possible pair of words in both the sentences is calculated using WordNet-based measure e.g. Lin.",
        "?",
        "Using the scores obtained in the second step, the semantic similarity/relatedness between the sentences is calculated by transforming the problem as that of computing the maximum total matching weight of a bipartite graph, which can be done by using Hungarian method."
      ]
    },
    {
      "heading": "4 System Description",
      "text": [
        "We submitted three runs in the semantic textual similarity task.",
        "The first two runs are based on the first approach i.e. TunedESA and they differ only in the machine learning algorithm used for obtaining the final similarity score based on all the considered scores/features.",
        "ESA was implemented on the current Wikipedia dump.",
        "WordNet based relatedness measure Lin was modified to give a better semantic similarity degree.",
        "Stanford Core-NLP library was used for obtaining the words with their syntactic roles.",
        "All the required scores/feature i.e. ESA based relatedness for the complete sentences and modified WordNet-based Lin similarity scores were calculated for the corresponding words lying in the same syntactic categories.",
        "Bagging and Linear Regression models were built using the training data for the first and second runs respectively.",
        "Based on the category of the test dataset, model was trained on the corresponding training dataset.",
        "For the surprise test datasets, we trained our model with the training dataset of the MSRvid data based on the fact that we obtained good results with this category.",
        "Then the built models were used for calculating the similarity scores for the test data.",
        "For the third run, WordNet Bipartite method was used to calculate the similarity scores.",
        "It didn't require any training."
      ]
    },
    {
      "heading": "5 Results and Discussion",
      "text": [
        "All above described runs are evaluated on STS test dataset.",
        "Table 1 shows the overall results2 of our three runs against the baseline system which follows the bag of words approach.",
        "Table 2 shows the Pearson correlation on different test datasets for all the three runs.",
        "It provides a comparison between corpus based relatedness measure ESA and our system TunedESA (Run 1 & Run 2).",
        "The results show significant improvement against ESA.",
        "Although, it can be seen that the baseline results are even better than of the ESA in the cases of MSRpar and SMTeuro.",
        "It may be because this implementation of ESA is not the standard one."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We presented a method to calculate the degree of sentence similarity based on tuning the corpus based relatedness measure with the knowledge-based similarity measure over the syntactic roles.",
        "The results show a definite improvement by the fusion.",
        "As future work, we plan to improve the syntactic role handling and considering more syntactical categories.",
        "Also, experimentation3 with standard ESA and other semantic similarity/relatedness measures needs to be performed."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work is supported in part by the European Union under Grant No.",
        "248458 for the Monnet project as well as by the Science Foundation Ireland under Grant No.",
        "SFI/08/CE/I1380 (Lion-2)."
      ]
    }
  ]
}
