{
  "info": {
    "authors": [
      "Stephen A. Della Pietra",
      "Vincent J. Della Pietra",
      "Robert L. Mercer",
      "Salim Roukos"
    ],
    "book": "Workshop on Speech and Natural Language",
    "id": "acl-H92-1020",
    "title": "Adaptive Language Modeling Using Minimum Discriminant Estimation",
    "url": "https://aclweb.org/anthology/H92-1020",
    "year": 1992
  },
  "references": [
    "acl-C88-1071",
    "acl-H89-1054",
    "acl-H91-1057"
  ],
  "sections": [
    {
      "heading": "ADAPTIVE LANGUAGE MODELING USING MINIMUM DISCRIMINANT ESTIMATION*",
      "text": [
        "P. 0.",
        "Box 704, Yorktown Heights, NY 10598 higher than if it had not occurred yet.",
        "In addition, if the words in the early part of a document suggest that the current document is from a particular subdomain, then we may expect other related words to occur at a higher rate than the static model may suggest.",
        "For example, the words \"inspection, fire, and insurance\" suggest an insurance report domain, and therefore increased probabilities to words such as \"stairwell and electrical\".",
        "Assume that from a partial document, denoted by h, we have an estimate of the unigram distribution pd(w I h) that a word w may be used in the remaining part of the document.",
        "We will denote pd(w h) by d(w), keeping in mind that this dynamic unigram distribution is continuously updated as the document is dictated and is estimated for a subset of R words of the vocabulary.",
        "(Typically R is on the order of a document size of about a few hundred words as compared to a vocabulary size of 20,000 words.)",
        "In general, the dynamic unigram distribution will be different from the static marginal unigram distribution, denoted by ps(w).",
        "In this paper, we propose a method for adapting the language model so that its marginal unigram distribution matches the desired dynamic unigram distribution d(w).",
        "The proposed approach consists of finding the model that requires the least pertubation of the static model and satisfies the set of constraints that have been derived from the partially observed document.",
        "By least pertubation we mean that the new model is closest to the static model, ps, using the non-symmetric Kullback-Liebler distortion measure (also known as discrimination information, relative entropy, etc.).",
        "The minimum discrimination information (MDI) p* distribution minimizes:",
        "over all p that satisfy a set of R linear constraints.",
        "In this paper, we consider marginal constraints of the"
      ]
    },
    {
      "heading": "ABSTRACT",
      "text": [
        "We present an algorithm to adapt a n-gram language model to a document as it is dictated.",
        "The observed partial document is used to estimate a unigram distribution for the words that already occurred.",
        "Then, we find the closest n-gram distribution to the static n-gram distribution (using the discrimination information distance measure) and that satisfies the marginal constraints derived from the document.",
        "The resulting minimum discrimination information model results in a perplexity of 208 instead of 290 for the static trigram model on a document of 321 words."
      ]
    },
    {
      "heading": "1 INTRODUCTION",
      "text": [
        "Statistical n-gram language models are useful for speech recognition and language translation systems because they provide an a-priori probability of a word sequence; these language models improve the accuracy of recognition or translation by a significant amount.",
        "In the case of trigram (n = 3) and bigram (n = 2) language models, the probability of the next word conditioned on the previous words is estimated from a large corpus of text.",
        "The resulting static language models (SLM) have fixed probabilities that are independent of the document being predicted.",
        "To improve the language model (LM), one can adapt the probabilities of the language model to match the current document more closely.",
        "The partially dictated (in the case of speech recognition) document provides significant clues about what words are more likely to be used next.",
        "One expects many words to be bursty.",
        "For example, if in the early part of a document the word fax has been used then the probability that it will be used again in the same document is significantly",
        "where we are summing over all events i in the set that correspond to the r-th constraint and dr is the desired value (for r = 1, 2, ..., R).",
        "In our case, the events i correspond to bigrams, (w1, w2), and the desired value for the r-th constraint, dr, is the marginal unigram probability, d(tur), for a word wr.",
        "The idea of using a window of the previous N words, called a cache, to estimate dynamic frequencies for a word was proposed in [5] for the case of a tri-part-ofspeech model and in [6] for a bigram model.",
        "In [4] a trigram language was estimated from the cache and interpolated with the static trigram model to yield about 20% lower perplexity and from 5% to 25% lower recognition error rate on documents ranging in length from 100 to 800 words."
      ]
    },
    {
      "heading": "2 MINIMUM DISCRIMINATION INFORMATION",
      "text": [
        "The discrimination information can be written as: D(p, ps) = – E p logps + E p log p (1)",
        "where Rps(p) is the bit rate in transmitting source p with model ps and H (p) is the entropy of source p. The MDI distribution p* satisfies the following Pythagorean inequality:",
        "for all distributions p in the set PR of distributions that satisfy the R constraints.",
        "So if we have an accurate estimate of the constraints then using the MDI distribution will result in a lower error by at least D(p* , ps).",
        "The MDI distribution is the Maximum Entropy (ME) distribution if the static model is the uniform distribution.",
        "Using Lagrange multipliers and differentiating with respect to pi the probability of the i-th event, we find that the optimum must have the form pt= where the factors fir are 1 if event i is not in the constraint set CT or some other value fr if event i belongs to constraint set Cr.",
        "So the MDI distribution is specified by the 13 factors f , r = 1, 2, ..., 11, that correspond to the 11 constraints, in addition to the original static model."
      ]
    },
    {
      "heading": "3 ALTERNATING MINIMIZATION",
      "text": [
        "Starting with an initial estimate of the factors, the following iterative algorithm is guaranteed to converge to the optimum distribution.",
        "At each iteration j, pick a constraint rj and adjust the corresponding factor so that the constraint is satisfied.",
        "In the case of marginal constraints, the update is:",
        "where pj-1(Cri) is the marginal of the previous estimate and dr, is the desired marginal.",
        "This iterative algorithm cycles through the constraints repeatadly until convergence hence the name alternating (thru the constraints) minimization.",
        "It was proposed by Darroch and R.atcliff in [3].",
        "A proof of convergence for linear constraints is given in [2] ."
      ]
    },
    {
      "heading": "4 ME CACHE",
      "text": [
        "We have applied the above approach to adapting a bigram model; we call the resulting model the ME cache.",
        "Using a cache window of the previous N words, we estimate the desired unigram probability of all 11 words that have occurred in the cache by:",
        "where A, is an adjustment factor taken to be the probability that the next word is already in the cache and f is the observed frequency of a word in the cache.",
        "Since any event (uq, w2) participates in 2 constraints one for the left marginal Awl) and the other for the right marginal d(w2) there are 2R+ 1 constraint, a left and right marginal for each word in the cache and the overall normalization, the ME bigram cache model is given by: pr„ ,(w , w2) ai(wi)a,.",
        "(w2)ps(wi, w2) We require the left and right marginals to be equal to get a stationary model.",
        "(Since all events participate in the normalization that factor is absorbed in the other two.)",
        "The iterations fall into two groups: those in which a left marginal is adjusted and those in which a right marginal is adjusted.",
        "In each of these iterations, we adjust two factors simultaneously: one for the desired unigram probability d(w) and the other so that the resulting ME model is a normalized distribution.",
        "The update for left marginals is",
        "where fc, are frequencies estimated from the cache window.",
        "The interpolating weights are Ai = 0.4, A2 = 0.5, and A3 = 0.1.",
        "For the ME cache we replace the dynamic unigram frequency fe1(w3) by the ME conditional bigrarn probability pnie(w31w2) given by:",
        "where pi – i(wi,.)",
        "denotes the left marginal of the (j – 1)-th estimate of the ME distribution and wi is the word that corresponds to the selected constraint at the j-th iteration.",
        "Similar equations can be derived for the updates for the right marginals.",
        "The process is started with po(wi, w2) = ps(wi,w2).",
        "Note that the marginal pi (w, .)",
        "can be computed by using R additions and multiplications.",
        "The algorithm requires order /12 operation to cycle thru all constraints once.",
        "11 is typically few hundred compared to the vocabulary size V which is 20,000 in our case.",
        "We have found that about 3 to 5 iterations are sufficient to achieve convergence."
      ]
    },
    {
      "heading": "5 EXPERIMENTAL RESULTS",
      "text": [
        "Using a cache window size of about 700 words, we estimated a desired unigram distribution and a corresponding ME bigram distribution with an MDI of about 2.2 bits (or 1.1 bits/word).",
        "Since the unigram distribution may not be exact, we do not expect to reduce our perplexity on the next sentence by a factor larger than 2.1 = 21.1.",
        "The actual reduction was a factor of 1.5 = 2°'62 on the next 93 words of the document.",
        "For a smaller cache size the discrepancy between the MDI and actual perplexity reduction is larger.",
        "To evaluate the ME cache model we compared it to the trigram cache model and the static trigram model.",
        "In all models we use linear interpolation between the dynamic and static components as:",
        "where .",
        "= 0.2.",
        "The static and cache trigram probabilities use the usual interpolation between unigram, bigram, and trigram frequencies [1].",
        "The cache trigram probability /), is given by: Note that the sum in the denominator is order 11 since the factors are unity for the words that are not in the cache.",
        "In 'Table 1, we compare the static, the ME cache, and the trigram cache models on three documents.",
        "Both cache models improve on the static.",
        "The ME and trigram cache are fairly close as would be expected since they both have the same dynamic unigram distribution.",
        "The second experiment illustrates how they are different.",
        "We compared the ME cache and the trigram cache on 2 nonsensical sentences made up from words that have occurred in the first sentence of a document.",
        "The 2 sentences are:",
        "• Si: the letter fire to to to • S2: building building building building",
        "Table 2 shows the perplexity of each sentence at 2 points in the document history: one after the first sentence (of length 33 words) is in the cache and the second after 10 sentences (203 words) are in the cache.",
        "We can see that the trigram cache can make some rare bigrams (w1, w2) more likely if both w1 and w2 have already occurred due to a term of the form d(w1)d(w2) whereas the ME cache still has the factor ps(wi, w2) which will tend to keep a rare bigram somewhat less probable.",
        "This is particularly pronounced for S2, where we expect d(building) to be quite accurate after 10 sentences, the ME cache penalizes the unlikely bigram by a factor of about 13 over the trigram cache.",
        "where ai and si are adjustments given by:"
      ]
    },
    {
      "heading": "6 CONCLUSION",
      "text": [
        "The MDI approach to adapting a language model can result in significant perplexity reduction without a leakage in the bigram probability model.",
        "We expect this fact to be important in adapting to a new domain where the unigram distribution d(w) can be estimated from possibly tens of documents.",
        "We are currently pursuing such experiments."
      ]
    },
    {
      "heading": "REFERENCES",
      "text": []
    }
  ]
}
