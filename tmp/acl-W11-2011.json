{
  "info": {
    "authors": [
      "Nina Dethlefs",
      "Heriberto Cuayáhuitl",
      "Jette Viethen"
    ],
    "book": "Proceedings of the SIGDIAL 2011 Conference",
    "id": "acl-W11-2011",
    "title": "Optimising Natural Language Generation Decision Making for Situated Dialogue",
    "url": "https://aclweb.org/anthology/W11-2011",
    "year": 2011
  },
  "references": [
    "acl-A00-2026",
    "acl-C00-1007",
    "acl-C00-1073",
    "acl-N07-1034",
    "acl-P01-1023",
    "acl-P07-1043",
    "acl-P10-1008",
    "acl-P10-1103",
    "acl-P10-1159",
    "acl-P11-2115",
    "acl-P98-1116",
    "acl-W03-2123",
    "acl-W06-1412",
    "acl-W08-1109",
    "acl-W10-4204"
  ],
  "sections": [
    {
      "text": [
        "Optimising Natural Language Generation Decision Making",
        "For Situated Dialogue",
        "Nina Dethlefs Heriberto Cuayahuitl Jette Viethen",
        "dethlefs@uni-bremen.de heriberto.cuayahuitl@dfki .de jviethen@ics .mq.edu.au",
        "Natural language generators are faced with a multitude of different decisions during their generation process.",
        "We address the joint optimisation of navigation strategies and referring expressions in a situated setting with respect to task success and human-likeness.",
        "To this end, we present a novel, comprehensive framework that combines supervised learning, Hierarchical Reinforcement Learning and a hierarchical Information State.",
        "A human evaluation shows that our learnt instructions are rated similar to human instructions, and significantly better than the supervised learning baseline."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Natural Language Generation (NLG) systems are typically faced with a multitude of decisions during their generation process due to nondeterminacy between a semantic input to a generator and its realised output.",
        "This is especially true in situated settings, where sudden changes of context can occur at anytime.",
        "Sources of uncertainty include (a) the situational context, such as visible objects, or task complexity, (b) the user, including their behaviour and reactions, and (c) the dialogue history, including shared knowledge or patterns of linguistic consistency (Halliday and Hasan, 1976) and alignment (Pickering and Garrod, 2004).",
        "Previous work on context-sensitive generation in situated domains includes Stoia et al.",
        "(2006) and Garoufi and Koller (2010).",
        "Stoia et al.",
        "present a supervised learning approach for situated referring expression generation (REG).",
        "Garoufi and Koller use techniques from AI planning for the combined generation of navigation instructions and referring expressions (RE).",
        "More generally, the NLG problem of non-deterministic decision making has been addressed from many different angles, including PENMAN-style choosers (Mann and Matthiessen, 1983), corpus-based statistical knowledge (Langk-ilde and Knight, 1998), tree-based stochastic models (Bangalore and Rambow, 2000), maximum entropy-based ranking (Ratnaparkhi, 2000), combinatorial pattern discovery (Duboue and McKeown, 2001), instance-based ranking (Varges, 2003), chart generation (White, 2004), planning (Koller and Stone, 2007) , or probabilistic generation spaces (Belz, 2008) to name just a few.",
        "More recently, there have been several approaches towards using Reinforcement Learning (RL) (Rieser et al., 2010; Janarthanam and Lemon, 2010) or Hierarchical Reinforcement Learning (HRL) (Deth-lefs and Cuayahuitl, 2010) for NLG decision making.",
        "All of these approaches have demonstrated that HRL/RL offers a powerful mechanism for learning generation policies in the absence of complete knowledge about the environment or the user.",
        "It overcomes the need for large amounts of handcrafted knowledge or data in rule-based or supervised learning accounts.",
        "On the other hand, RL can have difficulties to find an optimal policy in a large search space, and is therefore often limited to small-scale applications.",
        "Pruning the search space of a learning agent by including prior knowledge is therefore attractive, since it finds solutions faster, reduces computational demands, incorporates expert knowledge, and scales to complex problems.",
        "Suggestions to use such prior knowledge include Lithand-craft rules of prior knowledge obvious to the system designer.",
        "Cuayahuitl (2009) suggests using Hierarchical Abstract Machines to partially pre-specify dialogue strategies, and Heeman (2007) uses a combination of RL and Information State (IS) to also pre-specify dialogue strategies.",
        "Williams (2008) presents an approach of combining Partially-Observable Markov Decision Processes with conventional dialogue systems.",
        "The Information State approach is well-established in dialogue management (e.g., Bohlin et al.",
        "(1999) and Larsson and Traum (2000)).",
        "It allows the system designer to specify dialogue strategies in a principled and systematic way.",
        "A disadvantage is that random design decisions need to be made in cases where the best action, or sequence of actions, is not obvious.",
        "The contribution of this paper consists in a comprehensive account of constrained Hierarchical Reinforcement Learning through a combination with a hierarchical Information State (HIS), which is informed by prior knowledge induced from decision trees.",
        "We apply our framework to the generation of navigation strategies and referring expressions in a situated setting, jointly optimised for task success and linguistic consistency.",
        "An evaluation shows that humans prefer our learnt instructions to the supervised learning-based instructions, and rate them equal to human instructions.",
        "Simulation-based results show that our semi-learnt approach learns more quickly than the fully-learnt baseline, which makes it suitable for large and complex problems.",
        "Our approach differs from Heeman's in that we transfer it to NLG and to a hierarchical setting.",
        "Although Hee-man was able to show that his combined approach learns more quickly than pure RL, it is limited to small-scale systems.",
        "Our 'divide-and-conquer' approach, on the other hand, scales up to large search spaces and allows us to address complex problems."
      ]
    },
    {
      "heading": "2. The Generation Tasks",
      "text": [
        "Our domain is the generation of navigation instructions and referring expressions in a virtual 3D world in the GIVE scenario (Koller et al., 2010).",
        "In this task, two people engage in a 'treasure hunt', where an instruction giver (IG) navigates an instruction follower (IF) through the world, pressing a sequence of buttons and completing the task by obtaining a trophy.",
        "Pairs take part in three dialogues (in three different worlds); after the first dialogue, they switch roles.",
        "The GIVE-2 corpus (Gargett et al., 2010) provides transcripts of such dialogues in English and German.",
        "For this paper, we complemented the English dialogues of the corpus with a set of semantic annotations.",
        "The feature set is organised in five groups (Table 1).",
        "The first two groups cover manipulation instructions (i.e., instructions to press a button), including distractors and landmarks (Gargett et al., 2010).",
        "The third group describes high-and low-level navigation, the fourth group describes the user.",
        "The fifth group finally contains grammatical information.",
        "Navigation instructions can take many forms, even for the same route.",
        "For example, a way to another room can be described as 'go to the room with the lamp', 'go left and through the door', or 'turn 90 degrees, left, straight'.",
        "Choosing among these variants is a highly context-and speaker-dependent task.",
        "Figure 1 shows the six user strategies we identified from the corpus based on an analysis of the combination of navigation level ( 'high ' vs. 'low ') and content ( 'destination ', 'direction ', 'orientation ', 'path ', 'straight').",
        "User models are based on the navigation level and content decisions made in a sequence of instructions, so that different sequences, with a certain distribution, lead to different user model classifications.",
        "The proportions are shown in Figure 1.",
        "We found that 75% of all speakers use the same strategy in consecutive rounds/games.",
        "62.5% of pairs are consistent over all three dialogues, indicating inter-speaker alignment.",
        "These high measures of human consistency suggest that this phenomenon is worth modelling in a learning agent, and therefore provides the motivation of including linguistic consistency in our agent's behaviour.",
        "Manipulation instructions were treated as an REG task, which needs to be sensitive to the properties of the referent and distractors (e.g, size, colour, or spatial relation",
        "Table 1 : Corpus annotation features that were used as knowledge of the learning agent and the Information State.",
        "Features are presented in groups, describing the properties of referents in the environment (f1... f13) and their distractors (f14...f15), features of high-and low-level navigation (f16...f18), the user (f19...f23), and grammatical information about constituents (f24---f27).",
        "with respect to the referent) to be natural and distinguishing.",
        "We also considered the visual salience of objects, and the type of spatial relation involved, since recent studies indicate the potential relevance of these features (Viethen and Dale, 2008).",
        "Given these observations, we aim to optimise the task success and linguistic consistency of instructions.",
        "Task success is measured from user reactions after each instruction (Section 5.1).",
        "Linguistic consistency is achieved by rewarding the agent for generating instructions that belong to the same user model as the previous one.",
        "The agent has the same probability for choosing any pattern, but is then rewarded for consistency.",
        "Table 3 (in Section 5.2) presents an example dialogue generated by our system."
      ]
    },
    {
      "heading": "3. Constrained Hierarchical Reinforcement Learning for NLG",
      "text": [
        "Our idea of language generation as an optimisation problem is as follows: given a set of generation states, a set of actions, and an objective reward function, an optimal generation strategy maximises the objective function by choosing the actions leading to the highest reward for every reached state.",
        "Such states describe the system's knowledge about",
        "ID",
        "Feature",
        "Type",
        "Description",
        "h",
        "absolute_property(referent)",
        "boolean",
        "Is the colour of the referent mentioned?",
        "f2",
        "absolute_property(distractor)",
        "boolean",
        "Is the colour of the distractor mentioned?",
        "f3",
        "discriminative_colour(referent)",
        "boolean",
        "Is the colour of the referent discriminating?",
        "f4",
        "discriminative_colour(distractor)",
        "boolean",
        "Is the colour of the distractor discriminating?",
        "f5",
        "mention(distractor)",
        "boolean",
        "Is a distractor mentioned?",
        "f6",
        "first_mention(referent)",
        "boolean",
        "Is this the first reference to the referent?",
        "f7",
        "mention(macroJandmark)",
        "boolean",
        "Is a macro (non-movable) landmark mentioned?",
        "f8",
        "mention(microJandmark)",
        "boolean",
        "Is a micro (movable) landmark mentioned?",
        "f9",
        "num(distractors)",
        "integer",
        "How many distractors are present?",
        "f10",
        "num(microJandmarks)",
        "integer",
        "How many micro landmarks are present?",
        "fll",
        "spatial_rel(referent,obj)",
        "string",
        "Which spatial relation(s) are used in the RE?",
        "fl2",
        "taxonomic-property(referent)",
        "boolean",
        "Is the type of the distractor mentioned?",
        "fl3",
        "within_field_of_vision(referent)",
        "boolean",
        "Is the referent within the user's field of vision?",
        "fu",
        "mention(colour, lm)",
        "boolean",
        "Is the colour of a macro / micro lm mentioned?",
        "fl5",
        "mention(size, lm)",
        "boolean",
        "Is the size of a macro / micro lm mentioned?",
        "f16",
        "abstractness(navjnstruction)",
        "string",
        "Is the instruction explicit or implicit?",
        "fl7",
        "content(nav-instruction)",
        "string",
        "Vals: destination, direction, orientation,path, straight",
        "fl8",
        "level(navJnstruction)",
        "string",
        "Is the instruction high-or low-level?",
        "f19",
        "position(user)",
        "string",
        "Is the user onJrack or off-track?",
        "f20",
        "reaction(user)",
        "string",
        "Vais: take-action, take-wrong-action, wait, reqJielp",
        "f21",
        "type(user)",
        "string",
        "Vais: like s ^waiting, like s-exploring, in-between",
        "f22",
        "waits(user)",
        "boolean",
        "Is the user waiting for the next instruction?",
        "Î23",
        "model(user)",
        "string",
        "User model/navig.",
        "strategy used (cf.",
        "Fig.",
        "1)?",
        "f24",
        "actor(instruction)",
        "boolean",
        "Is the actor of the instruction inserted?",
        "f25",
        "mood(instruction)",
        "boolean",
        "Is the mood of the instruction inserted?",
        "f26",
        "process(instruction)",
        "boolean",
        "Is the process of the instruction inserted?",
        "Î27",
        "locationaLphrase(instruction)",
        "boolean",
        "Is the loc.",
        "phrase (path, straight, etc.)",
        "inserted?",
        "Figure 1: Decision tree for the classification of user models (UM) defined by the use of navigation level and content.",
        "UM 0=high-level, UM 1 =low-level (LL), UM 2=orientation-based LL, UM 3=orientation-based mixture (M), UM4=path-basedM, UM5=pureM.",
        "the generation task (e.g. navigation strategy, or referring expressions).",
        "The action set describes the system's capabilities (e.g. 'use high level navigation strategy' , 'mention colour of referent' , etc.).",
        "The reward function assigns a numeric value for each action taken.",
        "In this way, language generation can be seen as a finite sequence of states, actions and rewards {s0,a0,r\\,s\\,a\\, ...,rt-\\,st}, where the goal is to find an optimal strategy automatically.",
        "To do this we use RL with a divide-and-conquer approach in order to optimise a hierarchy ofgeneration policies rather than a single policy.",
        "The hierarchy of RL agents consists of L levels and N models per level, denoted as Mj, where j G {0,...,N – 1} and i G {0, ...,L – 1}.",
        "Each agent of the hierarchy is defined as a Semi-Markov Decision Process (SMDP) consisting of a 4-tuple < Sij, Aj ,Tij, Rj >.",
        "Sj is a set of states, Aj is a set of actions, Tj is a transition function that determines the next state s from the current state s and the performed action a, and Rj is a reward function that specifies the reward that an agent receives for taking an action a in state s lasting t time steps.",
        "The random variable t represents the number of time steps the agent takes to complete a subtask.",
        "Actions can be either primitive or composite.",
        "The former yield single rewards, the latter correspond to SMDPs and yield cumulative discounted rewards.",
        "The goal of each SMDP is to find an optimal policy that maximises the reward for each visited state, according to n* j (s) = argmaxa€Ai Q* j (s,a), where Q**i(s,a) specifies the expected cumulative reward for executing action a in state s and then following policy n*j.",
        "We use HSMQ-Learning (Dietterich, 1999) for learning a hierarchy of generation policies.",
        "This hierarchical approach has been applied successfully to dialogue strategy learning by Cuayahuitl et al.",
        "(2010).",
        "The notion of an Information State has traditionally been applied to dialogue, where it encodes all information relevant to the current state of the dialogue.",
        "This includes, for example, the context of the interaction, participants and their beliefs, and the status of grounding.",
        "An IS consists of a set of informational components, encoding the information of the dialogue, formal representations of these components, a set of dialogue moves leading to the update of the IS, a set of update rules which govern the update, and finally an update strategy, which specifies which update rule to apply in case more than one applies (Larsson and Traum (2000), p. 2-3).",
        "In this paper, we apply the theory of IS to language generation.",
        "For this purpose we define the informational components of an IS to represent the (situational and linguistic) knowledge of the generator (Section 4.2).",
        "Update rules are triggered by generator actions, such as the decision to insert a new constituent into the current logical form, or the decision to prefer one word order sequence over another.",
        "We use the DIPPER toolkit (Bos et al., 2003) for our implementation of the IS.",
        "3.3 Combining Hierarchical Reinforcement Learning and Information State",
        "Previous work has suggested the HSMQ-Learning algorithm for optimizing text generation strategies (Dethlefs and Cuayahuitl, 2010).",
        "Because such an algorithm uses all available actions in each state, an important extension is to constrain the actions available with some prior expert knowledge, aiming to combine behaviour specified by human designers and behaviour automatically inferred by reinforcement learning agents.",
        "To that end, we sug-",
        "(Manipulation ^ystemActyj^i",
        "(Macro LM) (Distractor) ( Micro LM Ml",
        "Figure 2: (Left:) Hierarchy of learning agents executed from top to bottom for generating instructions.",
        "(Right:) State representations for the agents shown in the hierarchy on the left.",
        "The features f1 ...f27 refer back to the features used in the annotation given in the first column of Table 1.",
        "Note that agents can share information across levels.",
        "gest combining the Information State approach with hierarchical reinforcement learning.",
        "We therefore redefine the characterisation of each Semi-Markov Decision Process (SMDP) in the hierarchy as a 5-tuple model Mj =< Sj,Aj,Tj,Rj,Ij >, where Sj, Aj, Tj and Rj are as before, and the additional element Ij is an Information State used as knowledge base and rule-based decision maker.",
        "In this extended model, action selection is based on a constrained set of actions provided by the IS update rules.",
        "We assume that the names of update rules in Ij represent the agent actions Aj.",
        "The goal of each SMDP is then to find an optimal policy that maximises the reward for each visited state, according to 7r*j(s) = argmaxa€Aim?",
        "Q*j(s,a), where Q*%(s,a) specifies the expected cumulative reward for executing constrained action a in state s and then following n*j thereafter.",
        "For learning such policies we use a modified version of HSMQ-Learning.",
        "This algorithm receives subtask Mj and Information State Ij used to initialise state s, performs similarly to Q-Learning for primitive actions, but for composite actions it invokes recursively with a child sub-task.",
        "In contrast to HSMQ-Learning, this algorithm chooses actions from a subset derived by applying the IS update rules to the current state of the world.",
        "When the subtask is completed, it returns a cumulative reward rt+r, and continues its execution until finding a goal state for the root subtask.",
        "This process iterates until convergence occurs to optimal context-independent policies, as in HSMQ-Learning."
      ]
    },
    {
      "heading": "4. Experimental Setting 4.1 Hierarchy of Agents",
      "text": [
        "Figure 2 shows a (hand-crafted) hierarchy of learning agents for navigating and acting in a situated environment.",
        "Each of these agents represents an individual generation task.",
        "Model MQ is the root agent and is responsible for ensuring that a set of navigation instructions guide the user to the next referent, where an RE is generated.",
        "Model Mq is responsible for the generation of the RE that best describes an intended referent.",
        "Subtasks MQ ... M| realise surface forms of possible distractors, or macro / micro landmarks.",
        "Model Mf is responsible for the generation of navigation instructions which smoothly fit into the linguistic consistency pattern chosen.",
        "Part of this task is choosing between a low-level (model Mf ) and a high-level (model M|) instruction.",
        "Sub-tasks Mq...M| realise the actual instructions, destination, direction, orientation, path, and 'straight', respectively.",
        "Finally, model can repair previous system utterances.",
        "Model",
        "State variables (features)",
        "M0\"",
        "/l9--/22",
        "Mo",
        "/l – /lO, /l2 – fl3, fia – /20, /22",
        "M{",
        "ha – fa",
        "Ml",
        "/17 – /23",
        "Ml",
        "hi /ll, /l3---/l5",
        "Ml",
        "h – h, hi, fi3",
        "M\\",
        "hi /ill /l3, /20 ,/22",
        "Ml, Mi",
        "/l6, /21, /20 ,/22.../23",
        "M0...M|",
        "/24\"./27",
        "Table 2: Action set of the learning agents and Information States.",
        "The HRL agent's knowledge base consists of all situational and linguistic knowledge the agent needs for decision making.",
        "Figure 2 shows the hierarchy of learning agents together with the knowledge base of the learning agent with respect to the semantic features shown in Table 1 that were used for the annotation of the GIVE-2 corpus dialogues.",
        "The first column of the table in Figure 2 indicates the respective model, also referred to as agent, or subtask, and the second column refers to the knowledge variable it uses (in the form of the feature index given in the first column of Table 1).",
        "In the agent, boolean values and strings were represented as integers.",
        "The HIS shares all information of the learning agent, but has an additional set of relational feature-value pairs for each slot.",
        "For example, if the agent knows that the slot content{nav-instruction) has value 1 (meaning 'filled'), the HIS knows also which value it was filled with, such as path.",
        "Such additional knowledge is required for the supervised learning baseline (Section 5).",
        "The action set of the hierarchical learning agent and the hierarchical information state is given in Table 2.",
        "The state-action space size of a flat learning agent would be \\S x A\\ = 10, the hierarchical setting has a state-action space size of 2.4 x 10.",
        "The average state-action space size of all subtasks is \\S x A\\/14 = 1.7 x 10.",
        "Generation actions can be primitive or composite.",
        "While the former correspond to single generation decisions, the latter represent separate generation subtasks (Fig.",
        "2).",
        "Prior knowledge can include decisions obvious to the system designer, expert knowledge, or general intuitions.",
        "In our case, we use a supervised learning approach to induce prior knowledge into our HRL agent.",
        "We trained decision trees on our annotated corpus data using Weka's (Witten and Frank, 2005) J48 decision tree classifer.",
        "A separate tree was trained for each semantic attribute (cf. Table 1).",
        "The obtained decision trees represent our supervised learning baseline.",
        "They achieved an accuracy of 91% in a tenfold cross-validation.",
        "For our semi-learnt combination ofHRL and HIS, we performed a manual analysis of the resulting rules to assess their impact on a learning agent.",
        "In the end, the following rules were used to constrain the agent's behaviour: (1) In REs, always use a referent's colour, except in cases of repair when colour is not discriminating; (2) mention a distractor or micro landmark, if the colour of the referent is not discriminating; (3) in navigation, always make orientation instructions explicit.",
        "All remaining behaviour was subject to learning.",
        "We use the following reward function to train the hierarchy of policies of our HRL agent.",
        "It aims to reduce discourse length at maximal task success using a consistent navigation strategy.",
        "Model(s)",
        "Actions",
        "M0°",
        "navigation, manipulation, confirmation, stop, repair_system_act, repair_no_system_act",
        "Mo",
        "insert_distractor, insert_no_distractor, insert_no_absolute_property, insert_micro_relatum, insert_macro_relatum insert_no-taxonomic-property, insert_absolute_property, insert_no_macro_relatum, insert_taxonomic_property",
        "M\\",
        "chooseJùghJevel, chooseJowJevel, get_route, choose_easy_route, choose_short_route",
        "Ml... Ml",
        "expJiead, exp_no_head, insert_colour, insert_no_colour, insert_size, insert_no_size, exp_spatial_relation",
        "Ml",
        "choose_explicit_abstractness, choose_implicit_abstractness, destination instruction, path-instruction",
        "M\\",
        "choose_explicit_abstractness, choose_implicit_abstractness, directionJnstr, orientationJnstr, straightinstr",
        "Ml... M\\",
        "exp_actor, exp_no_actor, expjnood, expJoc.phrase, exp_noJoc.phrase, exp.process, exp_no_process",
        "0",
        "for",
        "reaching the goal state",
        "-2",
        "for",
        "an already invoked subtask",
        "+1",
        "for",
        "generating instruction u con-",
        "sistent with instruction u-1",
        "-1",
        "otherwise.",
        "The third reward that encourages consistency of instructions rewards a sequence of actions that allow the last generated instruction to be classified as belonging to the same navigation strategy/user model as the previously generated instruction (cf. 2.2)."
      ]
    },
    {
      "heading": "5. Experiments and Results",
      "text": [
        "The simulated environment contains two kinds of uncertainties: (1) uncertainty regarding the state of the environment, and (2) uncertainty concerning the user's reaction to a system utterance.",
        "The first aspect is represented by a set of contextual variables describing the environment, and user behaviour.",
        "Altogether, this leads to 115 thousand different contextual configurations, which are estimated from data (cf.",
        "Section 2.1).",
        "The uncertainty regarding the user's reaction to an utterance is represented by a Naive Bayes classifier, which is passed a set of contextual features describing the situation, mapped with a set of semantic features describing the utterance.",
        "From these data, the classifier specifies the most likely user reaction (after each system act) of perforni-desired-action, perform-undesired-action, wait and request-help.",
        "The classifier was trained on the annotated data and reached an accuracy of 82% in a tenfold cross validation.",
        "With respect to REs, the fully-learnt policy (only HRL) uses colour when it is discriminating, and a distractor or micro landmark otherwise.",
        "The semi-learnt policy (HRL with HIS) behaves as defined in Section 4.3.",
        "The supervised learning policy (only HIS) uses the rules learnt by the decision trees.",
        "Both learnt policies learn to maximise task success, and to generate consistent navigation strategies.",
        "The",
        "Figure 3 : Comparison of fully-learnt, semi-learnt, and supervised learning (deterministic) behaviours.",
        "supervised learning policy generates successful instructions from the start.",
        "Note that we are not actually learning dialogue strategies, but rather generation strategies using dialogue features.",
        "Therefore the described policies, fully-learnt, semi-learnt and supervised-learning, exclusively guide the system's behaviour in the interaction with the simulated user.",
        "An example dialogue is shown in Table 3.",
        "We can observe that the agent starts using a low level navigation strategy, and then switches to high level.",
        "When the user gets confused, the system temporarily switches back to low level.",
        "For referring expressions, it first attempts to locate the referent by reference to a distractor, and then repairs by using a micro landmark.",
        "The surface forms of instructions were realised from templates, since the NLG system so far only generates a sequence of content selection decisions.",
        "We address surface realisation in Dethlefs and Cuayahuitl (2011).",
        "We compared our semi-learnt policy against a fully-learnt, and a supervised learning baseline.",
        "All policies were trained for 40 thousand episodes.",
        "For training, the step-size parameter a, which indicates the learning rate, was initiated with 1 and then reduced over time by a = j^, where t is the time step.",
        "The discount rate 7, which indicates the relevance of future rewards in relation to immediate rewards, was set to 0.99, and the probability of a random action e was 0.01.",
        "See (Sutton and Barto, 1998) for details on these parameters.",
        "w",
        "T",
        "«.--\"",
        "-//.....",
        "-",
        " – ",
        "Deterministic Semi-Learnt",
        "-",
        "-",
        "Fully-Learnt",
        "\\_______",
        "y",
        "*",
        "Table 3 : Sample dialogue in the GIVE-2 scenario showing the dynamics of generation policies.",
        "See Figure 2 for the corresponding hierarchy models, and Table 2 for the action set.",
        "See Section 5.2 for an explantation of the dialogue.",
        "while the semi-learnt behaviour is able to follow a near-optimal policy from the beginning, the fully-learnt policy takes about 40 thousand episodes to reach the same performance.",
        "In terms of simulated task success, we see that while the supervised learning behaviour follows a good policy from the start, it is eventually beaten by the learnt policies.",
        "We asked 11 participants to rate altogether 132 sets of instructions, where each set contained a spatial graphical scene containing a person, mapped with one human, one learnt, and one supervised learning instruction.",
        "Instructions consisted ofa navigation instruction followed by a referring expression.",
        "Subjects were asked to rate instructions on a 1-5 Likert scale (where 5 is the best) for their helpfulness on guiding the displayed person from its origin to pressing the intended button.",
        "We selected six different scenarios for the evaluation: (a) only one button is present, (b) two buttons are present, the referent and a distractor of the same colour as the referent, (c) two buttons are present, the referent and a distractor of a different colour than the referent, (d) one micro landmark is present and one dis-tractor of the same colour as the referent, (e) one micro landmark is present and one distractor of a different colour than the referent.",
        "All scenarios oc-",
        "Agent",
        "Policy",
        "Action (composite in italics)",
        "Utterance",
        "Usr",
        "request_route",
        "How do I get to the trophy?",
        "Sys",
        "root, navigation, dontRepair, orientation",
        "low level navigation, add direction+degrees",
        "First, turn 90 degrees left.",
        "*l TT?",
        "low level navigation, 'straight'",
        "Then go straight ahead.",
        "Usr",
        "perform-desired-action",
        "Sys",
        "vrf, vr|, 7r|",
        "confirmation",
        "high level navigation, destination, direction,",
        "OK.",
        "vrf, vr|, 7r|",
        "addLocation",
        "Turn left at the end of the hallway.",
        "Usr",
        "wait",
        "Sys",
        "*î",
        "repair system act, change navigation strategy",
        "low level navigation, direction",
        "Turn right",
        "high level navigation, path",
        "and go down the hallway.",
        "Usr",
        "perform-desired-action",
        "Sys",
        "low level navigation, direction",
        "Now turn left.",
        "Usr",
        "perform_desired_action",
        "Sys",
        "vr|, 7r|",
        "confirmation",
        "high level navigation, destination,",
        "OK.",
        "vrf, 7r|",
        "addLocation",
        "Enter to the room in front of you.",
        "Usr",
        "perform_desired_action",
        "Sys",
        "manipulation, use colour, use distractor",
        "Press the red button beside the blue.",
        "Usr",
        "requestJielp",
        "Sys",
        "repair system act, change granularity high level navigation, manipulation",
        "destination, use colour, use micro landmark",
        "Go to the red button beside the chair.",
        "Usr",
        "perform-desired-action",
        "Sys",
        "manipulation",
        "Press it.",
        "Usr",
        "perform-desired-action",
        "Sys",
        "*0°",
        "confirmation",
        "Well done.",
        "(1) Please rate each instruction for its helpfulness on a scale of 1 to 5, where 5 represents the best and 1 the worst.",
        "(a) Go to the room on the left.",
        "Press the green button.",
        "(2) Please circle the intended referent.",
        "curred twice in each evaluation sheet, their specific instances were drawn from the GIVE-2 corpus at random.",
        "Scenes and instructions were presented in a randomised order.",
        "Figure 4 presents an example evaluation scene.",
        "Finally, we asked subjects to circle the object they thought was the intended referent.",
        "Subjects rated the human instructions with an average of 3.82, the learnt instructions with an average of 3.55, and the supervised learning instructions with an average of 2.39.",
        "The difference between human and learnt is not significant.",
        "The difference between learnt and supervised learning is significant at p < 0.003, and the difference between human and supervised learning is significant at p < 0.0002.",
        "In 96% of all cases, users were able to identify the intended referent."
      ]
    },
    {
      "heading": "6. Conclusion and Discussion",
      "text": [
        "We have presented a combination of HRL with a hierarchical IS, which was informed by prior knowledge from decision trees.",
        "Such a combined framework has the advantage that it allows us to systematically pre-specify (obvious) generation strategies, and thereby find solutions faster, reduce computational demands, scale to complex domains, and incorporate expert knowledge.",
        "By applying HRL to the remaining (non-obvious) action set, we are able to learn a flexible, generalisable NLG policy, which will take the best action even under uncertainty.",
        "As an application of our approach and its generalisability across domains, we have presented the joint optimisation of two separate NLG tasks, navigation instructions and referring expressions, in situated dialogue under the aspects of task success and linguistic consistency.",
        "Based on an evaluation in a simulated environment estimated from data, we showed that our semi-learnt behaviour outperformed a fully-learnt baseline in terms of learning speed, and a supervised learning baseline in terms of average rewards.",
        "Human judges rated our instructions significantly better than the supervised learning instructions, and close to human quality.",
        "The study revealed a task success rate of 96%.",
        "Future work can transfer our approach to different applications to confirm its benefits, and induce the agent's reward function from data to test in a more realistic setting."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "Thanks to the German Research Foundation DFG and the Transregional Collaborative Research Centre SFB/TR8 'Spatial Cognition' and the EU-FP7 project ALIZ-E (ICT-248116) for partial support of this work.",
        "Also, thanks to John Bateman for comments on an earlier draft of this paper.",
        "i",
        "i",
        "mm",
        "II",
        "V",
        "□"
      ]
    }
  ]
}
