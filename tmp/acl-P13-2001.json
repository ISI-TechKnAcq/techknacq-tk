{
  "info": {
    "authors": [
      "Hassan Sajjad",
      "Kareem Darwish",
      "Yonatan Belinkov"
    ],
    "book": "ACL",
    "id": "acl-P13-2001",
    "title": "Translating Dialectal Arabic to English",
    "url": "https://aclweb.org/anthology/P13-2001",
    "year": 2013
  },
  "references": [
    "acl-A00-1002",
    "acl-D09-1141",
    "acl-D11-1128",
    "acl-E06-1047",
    "acl-J03-1002",
    "acl-N03-1017",
    "acl-N12-1006",
    "acl-N13-1044",
    "acl-P08-2030",
    "acl-P10-1048",
    "acl-P11-2007",
    "acl-P12-2035",
    "acl-P12-2059",
    "acl-W11-2123",
    "acl-W11-2602"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a dialectal Egyptian Arabic to English statistical machine translation system that leverages dialectal to Modern Standard Arabic (MSA) adaptation.",
        "In contrast to previous work, we first narrow down the gap between Egyptian and MSA by applying an automatic character-level transformational model that changes Egyptian to EG?, which looks similar to MSA.",
        "The transformations include morphological, phonological and spelling changes.",
        "The transformation reduces the out-of-vocabulary (OOV) words from 5.2% to 2.6% and gives a gain of 1.87 BLEU points.",
        "Further, adapting large MSA/English parallel data increases the lexical coverage, reduces OOVs to 0.7% and leads to an absolute BLEU improvement of 2.73 points."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Modern Standard Arabic (MSA) is the lingua franca for the Arab world.",
        "Arabic speakers generally use dialects in daily interactions.",
        "There are 6 dominant dialects, namely Egyptian, Moroccan, Levantine, Iraqi, Gulf, and Yemeni1.",
        "The dialects may differ in vocabulary, morphology, syntax, and spelling from MSA and most lack spelling conventions.",
        "Different dialects often make different lexical choices to express concepts.",
        "For example, the concept corresponding to ?Oryd?",
        "YK",
        "expressed as ?EAwz?",
        "P?A?",
        "in Egyptian, ?Abgy?",
        "?",
        "?K.",
        "@ in Gulf, ?Aby?",
        "?",
        "G. @ in Iraqi, and ?bdy?",
        "?",
        "Arabic dialects may differ morphologically from MSA.",
        "For example, Egyptian Arabic uses a negation construct similar to the French ?ne pas?",
        "negation construct.",
        "The Egyptian word ?mlEbt$?",
        "?",
        "J.???",
        "(or alternatively spelled ?J.??A?)",
        "(?I did not play?)",
        "is composed of ?m+lEbt+$?.",
        "The pronunciations of letters often differ from one dialect to another.",
        "For example, the letter ?q?",
        "?",
        "is typically pronounced in MSA as an unvoiced uvular stop (as the ?q?",
        "in ?quote?",
        "), but as a glottal stop in Egyptian and Levantine (like ?A?",
        "in ?Alpine?)",
        "and a voiced velar stop in the Gulf (like ?g?",
        "in ?gavel?).",
        "Differing pronunciations often reflect on spelling.",
        "Social media platforms allowed people to express themselves more freely in writing.",
        "Although MSA is used in formal writing, dialects are increasingly being used on social media sites.",
        "Some notable trends on social platforms include (Darwish et al., 2012): - Mixed language texts where bilingual (or multilingual) users code switch between Arabic and English (or Arabic and French).",
        "In the example ?wSlny mrsy?",
        "?",
        "??Q?",
        "?",
        "????",
        "(?got it thank you?",
        "), ?thank you?",
        "is the transliterated French word ?merci?.",
        "?",
        "The use of phonetic transcription to match dialectal pronunciation.",
        "For example, ?Sdq?",
        "?Y?",
        "(?truth?)",
        "is often written as ?Sj?",
        "l .?",
        "in Gulf dialect.",
        "?",
        "Creative spellings, spelling mistakes, and word elongations are ubiquitous in social texts.",
        "?",
        "The use of new words like ?lol?",
        "???",
        "(?LOL?).",
        "?",
        "The attachment of new meanings to words such as using ?THn?",
        "?j?",
        "to mean ?very?",
        "while it means ?grinding?",
        "in MSA.",
        "The Egyptian dialect has the largest number of speakers and is the most commonly understood dialect in the Arab world.",
        "In this work, we focused on translating dialectal Egyptian to English us-1 ing Egyptian to MSA adaptation.",
        "Unlike previous work, we first narrowed the gap between Egyptian and MSA using character-level transformations and word n-gram models that handle spelling mistakes, phonological variations, and morphological transformations.",
        "Later, we applied an adaptation method to incorporate MSA/English parallel data.",
        "The contributions of this paper are as follows: ?",
        "We trained an Egyptian/MSA transformation model to make Egyptian look similar to MSA.",
        "We publicly released the training data.",
        "?",
        "We built a phrasal Machine Translation (MT) system on adapted Egyptian/English parallel data, which outperformed a non-adapted baseline by 1.87 BLEU points.",
        "?",
        "We used phrase-table merging (Nakov and Ng, 2009) to utilize MSA/English parallel data with the available in-domain parallel data."
      ]
    },
    {
      "heading": "2 Previous Work",
      "text": [
        "Our work is related to research on MT from a resource poor language (to other languages) by pivoting on a closely related resource rich language.",
        "This can be done by either translating between the related languages using word-level translation, character level transformations, and language specific rules (Durrani et al., 2010; Hajic?",
        "et al, 2000; Nakov and Tiedemann, 2012), or by concatenating the parallel data for both languages (Nakov and Ng, 2009).",
        "These translation methods generally require parallel data, for which hardly any exists between dialects and MSA.",
        "Instead of translating between a dialect and MSA, we tried to narrow down the lexical, morphological and phonetic gap between them using a character-level conversion model, which we trained on a small set of parallel dialect/MSA word pairs.",
        "In the context of Arabic dialects3, most previous work focused on converting dialects to MSA and vice versa to improve the processing of dialects (Sawaf, 2010; Chiang et al., 2006; Mohamed et al., 2012; Utiyama and Isahara, 2008).",
        "Sawaf (2010) proposed a dialect to MSA normalization that used character-level rules and morphological analysis.",
        "Salloum and Habash (2011) also used a rule-based method to generate MSA paraphrases of dialectal out-of-vocabulary (OOV) and low frequency words.",
        "Instead of rules, we automatically 3Due to space limitations, we restrict discussion to work on dialects only.",
        "learnt character mappings from dialect/MSA word pairs.",
        "Zbib et al. (2012) explored several methods for dialect/English MT.",
        "Their best Egyptian/English system was trained on dialect/English parallel data.",
        "They used two language models built from the English GigaWord corpus and from a large web crawl.",
        "Their best system outperformed manually translating Egyptian to MSA then translating using an MSA/English system.",
        "In contrast, we showed that training on in-domain dialectal data irrespective of its small size is better than training on large MSA/English data.",
        "Our LM experiments also affirmed the importance of in-domain English LMs.",
        "We also showed that a conversion does not imply a straight forward usage of MSA resources and there is a need for adaptation which we fulfilled using phrase-table merging (Nakov and Ng, 2009)."
      ]
    },
    {
      "heading": "2.1 Baseline",
      "text": [
        "We constructed baselines that were based on the following training data: - An Egyptian/English parallel corpus consisting of ?38k sentences, which is part of the LDC2012T09 corpus (Zbib et al., 2012).",
        "We randomly divided it into 32k sentences for training, 2k for development and 4k for testing.",
        "We henceforth refer to this corpus as EG and the English part of it as EGen.",
        "We did not have access to the training/test splits of Zbib et al. (2012) to directly compare to their results.",
        "- An MSA/English parallel corpus consisting of 200k sentences from LDC4.",
        "We refer to this corpus as the AR corpus.",
        "For language modeling, we used either EGen or the English side of the AR corpus plus the English side of NIST12 training data and English GigaWord v5.",
        "We refer to this corpus as GW.",
        "We tokenized Egyptian and Arabic according to the ATB tokenization scheme using the MADA+TOKAN morphological analyzer and to-kenizer v3.1 (Roth et al., 2008).",
        "Word elongations were already fixed in the corpus.",
        "We word-aligned the parallel data using GIZA++ (Och and Ney, 2003), and symmetrized the alignments using grow-diag-final-and heuristic (Koehn et al., 2003).",
        "We trained a phrasal MT system (Koehn et al., 2003).",
        "We built five-gram LMs using KenLM",
        "training sets with GW and EGen corpora for LM training with modified Kneser-Ney smoothing (Heafield, 2011).",
        "In case of more than one LM, we tuned their weights on a development set using Minimum Error Rate Training (Och and Ney, 2003).",
        "We built several baseline systems as follows: ?",
        "B1 used AR for training a translation model and GW for LM.",
        "?",
        "B2-B4 systems used identical training data, namely EG, with the GW, EGen, or both for B2, B3, and B4 respectively for language modeling.",
        "Table 1 reports the baseline results.",
        "The system trained on AR (B1) performed poorly compared to the one trained on EG (B2) with a 6.75 BLEU points difference.",
        "This highlights the difference between MSA and Egyptian.",
        "Using EG data for training both the translation and language models was effective.",
        "B4 used two LMs and yielded the best results.",
        "For later comparison, we only use the B4 baseline."
      ]
    },
    {
      "heading": "3 Proposed Methods",
      "text": []
    },
    {
      "heading": "3.1 Egyptian to EG? Conversion",
      "text": [
        "As mentioned previously, dialects differ from MSA in vocabulary, morphology, and phonology.",
        "Dialectal spelling often follows dialectal pronunciation, and dialects lack standard spelling conventions.",
        "To address the vocabulary problem, we used the EG corpus for training.",
        "To address the spelling and morphological differences, we trained a character-level mapping model to generate MSA words from dialectal ones using character transformations.",
        "To train the model, we extracted the most frequent words from a dialectal Egyptian corpus, which had 12,527 news comments (containing 327k words) from Al-Youm Al-Sabe news site (Zaidan and CallisonBurch, 2011) and translated them to their equivalent MSA words.",
        "We hired a professional translator, who generated one or more translations of the most frequent 5,581 words into MSA.",
        "Out of these word pairs, 4,162 involved character-level transformations due to phonological, morphological, or spelling changes.",
        "We aligned the translated pairs at character level using GIZA++ and Moses in the manner described in Section 2.1.",
        "As in the baseline of Kahki et al. (2011), given a source word, we produced all of its possible segmentations along with their associated character-level mappings.",
        "We restricted individual source character sequences to be 3 characters at most.",
        "We retained all mapping sequences leading to valid words in a large lexicon.",
        "We built the lexicon from a set of 234,638 Aljazeera articles5 that span a 10 year period and contain 254M tokens.",
        "Spelling mistakes in Aljazeera articles were very infrequent.",
        "We sorted the candidates by the product of the constituent mapping probabilities and kept the top 10 candidates.",
        "Then we used a trigram LM that we built from the aforementioned Aljazeera articles to pick the most likely candidate in context.",
        "We simply multiplied the character-level transformation probability with the LM probability ?",
        "giving them equal weight.",
        "Since Egyptian has a ?ne pas?",
        "like negation construct that involves putting a ???",
        "and ?",
        "??",
        "at the beginning and end of verbs, we handled words that had negation by removing these two letters, then applying our character transformation, and lastly adding the negation article ?lA?",
        "B before the verb.",
        "We converted theEG train, tune, and test parts.",
        "We refer to the converted corpus as EG?.",
        "As an example, our system transformed Yg ?.",
        "j.",
        "?J",
        "?",
        "@ to ?Al*y?",
        "?",
        "Y?",
        "@ involved a spelling correction.",
        "The transformation of ?byHSlhm?",
        "???",
        "?jJ K. to ?yHSl lhm?",
        "???",
        "?",
        "?m' involved a morphological change and word splitting.",
        "Changing ?myEjb$?",
        "?.",
        "j.",
        "?J ?",
        "to ?lA yEjb?",
        "I. j.",
        "?K B involved morphologically transforming a negation construct."
      ]
    },
    {
      "heading": "3.2 Combining AR and EG?",
      "text": [
        "The aforementioned conversion generated a language that is close, but not identical, to MSA.",
        "In order to maximize the gain using both parallel corpora, we used the phrase merging technique described in Nakov and Ng (2009) to merge the phrase tables generated from theAR andEG?",
        "corpora.",
        "If a phrase occurred in both phrase tables, we",
        "adopted one of the following three solutions: - Only added the phrase with its translations and their probabilities from the AR phrase table.",
        "This assumed AR alignments to be more reliable.",
        "- Only added the phrase with its translations and their probabilities from the EG?",
        "phrase table.",
        "This assumed EG?",
        "alignments to be more reliable.",
        "- Added translations of the phrase from both",
        "phrase tables and left the choice to the decoder.",
        "We added three additional features to the new phrase table to avail the information about the origin of phrases (as in Nakov and Ng (2009))."
      ]
    },
    {
      "heading": "3.3 Evaluation and Discussion",
      "text": [
        "We performed the following experiments: - S0 involved translating the EG?",
        "test using AR.",
        "- S1 and S2 trained on the EG?",
        "with EGen and both EGen and GW for LM training respectively.",
        "- S?",
        "used phrase merging technique.",
        "All systems trained on both EG?",
        "and AR corpora.",
        "We built separate phrase tables from the two corpora and merged them.",
        "When merging, we preferred AR or EG?",
        "for SAR and SEG?",
        "respectively.",
        "For SALL, we kept phrases from both phrase tables.",
        "Table 2 summarizes results of using EG?",
        "and phrase table merging.",
        "S0 was slightly better than B1, but lagged considerably behind training using EG or EG?.",
        "S1, which used only EG?",
        "for training showed an improvement of 1.67 BLEU points from the best baseline system (B4).",
        "Using both language models (S2) led to slight improvement.",
        "Phrase merging that preferred phrases learnt from EG?",
        "data over AR data performed the best with a",
        "We analyzed 100 test sentences that led to the greatest absolute change in BLEU score, whether positive or negative, between training with EG and EG?.",
        "The largest difference in BLEU was 0.69 in favor of EG?.",
        "Translating the Egyptian sentence ?wbyHtrmwA AlnAs AltAnyp?",
        "the second people?",
        "(BLEU = 0.31).",
        "Conversion changed ?wbyHtrmwA?",
        "to ?wyHtrmwA?",
        "and ?AltAnyp?",
        "?J K AJ?",
        "@ to ?AlvAnyp?",
        "?J K AJ?",
        "@, leading to ?and they respect other people?",
        "(BLEU = 1).",
        "Training with EG?",
        "outperformed EG for 63 of the sentences.",
        "Conversion improved MT, because it reduced OOVs, enabled MADA+TOKAN to successfully analyze words, and reduced spelling mistakes.",
        "In further analysis, we examined 1% of the sentences with the largest difference in BLEU score.",
        "Out of these, more than 70% were cases where the EG?",
        "model achieved a higher BLEU score.",
        "For each observed conversion error, we identified its linguistic character, i.e. whether it is lexical, syntactic, morphological or other.",
        "We found that in more than half of the cases (?57%) using morphological information could have improved the conversion.",
        "Consider the following example, where",
        "(1) is the original EG sentence and its EG/EN translation, and (2) is the converted EG?",
        "sentence and its EG?/EN translation: 1.",
        "?JJ.",
        "?P I.",
        "?k ?",
        "lOn h*h Hsb rgbth because this is according to his desire In this case, ?rgbtk?",
        "?JJ.",
        "?P (?your wish?)",
        "was converted to ?rgbth?",
        "?JJ.",
        "?P (?his wish?)",
        "leading to an unwanted change in the translation.",
        "This could be avoided, for instance, by running a morphological analyzer on the original and converted word, and making sure their morphological features (in this case, the person of the possessive) correspond.",
        "In a similar case, the phrase ?mEndy$ AEdA?",
        "J?, thereby changing the translation from ?I don't have enemies?",
        "to ?I have enemies?.",
        "Here, again, a morphological analyzer could verify the retaining of negation after conversion.",
        "In another sentence, ?knty?",
        "?",
        "?",
        "J?",
        "(?you (fm.)",
        "were?)",
        "was correctly converted to the MSA ?knt?",
        "I J?, which is used for feminine and masculine forms.",
        "However, the induced ambiguity ended up hurting translation.4 Aside from morphological mistakes, conversion often changed words completely.",
        "In one sentence, the word ?lbAnh?",
        "?",
        "KAJ.",
        "?",
        "(?chewing gum?)",
        "was wrongly converted to ?lOnh?",
        "?",
        "K B (?because it?",
        "), resulting in a wrong translation.",
        "Perhaps a morphological analyzer, or just a part-of-speech tagger, could enforce (or probabilistically encourage) a match in parts of speech.",
        "The conversion also faces some other challenges.",
        "Consider the following example:",
        "1.",
        "?J K @ A J???",
        "A Jk@ @??",
        "hwA AHnA EmlnA Ayyyh he is we did we What ?",
        "?",
        "2.",
        "?K @ A J???",
        "?m ' ?",
        "?",
        "he we did we do ?",
        "?",
        "While the first two words ?hwA AHnA?",
        "A Jk@ @??",
        "were correctly converted to ?hw nHn?",
        "?m ' ?",
        "?, the final word ?Ayyyh?",
        "?J K @ (?what?)",
        "was shortened but remained dialectal ?Ayh?",
        "?K @ rather than MSA ?mA/mA*A?",
        "A?/ @ XA?.",
        "There is a syntactic challenge in this sentence, since the Egyptian word order in interrogative sentences is normally different from the MSA word order: the interrogative particle appears at the end of the sentence instead of at the beginning.",
        "Addressing this problem might have improved translation.",
        "The above analysis suggests that incorporating deeper linguistic information in the conversion procedure could improve translation quality.",
        "In particular, using a morphological analyzer seeems like a promising possibility.",
        "One approach could be to run a morphological analyzer for dialectal Arabic (e.g. MADA-ARZ (Habash et al., 2013)) on the original EG sentence and another analyzer for MSA (such as MADA) on the converted EG?",
        "sentence, and then to compare the morphological features.",
        "Discrepancies should be probabilistically incorporated in the conversion.",
        "Exploring this approach is left for future work."
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "We presented an Egyptian to English MT system.",
        "In contrast to previous work, we used an automatic conversion method to map Egyptian close to MSA.",
        "The converted Egyptian EG?",
        "had fewer OOV words and spelling mistakes and improved language handling.",
        "The MT system built on the adapted parallel data showed an improvement of 1.87 BLEU points over our best baseline.",
        "Using phrase table merging that combined AR and EG?",
        "training data in a way that preferred adapted dialectal data yielded an extra 0.86 BLEU points.",
        "We will make the training data for our conversion system publicly available.",
        "For future work, we want to expand our work to other dialects, while utilizing dialectal morphological analysis to improve conversion.",
        "Also, we believe that improving English language modeling to match the genre of the translated sentences can have significant positive impact on translation quality."
      ]
    }
  ]
}
