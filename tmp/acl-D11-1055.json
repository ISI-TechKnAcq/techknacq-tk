{
  "info": {
    "authors": [
      "Dani Yogatama",
      "Michael Heilman",
      "Brendan O'Connor",
      "Chris Dyer",
      "Bryan R. Routledge",
      "Noah A. Smith"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1055",
    "title": "Predicting a Scientific Community's Response to an Article",
    "url": "https://aclweb.org/anthology/D11-1055",
    "year": 2011
  },
  "references": [
    "acl-C08-1087",
    "acl-D08-1038",
    "acl-J96-1002",
    "acl-N09-1031",
    "acl-N10-1038",
    "acl-W09-3607"
  ],
  "sections": [
    {
      "text": [
        "Dani Yogatama Michael Heilman Brendan O'Connor Chris Dyer",
        "Tepper School of Business Carnegie Mellon University Pittsburgh, PA 15213, USA",
        "We consider the problem of predicting measurable responses to scientific articles based primarily on their text content.",
        "Specifically, we consider papers in two fields (economics and computational linguistics) and make predictions about downloads and within-community citations.",
        "Our approach is based on generalized linear models, allowing interpretability; a novel extension that captures first-order temporal effects is also presented.",
        "We demonstrate that text features significantly improve accuracy of predictions over metadata features like authors, topical categories, and publication venues."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Written communication is an essential component of the complex social phenomenon of science.",
        "As such, natural language processing is well-positioned to provide tools for understanding the scientific process, by analyzing the textual artifacts (papers, proceedings, etc.)",
        "that it produces.",
        "This paper is about modeling collections of scientific documents to understand how their textual content relates to how a scientific community responds to them.",
        "While past work has often focused on citation structure (Borner et al., 2003; Qazvinian and Radev, 2008), our emphasis is on the text content, following Ramage et al.",
        "(2010) and Gerrish and Blei (2010).",
        "Instead of task-independent exploratory data analysis (e.g., topic modeling) or multi-document summarization, we consider supervised models of the collective response of a scientific community to a published article.",
        "There are many measures of impact of a scientific paper; ours come from direct measurements of the number of downloads (from an established website where prominent economists post papers before formal publication) and citations (within a fixed scientific community).",
        "We adopt a discriminative approach based on generalized linear models that can make use of any text or metadata features, and show that simple lexical features offer substantial power in modeling out-of-sample response and in forecasting response for future articles.",
        "Realistic forecasting evaluations require methodological care beyond the usual best practices of train/test separation, and we elucidate these issues.",
        "In addition, we introduce a new regularization technique that leverages the intuition that the relationship between observable features and response should evolve smoothly over time.",
        "This regularizer allows the learner to rely more strongly on more recent evidence, while taking into account a long history of training data.",
        "Our time series-inspired regu-larizer is computationally efficient in learning and is a significant advance over earlier text-driven forecasting models that ignore the time variable altogether (Kogan et al., 2009; Joshi et al., 2010).",
        "We evaluate our approaches in two novel experimental settings: predicting downloads of economics articles and predicting citation of papers at ACL conferences.",
        "Our approaches substantially outper-",
        "Figure 1: Left: the distribution of log download counts for papers in the NBER dataset one year after posting.",
        "Right: the distribution of within-dataset citations of ACL papers within three years of publication (outliers excluded for readability).",
        "form text-ignorant baselines on ground-truth predictions.",
        "Our time series models permit flexibility in features and offer a novel and perhaps more interpretable view of the data than summary statistics.",
        "We make use of two collections of scientific literature, one from the economics domain, and the other from computational linguistics and natural language processing.",
        "Statistics are summarized in Table 1.",
        "Our first dataset consists of research papers in economics from the National Bureau of Economic Research (NBER) from 1999 to 2009 (http:// www.nber.org).",
        "Approximately 1,000 research economists are affiliated with the NBER.",
        "New NBER working papers are posted to the website weekly.",
        "The papers are not yet peer-reviewed, but given the prominence of many economists affiliated with the NBER, many of the papers are widely read.",
        "Text from the abstracts of the papers and related metadata are publicly available.",
        "Full text is available to subscribers (universities typically have access).",
        "The NBER provided us with download statistics for these papers.",
        "For each paper, we computed the total number of downloads in the first year after each paper's posting.",
        "The download counts are log-normally distributed, as shown in Figure 1, and so our regression models (§3) minimize squared errors in the log space.",
        "Our download logs begin in 1999.",
        "We use the 8,814 papers from 1999-2009 period (there are 16,334 papers in the full dataset dating back to 1985).",
        "We only use text from the abstracts, since we were able to obtain full texts for just a portion of the papers, and since the OCR of the full texts we do have is very noisy.",
        "Our second dataset consists of research papers from the Association for Computational Linguistics (ACL) from 1980 to 2006 (Radev et al., 2009a; Radev et al., 2009b).",
        "We have the full texts for papers (OCR output) as well as structured citation data.",
        "There are 15,689 papers in the whole dataset.",
        "For the citation prediction task, we include conference papers from ACL, EACL, HLT, and NAACL.",
        "We remove journal papers, since they are characteristically different from conference papers, as well as workshop papers.",
        "We do include short papers, interactive demo session papers, and student research papers that are included in the companion volumes for these conferences (such papers are cited less than full papers, but many are still cited).",
        "The resulting dataset contains 4,026 papers.",
        "The number of papers in each year varies because not all conferences are annual.",
        "We look at citations in the three-year window following publication, excluding self-citations and only considering citations from papers within these conferences.",
        "Figure 1 shows a histogram; note that many papers (54%) are not cited at all, and the distribution of citations per paper is neither normal nor log-normal.",
        "We organize the papers into two classes: those with zero citations and those with non-zero citations in the three-year window.",
        "1",
        "Dataset",
        "# Docs.",
        "Avg.",
        "#",
        "Response",
        "Words",
        "NBER",
        "8,814",
        "155",
        "# downloads in first",
        "year (mean 761)",
        "ACL",
        "4,026",
        "3,966",
        "at least 1 citation in",
        "first 3 years?",
        "(54% no)"
      ]
    },
    {
      "heading": "3. Model",
      "text": [
        "Our forecasting approach is based on generalized linear models for regression and classification.",
        "The models are trained with an .^-penalty, often called a \"ridge\" model (Hoerl and Kennard, 1970).",
        "For the NBER data, where (log) number of downloads is nearly a continuous measure, we use linear regression.",
        "For the ACL data, where response is the binary cited-or-not variable we use logistic regression, often referred to as a \"maximum entropy\" model (Berger et al., 1996) or a log-linear model.",
        "We briefly review the class of models.",
        "Then, we describe a time series model appropriate for time series data.",
        "Consider a model that predicts a response y given a vector input x = (x1;..., xd) G Rd.",
        "Our models are linear functions of x and parameterized by the vector ß.",
        "Given a corpus of M document features, X, and responses Y, we estimate:",
        "where L is a model-dependent loss function and R is a regularization penalty to encourage models with small weight vectors.",
        "We describe models and loss functions first and then turn to regularization.",
        "For the NBER data, the (log) number of downloads is continuous, and so we use least-squares linear regression model.",
        "The loss function is the sum of the squared errors for the M documents in our training data: L(ß, X, Y) = E^y* - y*), where the prediction rule for new documents is: y = Ed=o ßjXj.",
        "Probabilistically, this equates to an assumption that ßTx is the mean of a normal (i.e., Gaussian) distribution from which random variable y is drawn.",
        "For the ACL data, we predict y from a discrete set C (specifically, the binary set of zero citations or more than zero citations), and we use logistic regression.",
        "This model assumes that for the ith training input Xj, the output y* is drawn according to:",
        "where there is a feature vector ßc for each class c G C. Under this interpretation, parameter estimation is maximum a posteriori inference for ß, and R(ß) is a log-prior for the weights.",
        "The loss function is the negative log likelihood for the M documents: L(ß, X,Y) = - Ya=\\logp(y* I x*).",
        "The prediction rule for a new document is: y = argmaxc€C Xj=0 ßcjXj.",
        "Generalized linear models and penalized regression are well-studied with an extensive literature (Mccullagh and Nelder, 1989; Hastie et al., 2009).",
        "We leave other types of models, such as Poisson (Cameron and Trivedi, 1998) or ordinal (McCullagh, 1980) regression models, to future work.",
        "With large numbers of features, regularization is crucial to avoid overfitting.",
        "In ridge regression (Ho-erl and Kennard, 1970), a standard method to which we compare the time series regularization discussed in §3.3, the penalty R(ß) is proportional to the .",
        "2-norm of ß:",
        "where A is a regularization hyperparameter that is tuned on development data or by cross-validation.This penalty pushes many ßj close (but not completely) to zero.",
        "In practice, we multiply the penalty by the number of examples M to facilitate tuning of",
        "A.",
        "The ridge linear regression model can be interpreted probabilistically as each coefficient ßj is drawn i.i.d.",
        "from a normal distribution with mean 0 and variance 2A-1.",
        "A simple way to capture temporal variation is to conjoin traditional features with a time variable.",
        "Here, we divide the dataset into T time steps (years).",
        "In the new representation, the feature space expands from Rd to RTxd.",
        "For a document published at year t, the elements of x are non-zero only for those features that correspond to year-t; that is xt/;j = 0 for all f = t.",
        "Estimating this model with the new features using the .",
        "2-penalty would be effectively estimating separate models for each year under the assumption that each ßt j is independent; even for features that differed only temporally (e.g., ßtj and ßt+1jj).",
        "In this work, we apply time series regularization to GLMs, enabling models that have coefficients that change over time but prefer gradual changes across time steps.",
        "Boyd and Vandenberghe (2004, §6.3) describe a general version of this sort of regularizer.",
        "To our knowledge, such regularizers have not previously been applied to temporal modeling of text.",
        "The time series regularization penalty becomes:",
        "It includes a standard ^2-penalty on the coefficients, and a penalty for differences between coefficients for adjacent time steps to induce smooth changes.Similar to the previous model, in practice, we multiply the regularization constant A by f to facilitate tuning of A for datasets with different numbers of examples M and numbers of time steps T. The new parameter, a, controls the smoothness of the estimated coefficients.",
        "Setting a to zero imposes no penalty for time-variation in the coefficients and results in independent ridge regressions at each time step.",
        "Also, when the number of examples is constant across time steps, setting a large a parameter (a – oo) results in a single ridge regression over all years since it imposes ßtj = ßt+\\,j for all t G T. The partial derivative is:",
        "This time series regularization can be applied more generally, not just to linear and logistic regression.",
        "With either ridge regularization or this time series regularization scheme, Eq.",
        "1 is an unconstrained convex optimization problem for the linear models",
        "Figure 2: Time series regression as a graphical model; the variables Xt and Yt are the sets of feature vectors and response variables from documents dated t.",
        "we describe here.",
        "There exist a number of optimization procedures for it; we use the L-BFGS quasiNewton algorithm (Liu and Nocedal, 1989).",
        "Probabilistic Interpretation",
        "We can interpret the time series regularization probabilistically as follows.",
        "Let the coefficient for the jth feature over time be ßj = (ß1)j-, ß2j,ßTj).",
        "ßj are draws from a multivariate normal distribution with a tridiagonal precision matrix S-1 = A G",
        "The form of R(ß) follows from noting:",
        "The squared difference between adjacent time steps comes from the off-diagonal entries in the precision matrix.",
        "Figure 2 shows a graphical representation of the time series regularization in our model.",
        "Its Markov chain structure corresponds to the off-diagonals.",
        "There is a rich literature on time series analysis (Box et al., 2008; Hamilton, 1994).",
        "The prior distribution over the sequence (ß1;j,..., ßT;j ) that our regularizer posits is closely linked to a first-order autoregressive process, AR(1).",
        "1+a",
        "-a",
        "0",
        "0",
        "-a",
        "1 + 2a",
        "-a",
        "0",
        "0",
        "-a",
        "1 + 2a",
        "-a",
        "0",
        "0",
        "-a",
        "1 + 2a"
      ]
    },
    {
      "heading": "4. Features",
      "text": [
        "NBER metadata features",
        "• Authors' last names.",
        "We treat each name as a binary feature.",
        "If a paper has multiple authors, all authors are used and they have equal weights regardless of their ordering.",
        "• NBER program(s).",
        "There are 19 major research programs at the NBER (e.g., Monetary Economics, Health Economics, etc.",
        ").",
        "ACL metadata features• Authors' last names as binary features.",
        "• Conference venues.",
        "We use first letter of the ACL anthology paper ID, which correlates with its conference venue (e.g., P for the ACL main conference, H for the HLT conference, etc.",
        ").",
        "Text features",
        "• Binary indicator features for the presence of each unigram, bigram, and trigram.",
        "For the NBER data, we have separate features for titles and abstracts.",
        "For the ACL data, we have separate features for titles and full texts.",
        "We pruned text features by document frequency (details in §5).",
        "• Log transformed word counts.",
        "We include features for the numbers of words in the title and the",
        "abstract (NBER) or the full text (ACL)."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "For each of the datasets in §2, we test our models for two tasks: forecasting about future papers (i.e., making predictions about papers that appeared after a training dataset) and modeling held-out papers from the past (i.e., making predictions within the same time period as the training dataset, on held-out examples).",
        "For the NBER dataset, the task is to predict the number of downloads a paper will receive in its first year after publication.",
        "For the ACL dataset, the task is to predict whether a paper will be cited at all (by another ACL paper in our dataset) within the first three years after its publication.",
        "To our knowledge, clean, reliable citation counts are not available for the NBER dataset; nor are download statistics available for the ACL dataset.",
        "Table 2 summarizes the variables of interest, model types, and evaluation metrics for the tasks.",
        "The lag between a paper's publication and when its outcome (download or citation count) can be observed poses a unique methodological challenge.",
        "Consider predicting the number of downloads over g future time steps.",
        "If t is the time of forecasting, we can observe the texts of all articles published before t. However, any article published in the interval [t – g, t] is too recent for the outcome measurement of y to be taken.",
        "We refer to the interval [t – g, t] as the \"forecast gap\".",
        "Since recent articles are sometimes the most relevant predictions at t, we do not want to ignore them.",
        "Consider a paper at time step t', t – g < t' <t. To extrapolate its number of downloads, we consider the observed number in [t', t], and then estimate the ratio r of downloads that occur in the first t – t' time steps, against the first g time steps, using the fully observed portion of the training data.",
        "We then scale the observed downloads during [t', t] by r-1 to extrapolate.",
        "The same method is used to extrapolate citation counts.",
        "In preliminary experiments, we observed that extrapolating responses for papers in the forecast gap led to better performance in general.",
        "For example, for the ridge regressions trained on all past years with the full feature set, the error dropped from 262 to 259 when using extrapolation compared to without extrapolation.",
        "Also, the extrapolated download counts were quite close to the true values (which we have but do not use because of the forecast gap): for example, the mean absolute error of the extrapolated responses was 99 when extrapolated based on the median of the fully observed portion of the training data (measured monthly).",
        "NBER",
        "ACL",
        "Response",
        "log(#downloads+1)",
        "1{#citations > 0}",
        "GLM type",
        "normal / squared-loss",
        "logistic / log-loss",
        "Metric 1",
        "mean absolute error",
        "accuracy",
        "Metric 2",
        "Kendall's t",
        "Kendall's t",
        "In our first set of experiments, we predict the number of downloads of an NBER paper within one year of its publication.",
        "We compare four approaches for predicting downloads.",
        "The first is a baseline that simply uses the median of the log of the training and development data as the prediction.",
        "The second and third use GLMs with ridge regression-style regularization (§3.2), trained on all past years (\"all years\") and on the single most recent past year (\"one year\"), respectively.",
        "The last model (\"time series\") is a GLM with time series regularization (§3.3).",
        "We divided papers by year.",
        "Figure 3 illustrates the experimental setup.",
        "We held out a random 20% of papers for each year from 1999-2007 as a test set for the task of modeling the past.",
        "To define the feature set and tune hyperparameters, we used the remaining 80% of papers from 1999-2005 as our training data and the remaining papers in 2006 as our development data.",
        "After pruning, we have 37,251 total features, of which 2,549 are metadata features.",
        "When tuning hyperparameters, we simulated the existence of a forecast gap by using extrapolated responses for papers in the last year of the training data instead of their true responses.",
        "We considered A G 5{2,i,...,-5-6}, and a G 5{3,2,...)-i,-2} and selected those that led to the best performance on the development set.",
        "We then used the selected feature set and hyperparameters to test the forecasting and modeling capabilities of each model.",
        "For forecasting, we predicted numbers of downloads of papers in 2008 and 2009.",
        "We used the baseline median, ridge regression, and time series regularization models trained on papers in 1999-2007 and 1999-2008, respectively.",
        "We treated the last year of the training data (2007 and",
        "NBER Experiments ACL Experiments",
        "20% modeling test (unused) 20% modeling test",
        "Figure 3: An illustration of how the datasets were segmented for the experiments.",
        "Portions of data for which we report results are shaded.",
        "Time spans are not to scale.",
        "2008, respectively) as a forecast gap, since we would not have observed complete responses of papers in these years when forecasting.",
        "For the \"one year\" models, we trained ridge regressions only on the most recent past year, using papers in 2007 and 2008, respectively, as training data.",
        "To test the additive benefit of text features, we trained models with just metadata features (NBER programs and authors, denoted \"Meta\") and with both metadata",
        "'99 ... '04",
        "'05",
        "'06",
        "80%",
        "training",
        "dev.",
        "(tuning, feature",
        "20%",
        "modeling test (unused)",
        "pruning)",
        "'99 ...",
        "'06",
        "'07",
        "'08",
        "80%",
        "training",
        "test",
        "20%",
        "modeling test (unused)",
        "'99 ...",
        "'07",
        "'08",
        "'09",
        "80%",
        "training",
        "gap",
        "test",
        "20%",
        "modeling test",
        "Table 3: Mean absolute errors for the NBER download predictions.",
        "\"*\" indicates statistical significance between time series models using metadata features and the full feature set.",
        "\"f\" indicates statistical significance between the time series and ridge regression models using the full feature set (Wilcoxon signed-rank test, p < 0.01).",
        "and text features (denoted \"Full\").",
        "To evaluate the modeling capabilities, we trained the ridge regression and time series regularization models on papers from 1999-2008 and predicted the numbers of downloads of held-out papers in 19992007.",
        "For comparison, we also trained ridge regression models on each individual year (\"one year\") and predicted the numbers of downloads ofthe held-out papers in the corresponding year.",
        "Table 3 shows mean absolute errors for each method on both forecasting test splits, and mean absolute errors averaged across papers over nine modeling test splits.",
        "For interpretability, we report predictions in terms of download counts, though the models were trained with log counts (§2.1).",
        "The results show that even a simple n-gram representation of text contains a valuable, learnable signal that is predictive of future downloads.",
        "While the time series model did not significantly outperform ridge regression at predicting future downloads, it did result in significantly better performance for modeling papers in the past.",
        "We now turn to the problem of predicting citation levels.",
        "Recall that here we aim to predict whether an ACL paper will be cited within our dataset within three years.",
        "Our experimental setup (Figure 3) is similar to the setup for the NBER dataset, except that we use logistic regression to model the discrete cited-or-not response variable.",
        "We also make the simplifying assumption that all citations occur at the end of each year.",
        "Therefore, the forecast gap is only",
        "Table 4: Classification accuracy (%) for predicting whether ACL papers will be cited within three years.",
        "\"*\" indicates statistical significance between time series models using metadata features and the full feature set (binomial sign test, p < 0.01).",
        "With the full feature set, differences between the time series and ridge (all years) models are not statistically significant at the 0.01 level, but for the modeling taskp is estimated at 0.026, and for the 2006 forecasting task, p is estimated at 0.050.",
        "two years (we have observed complete citations in the test year).",
        "After feature pruning, there were 30,760 total features, of which 1,694 are metadata features.",
        "We considered A G 5{2,1>---8'-9} (\"Full\") and A G 5{2>>->-11>-12> (\"Meta\"); and a G 5{6>>->>-1> (both \"Full\" and \"Meta\"), selecting the best values using the development data.",
        "Again, we compare four methods: a baseline of always predicting the most frequent class in the training data, \"all years\" and \"one year\" logistic regression models, and a logistic regression with the time series regularizer.",
        "For the forecasting task, we used papers in 2004, 2005, and 2006 as test sets.",
        "As the training sets for the \"all years\" and time series models, we used papers from 1980 up to the last year before each test set, with the last two years extrapolated.",
        "As the training sets for the \"one year\" models, we used papers from the year immediately before the test set, with extrapolated responses.",
        "To evaluate modeling capabilities, we predicted citation levels of held-out papers in 1980-2003.",
        "We used the \"all years\" and time series models trained on 1980-2005.",
        "We trained \"one year\" models separately for each year and predicted downloads for the held-out papers in that year.",
        "Table 4 shows classification accuracy for each model on the test data for both the forecasting and modeling tasks.",
        "It is again clear that adding text significantly improved the performance of the model.",
        "Also, the time series regression model shows a small, though not statistically significant, gain for modeling whether past papers will be cited – as well as similarly small gains on two of the three forecasting test years.",
        "Features",
        "Model",
        "Modeling",
        "Forecasting",
        "1999-07",
        "2008",
        "2009",
        "-",
        "median",
        "333",
        "371",
        "397",
        "Meta",
        "one year",
        "279",
        "354",
        "375",
        "Meta",
        "all years",
        "303",
        "334",
        "378",
        "Meta",
        "time series",
        "279",
        "353",
        "375",
        "Full",
        "one year",
        "271",
        "346",
        "351",
        "Full",
        "all years",
        "265",
        "+300",
        "339",
        "Full",
        "time series",
        "*f245",
        "*321",
        "*332",
        "Feat.",
        "Model",
        "Modeling",
        "Forecasting",
        "1980-03",
        "2004",
        "2005",
        "2006",
        "-",
        "majority",
        "55",
        "56",
        "60",
        "50",
        "Meta",
        "one year",
        "61",
        "56",
        "54",
        "62",
        "Meta",
        "all years",
        "65",
        "58",
        "53",
        "60",
        "Meta",
        "time series",
        "66",
        "56",
        "53",
        "56",
        "Full",
        "one year",
        "69",
        "70",
        "64",
        "67",
        "Full",
        "all years",
        "67",
        "69",
        "70",
        "70",
        "Full",
        "time series",
        "70",
        "*69",
        "*70",
        "*72",
        "We can also use the models for ranking to help decide which papers are expected to have the greatest impact.",
        "With rankings, we can use the same metric both for download and citation predictions.",
        "For the NBER data, we ranked test-set papers based on the predicted numbers of downloads and computed the correlation to the actual numbers of downloads.",
        "For the ACL data, we ranked papers based on the probability of being cited (within the next three years) and computed the correlation to the actual numbers of citations.",
        "To measure ranking models' ranking quality, we used Kendall's t, a nonparametric statistic that measures the similarity of two different orderings over the same set of items.",
        "Here, the items are scientific papers and the two metrics are the gold standard numbers of downloads (or citations) and model predictions for the numbers of downloads, or citation probabilities.",
        "If q is the chance that a randomly drawn pair of items will be ranked in the same way by the two metrics, then t = 2(q – 0.5).",
        "Table 5 shows Kendall's t for each model for the forecasting tasks (i.e., prediction of future citations or downloads) in both datasets.",
        "As in the previous experiments, we see small benefits for the time series regression model on most held-out data splits – and larger benefits for including text features along with metadata features."
      ]
    },
    {
      "heading": "6. Analysis",
      "text": [
        "An advantage of the time series regularized regression model is its interpretability.",
        "Inspecting feature coefficients in the model allows us to identify trends and changes of interests over time within a scientific community.",
        "unemploymentrate inflationrate",
        "First, we illustrate the difference between the time series and the other models in Figure 4, for NBER models' weights for unemployment rate and inflation rate appearing in a paper's abstract.",
        "The year-to-year weights of \"one year\" models fluctuate substantially, and the \"all years\" model is necessarily constant, but the time series regularizer gives a smooth trajectory.",
        "Previous work has examined the flow of ideas as trends in word and phrase frequencies, as in the Google Books Ngram Viewer (Michel et al., 2011).",
        "Topic models have been used extensively to explore trends in low-dimensional spaces (Blei and Lafferty, 2006; Wang et al., 2008; Wang and McCal-lum, 2006; Ahmed and Xing, 2010).",
        "By contrast, our approach allows us to examine trends in the impact of text related to specific observation variables: the coefficient trendline for a feature illustrates its association with measurements of scholarly impact (citation and download frequency).",
        "Text frequencies can be quite different from the discriminative weights our model assigns to features.",
        "Figure 5 illustrates the ßtj trends in the ACL time series model for some selected terms that oc-",
        "Feat.",
        "Model",
        "NBER '08 '09",
        "'04",
        "ACL",
        "'05",
        "'06",
        "Meta",
        "one year",
        ".29",
        ".22",
        ".17",
        ".08",
        ".16",
        "Meta",
        "all years",
        ".31",
        ".22",
        ".15",
        ".12",
        ".21",
        "Meta",
        "time series",
        ".29",
        ".22",
        ".14",
        ".10",
        ".17",
        "Full",
        "one year",
        ".35",
        ".31",
        ".44",
        ".39",
        ".33",
        "Full",
        "all years",
        ".43",
        ".37",
        ".42",
        ".43",
        ".40",
        "Full",
        "time series",
        ".43",
        ".38",
        ".47",
        ".44",
        ".43",
        "m",
        "time series",
        "m",
        "all years",
        "m",
        "one year",
        "Time series coef.",
        "Term frequencies generation■machine translation",
        "Figure 5: Feature trends: model coefficients vs. term frequencies over time in the ACL corpus.",
        "Term freq.",
        "is the fraction of tokens (or bigrams for m.t.)",
        "that year, that are the term, averaged over a centered five-year window.",
        "cur frequently in conference session titles.",
        "On the right are term frequencies (with smoothing, since year-to-year frequencies are bumpy).",
        "Most terms decline over time.",
        "On the left, by contrast, are the weights learned by our time series model.",
        "They tell a very different story: for example, parsing has shown a definite increase in interest, while interest in grammars (e.g., formalisms) has declined somewhat.",
        "These trends have face validity, giving credence to our analysis; they also broadly agree with",
        "Hall et al.",
        "(2008).",
        "The regression method also allows analysis ofauthor influence, since we fit a coefficient for each of the authors in the ACL dataset.",
        "Figure 6(a) addresses the following question: do prolific authors get cited more often, even after accounting for the content of their papers?",
        "The effect is present but relatively small according to our model: the total number of papers co-authored by an author has a weak correlation to the author's citation prediction coefficient (t = 0.16).",
        "Next, does the model provide more information than the simple citation probability of an author?",
        "Figure 6(b) compares coefficients to an author's papers' probability of being cited.",
        "Since we did not prune author features, there are many authors with",
        "Doc.",
        "frequency",
        "(a) Doc.",
        "freq.",
        "vs. coef.",
        "Citation proportion",
        "(b) Citation prop.",
        "vs. coef.",
        "Figure 6: Analysis of author citation coefficients.",
        "Every point is one ACL author, and the vertical axis shows the citation coefficient, compared to (a) the number of documents co-authored by the author; and (b) the proportion of an author's papers that are cited within three years.",
        "The vertical bar is the macro-averaged citation proportion across authors, 41%.",
        "only a few papers, resulting in unsmoothed probabilities of 0, 0.5, 1, etc.",
        "(these correspond to the vertical \"bands\" in the plot).",
        "By contrast, the ^2-penalty of the model naturally assigned coefficients close to zero for such authors if it is justified.",
        "In general, the simple probability agrees with the coefficient, but there are differences.",
        "The semantics of the regression imply we are measuring the relative citation probability of an author, controlling for text and venue effects.",
        "If an author has a high citation prediction coefficient but a low citation probability, that implies the author has better-cited work than would be expected according to the n-grams in his or her papers.",
        "We have omitted names of authors from the figure for clarity and confidentiality, but high outlier authors tend to be well-known researchers in the ACL community.",
        "Obviously, since the prediction model is not perfect, it is not possible to completely verify this hypothesis, but we feel this analysis is reasonably suggestive."
      ]
    },
    {
      "heading": "7. Related Work",
      "text": [
        "Previous work on modeling scientific literature mostly focused on citation graphs (Borner et al., 2003; Qazvinian and Radev, 2008).",
        "Some researchers, e.g., Erosheva et al.",
        "(2004), have used text content.",
        "Most of these are based on topic models: Gerrish and Blei (2010) measure scholarly impact, Hall et al.",
        "(2008) study the \"history of ideas\", and Ramage et al.",
        "(2010) rank universities based on scholarly output using topic models.",
        "Download rates and citation prediction were two of the main tasks in the KDD Cup 2003 (McGovern and Jurafsky (2010) considered the problem slightly differently and proposed an information retrieval approach to citation prediction.",
        "Our approach is novel in that we formulate the problem as a forecasting task and we seek to predict future impact of articles.",
        "Linear regression with text features has been used to predict financial risk (Kogan et al., 2009) and movie revenues (Joshi et al., 2010).",
        "While the forecasts in those papers are similar to ours, those authors did not consider a forecast gap or allowing the parameters of the model to vary over time.",
        "Our time series regularization is closely related to the fused lasso (Tibshirani et al., 2005).",
        "It penalizes a loss function by the ^1-norm of the coefficients and their differences.",
        "The ^1-penalty for differences between coefficients encourages sparsity in the differences.",
        "We use the ^2-norm to induce smooth changes across time steps."
      ]
    },
    {
      "heading": "8. Conclusions",
      "text": [
        "We presented a statistical approach to predicting a scientific community's response to an article, based on its textual content.",
        "To improve the interpretability of the linear model, we developed a novel time series regularizer that encourages gradual changes across time steps.",
        "Our experiments showed that text features significantly improve accuracy of predictions over baseline models, and we found that the feature weights learned with the time series regularizer reflect important trends in the literature."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We thank the National Bureau of Economic Research for providing the NBER dataset for this research, Fallaw Sowell for helpful discussions, and three anonymous reviewers for comments on an earlier draft of this paper.",
        "This research was supported by the Intelligence Advanced Research Projects Activity under grant number N10PC20222 and TeraGrid resources provided by the Pittsburgh Supercomputing Center under grant number TG-",
        "DBS110003."
      ]
    }
  ]
}
