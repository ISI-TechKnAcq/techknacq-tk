{
  "info": {
    "authors": [
      "Hongqiao Li",
      "Changning Huang",
      "Jianfeng Gao",
      "Xiaozhong Fan"
    ],
    "book": "SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-W04-1107",
    "title": "Chinese Chunking With Another Type of Spec",
    "url": "https://aclweb.org/anthology/W04-1107",
    "year": 2004
  },
  "references": [
    "acl-C02-1145",
    "acl-J95-4004",
    "acl-P00-1015",
    "acl-P01-1043",
    "acl-P02-1055",
    "acl-P03-1063",
    "acl-W00-0726",
    "acl-W00-0730",
    "acl-W01-0706",
    "acl-W95-0107"
  ],
  "sections": [
    {
      "text": [
        "Beijing 100080 China cnhuang@msrchina.",
        "research.microsoft.com"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "Spec is a critical issue for automatic chunking.",
        "This paper proposes a solution of Chinese chunking with another type of spec, which is not derived from a complete syntactic tree but only based on the un-bracketed, POS tagged corpus.",
        "With this spec, a chunked data is built and HMM is used to build the chunker.",
        "TBL-based error correction is used to further improve chunking performance.",
        "The average chunk length is about 1.38 tokens, F measure of chunking achieves 91.13%, labeling accuracy alone achieves 99.80% and the ratio of crossing brackets is 2.87%.",
        "We also find that the hardest point of Chinese chunking is to identify the chunking boundary inside noun-noun sequences1."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Abney (1991) has proposed chunking as a useful and relative tractable median stage that is to divide sentences into non-overlapping segments only based on superficial analysis and local information.",
        "(Ramshaw and Marcus, 1995) represent chunking as tagging problem and the CoNLL2000 shared task (Kim Sang and Buchholz, 2000) is now the standard evaluation task for chunking English.",
        "Their work has inspired many others to study chunking for other human languages.",
        "Besides the chunking algorithm, spec (the detailed definitions of all chunk types) is another critical issue for automatic chunking development.",
        "The well-defined spec can induce the chunker to perform well.",
        "Currently chunking specs are defined as some rules or one program to extract phrases from Treebank such as (Li, 2003) and (Li, 2004) in order to save the cost of manual annotation.",
        "We name it as Treebank-derived spec.",
        "However, we find that it is more valuable to compile another type of chunking spec according to the observation from unbracketed corpus instead of Treebank.",
        "Based on the problems of chunking Chinese that are found with our observation, we explain the reason why another type of spec is needed and then propose our spec in which the shortening and extending strategies are used to resolve these problems.",
        "We also compare our spec with a Treebank-derived spec which is derived from Chinese Treebank (CTB) (Xue and Xia, 2000).",
        "An annotated chunking corpus is built with the spec and then a chunker is also constructed accordingly.",
        "For annotation, we adopt a two-stage processing, in which text is first chunked manually and then the potential inconsistent annotations are checked semi-automatically with a tool.",
        "For the chunker, we use HMM model and TBL (Transform-based Learning) (Brill, 1995) based error correction to further improve chunking performance.",
        "With our spec the overall average length of chunks arrives 1.38 tokens, in open test, the chunking F measure achieves 91.13% and 95.45% if under-combining errors are not counted.",
        "We also find the hardest point of Chinese chunking is to identify the chunking boundary inside a noun-noun sequence.",
        "In the remainder of this paper section 2 describes some problems in chunking Chinese text, section 3 discusses the reason why another type of spec is needed and proposes our chunking spec, section 4 discusses the annotation of our chunking corpus, section 5 describes chunking model, section 6 gives experiment results, section 7, 8 recall some related work and give our conclusions respectively."
      ]
    },
    {
      "heading": "2 Problems of Chunking Chinese Text",
      "text": [
        "The purpose of Chinese chunking is to divide sentence into syntactically correlated parts of words after word segmentation and part-of-speech (POS) tagging.",
        "For example: ‘solid’ /n ‘traffic’ /n ‘frame’] [VP /d ‘already’ /v ‘achieve considerable scale ’ /u] ‘Zhuhai has achieved considerable scale in solid traffic frame.’ According to Abney’s definition, most chunks are modifier-head structures and non-overlapping.",
        "However, some syntactic structures in Chinese are very hard to be chunked correctly due to /ns ‘Zhuhai’] /u ‘of’ [NP /a [NP characteristics of Chinese language, for example, less using of function words and less inflection formats.",
        "Table 1 shows the most common structural ambiguities occurred during Chinese chunking.",
        "Their occurrences and distributions of each possible structure are also reported.",
        "As can be seen in Table 1, only 77% neighboring nouns can be grouped inside one chunk; if the left word is ‘ /of’ or a verb, this figure will ascend to 80% and 94% respectively; but if the left word is an adjective or a numeral, it will descend to 70% and 59% respectively; for ‘n _c_n’, only 52% are word level coordination.",
        "In contrast with English chunking, several hard problems are described in detail as following.",
        "(1) Noun-noun compounds",
        "Compounds formed by more than two neighboring nouns are very common in Chinese and not always all the left nouns modify the head of the compound.",
        "Some compounds consist of several shorter sub-compounds.",
        "For example: ( /younger /volunteer /science and technology /service team) ‘young volunteer service team of science and technology’ ‘ ’ and ‘ ’ are two sub-compounds and the former modifies the latter.",
        "But sometimes it is impossible to distinguish the inner structures, for example: /world /peace /career It is impossible to distinguish whether it is { { } } or { { }}.",
        "English chunking also shows such problem, and the common solution for English is not to identify their inner structure and treat them as a flat noun phrase.",
        "Following is an example in CoNLL2000 shared task: [NP employee assistance program directors]",
        "(2) Coordination",
        "Coordination in all cases can be divided into two types: with conjunctions and without conjunctions.",
        "The former can be further divided into two subcategories: word-level and phrase-level coordinations.",
        "For example: { /policy /bank /and /commercial /bank} /of { /relationship /and /cooperation} ‘the relationship and cooperation between policy banks and commercial banks’.",
        "The former coordination is phrase-level and the latter is word-level.",
        "Unfortunately, sometimes it is difficult or even impossible to distinguish whether it is word-level or phrase-level at all, for example: /least /salary /and /living maintenance ‘the least salary and living maintenance’ It is impossible to distinguish ‘ ’ is a shared modifier or not.",
        "English chunking also has such kind of problems.",
        "The solution of CoNLL2000 is to leave the conjunctions outside chunks for phrase-level coordinations and to group the conjunction inside a chunk when it is word-level or impossibly distinguished phrase-level.",
        "For example: [NP enough food and water] In Chinese, some coordinate construction has no conjunction or punctuation inside, and also could not be distinguished from a modifier-head construction with syntactic knowledge only.",
        "For example:",
        "v nn 154 6% (v_n modify the last noun) /enter /factory /worker _ _ 94 % (others) ‘avoid /avoid /law /duty legal duties’ n n 98 80% ( n_n is modifier _head) 勤/watch /of /traffic /cop ‘a traffic _ _ 20% (others) orderly cop’ 痪/paralytic /of /body /function a nn 27 70% ( a modify the first n) /high /technology /company ‘high-tech company’ _ _ 30% (others) ‘old /old /news /worker news worker’ m nn 17 41% ( m modify the first n) /two /nation /people ‘our two peoples’ _ _ 59% (others) /some /country /area ‘some rural areas’ n c n 88 52%(word level coordination) /economy /and /society ‘economy and society’ _ _ 48%(others) /quality /and /technology 求/requirement n, v, a, d, m, q, p, f , c are the POS tags of noun, verb, adjective, adverb, number, measure, preposition, localizer,",
        "wagons, caution lights and alarm whistles’ Such problem does not exist in English because almost all coordinations have certain conjunctions or punctuations between words or phrases of the same syntactic categories in formal English.",
        "(3) Structural ambiguities",
        "In Chinese, some structural ambiguities in phrase level are impossible or unnecessary to be distinguished during chunking.",
        "There is an example of ‘a_n_n’:",
        "identically acceptable.",
        "English also has such problem.",
        "The solution of CoNLL2000 is not to distinguish inner structure and group the given sequence as a single chunk.",
        "For example, the inner structure of ‘[NP heavy truck production]’ is ‘{{heavy truck} production}’, whereas one reading of ‘[NP heavy quake damage]’ is ‘{heavy {quake damage}}’.",
        "Besides, ‘a_n_n’, ‘m_n_n’ and ‘m_q_n_n’ also have the similar problem."
      ]
    },
    {
      "heading": "3 Chinese Chunking Spec",
      "text": [
        "As a kind of shallow parsing, the principles of chunking are to make chunking much more efficient and precise than full parsing.",
        "Obviously, one can shorten the length of chunks to leave ambiguities outside of chunks.",
        "For example, if we let noun-noun sequences always chunk into single word, those ambiguities listed in Table 1 would not be encountered and the performance would be greatly improved.",
        "In fact, there is an implicit requirement in chunking, no matter which language it is, the average length of chunks is as longer as possible without violating the general principle of chunking.",
        "So a trade-off between the average chunk length and the chunking performance exists."
      ]
    },
    {
      "heading": "3.1 Why another type of spec is needed",
      "text": [
        "A convenient spec is to extract the lowest non-terminal nodes from a Treebank (e.g. CTB) as Chinese chunked data.",
        "But there are some problems.",
        "The trees are designed for full parsing instead of shallow parsing, thus some of these problems listed in section 2 could not be resolved well in chunking.",
        "Maybe we can compile some rules to prune the tree or break some non-terminal nodes in order to properly resolve these problems just like CoNLL2000.",
        "However, just as (Kim Sang and Buchholz, 2000) noted: “some trees are very complex and some annotations are inconsistent”.",
        "So these rules are complex, the extracted data are inconsistent and manual check is also needed.",
        "In addition, the resource of Chinese Treebank is limited and the extracted data is not enough for chunking.",
        "So we compile another type of chunking spec according to the observation from un-bracket corpus instead of Treebank.",
        "The only shortcoming is the cost of annotation, but there are some advantages for us to explore.",
        "1) It coincides with auto chunking procedure, and we can select proper solutions to these problems without constraints of the exist Treebank.",
        "The purpose of drafting another type of chunking spec is to keep chunking consistency as high as possible without hurting the performance of auto-chunking in whole.",
        "2) Through spec drafting and text annotating most frequent and significant syntactic ambiguities could be studied, and those observations are in turn described in the spec carefully.",
        "3) With a proper spec and certain mechanical approaches, a large-scale chunked data could be produced without supporting from the Treebank."
      ]
    },
    {
      "heading": "3.2 Our spec",
      "text": [
        "Our spec and chunking annotation are based on PK corpus2 (Yu et al.",
        "1996).",
        "The PK corpus is un-bracketed, but in which all words are segmented and only one POS tag is assigned to each word.",
        "We define 11 chunk types that are similar with CoNLL2000.",
        "They are NP (noun chunk), VP (verb chunk), ADJP (adjective chunk), ADVP (adverb chunk), PP (prepositional chunk), CONJP (conjunction), MP (numerical chunk), TP (temporal chunk), SP (spatial chunk), INTJP (interjection) and INDP (independent chunk).",
        "During spec drafting we try to find a proper chunk spec to solve these problems by two ways: either merging neighboring chunks into one chunk or shortening them.",
        "Besides those structural ambiguities, we also extend boundary of the chunks with minor structural ambiguities in order to make the chunks close to the constituents."
      ]
    },
    {
      "heading": "3.2.1 Shortening",
      "text": [
        "The auxiliary ‘ /of’ is one of the most frequent words in Chinese and used to connect a pre-modifier with its nominal head.",
        "However the left boundary of such a construction is quite complicated: almost all kinds of preceding clauses, phrases and words can be combined with it to form such a pre-modifier, and even one construction can embed into another.",
        "So we definitely leave it outside any chunk.",
        "Similarly, conjunctions, ‘ /and’, ‘ /or’ and ‘ /and’ et al., are also left outside any chunk no matter they are word-level or",
        "/n phrase-level coordinations.",
        "For instances, the examples in Section 2 are chunked as ‘[NP",
        "Similar with the shared task of CoNLL2000, we define noun compound that is formed by a noun-sequence: ‘a_n_n’, ‘m_n_n’ or ‘m_q_n_n’, as one chunk, even if there are sub-compounds, sub-phrase or coordination relations inside it.",
        "For instances, ‘[NP ]’,",
        "]’ are grouped into single chunks respectively.",
        "However, it does not mean that we blindly bind all neighboring nouns into a flat NP.",
        "If those neighboring nouns are not in one constituent or cross the phrase boundary, they will be chunked separately, such as following two examples in",
        "] /u [NP ] [NP ]’.",
        "So our solution does not break the grammatical phrase structure in a given sentence.",
        "With this chunking strategy, we not only properly resolved these problems, but also get longer chunks.",
        "Longer chunks can make successive parsing easier based on chunking.",
        "For example, if we chunked the sentence as:",
        "There would be three possible syntactic trees which are difficult to be distinguished:",
        "Whereas with above chunking strategy of our spec, there is only one syntactic tree remained:",
        "Another reason of the chunking strategy is that for some NLP applications such as IR, IE or QA, it is unnecessary to analyze these ambiguities at the early stage of text analysis.",
        "(2) PP",
        "Most PP consists of only the preposition itself because the right boundary of a preposition phrase is hard to identify or far from the preposition.",
        "But certain prepositional phrases in Chinese are formed with a frame-like construction, such as [PP /p ‘at’ ... /f ‘middle’], [PP /p ... /f ‘top’], etc.",
        "Statistics shows that more than 90% of those frame-like PPs are un-ambiguous, and others commonly have certain formal features such as an auxiliary or a conjunction immediately following the localizer.",
        "Table 2 shows the statistic result.",
        "Thus with those observations, those frame-like constructions could be chunked as PP.",
        "The length of such kind of PP frames is restricted to be at most two words inside in order to keep the distribution of chunk length more even and the chunking annotation more consistent.",
        "as a chunk without any ambiguity",
        "(3) SP",
        "Most spatial chunks consist of only the localizer(with POS tag ‘/s’ or ‘/f’).",
        "But if the spatial phrase is in the beginning of a sentence, or there is a punctuation (except “ ”) in front of it, then the localizer and its preceding words could be chunked as a SP.",
        "And the number of words in front of the localizer is also restricted to at most two for the same reason.",
        "(4) VP",
        "Commonly, a verb chunk VP is a pre-modifier verb construction, or a head-verb with its following verb-particles which form a morphologically derived word sequence.",
        "The pre-modifier is formed by adverbial phrases and/or auxiliary verbs.",
        "In order to keep the annotation consistent those verb particles and auxiliary verbs could be found in a closed list respectively only.",
        "Post-modifiers of a verb such as object and complement should be excluded in a verb chunk.",
        "We find that although a head verb groups more than one preceding adverbial phrases, auxiliary verbs and following verb-particles into one VP, its chunking performance is still high.",
        "For example: [CONJP /c ‘if’] [VP /d ‘lately’ /d ‘not’ /v ‘can’ /v ‘build’ /v ‘up’] [NP /n ‘diplomat /n ‘relation’] ‘If we could not build up the foreign relations soon’"
      ]
    },
    {
      "heading": "3.3 Spec Comparison",
      "text": [
        "We compare our spec with the Treebank-derived spec, named as S1, which is to extract the lowest non-terminal nodes from CTB as chunks from the aspect of the solutions of these problems in section 2.",
        "Noun-noun compound and the coordination which has no conjunction are chunked identically in both specs.",
        "But for others, there are different.",
        "In S1, the conjunctions of phrase-level coordination are outside of chunks and the ones of word-level are inside a chunk, all adjective or numerical modifiers are separate from noun head.",
        "According to S1, the example in 3.2.1 should be chunked as following.",
        "But these phrases that are impossible to distinguish inner structures during the early stage of text analysis are hard to be chunked and would cause some inconsistency.",
        "‘[ADJP ] [NP ]",
        "In addition, with our spec outside words are only punctuations, structural auxiliary ‘ /of’, or conjunctions, whereas with S1, outside words are defined as all left words after lowest non-terminal extraction."
      ]
    },
    {
      "heading": "4 Chunking Annotation",
      "text": [
        "Four graduate students of linguistics were assigned to annotate manually the PK corpus with the proposed chunking spec.",
        "Many discussions between authors and those annotators were conducted in order to define a better chunking spec for Chinese.",
        "Through the spec drafting and text annotating most significant syntactic ambiguities in Chinese, such as those structural ambiguities discussed in section 2 and 3, have been studied, and those observations are carefully described in the spec in turn.",
        "Consistency control is another important issue during annotation.",
        "Besides the common methods: manual checking, double annotation, post annotation checking, we explored a new consistency measure to help us find the potential inconsistent annotations, which is hinted by (Kenneth and Ryszard.",
        "2000), who defined consistency gain as a measure of a rule in learning from noisy data.",
        "The consistency of an annotated corpus in whole could be divided down into consistency of each chunk.",
        "If the same chunks appear in the same context, they should be identically annotated.",
        "So we define the consistency of one special chunk as the ratio of identical annotation in the same context.",
        "Where P represents a pattern of the chunk (POS or/and lexical sequence), context(P) represents the needed context to annotate this chunk, N represents the number of chunks in the whole corpus S. In order to improve the efficiency we also develop a semi-automatic tool that not only check mechanical errors but also detect those potential inconsistent annotations.",
        "For example, one inputs a POS pattern: ‘a_n_n’, and an expected annotation result: ‘B-NP_I-NP_E-NP3’, the tool will list all the consistent and inconsistent sentences in the annotated text respectively.",
        "Based on the output one can revise those inconsistent results one by one, and finally the consistency of the chunked text will be improved step by step."
      ]
    },
    {
      "heading": "5 Chunking Model",
      "text": [
        "After annotating the corpus, we could use various learning algorithms to build the chunking model.",
        "In this paper, HMM is selected because not only its training speed is fast, but also it has comparable performance (Xun and Huang, 2000).",
        "Automatic chunking with HMM should conduct the following two steps.",
        "1) Identify boundaries of each chunk.",
        "It is to assign each word a chunk mark, named M, which contains 5 classes: B, I, E, S (a single word chunk) and O (outside all chunks).",
        "2) Tag the chunk type, named X, which contains 11 types defined in Section 3.",
        "So each word will be tagged with two tags: M and X (the words excluding from any chunk only have M).",
        "So the result after chunking is a sequence of triples (t, m, x), where t, m, x represent POS tag, chunk mark and chunk type respectively.",
        "All the triples of a chunk are combined as an item ni, which also could be named as a chunk rule.",
        "Let W as the word segmentation result of a given sentence, T as POS tagging result and C (C= n1 n2 ... nj) as the chunking result.",
        "The statistical chunking model could be described as following:",
        "If the triple is unseen, formula 5 is used.",
        "For P(C, T), trigrams among chunks and outside words are used to approximate, that is:",
        "Smoothing follows the method of (Gao et al., 2002).",
        "In order to improve the performance we use N-fold error correction (Wu, 2004) technique to reduce the error rate and TBL is used to learn the error correction rules based on the output of HMM."
      ]
    },
    {
      "heading": "6 Data and Evaluation",
      "text": [
        "The performance of chunking is commonly measured with three figures: precision (P), recall (R) and F measure that are defined in CoNLL2000.",
        "Besides these, we also use two other measurements to evaluate the performance of bracketing and labeling respectively: RCB(ratio of crossing brackets), that is the percentage of the found brackets which cross the correct brackets; LA(labeling accuracy), that is the percentage of the found chunks which have the correct labels.",
        "No.",
        "of the chunks crossed chunk boundaries No.",
        "of chunks in test data",
        "No.",
        "of correct chunks"
      ]
    },
    {
      "heading": "6.1 Chunking performance with our spec",
      "text": [
        "Training and test was done on the PK corpus.",
        "Table 3 shows the detail information.",
        "We use the uni-gram of chunk POS rules as the baseline."
      ]
    },
    {
      "heading": "No. of the chunks with correctboundari",
      "text": [
        "es The average length(ALen) ofchunks foreach type is the average numberoftokens in eachchunk ofgiven type.",
        "The overall average length is the average numberoftokens in each chunk.",
        "To be more disinterested, outside tokens (including outside punctuations) are also concernedan d each of them is counted as one chunk.",
        "conclude that the hardest issue of Chinese chunking is to identify boundaries of NPs.",
        "Table 5:The result of each type with our spec All the chunking errors could be classified into four types: wrong labeling, under-combining, over-combining and overlapping.",
        "Table 6 lists the number and percentage of each type of errors.",
        "Under-combining errors count about a half number of overall chunking errors, however it is",
        "not a problem in certain applications because they does not cross the brackets, thus there are still opportunities to combine them later with additional knowledge.",
        "If we evaluate the chunking result without counting those under-combining errors, the F score of the proposed chunker achieves 95.45%.",
        "With comparison we also use some other learning methods, MBL(Bosch and Buchholz, 2002), SVM(Kudoh and Matsumoto, 2001) and TBL to build the chunker.",
        "The features for MBL and SVM are the POS of current, left two and right two words, lexical of current, left one and right one word.",
        "TiMBL4 and SVM-light5 are used as the tools.",
        "For SVM, we convert the chunk marks BIOES to BI and the binary class SVM is used to classifier the chunk boundary, then some rules are used to identify its label.",
        "For TBL, the rule templates are all the possible combinations of the features and the initial state is that each word is a chunk.",
        "Table 7 shows the result.",
        "As seen, without error correction all these models do not perform well and our HMM gets the best performance."
      ]
    },
    {
      "heading": "6.2 Further applications",
      "text": [
        "The length of chunks with our spec (AoL is 1.38) is longer than other Treebank-derived specs (AoL of S1 is 1.239) and closer to the constituents of sentence.",
        "Thus there are several applications benefit from the fact, such as: 1) The longest/full noun phrase identification.",
        "According to our statistics, due to including noun-noun compounds, ‘a_n_n’ and ‘m_n_n’ inside NPs, 65% noun chunks are already the longest/full noun phrases and other 22% could become the longest /full noun phrases by only one next combining step.",
        "2) The predicate-verb identification.",
        "By extending the average length of VPs, the main verb (or predicate-verb, also called tensed verb in English) of a given sentence could be identified based on certain surface evidences with a relatively high accuracy.",
        "With certain definition our statistics based on our test set show that 84.88% of those main verbs are located in the first longest VPs among all VPs in a sentence."
      ]
    },
    {
      "heading": "7 Related Work",
      "text": [
        "For chunking spec, the CoNLL2000 shared task defines a program chunklink to extract chunks from English Treebank.",
        "(Li, 2003) defines the similar Treebank-derived spec for Chinese and she reports manual check is also needed to make data consistent.",
        "Part of the Sparkle project has concentrates on a spec based on unbracketed corpus of English, Italian, French and German(Carroll et al., 1997).",
        "(Zhou, 2002) defines base phrase which is similar as chunk for Chinese, but his annotation and experiment are on his own corpus.",
        "For chunking algorithm, many machine learning (ML) methods have been applied and got promising results after chunking is represented as tagging problem, such as: SVM (Kudoh and Matsumoto, 2001), Memory-based (Bosch and Buchholz, 2002), SNoW (Li and Roth), et al..",
        "Some rule-base chunking (Kinyon, 2003) and combining rules with learning (Park and Zhang, 2003) are also reported.",
        "For annotation, (Brants, 2000) reports the inter-annotator agreement of part-of-speech annotations is 98.57%, the one of structural annotations is 92.43% and some consistency measures.",
        "(Xue et al., 2002) also address some issues related to building a large-scale Chinese corpus."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "We propose a solution of Chinese chunking with another type of spec that is based on unbracketed corpus rather than derived from a Treebank.",
        "Through spec drafting and annotating, most significant syntactic ambiguous patterns have been studied, and those observations in turn have been described in the spec carefully.",
        "The proposed method of defining a chunking spec helps us find a proper solution for the hard problems of chunking Chinese.",
        "The experiments show that with our spec, the overall Chinese chunking F-measure achieves 91.13% and 95.45% if under-combining errors are not counted."
      ]
    },
    {
      "heading": "9 Acknowledgements",
      "text": [
        "We would like to thank the members of the Natural Language Computing Group at Microsoft Research Asia.",
        "Especial acknowledgement is to the anonymous reviewers for their insightful comments and suggestions.",
        "Based on that we have revised the paper accordingly."
      ]
    }
  ]
}
