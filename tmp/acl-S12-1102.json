{
  "info": {
    "authors": [
      "Sergio Jimenez",
      "Claudia Becerra",
      "Alexander Gelbukh"
    ],
    "book": "SemEval",
    "id": "acl-S12-1102",
    "title": "Soft Cardinality + ML: Learning Adaptive Similarity Functions for Cross-lingual Textual Entailment",
    "url": "https://aclweb.org/anthology/S12-1102",
    "year": 2012
  },
  "references": [
    "acl-N04-3012",
    "acl-P09-3004",
    "acl-P99-1004",
    "acl-S12-1053",
    "acl-W02-0109",
    "acl-W05-1203",
    "acl-W07-1407"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper presents a novel approach for building adaptive similarity functions based on cardinality using machine learning.",
        "Unlike current approaches that build feature sets using similarity scores, we have developed these feature sets with the cardinalities of the commonalities and differences between pairs of objects being compared.",
        "This approach allows the machine-learning algorithm to obtain an asymmetric similarity function suitable for directional judgments.",
        "Besides using the classic set cardinality, we used soft cardinality to allow flexibility in the comparison between words.",
        "Our approach used only the information from the surface of the text, a stop-word remover and a stemmer to address the cross-lingual textual entailment task 8 at SEMEVAL 2012.",
        "We have the third best result among the 29 systems submitted by 10 teams.",
        "Additionally, this paper presents better results compared with the best official score."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Adaptive similarity functions are those functions that, beyond using the information of two objects being compared, use information from a broader set of objects (Bilenko and Mooney, 2003).",
        "Therefore, the same similarity function may return different results for the same pair of objects, depending on the context of where the objects are.",
        "Adaptability is intended to improve the performance of the similarity function in relation to the task in question associated with the entire set of objects.",
        "For example, adaptiveness improves relevance of documents retrieved for a query in an information retrieval task for a particular document collection.",
        "In text applications there are mainly three methods to provide adaptiveness to similarity functions: term weighting, adjustment or learning the parameters of the similarity function, and machine learning.",
        "Term weighting is a common practice that assigns a degree of importance to each occurrence of a term in a text collection (Salton and Buckley, 1988; Lan et al., 2005).",
        "Secondly, if a similarity function has parameters, these can be adjusted or learned to adapt to a particular data set.",
        "Depending on the size of the search space defined by these parameters, they can be adjusted either manually or using a technique of AI.",
        "For instance, Jimenez et al.",
        "manually adjusted a single parameter in the generalized measure of Monge-Elkan (1996) (Jimenez et al., 2009) and Ristrad and Yanilios (1998) learned the costs of editing operations between particular characters for the Levenshtein distance (1966) using HMMs.",
        "Thirdly, the machine-learning approach aims to learn a similarity function based on a vector representation of texts using a subset of texts for training and a learning function (Bilenko and Mooney, 2003).",
        "The three methods of adaptability can also be used in a variety of combinations, e.g. term weighting in combination with machine learning (Debole and Sebastiani, 2003; Lan et al., 2005).",
        "Finally, to achieve adaptability, other approaches use data sets considerably larger, such as large corpora or the Web, e.g. distributional similarity (Lee, 1999).",
        "In the machine-learning approach, a vector representation of texts is used in conjunction with an algorithm of classification or regression (Alpaydin, 2004).",
        "Each vector of features ?f1, f2, .",
        ".",
        ".",
        ", fm?",
        "is associated to each pair ?Ti, Tj?",
        "of texts.",
        "Thus, Bilenko et al. (2003) proposed a set of features indexed by the data set vocabulary, similar to Zanzotto et al., (2009) who used fragments of parse trees.",
        "However, a more common approach is to select as features the scores of different similarity functions.",
        "Using these features, the machine-learning algorithm discovers the relative importance of each feature and a combination mechanism that maximizes the alignment of the final result with a gold standard for the particular task.",
        "In this paper, we propose a novel approach to extract feature sets for a machine-learning algorithm using car",
        "dinalities rather than scores of similarity functions.",
        "For instance, instead of using as a feature the score obtained by the Dice's coefficient (i.e. 2?|Ti?Tj |/|Ti|+|Tj |), we use |Ti|, |Tj |and |Ti ?",
        "Tj |as features.",
        "The rationale behind this idea is that despite the similarity scores being suitable for learning a combined function of similarity, they hide the information imbalance between the original pair of texts.",
        "Our hypothesis is that the information coded in this imbalance could provide the machine-learning algorithm with better information to generate a combined similarity score.",
        "For instance, consider these pairs of texts: ?",
        "?The beach house is white.",
        "?, ?The house was completely empty.?",
        "?",
        "and ?",
        "?The house?, ?The beach house was completely empty and isolated?",
        "?.",
        "Both pairs have the same similarity score using the Dice coefficient, but it is evident that the latter has an imbalance of information lost in that single score.",
        "This imbalance of information is even more important if the task requires to identify directional similarities, such as ?T1 is more similar to T2, than T2 is to T1?.",
        "However, unlike the similarity functions, which are numerous, there is only one set cardinality.",
        "This issue can be addressed using the soft cardinality proposed by Jimenez et al. (2010), which uses an auxiliary function of similarity between elements to make a soft count of the elements in a set.",
        "For instance, the classic cardinality of the set A = { ?Sunday?, ?Saturday? }",
        "is |A |= 2; and the soft cardinality of the same set, using a normalized edit-distance as auxiliary similarity function, is |A| ?",
        "sim = 1.23 because of the commonalities between both words.",
        "Furthermore, soft cardinality allows weighting of elements giving it additional capacity to adapt.",
        "We used the proposed approach to participate in the cross-lingual textual-entailment task 8 at SEMEVAL 2012.",
        "The task was to recognize bidirectional, forward, backward or lack of entailment in pairs of texts written in five languages.",
        "We built a system based on the proposed method and the use of surface information of the text, a stop-word remover and a stemmer.",
        "Our system achieved the third best result in official classification and, after some debugging, we are reporting better results than the best official scores.",
        "This paper is structured as follows.",
        "Section 2 briefly describes soft cardinality and other cardinalities for text applications.",
        "Section 3 presents the proposed method.",
        "Experimental validation is presented in Section 4.",
        "A brief discussion is presented in Section 5.",
        "Finally, conclusions are drawn in Section 6."
      ]
    },
    {
      "heading": "2 Cardinalities for text",
      "text": [
        "Cardinality is a measure of counting the number of elements in a set.",
        "The cardinality of classical set theory represents the number of non-repeated elements in a set.",
        "However, this cardinality is rigid because it counts in the same manner very similar or highly differentiated elements.",
        "In text applications, text can be modeled as a set of words and a desirable cardinality function should take into account the similarities between words.",
        "In this section, we present some methods to soften the classical concept of cardinality."
      ]
    },
    {
      "heading": "2.1 Lemmatizer Cardinality",
      "text": [
        "The simplest approach is to use a stemmer that collapses words with common roots in a single lemma.",
        "Consider the sentence: ?I loved, I am loving and I will love you?.",
        "The plain word counting of this sentence is 10 words.",
        "The classical cardinality collapses the three occurrences of the pronoun ?I?",
        "giving a count of 8.",
        "However, a lemmatizer such as Porter's stemmer (1980) also collapses the words ?loved?, ?loving?",
        "and ?love?",
        "in a single lemma ?love?",
        "for a count of 6.",
        "Thus, when a text is lemmatized, it induces a relaxation of the classical cardinality of a text.",
        "In addition, to provide corpus adaptability, a weighted version of this cardinality can add weights associated with each word occurrence instead of adding 1 for each word (e.g. tf-idf)."
      ]
    },
    {
      "heading": "2.2 LCS cardinality",
      "text": [
        "Longest common subsequence (LCS) length is a measure of the commonalities between two texts, unlike set intersection, taking into account the order.",
        "Therefore, a cardinality function of a pair of texts A and B could be |A ?",
        "B |= len(LCS(A,B)), |A |= len(A) and |B |= len(B).",
        "Functions len(?)",
        "and LCS(?, ?)",
        "calculate length and LCS respectively, either in character or word granularity."
      ]
    },
    {
      "heading": "2.3 Soft Cardinality",
      "text": [
        "Soft cardinality is a function that uses an auxiliary similarity function to make a soft count of the elements (i.e. words) in a set (i.e. text) (Jimenez et al., 2010).",
        "The auxiliary similarity function can be any measure or metric that returns scores in the interval [0, 1], with 0 being the lowest degree of similarity and 1 the highest (i.e. identical words).",
        "Clearly, if the auxiliary similarity function is a rigid comparator that returns 1 for identical words and 0 otherwise, the soft cardinality becomes the classic set cardinality.",
        "The soft cardinality of a set A = {a1, a2, .",
        ".",
        ".",
        ", a|A|} can be calculated by the following expression: |A|",
        "the auxiliary similarity function for approximate word comparison, wai are weights associated with each word ai, and p is a tuning parameter that controls the degree of smoothness of the cardinality, i.e. if 0 ?",
        "p all elements in a set are considered identical and if p??",
        "soft cardinality becomes classic cardinality."
      ]
    },
    {
      "heading": "2.4 Dot-product VSM ?Cardinality?",
      "text": [
        "Resemblance coefficients are cardinality-based similarity functions.",
        "For instance, the Dice coefficient is the ratio between the cardinality of the intersection divided by the arithmetic mean of individual cardinalities:2?|A?B|/|A|+|B|.",
        "The cosine coefficient is similar but instead of using the arithmetic mean it uses the geometric mean: |A?B|/",
        "|B|.",
        "Furthermore, the cosine similarity is a well known metric used in the vector space model (VSM) proposed by Salton et al. (1975)",
        ".",
        "Clearly, this expression can be compared with the cosine coefficient interpreting the dot-product operation in the cosine similarity as a cardinality.",
        "Thus, the obtained cardinalities are:",
        "of weighting providing no effect if 0?",
        "p or emphasising the weights if p > 0.",
        "In a similar application, Gonzalez and Caicedo (2011) used p = 0.5 and normalization justified by the quantum information retrieval theory."
      ]
    },
    {
      "heading": "3 Learning Similarity Functions from Cardinalities",
      "text": [
        "Different similarity measures use different knowledge, identify different types of commonalities, and compare objects with different granularity.",
        "In many of the automatic text-processing applications, the qualities of several similarity functions may be required to achieve the final task.",
        "The combination of similarity scores with a machine-learning algorithm to obtain a unified effect for a particular task is a common practice (Bilenko et al., 2003; Malakasiotis and Androutsopoulos, 2007; Malakasiotis, 2009).",
        "For each pair of texts for comparison, there is provided a vector representation based on multiple similarity scores as a set of features.",
        "In addition, a class attribute is associated with each vector which contains the objective of the task or the gold standard to be learned by the machine-learning algorithm.",
        "However, the similarity scores conceal important information when the task requires dealing with directional problems, i.e. whenever the order of comparing each pair of texts is related with the class attribute.",
        "For instance, textual entailment is a directional task since it is necessary to recognize whether the first text entails the second text or vice versa.",
        "This problem can be addressed using asymmetric similarity functions and including scores for sim(A,B) and sim(B,A) in the resulting vector for each pair ?A,B?.",
        "Nevertheless, the similarity measures that are more commonly used are symmetric, e.g. edit-distance (Levenshtein, 1966), LCS (Hirschberg, 1977), cosine similarity, and many of the current semantic relatedness measures (Pedersen et al., 2004).",
        "Although, there are asymmetric measures such as the Monge-Elkan measure (1996) and the measure proposed by Corley and Mihalcea (Corley and Mihalcea, 2005), they are outnumbered by the symmetric measures.",
        "Clearly, this situation restricts the use of the machine learning as a method of combination for directional problems.",
        "Alternatively, we propose the construction of a vector for each pair of texts using cardinalities instead of similarity scores.",
        "Moreover, using cardinalities rather than similarity scores allows the machine-learning algorithm to discover patterns to cope with directional tasks.",
        "Basically, we propose to use a set with six features for each cardinality function: |A|, |B|, |A ?",
        "B|, |A ?",
        "B|,"
      ]
    },
    {
      "heading": "|A?B |and |B ?A|. 4 Experimental Setup",
      "text": []
    },
    {
      "heading": "4.1 Cross-lingual Textual Entailment (CLTE) Task",
      "text": [
        "This task consist of recognizing in a pair of topically related text fragments T1 and T2 in different languages, one of the following possible entailment relations: i) bidi",
        "sumed to be true statements; hence contradictory pairs are not allowed.",
        "Data sets consist of a collection of 1,000 text pairs (500 for training and 500 for testing) each one labeled with one of the possible entailment types.",
        "Four balanced data sets were provided using the following language pairs: German-English (deu-eng), French-English (fra-eng), Italian-English (ita-eng) and Spanish-English (spa-eng).",
        "The evaluation measure for experiments was accuracy, i.e. the ratio of correctly predicted pairs by the total number of predictions.",
        "For a comprehensive description of the task see (Negri et al., 2012)."
      ]
    },
    {
      "heading": "4.2 Experiments",
      "text": [
        "Given that each pair of texts ?T1, T2?",
        "are in different lan",
        "guages, a pair of translations ?T t1 , T t 2?",
        "were provided using Google Translate service.",
        "Thus, each one of the text",
        "Then, all produced pairs were preprocessed by removing stop-words in their respective languages.",
        "Finally, all texts were lemmatized using Porter's stemmer (1980) for English and Snowball stemmers for other languages using an implementation provided by the NLTK (Loper and Bird, 2002).",
        "Then, different set of features were generated using similarity scores or cardinalities.",
        "While each symmetric similarity function generates 2 features i)sim(T1, T t2) and ii)sim(T t1 , T2), asymmetric functions generate two additional features iii)sim(T t2 , T1) and iv)sim(T2, T",
        "On the other hand, each cardinality function generates",
        "Various combinations of cardinalities, symmetric and asymmetric functions were used to generate the following feature sets: Sym.simScores: scores of the following symmetric similarity functions: Jaccard, Dice, and cosine coefficients using classical cardinality and soft cardinality (edit-distance as auxiliar sim.",
        "function).",
        "In addition, cosine similarity, softTFIDF (Cohen et al., 2003) and edit-distance (total 18 features).",
        "Asym.LCS.sim: scores of the following asymmetric similarity functions: sim(T1, T2) = lcs(T1,T2)/len(T1) and sim(T1, T2) = lcs(T1,T2)/len(T2) at character level (4 features).",
        "Classic.card: cardinalities using classical set cardinality (12 features).",
        "Dot.card.w: dot-product cardinality using idf weights as described in Section 2.4, using p = 1 (12 features).",
        "LCS.card: LCS cardinality at word-level using idf weights as described in Section 2.1 (12 features).",
        "SimScores: combined features sets from Sym.SimScores, Asym.LCS.sim and the generalized Monge-Elkan measure (Jimenez et al., 2009) using p = 1, 2, 3 (30 features).",
        "Dot.card.w.0.5: same as Dot.card.w using p = 0.5.",
        "Classic.card.w: classical cardinality using idf weights (12 features).",
        "Soft.card.w: soft cardinality using idf weights as described in Section 2.3 using p = 1, 2, 3, 4, 5 (60 features).",
        "The machine-learning classification algorithm for all feature sets was SVM (Cortes and Vapnik, 1995) with the complexity parameter C = 1.5 and a linear polynomial kernel.",
        "All experiments were conducted using WEKA (Hall et al., 2009)."
      ]
    },
    {
      "heading": "4.3 Results",
      "text": [
        "In Semeval 2012 exercise, participants were given a particular subdivision into training and test subsets for each data set.",
        "For official results, participants received only the gold-standard labels for the subset of training, and accuracies of each system in the test subset was measured by the organizers.",
        "In Table 1, the results for that particular division are shown.",
        "At the bottom of that table, the official results for the first three systems are shown.",
        "Our system, ?3rd.Softcard?",
        "was configured using soft cardinality with edit-distance as auxiliary similarity function and p = 2.",
        "Erroneously, at the time of the submission, all texts in the 5 languages were lemmatized using an English stemmer and stop-words in all languages were aggregated into a single set before the withdrawal.",
        "In spite of these bugs, our system was the third best score.",
        "To compare our approach of using feature sets based on soft cardinality versus other approaches, we generated 100 random training-test subdivisions (50%-50%) of each data set.",
        "The average results were compared and tested statistically with the paired T-tested corrected test.",
        "Results, deviations, the percentage of improvement, and its significance in comparison with the Soft.card.w system are shown in Table2."
      ]
    },
    {
      "heading": "5 Discusion",
      "text": [
        "Results in Table 2 show that our hypothesis that feature sets obtained from cardinalities should outperform features sets obtained from similarity scores was de-mostrated when compared versus similarity functions alternatively symmetrical or asymetrical.",
        "However, when our approach is compared with a feature set obtained by combining symmetric and asymmetric functions, we obtained an improvement of 5.8% but only with a significance of 0.124.",
        "Regarding soft cardinality compared to alternative cardinalities, soft cardinality outperformed others in all cases with significance <0.059."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "We have proposed a new method to compose feature sets using cardinalities rather than similarity scores.",
        "Our approach proved to be effective for directional text comparison tasks such as textual entailment.",
        "Furthermore, the soft cardinality function proved to be the best for obtaining such sets of features."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This research was funded by the Systems and Industrial Engineering Department, the Office of Student Welfare of the National University of Colombia, Bogot?, and throught a grant from the Colombian Department for"
      ]
    }
  ]
}
