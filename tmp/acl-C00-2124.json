{
  "info": {
    "authors": [
      "Erik F. Tjong Kim Sang",
      "Walter Daelemans",
      "Herve Dejean",
      "Rob Koeling",
      "Yuval Krymolowski",
      "Vasin Punyakanok",
      "Dan Roth"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-2124",
    "title": "Applying System Combination to Base Noun Phrase Identification",
    "url": "https://aclweb.org/anthology/C00-2124",
    "year": 2000
  },
  "references": [
    "acl-A00-2007",
    "acl-C00-1034",
    "acl-J93-2004",
    "acl-J96-1002",
    "acl-P98-1029",
    "acl-P98-1081",
    "acl-W95-0107",
    "acl-W99-0621"
  ],
  "sections": [
    {
      "text": [
        "{ erikt,daelem}tluia.ua.ac.be TUniversitat Tubingen Kleine WilhelmstraBe 113 1)-72074 Tubingen, Germany dejean4sfs.nphiLuni-tuebingen.de 7Slii Cambridge 23 Millers Yard,Mill Lane Cambridge, 0132 1.1i,Q, UK koelingcam.sri.com /3Bar-Ilan University Ramat Gan, 52900, Israel yuvalkOmaes.1)iu.ac.il Abstract We use seven machine learning algorithms for one task: identifying base noun phrases.",
        "The results have been processed by different system.",
        "combination methods and all of these outperformed the best individual.",
        "result.",
        "We have applied the seven learners with the best combinator, a majority vote of the top five systems, to a standard data set and managed to improve the best published result; for this data set."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Van Halteren et a].",
        "(1998) and Brill and Wu (1998) show that part-of-speech tagger performance can be improved by combining different taggers.",
        "13y using techniques such as majority voting, errors made by the minority of the taggers can be removed.",
        "Van Balteren et al.",
        "(1998) ref)ort that the results of such a combined approach can improve upon the accuracy error of the best individual system with as much as 19%.",
        "The positive effect of system combination for non-language processing tasks has been shown.",
        "in a large body of machine learning work.",
        "In this paper we will use system combination for identifying base noun phrases (baseNPs).",
        "We will apply seven machine learning algorithms to the same baseNP task.",
        "At two points we will apply combination methods.",
        "We will start with making the systems process five output; representations and combine the results by choosing the majority of the output features.",
        "Three of the seven systems use this approach.",
        "After this we will make an overall combination of the results of the seven systems.",
        "There we will evaluate several system combination meth-'University of Illinois 1304 W. Springfield Ave. Urbana, II, 61.801, USA fpunyakan,danr1((ks.uitic.edu ()cis.",
        "The best; performing method will be applied to a standard data set for baseNP identification."
      ]
    },
    {
      "heading": "2 Methods and experiments",
      "text": [
        "In this section we will describe our learning task: recognizing base noun phrases.",
        "After this we will describe the data representations we used and the machine learning algorithms that we will apply to the task.",
        "We will conclude with an overview of the combination methods that we will test."
      ]
    },
    {
      "heading": "2.1 Task description",
      "text": [
        "Base noun phrases (baseNPs) are 1101111 phrases which (10 not contain another noun phrase.",
        "For example, the sentence In [ early trading ] in [ Hong Kong [ Monday ] , [ gold ] was quoted at [ $ 366.50 ] [ an ounce .",
        "contains six baseNPs (marked as phrases between square brackets).",
        "The phrase $ 866.50 On ounce is a noun phrase as well.",
        "However, it is not a baseNP since it contains two other noun phrases.",
        "Two baseNP data sets have been put forward by Ramshaw and Marcus (1995).",
        "The main data set consist of four sections of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1.993) as training material (sections 15-18, 211727 tokens) and one section as test; material (section 20, 47377 tokens)1.",
        "The data contains words, their part-of-speech",
        "(POS) tags as computed by the Brill tagger and their baseNP segmentation as derived from the Treebank (with some modifications).",
        "In the baseNP identification task, performance is measured with three rates.",
        "First, with the percentage of detected noun phrases that are correct (precision).",
        "Second, with the percentage of noun phrases in the data that were found by the classifier (recall).",
        "And third, with the Fi3=1 rate which is equal to (2*preci-sion*recall)/(precision+recall).",
        "The latter rate has been used as the target for optimization."
      ]
    },
    {
      "heading": "2.2 Data representation",
      "text": [
        "In our example sentence in section 2.1, noun phrases are represented by bracket structures.",
        "It has been shown by Munoz et al.",
        "(1999) that for baseNP recognition, the representation with brackets outperforms other data representations.",
        "One classifier can be trained to recognize open brackets (0) and another can handle close brackets (C).",
        "Their results can be combined by making pairs of open and close brackets with large probability scores.",
        "We have used this bracket representation (0+C) as well.",
        "However, we have not used the combination strategy from Munoz et al.",
        "(1999) but instead used the strategy outlined in Tjong Kim Sang (2000): regard only the shortest possible phrases between candidate open and close brackets as base noun phrases.",
        "An alternative representation for baseNPs has been put forward by Ramshaw and Marcus (1995).",
        "They have defined baseNP recognition as a tagging task: words can be inside a baseNP (I) or outside a baseNP (0).",
        "In the case that one baseNP immediately follows another baseNP, the first word in the second baseNP receives tag B.",
        "Example: In0 early/ trading/ in0 Hong' Kong/ MondayB ,o gold/ was0 quoted° ato $1 366.50/ anB ounce/ •0 This set of three tags is sufficient for encoding baseNP structures since these structures are nonrecursive and nonoverlapping.",
        "Tjong Kim Sang (2000) outlines alternative versions of this tagging representation.",
        "First, the B tag can be used for the first word of every baseNP (I0B2 representation).",
        "Second, instead of the B tag an E tag can be used to mark the last word of a baseNP immediately before another baseNP (I0E1).",
        "And third, the E tag can be used for every noun phrase final word (10E2).",
        "He used the Ramshaw and Marcus (1995) representation as well (IOB1).",
        "We will use these four tagging representations and the 0+C representation for the system-internal combination experiments."
      ]
    },
    {
      "heading": "2.3 Machine learning algorithms",
      "text": [
        "This section contains a brief description of the seven machine learning algorithms that we will apply to the baseNP identification task: AL-LiS, c5.0, IGTree, MaxEnt, MBL, MBSL and SNoW.",
        "ALLiS2 (Architecture for Learning Linguistic Structures) is a learning system which uses theory refinement in order to learn non-recursive NP and VP structures (Dejean, 2000).",
        "ALLiS generates a regular expression grammar which describes the phrase structure (NP or VP).",
        "This grammar is then used by the CASS parser (Ab-ney, 1996).",
        "Following the principle of theory refinement, the learning task is composed of two steps.",
        "The first step is the generation of an initial grammar.",
        "The generation of this grammar uses the notion of default values and some background knowledge which provides general expectations concerning the inner structure of NPs and VPs.",
        "This initial grammar provides an incomplete and/or incorrect analysis of the data.",
        "The second step is the refinement of this grammar.",
        "During this step, the validity of the rules of the initial grammar is checked and the rules arc improved (refined) if necessary.",
        "This refinement relies on the use of two operations: the contextualization (in which contexts such a tag always belongs to the phrase) and lexical-ization (use of information about the words and not only about POS).",
        "C5.03, a commercial version of c4.5 (Quinlan, 1993), performs top-down induction of decision trees (TDIDT).",
        "On the basis of an instance base of examples, c5.0 constructs a decision tree which compresses the classification information in the instance base by exploiting differences in relative importance of different features.",
        "Instances are stored in the tree as paths",
        "of connected nodes ending in leaves which contain classification information.",
        "Nodes are connected via arcs denoting feature values.",
        "Feature information gain (mutual information between features and class) is used to determine the order in which features are employed as tests at; all levels of the tree (Quinlan, 1993).",
        "With the full input representation (words and POS tags), we were not able to run complete experiments.",
        "We therefore experimented only with the POS tags (with a context of two left and right).",
        "We have used the default; parameter setting with decision trees combined with value grouping.",
        "We have used a nearest neighbor algorithm (1131.-IG, here listed as MBL) and a decision tree algorithm (IGnee) from the TiMBL learning package (Daelemans et al., 1.999b).",
        "Both algorithms store the training data and classify new items by choosing the most frequent classification among training items which are closest to this new item.. Data items are represented as sets of feature-value pairs.",
        "Each feature receives a weight which is based on the amount of information which it provides for computing the classification of the items in the training data.",
        "11.31-IG uses these weights for computing the distance between a pair of data items and IGTree uses them for deciding which feature-value decisions should be made in the top nodes of the decision tree (Daelemans et al., 1999b).",
        "We will use their default parameters except for the tull-IG parameter for the number of examined nearest neighbors (k) which we have set to 3 (Daelemans et al., 1999a).",
        "The classifiers use a left and right; context of four words and part-of-speech tags.",
        "For the four IO representations we have used a second processing stage which used a smaller context but which included in-thrmation about the IO tags predicted by the first processing phase (Tjong Kim Sang, 2000).",
        "When building a classifier, one must gather evidence for predicting the correct class of an item from its context.",
        "The Maximum Entropy (MaxEnt) framework is especially suited for integrating evidence from various information sources.",
        "Frequencies of evidence/class combinations (called features) are extracted from a sample corpus and considered to be properties of the classification process.",
        "Attention is constrained to models with these properties.",
        "The 1VlaxEnt principle now demands that; among all the probability distributions that obey these constraints, the most uniform is chosen.",
        "During training, features are assigned weights in such a way that;, given the MaxEnt principle, the training data is matched as well as possible.",
        "During evaluation it is tested which features are active (i.e. a feature is active when the context; meets the requirements given by the feature).",
        "For every class the weights of the active features are combined and the best scoring class is chosen (Berger et al., 1996).",
        "For the classifier built here the surrounding words, their POS tags and baseNP tags predicted for the previous words are used as evidence.",
        "A mixture of simple features (consisting of one of the mentioned information sources) and complex features (combinations thereof) were used.",
        "The left context never exceeded 3 words, the right; context; was maximally 2 words.",
        "The model was calculated using existing software (Dehaspe, 1997).",
        "MI3SL (Argamon et al., 1999) uses POS data in order to identify baseNPs.",
        "Inference relies on a memory which contains all the occurrences of POS sequences which appear in the beginning, or the end, of a baseNP (including complete phrases).",
        "These sequences may include a few context; tags, up to a pre-specified max_context.",
        "During inference, MBSL tries to 'tile' each POS string with parts of noun-phrases from the memory.",
        "If the string could be fully covered by the tiles, it becomes part of a candidate list, ambiguities between candidates are resolved by a constraint propagation algorithm.",
        "Adding a context extends the possibilities for tiling, thereby giving more opportunities to better candidates.",
        "The approach of MBSI, to the problem of identifying baseNPs is sequence-based rather than word-based, that is, decisions are taken per POS sequence, or per candidate, but not for a single word.",
        "In addition, the tiling process gives no preference to any direction in the sentence.",
        "The tiles may be of any length, up to the maximal length of a phrase in the training data, which gives MBSL a generalization power that compensates for the setup of using only POS tags.",
        "The results presented here were obtained by optimizing MBSL parameters based on 5-fold CV on the training data.",
        "SNoW uses the Open/Close model, described in Munoz et; al.",
        "(1999).",
        "As is shown there, this",
        "model produced better results than the other paradigm evaluated there, the Inside/Outside paradigm.",
        "The Open/Close model consists of two SNoW predictors, one of which predicts the beginning of baseNPs (Open predictor), and the other predicts the end of the phrase (Close predictor).",
        "The Open predictor is learned using SNoW (Carlson et al., 1999; Roth, 1998) as a function of features that utilize words and POS tags in the sentence and, given a new sentence, will predict for each word whether it is the first word in the phrase or not.",
        "For each Open, the Close predictor is learned using SNoW as a function of features that utilize the words in the sentence, the POS tags and the open prediction.",
        "It will predict, for each word, whether it can be the end of the phrase, given the previously predicted Open.",
        "Each pair of predicted Open and Close forms a candidate of a baseNP.",
        "These candidates may conflict due to overlapping; at this stage, a graph-based constraint satisfaction algorithm that uses the confidence values SNoW associates with its predictions is employed.",
        "This algorithm (\"the combinator\") produces the list of the final baseNPs for each sentence.",
        "Details of SNoW, its application in shallow parsing and the combinator's algorithm are in Munoz et al.",
        "(1999)."
      ]
    },
    {
      "heading": "2.4 Combination techniques",
      "text": [
        "At two points in our noun phrase recognition process we will use system combination.",
        "We will start with system-internal combination: apply the same learning algorithm to variants of the task and combine the results.",
        "The approach we have chosen here is the same as in Tjong Kim Sang (2000): generate different variants of the task by using different representations of the output (I0B1, 10B2, IOE1, 10E2 and 0+C).",
        "The five outputs will converted to the open bracket representation (0) and the close bracket representation (C) and after this, the most frequent of the five analyses of each word will chosen (majority voting, see below).",
        "We expect the systems which use this combination phase to perform better than their individual members (Tjong Kim Sang, 2000).",
        "Our seven learners will generate different classifications of the training data and we need to find out which combination techniques are most appropriate.",
        "For the system-external combination experiment, we have evaluated different voting mechanisms, effectively the voting methods as described in Van Halteren et al.",
        "(1998).",
        "In the first method each classification receives the same weight and the most frequent classification is chosen (Majority).",
        "The second method regards as the weight of each individual classification algorithm its accuracy on sonic part of the data, the tuning data (TotPrecision).",
        "The third voting method computes the precision of each assigned tag per classifier and uses this value as a weight for the classifier in those cases that it chooses the tag (TagPrecision).",
        "The fourth method uses both the precision of each assigned tag and the recall of the competing tags (Precision-Recall).",
        "Finally, the fifth method uses not only a weight for tine current classification but it also computes weights for other possible classifications.",
        "The other classifications are determined by examining the tun",
        "ing data and registering the correct values for every pair of classifier results (pair-wise voting, see Van Halteren et al.",
        "(1998) for an elaborate explanation).",
        "Apart from these five voting methods we have also processed the output streams with two classifiers: MBL and IGTree.",
        "This approach is called classifier stacking.",
        "Like Van Halteren et M. (1998), we have used different input versions: one containing only the classifier output and another containing both classifier output and a compressed representation of the data item under consideration.",
        "For the latter purpose we have used the part-of-speech tag of the current word.",
        "3 Res Ult S4 We want to find out whether system combination could improve performance of baseNP recognition and, if this is the fact, we want to select the best combination technique.",
        "For tins purpose we have performed an experiment with sections 15-18 of the WSJ part; of the Penn Treebank as training data (211727 tokens) and section 21 as test data (40039 tokens).",
        "Like the data used by Ramshaw and Marcus (1995), this data was retagged by the Brill tagger in order to obtain realistic part-of-speech (POS) tags'.",
        "The data was segmented into baseNP parts and non-baseNP parts in a similar fashion as the data used by Ramshaw and Marcus (1995).",
        "Of the training data, only 90% was used for training.",
        "The remaining 10% was used as tuning data for determining the weights of the combination techniques.",
        "For three classifiers (MBL, MaxEnt and IGTree) we have used system-internal combination.",
        "These learning algorithms have processed five different representations of the output; (10131, 10B2, I0E1, 10E2 and 0-I--C) and the results have been combined with majority voting.",
        "The test data results can be found in Table 1.",
        "In all cases, the combined results were better than that of the best included system.",
        "The results of ALLiS, c5.0, MBSL and SNoW have been converted to the 0 and the C repre",
        "for section WSJ 21 of the Penn Treebank with seven individual classifiers and combinations of them.",
        "Each combination performs better than its best individual member.",
        "The stacked classifiers without context information perform best.",
        "sentation.",
        "Together with the bracket; representations of the other three techniques, this gave us a total of seven 0 results and seven C results.",
        "These two data streams have been combined with the combination techniques described in section 2.4.",
        "After tins, we built baseNPs from the 0 and C results of each combination technique, like described in section 2.2.",
        "The bracket accuracies and the Fo=1 scores for test data can be found in Table 2.",
        "All combinations improve the results of the best individual classifier.",
        "The best results were obtained with a memory-based stacked classifier.",
        "This is different; from the combination results presented in Van Halteren et al.",
        "(1998), in which pairwise voting performed best.",
        "However, in their later work stacked classifiers outperform voting methods as well (Van Halteren et al., to appear).",
        "and Marcus (1995) together with an overview of earlier work.",
        "The accuracy scores indicate how often a word was classified correctly with the representation used (0, C or IOB1).",
        "The combined system outperforms all earlier reported results for this data set.",
        "Based on an earlier combination study (Tjong Kim Sang, 2000) we had expected the voting methods to do better.",
        "We suspect that their performance is below that of the stacked classifiers because the difference between the best and the worst individual system is larger than in our earlier study.",
        "We assume that the voting methods might perform better if they were only applied to the classifiers that perform well on this task.",
        "In order to test this hypothesis, we have repeated the combination experiments with the best n classifiers, where n took values from 3 to 6 and the classifiers were ranked based on their performance on the tuning data.",
        "The best performances were obtained with five classifiers: F1=1=93.44 for all five voting methods with the best stacked classifier reaching 93.24.",
        "With the top five classifiers, the voting methods outperform the best combination with seven systems.",
        "Adding extra classification results to a good combination system should not make overall performance worse so it is clear that there is some room left for improvement of our combination algorithms.",
        "We conclude that the best results in this task can be obtained with the simplest voting method, majority voting, applied to the best five of our classifiers.",
        "Our next task was to apply the combination approach to a standard data set so that we could compare our results with other work.",
        "For this purpose we have used 6Wc are unaware of a good method for determining the significance of 170=1 differences but we assume that this F0=1 difference is not significant.",
        "However, we believe that the fact that more combination methods perform well, shows that it easier to get a good performance out of the best five systems than with all seven.",
        "the data put forward by Ramshaw and Marcus (1995).",
        "Again, only 90% of the training data was used for training while the remaining 10% was reserved for ranking the classifiers.",
        "The seven learners were trained with the same parameters as in the previous experiment.",
        "Three of the classifiers (MBL, MaxEnt and IGTree) used system-internal combination by processing different output representations.",
        "The classifier output was converted to the 0 and the C representation.",
        "Based on the tuning data performance, the classifiers ALLiS, IGTIIEE, MaxEnt, M13L and SNoW were selected for being combined with majority voting.",
        "After this, the resulting 0 and C representations were combined to baseNPs by using the method described in section 2.2.",
        "The results can be found in Table 3.",
        "Our combined system obtains an F0,1 score of 93.86 which corresponds to an 8% error reduction compared with the best published result for tins data set (93.26)."
      ]
    },
    {
      "heading": "4 Concluding remarks",
      "text": [
        "In tins paper we have examined two methods for combining the results of machine learning algorithms for identifying base noun phrases.",
        "In the first method, the learner processed different output data representations and the results were combined by majority voting.",
        "This approach yielded better results than the best included classifier.",
        "In the second combination approach we have combined the results of seven learning systems (ALLiS, c5.0, IGTree, MaxEnt, MBL, MBSL and SNoW).",
        "Here we have tested different combination methods.",
        "Each combination",
        "method outperformed the best individual learning algorithm and a majority vote of the top five systems performed best.",
        "We have applied this approach of system-internal and system-external combination to a standard data set for base noun phrase identification and the performance of our system was better than any other published result for this data set.",
        "Our study shows that the combination methods that we have tested are sensitive for the inclusion of classifier results of poor quality.",
        "This leaves room for improvement of our results by evaluating other combinators.",
        "Another interesting approach which might lead to a better performance is taking into account more context information, for example by combining complete phrases instead of independent brackets.",
        "It would also be worthwhile to evaluate using more elaborate methods for building baseNPs out of open and close bracket candidates."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "Dejean Koeling and Tjong Kim.",
        "Sang are funded by the TMR.",
        "network Learning Computational Grammars7.",
        "Punyakanok and Roth are supported by NFS grants IIS-9801638 and SBR9873450."
      ]
    }
  ]
}
