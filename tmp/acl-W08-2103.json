{
  "info": {
    "authors": [
      "Tomoya Iwakura",
      "Seishi Okamoto"
    ],
    "book": "CoNLL 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning",
    "id": "acl-W08-2103",
    "title": "A Fast Boosting-based Learner for Feature-Rich Tagging and Chunking",
    "url": "https://aclweb.org/anthology/W08-2103",
    "year": 2008
  },
  "references": [
    "acl-D07-1033",
    "acl-D07-1083",
    "acl-H05-1059",
    "acl-J05-1003",
    "acl-J93-2004",
    "acl-N01-1025",
    "acl-N03-1033",
    "acl-P03-1004",
    "acl-P05-1001",
    "acl-P05-1024",
    "acl-P07-1096",
    "acl-W02-1001",
    "acl-W04-3239"
  ],
  "sections": [
    {
      "text": [
        "Tomoya Iwakura Seishi Okamoto",
        "Fujitsu Laboratories Ltd. 1-1, Kamikodanaka 4-chome, Nakahara-ku, Kawasaki 211-8588, Japan",
        "Combination of features contributes to a significant improvement in accuracy on tasks such as part-of-speech (POS) tagging and text chunking, compared with using atomic features.",
        "However, selecting combination of features on learning with large-scale and feature-rich training data requires long training time.",
        "We propose a fast boosting-based algorithm for learning rules represented by combination of features.",
        "Our algorithm constructs a set of rules by repeating the process to select several rules from a small proportion of candidate rules.",
        "The candidate rules are generated from a subset of all the features with a technique similar to beam search.",
        "Then we propose POS tagging and text chunking based on our learning algorithm.",
        "Our tagger and chunker use candidate POS tags or chunk tags of each word collected from automatically tagged data.",
        "We evaluate our methods with English POS tagging and text chunking.",
        "The experimental results show that the training time of our algorithm are about 50 times faster than Support Vector Machines with polynomial kernel on the average while maintaining state-of-the-art accuracy and faster classification speed."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Several boosting-based learning algorithms have been applied to Natural Language Processing problems successfully.",
        "These include text categorization (Schapire and Singer, 2000), Natural Language Parsing (Collins and Koo, 2005), English syntactic chunking (Kudo et al., 2005) and so on.",
        "© 2008.",
        "Some rights reserved.",
        "Furthermore, classifiers based on boosting-based learners have shown fast classification speed (Kudo et al., 2005).",
        "However, boosting-based learning algorithms require long training time.",
        "One of the reasons is that boosting is a method to create a final hypothesis by repeatedly generating a weak hypothesis in each training iteration with a given weak learner.",
        "Lhese weak hypotheses are combined as the final hypothesis.",
        "Furthermore, the training speed of boosting-based algorithms becomes more of a problem when considering combination of features that contributes to improvement in accuracy.",
        "Lhis paper proposes a fast boosting-based algorithm for learning rules represented by combination of features.",
        "Our learning algorithm uses the following methods to learn rules from large-scale training samples in a short time while maintaining accuracy; 1) Using a rule learner that learns several rules as our weak learner while ensuring a reduction in the theoretical upper bound of the training error of a boosting algorithm, 2) Repeating to learn rules from a small proportion of candidate rules that are generated from a subset of all the features with a technique similar to beam search, 3) Changing subsets of features used by weak learner dynamically for alleviating overfitting.",
        "We also propose feature-rich POS tagging and text chunking based on our learning algorithm.",
        "Our POS tagger and text chunker use candidate tags of each word obtained from automatically tagged data as features.",
        "The experimental results with English POS tagging and text chunking show drastically improvement of training speeds while maintaining competitive accuracy compared with previous best results and fast classification speeds."
      ]
    },
    {
      "heading": "2. Boosting-based Learner 2.1 Preliminaries",
      "text": [
        "We describe the problem treated by our boosting-based learner as follows.",
        "Let X be the set of examples and y be a set of labels { – 1, +1}.",
        "Let T = {/1, /2,/m} be types of features represented by strings.",
        "Let be a set of training sam-",
        "## a smoothing value e =1 ## rule number r: the initial value is 1.",
        "While (r<R) ## Train weak-learner using (S, {wr,i}T=i) ## Get f types of rules: {f,- }vj=1{fj}j=i <- weak-learner^,!^}™ x); ## Update weights with confidence value Foreachf e {fjYj=i",
        "pies {(xi, in),(xm, ym)}, where each example X; e ^ consists of features in F, which we call a feature-set, and yi e y is a class label.",
        "The goal is to induce a mapping from 5.",
        "Let |xj| (0 < |xj| < M) be the number of features included in a feature-set Xj, which we call the size of Xi, and Xij G F (1 < j < |xj| ) be a feature included in Xj.",
        "We call a feature-set of size A; a fc-feature-set.",
        "Then we define subsets of feature-sets as follows.",
        "Definition 1 Subsets of feature-sets",
        "If a feature-set Xj contains all the features in a feature-set Xj, f/zen we call x» is a subset ofitj and denote it as",
        "xi C xj.",
        "Then we define weak hypothesis based on the idea of the real-valued predictions and abstaining (RVPA, for short) (Schapire and Singer, 2000).",
        "Definition 2 Weak hypothesis for feature-sets",
        "Let f be a feature-set, called a rule, x be a feature-set, and c be a real number, called a confidence value, then a weak-hypothesis for feature-sets is defined as",
        "otherwise'",
        "!Our learner can handle binary vectors as in (Morishita, 2002).",
        "When our learner treats binary vectors for M attributes {Xi,...,Xm}, the learner converts each vector to the corresponding feature-set as x; < – {fi\\Xij e X; A Xij = 1} (1 < i < rn, 1 < j < M).",
        "Our boosting-based learner selects R types of rules for creating a final hypothesis F on several training iterations.",
        "The F is defined as",
        "We use a learning algorithm that generates several rules from a given training samples S = {(xi,j/i)}™1 and weights over samples {«V;i, ...,wr>m\\ as input of our weak learner.",
        "wr^ is the weight of sample number i after selecting r – 1 types of rules, where 0<«v;j, 1 < i < m and 1 < r < R.",
        "Given such input, the weak learner selects v types of rules {fj}j=1 (f) C F) with gain:",
        "where f is a feature-set, and Wr,y(i) is",
        "W™(f) = TZi «V,;[[f C Xi A yi = y}}, and [[it]] is 1 if a proposition 7r holds and 0 otherwise.",
        "The weak learner selects a feature-set having the highest gain as the first rule, and the weak learner finally selects v types of feature-sets having gain in top v as {fj}j=1 at each boosting iteration.",
        "Then the boosting-based learner calculates the confidence value of each f in {f/}J=1 and updates the weight of each sample.",
        "The confidence value Cj for fj is defined as",
        "After the calculation of Cj for fj, the learner updates the weight of each sample with",
        "r+i,i = Wr,iexp(-yih{tj>c.)).",
        "Then the learner adds (fj, cf) to F as the r-th rule and its confidence value.",
        "When we calculate the confidence value cJ+i for fJ+i, we use {wr+1,1,«v+i,m}- The learner adds (fj+i, Cj+\\) to F as the r+l-th rule and confidence value.",
        "After the updates of weights with {fj}J=1, the learner starts the next boosting iteration.",
        "The learner continues training until obtaining R rules.",
        "Our boosting-based algorithm differs from the other boosting algorithms in the number of rules learned at each iteration.",
        "The other boosting-based algorithms usually learn a rule at each iteration",
        "##sortByW(F,fq): Sort features (/ e T) ## in ascending order based on weights of features ## (a % b): Return the reminder of (a f b) ##\\B\\-buckets: B = {B[0],B[\\B\\ - 1]} procedure distFT(5, ##Calculate the weight of each feature Foreach (feF) Wr(f) = YZi Q *]]",
        "Fi sure 2: Distribute features to buckets based on weights (Schapire and Singer, 2000; Freund and Mason, 1999).",
        "Despite the difference, our boosting-based algorithm ensures a reduction in the theoretical upper bound of training error of the AdaBoost.",
        "We list the detailed explanation in Appendix.A.",
        "Figure 1 shows an overview of our boosting-based rule learner.",
        "To avoid to happen that Wr.",
        ")+i(f) or Wr.",
        ")_i(f) is very small or even zero, we use the smoothed values e (Schapire and Singer, 1999).",
        "Furthermore, to reflect imbalance class distribution, we use the default rule (Freund and Mason, 1999), defined as \\ log(^i), where Wy = ZT=M = V\\\\ for V e {±1}.",
        "The initial weights are defined with the default rule."
      ]
    },
    {
      "heading": "3. Fast Rule Learner",
      "text": [
        "We use a method to generate candidate rules without duplication (Iwakura and Okamoto, 2007).",
        "We denote f' = f + / as the generation of k + 1-feature-set f consisting of a feature / and a k-feature-set f. Let ID(f) be the integer corresponding to /, called id, and <fi be 0-feature-set.",
        "Then we define gen generating a feature-set as",
        "We assign smaller integer to more infrequent features as id.",
        "If there are features having the same frequency, we assign id to each feature with lexicographic order of features.",
        "Training based on this candidate generation showed faster training speed than generating candidates by an arbitrary order (Iwakura and Okamoto, 2007).",
        "We propose a method for learning rules by repeating to select a rule from a small portion of candidate rules.",
        "We evaluated the effectiveness of four types of methods to learn a rule from a subset of features on boosting-based learners with a text chunking task (Iwakura and Okamoto, 2007).",
        "The results showed that Frequency-based distribution (F-dist) has shown the best accuracy.",
        "F-dist",
        "## Fk : A set of k-feature-sets ## 1Z0 : v optimal rules (feature-sets) ## Rk,ui : w k-feature-sets for generating candidates ## selectNBestCK, n, S, Wr): n best rules from TZ ## with gain on {wi,r}™ 1 and training samples S procedure weak-learner^, S, Wr) ## v best feature-sets as rules Ho = selectNBest( TZ0 U Fk, v, S, Wr)\\ if (£ < k) return TZ0; ## Size constraint ## uj best feature-sets in Fk for generating candidates Rk^ = selectNBest(JFfc, uj, S, Wr); t = min gain(f); ## The gain of v-lh optimal rule",
        "Foreach ( fk e Rk,u) if (it(fk) < t) continue; ## Upper bound of gain Foreach (/gF) ## Generate candidates fk+i = gen(fk,f); return weak-learner (Fk+i, S, W);",
        "Fi gure 3l Find optimal feature-sets with given weights distributes features to subsets of features, called buckets, based on frequencies of features.",
        "However, we guess training using a subset of features depends on how to distribute features to buckets like online learning algorithms that generally depend on the order of the training examples (Kazama and Torisawa, 2007).",
        "To alleviate the dependency on selected buckets, we propose a method that redistributes features, called Weight-based distribution (W-dist).",
        "W-dist redistributes features to buckets based on the weight of feature defined as",
        "for each / e F after examining all buckets.",
        "Figure 2 describes an overview of W-dist.",
        "We propose a weak learner that learns several rules from a small portion of candidate rules.",
        "Figure 3 describes an overview of the weak learner.",
        "At each iteration, one of the \\F>\\-buckets is given as an initial 1-feature-sets F\\.",
        "The weak learner finds v best feature-sets as rules from candidates consisting of F\\ and feature-sets generated from F\\.",
        "The weak learner generates candidates k-feature-sets (1 < k) from uj best (fc-l)-feature-sets in Fk_ i with gain.",
        "We also use the following pruning techniques (Morishita, 2002; Kudo et al., 2005).",
        "• Frequency constraint: We examine candidates seen on at least £ different examples.",
        "• Size constraint: We examine candidates whose size is no greater than a size threshold (.",
        "• Upper bound of gain: We use the upper bound of gain defined as",
        "## Wr = {vjr,i}l= i'- Weights of samples after learning ## b,r: The current bucket and rule number procedure AdaBoost.SDF() B = distFT(S', \\B\\); ## Distributing features into B ## Initialize values and weights:",
        "While (r < R) ## Learning R types of rules ##Select v rules and increment bucket id b",
        "Foreach (f e ##Update weights with each rule c ilogf^.+iW+iV",
        "Figure 4: An overview of AdaBoost.SDF",
        "• words, words that are turned into all capitalized, prefixes and suffixes (up to 4) in a 7-word window.",
        "• labels assigned to three words on the right.",
        "• whether the current word has a hyphen, a number, a capital letter",
        "• whether the current word is all capital, all small",
        "• candidate POS tags of words in a 7-word window",
        "is less than the gain of the current optimal rule r, candidates containing f are safely pruned.",
        "Figure 4 describes an overview of our algorithm, which we call AdaBoost for a weak learner learning Several rules from Distributed Features (AdaBoost.SDF, for short).",
        "The training of AdaBoostSDF with (y = l,w = oo,l<|£>|)is equivalent to the approach of AdaBoostDF (Iwakura and Okamoto, 2007).",
        "If we use (\\F>\\ = \\,v = 1), AdaBoost.SDF examines all features on every iteration like (Freund and Mason, 1999; Schapire and Singer, 2000)."
      ]
    },
    {
      "heading": "4. POS tagging and Text Chunking 4.1 English POS Tagging",
      "text": [
        "We used the Penn Wall Street Journal treebank (Marcus et al., 1994).",
        "We split the treebank into training (sections 0-18), development (sections 19-21) and test (sections 22-24) as in (Collins, 2002).",
        "We used the following candidate POS tags, called candidate feature, in addition to commonly used features (Gimenez and Marquez, 2003; Toutanova et al., 2003) shown in Figure 5.",
        "We collect candidate POS tags of each word from the automatically tagged corpus provided for the shared task of English Named Entity recognition in CoNLL 2003.",
        "The corpus includes 17,003,926 words with POS tags and chunk tags",
        "• words and POS tags in a 5-word window.",
        "• labels assigned to two words on the right.",
        "• candidate chunk tags of words in a 5-word window",
        "Figure 6: Feature types for text chunking annotated by a POS tagger and a text chunker.",
        "Thus, the corpus includes wrong POS tags and chunk tags.",
        "We collected candidate POS tags of words that appear more than 9 times in the corpus.",
        "We express these candidates with one of the following ranges decided by their frequency fq; 10 < fq < 100, 100 <fq< 1000 and 1000 < fq.",
        "For example, we express 'work' annotated as NN 2000 times like \"1000<NN\".",
        "If 'work' is current word, we add 1000<NN as a candidate POS tag feature of the current word.",
        "If 'work' appears the next of the current word, we add 1000<NN as a candidate POS tag of the next word.",
        "We used the data prepared for CoNLL-2000 shared tasks.",
        "This task aims to identify 10 types of chunks, such as, NP, VP and PP, and so on.",
        "The data consists of subsets of Penn Wall Street Journal treebank; training (sections 15-18) and test (section 20).",
        "We prepared the development set from section 21 of the treebank as in (Tsuruoka and Tsujii, 2005).",
        "Each base phrase consists of one word or more.",
        "To identify word chunks, we use IOE2 representation.",
        "The chunks are represented by the following tags: EX is used for end word of a chunk of class X. IX is used for non-end word in an X chunk.",
        "O is used for word outside of any chunk.",
        "For instance, \"[He] (NP) [reckons] (VP) [the current account deficit] (NP)...\" is represented by IOE2 as follows; \"He/E-NP reckons/E-VP the/I-NP current/I-NP account/I-NP deficit/E-NP\".",
        "We used features shown in Figure 6.",
        "We collected the followings as candidate chunk tags from the same automatically tagged corpus used in POS tagging.",
        "• Candidate tags expressed with frequency information as in POS tagging",
        "• The ranking of each candidate decided by frequencies in the automatically tagged data",
        "• Candidate tags of each word",
        "For example, if we collect \"work\" annotated as I-NP 2000 times and as E-VP 100 time, we generate the following candidate features for \"work\"; 1000<I-NP, 100<E-VP<1000, rank:I-NP=l rank:E-NP=2, candidate=I-NP and candidate=E-VP.",
        "Table 1: Training data for experiments: jj of S, M, jj of cl and av.",
        "jj of ft indicate the number samples, the distinct number of feature types, the number of class in each data set, and the average number of features, respectively.",
        "POS and ETC indicate POS-tagging and text chunking.",
        "The \"-c\" indicates using candidate features collected from parsed unlabeled data.",
        "We converted the chunk representation of the automatically tagged corpus to IOE2 and we collected chunk tags of each word appearing more than nine times.",
        "AdaBoostSDF treats the binary classification problem.",
        "To extend AdaBoost.SDF to multi-class, we used the one-vs-the-rest method.",
        "To identify proper tag sequences, we use Viterbi search.",
        "We map the confidence value of each classifier into the range of 0 to 1 with sigmoid function , and select a tag sequence which maximizes the sum of those log values by Viterbi search."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "We compared AdaBoost.SDF with Support Vector Machines (SVM).",
        "SVM has shown good performance on POS tagging (Gimenez and Marquez, 2003) and Text Chunking (Kudo and Matsumoto, 2001).",
        "Furthermore, SVM with polynomial kernel implicitly expands all feature combinations without increasing the computational costs.",
        "Thus, we compared AdaBoost.SDF with SVM.",
        "To evaluate the effectiveness of candidate features, we examined two types of experiments with candidate features and without them.",
        "We list the statics of training sets in Table 1.",
        "We used the followings for comparison; Training time is time to learn 100,000 rules.",
        "Best training time is time for generating rules to show the best F-measure {Fp={) on development data.",
        "Accuracy is Fp=\\ on a test data with the rules at best training time.",
        "Table 2: Experimental results of POS tagging and Text Chunking (TC) with candidate features.",
        "F and time indicate the average Fp=i of test data and time (hour) to learn 100,000 rules for all classes with F-dist.",
        "These results are listed separately with respect to each £ = {1,5}.",
        "Table 2 shows average accuracy and training time.",
        "We used F-dist as the distribution method.",
        "These average accuracy obtained with rules learned by AdaBoost.SDF G,=io) on both tasks are competitive with the average accuracy obtained with rules learned by AdaBoostSDF G,=i).",
        "These results have shown that learning several rules at each iteration contributes significant improvement of training time.",
        "These results have also shown that the learning several rule at each iteration methods are more efficient than training by just using the frequency constraint £.",
        "Figure 7 shows a snapshot for accuracy obtained with chunkers using different number of rules.",
        "This graph shows that chunkers based on AdaBoost.SDF G.=io,ioo) and AdaBoost.SDF G/=i,w={i,io,ioo}) have shown better accuracy than chunkers based on AdaBoost.SDF G.=1,^=00) at each training time.",
        "These result have shown that learning several rules at each iteration and learning combination of features as rules with a technique similar to beam search are effective in improving training time while giving a better convergence.",
        "Figure 7 also implies that taggers and chunkers based on AdaBoostSDF G,=ioo) will show better or competitive accuracy than accuracy of the others by increasing numbers of rules to be learned while maintaining faster convergence speed.",
        "V",
        "POS(£",
        "= 1)",
        "POS (i = 5)",
        "TC(Î",
        "= 1)",
        "TC(Î",
        "= 5)",
        "F",
        "time",
        "F",
        "time",
        "F",
        "time",
        "F",
        "time",
        "1",
        "97.27",
        "196.3",
        "97.23",
        "195.7",
        "93.98",
        "145.3",
        "93.95",
        "155.8",
        "10",
        "97.23",
        "23.05",
        "97.17",
        "22.35",
        "93.96",
        "2.69",
        "93.88",
        "2.70",
        "100",
        "96.82",
        "2.99",
        "96.83",
        "2.91",
        "93.16",
        "0.74",
        "93.14",
        "0.56",
        "data",
        "It of S",
        "M",
        "1) of cl",
        "av.",
        "J of ft",
        "POS",
        "912,344",
        "579,052",
        "45",
        "22.09",
        "POS-c",
        "912,344",
        "579,793",
        "45",
        "35.39",
        "ETC",
        "211,727",
        "92,825",
        "22",
        "11.37",
        "ETC-c",
        "211,727",
        "93,333",
        "22",
        "45.49",
        "Table 3: Experimental results on POS tagging and Text Chunking.",
        "Accuracies (Fp=i) on test data and training time (hour) of AdaBoost.SDF are averages of w={l,10,100,oo} for each £ with F-dist and £ = 1.",
        "Fp=i and time (hour) of SVMs are averages of C={0.1,1,10} for each kernel parameter d.",
        "POS tagging without candidate features",
        "Table 3 lists average accuracy and training time on POS tagging and text chunking with respect to each (y, Q for AdaBoost.SDF and d for SVM.",
        "AdaBoostSDF with v=\\q and v=\\oo have shown much faster training speeds than SVM and AdaBoost.SDF ( i.=1,^=00) that is equivalent to the AdaBoostDF (Iwakura and Okamoto, 2007).",
        "Furthermore, the accuracy of taggers and chun-kers based on AdaBoost.SDF G,=io) have shown competitive accuracy with those of SVM-based and AdaBoost.DF-based taggers and chunkers.",
        "AdaBoostSDF (^=10) showed about 6 and 54 times faster training speeds than those of AdaBoostDF on the average in POS tagging and text chunking.",
        "AdaBoost.SDF G,=io) showed about 147 and 9 times faster training speeds than the training speeds of SVM on the average of POS tagging and text chunking.",
        "On the average of the both tasks, AdaBoost.SDF G.=io) showed about 25 and 50 times faster training speed than AdaBoostDF and SVM.",
        "These results have shown that AdaBoostSDF with a moderate parameter v can improve training time drastically while maintaining accuracy.",
        "These results in Table 3 have also shown that rules represented by combination of features and the candidate features collected from automatically tagged data contribute to improved accuracy.",
        "We compared Fp=\\ and best training time of F-dist and W-dist.",
        "We used ( = 2 that has shown",
        "Table 4: Results obtained with taggers and chunkers based on F-dist and W-dist.",
        "These results obtained with taggers and chunkers trained with uj = {1,10,100, 00} and Q = 2.",
        "F and time indicate average Fp=i on test data and average best training time.",
        "better average accuracy than ( = {1,3} in both tasks.",
        "Table 4 lists comparison of F-dist and W-dist on POS tagging and text chunking.",
        "Most of accuracy obtained with W-dist-based taggers and parsers better than accuracy obtained with F-dist-based taggers and parsers.",
        "These results have shown that W-dist improves accuracy without drastically increasing training time.",
        "The text chunker and the tagger trained with AdaBoostSDF (y = 10, uj = 10 and W-dist) has shown competitive accuracy with that of the chunker trained with AdaBoost.SDF (y = 1, uj = 00 and F-dist) while maintaining about 7.5 times faster training speed.",
        "We measured testing speeds of taggers and chunkers based on rules or models listed in Table 5.",
        "We examined two types of fast classification algorithms for polynomial kernel: Polynomial Kernel Inverted (PKI) and Polynomial Kernel Expanded (PKE).",
        "The PKI leads to about 2 to 12 times improvements, and the PKE leads to 30 to 300 compared with normal classification approach of SVM (Kudo and Matsumoto, 2003).",
        "The POS-taggers based on AdaBoost.SDF, SVM with PKI, and SVM with PKE processed 4,052 words, 159 words, and 1,676 words per second, respectively.",
        "The chunkers based on these three methods processed 2,732 words, 113 words, and 1,718 words per second, respectively.",
        "Speeds based On PKI Or PKE (http://www.chasen.org/-taku/software/",
        "yamcha/).",
        "We list the average speeds of SVM-based tagger and chunker with PKE of a threshold parameter a = 0.0005 for rule selection in both task.",
        "The accuracy obtained with models converted by PKE are slightly lower than the accuracy obtained with their original models in our experiments.",
        "POS tagging with F-dist",
        "„",
        "_J=1",
        "_j=10",
        "_j=100",
        "0J = OO",
        "t' 1 time",
        "t' 1 time",
        "t' 1 time",
        "t' 1 time",
        "1",
        "97.31",
        "30.03",
        "97.31",
        "64.25",
        "97.32",
        "142.9",
        "97.26",
        "89.59",
        "10",
        "97.26",
        "3.21",
        "97.32",
        "9.57",
        "97.30",
        "15.54",
        "97.30",
        "19.64",
        "100",
        "96.86",
        "0.62",
        "96.95",
        "1.32",
        "96.95",
        "2.13",
        "96.96",
        "2.43",
        "POS tagging with W-dist",
        "„",
        "_j=l",
        "_j=10",
        "_j=100",
        "0J = OO",
        "t' 1 time",
        "t' 1 time",
        "t' 1 time",
        "t' 1 time",
        "1",
        "97.32",
        "29.96",
        "97.31",
        "57.05",
        "97.31",
        "163.2",
        "97.32",
        "98.71",
        "10",
        "97.24",
        "2.66",
        "97.30",
        "25.70",
        "97.28",
        "16.20",
        "97.29",
        "20.49",
        "100",
        "97.00",
        "0.54",
        "97.02",
        "1.31",
        "97.07",
        "2.22",
        "97.08",
        "2.58",
        "Text Chunking with F-dist",
        "„",
        "_j=l",
        "_j=10",
        "_j=100",
        "0J = OO",
        "t' 1 time",
        "t' 1 time",
        "t' 1 time",
        "t' 1 time",
        "1",
        "93.95",
        "7.42",
        "94.30",
        "23.30",
        "94.22",
        "34.74",
        "94.31",
        "21.26",
        "10",
        "93.99",
        "0.98",
        "94.08",
        "2.44",
        "94.19",
        "3.11",
        "94.18",
        "3.18",
        "100",
        "93.32",
        "0.16",
        "93.33",
        "0.32",
        "93.42",
        "0.40",
        "93.42",
        "0.40",
        "Text Chunking with W-dist",
        "„",
        "_j=l",
        "_j=10",
        "_j=100",
        "0J = OO",
        "b' 1 time",
        "b' 1 time",
        "b' 1 time",
        "b' 1 time",
        "1",
        "93.99",
        "2.93",
        "94.24",
        "24.77",
        "94.32",
        "35.72",
        "94.32",
        "35.61",
        "10",
        "93.98",
        "0.71",
        "94.30",
        "2.82",
        "94.29",
        "3.60",
        "94.30",
        "4.05",
        "100",
        "93.66",
        "0.17",
        "93.65",
        "0.36",
        "93.50",
        "0.42",
        "93.50",
        "0.42",
        "Alg.",
        "/",
        "C(d)",
        "1",
        "2",
        "3",
        "*'ß=l",
        "time",
        "*'ß=l",
        "time",
        "*'ß=l",
        "time",
        "„=1",
        "96.96",
        "5.09",
        "97.10",
        "27.90",
        "97.10",
        "30.92",
        "„=10",
        "96.89",
        "0.79",
        "97.12",
        "4.56",
        "97.07",
        "4.74",
        "„=100",
        "96.57",
        "0.10",
        "96.82",
        "0.81",
        "96.73",
        "0.81",
        "SVMs || 96.60 | 101.63 || 97.15 | 166.76 || 96.93 | 625.32",
        "POS tagging with candidate features",
        "Alg./",
        "C(d)",
        "1",
        "2",
        "3",
        "*'ß=l",
        "time",
        "*'ß=l",
        "time",
        "*'ß=l",
        "time",
        "„=1",
        "97.06",
        "6.65",
        "97.30",
        "109.20",
        "97.29",
        "330.82",
        "„=10",
        "96.98",
        "1.27",
        "97.29",
        "13.26",
        "97.23",
        "38.27",
        "„=100",
        "96.61",
        "0.14",
        "96.93",
        "1.64",
        "96.76",
        "5.05",
        "SVMs || 96.76 | 170.24 || 97.31 | 206.39 || 97.23 | 1346.04",
        "Text Chunking without candidate features",
        "Alg./",
        "C(d)",
        "1",
        "2",
        "3",
        "*'ß=l 1 lim<>",
        "*'ß=l 1 üme",
        "*'ß=l 1 üme",
        "„=1",
        "92.50",
        "0.12",
        "93.60",
        "0.26",
        "93.47",
        "0.41",
        "„=10",
        "92.34",
        "0.02",
        "93.50",
        "0.05",
        "93.39",
        "0.07",
        "„=100",
        "89.70",
        "0.008",
        "92.31",
        "0.02",
        "92.03",
        "0.02",
        "SVMs",
        "92.14",
        "8.55",
        "93.91",
        "7.38",
        "93.49",
        "9.82",
        "Text Chunking with candidate features",
        "Alg./",
        "C(d)",
        "1",
        "2",
        "3",
        "*'ß=l",
        "time",
        "*'ß=l",
        "time",
        "*'ß=l",
        "time",
        "„=1",
        "92.89",
        "0.25",
        "94.19",
        "26.10",
        "94.04",
        "300.77",
        "„=10",
        "92.85",
        "0.04",
        "94.11",
        "2.97",
        "94.08",
        "3.06",
        "„=100",
        "91.99",
        "0.01",
        "93.37",
        "0.32",
        "93.24",
        "0.34",
        "Table 5: Comparison with previous best results: (Top : POS tagging, Bottom: Text Chunking )",
        "One of the reasons that boosting-based classifiers realize faster classification speed is sparseness of rules.",
        "SVM learns a final hypothesis as a linear combination of the training examples using some coefficients.",
        "In contrast, this boosting-based rule learner learns a final hypothesis that is a subset of candidate rules (Kudo and Matsumoto, 2004)."
      ]
    },
    {
      "heading": "6. Related Works",
      "text": [
        "We list previous best results on English POS tagging and Text chunking in Table 5.",
        "These results obtained with the taggers and chunkers based on AdaBoostSDF and SVM showed competitive F-measure with previous best results.",
        "These show that candidate features contribute to create state-of-the-art taggers and chunkers.",
        "These results have also shown that AdaBoostSDF-based taggers and chunkers show competitive accuracy by learning combination of features automatically.",
        "Most of these previous works manually selected combination of features except for SVM with polynomial kernel and (Kudo and Matsumoto, 2001) a boosting-based re-ranking (Kudo et al., 2005).",
        "LazyBoosting randomly selects a small proportion of features and selects a rule represented by a feature from the selected features at each iteration (Escuderoetal.,2000).",
        "Collins and Koo proposed a method only updates values of features co-occurring with a rule feature on examples at each iteration (Collins and Koo, 2005).",
        "Kudo et al.",
        "proposed to perform several pseudo iterations for converging fast (Kudo et al., 2005) with features in the cache that maintains the features explored in the previous iterations.",
        "AdaBoost.MH^ learns a weak-hypothesis represented by a set of rules at each boosting iteration (Sebastianietal.,2000).",
        "AdaBoostSDF differs from previous works in the followings.",
        "AdaBoostSDF learns several rules at each boosting iteration like AdaBoost.MH^.",
        "However, the confidence value of each hypothesis in AdaBoost.MHXi?",
        "does not always minimize the upper bound of training error for AdaBoost because the value of each hypothesis consists of the sum of the confidence value of each rule.",
        "Compared with AdaBoost.MIF™, AdaBoostSDF computes the confidence value of each rule to minimize the upper bound of training error on given weights of samples at each update.",
        "Furthermore, AdaBoost.",
        "SDF learns several rules represented by combination of features from limited search spaces at each boosting iteration.",
        "The creation of subsets of features in AdaBoost.",
        "SDF enables us to recreate the same classifier with same parameters and training data.",
        "Recreation is not ensured in the random selection of subsets in LazyBoosting."
      ]
    },
    {
      "heading": "7. Conclusion",
      "text": [
        "We have proposed a fast boosting-based learner, which we call AdaBoostSDF.",
        "AdaBoostSDF repeats to learn several rules represented by combination of features from a small proportion of candidate rules.",
        "We have also proposed methods to use candidate POS tags and chunk tags of each word obtained from automatically tagged data as features in POS tagging and text chunking.",
        "The experimental results have shown drastically improvement of training speed while maintaining competitive accuracy compared with previous best results.",
        "Future work should examine our approach on several tasks.",
        "Future work should also compare our algorithm with other learning algorithms.",
        "The upper bound of the training error for AdaBoost of (Freund and Mason, 1999), which is used in AdaBoost.",
        "SDF, is induced by adopting THEOREM 1 presented in (Schapire and Singer, 1999).",
        "Let Zr be Y^iLiwr+i,i ^at is a sum of weights updated with R rules.",
        "The bound holds on the training error after selecting R rules, is induced as follows.",
        "POS tagging",
        "Fff=i",
        "Perception (Collins, 2002)",
        "Dep.",
        "Networks (Toutanova et al., 2003)",
        "SVM (Gimenez and Marquez, 2003)",
        "ME based a bidirectional inference (Tsuruoka and Tsujii, 2005)",
        "Guided learning for bidirectional sequence classification (Shen et al., 2007)",
        "97.11 97.24 97.05 97.15 97.33",
        "AdaBoostSDF with candidate features (C=2,„=l,u=100, W-dist) AdaBoostSDF with candidate features (C=2,„=10,u=10, F-disi) SVM with candidate features (C=0.1, d=2)",
        "97.32 97.32 97.32",
        "Text Chunking",
        "Regularized Winnow + full parser output (Zhang et al., 2001) SVM-voting (Kudo and Matsumoto, 2001) ASO + unlabeled data (Ando and Zhang, 2005) CRF+Reranking(Kudo et al., 2005)",
        "ME based a bidirectional inference (Tsuruoka and Tsujii, 2005)",
        "LaSo (Approximate Large Margin Update) (Daumé III and Marcu, 2005)",
        "HySOL (Suzuki et al., 2007)",
        "94.17 93.91 94.39 94.12 93.70 94.4 94.36",
        "AdaBoostSDF with candidate featuers (£=2,„=l,oj=oo, W-dist) AdaBoostSDF with candidate featuers (£=2,v=10,oj=10,W-dist) SVM with candidate features (C=l, d=2)",
        "94.32 94.30 94.31",
        "Then we show that the upper bound of training error Zr for R rules shown in Eq.",
        "(2) is less than or equal to the upper bound of the training error Zr_\\ for R-\\ rules.",
        "By unraveling the (2) and plugging the confidence values cR = {|log(^r-+^((^))), o } given by the weak hypothesis into the unraveled equation, we obtain Zr<Zr_\\, since"
      ]
    }
  ]
}
