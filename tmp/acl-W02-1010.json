{
  "info": {
    "authors": [
      "Dmitry Zelenko",
      "Chinatsu Aone",
      "Anthony Richardella"
    ],
    "book": "Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W02-1010",
    "title": "Kernel Methods for Relation Extraction",
    "url": "https://aclweb.org/anthology/W02-1010",
    "year": 2002
  },
  "references": [
    "acl-A00-1011",
    "acl-M98-1009",
    "acl-M98-1012"
  ],
  "sections": [
    {
      "heading": "SRA International 4300 Fair Lakes Ct. Fairfax VA 22033 USA Abstract",
      "text": [
        "We present an application of kernel methods to extracting relations from unstructured natural language sources.",
        "We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels.",
        "We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text.",
        "We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Information extraction is an important unsolved problem of NLP.",
        "It is the problem of extracting entities and relations among them from text documents.",
        "Examples of entities are people, organizations, and locations.",
        "Examples of relations are person-affiliation and organization-location.",
        "The person-affiliation relation means that a particular person is affiliated with a certain organization.",
        "For instance, the sentence, \"John Smith is the chief scientist of the Hardcom Corp.\", contains the person-affiliation relation between the person \"John Smith\" and the organization \"Hardcom Corp.\".",
        "In this paper, we address the problem of extracting such relations from natural language text.",
        "We propose a machine learning approach to relation extraction and a novel methodology for information extraction based on kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000).",
        "We believe that shallow parsing is an important prerequisite for information extraction.",
        "Shallow parsing provides a robust mechanism for producing text representations that can be effectively used for entity and relation extraction.",
        "Indeed, the first step of our relation extraction approach is a powerful shallow parsing component of an information extraction system (Aone and Ramos- S antacruz, 2000).",
        "The system comprises cascading finite state machines that identify names, noun phrases, and a restricted set of parts of speech in text.",
        "The system also classifies noun phrases and names as to whether they refer to people, organizations and locations, thereby producing entities.",
        "Thus, the input to the relation extraction system is a shallow parse, with noun phrases and names marked with relevant entity types.",
        "We formalize a relation extraction problem as a shallow parse classification problem in section 4.",
        "A shallow parse is turned into an example whose label reflects whether a relation of interest is expressed by the shallow parse.",
        "The learning system uses the labeled examples to output a model that is applied to shallow parses to obtain labels, and thus extract relations.",
        "A unique property of the kernel methodology is that we do not explicitly generate features.",
        "More precisely, an example is no longer a feature vector as is common in machine learning algorithms.",
        "Instead, examples retain their original representations (of shallow parses) and are used within learning algorithms only via computing a similarity (or kernel') function between them.",
        "Such a use of examples allows our learning system to implicitly explore a much larger feature space than one computationally feasible for processing with feature-based learning algorithms.",
        "We conduct an experimental evaluation of our approach in section 6.",
        "We compare our approach with the feature-based linear methods (Roth, 1999), with promising results."
      ]
    },
    {
      "heading": "2 Related Work on Information Extraction",
      "text": [
        "The problem of relation extraction from natural language texts was previously addressed by Message Understanding Conferences (MUC).",
        "A number of systems were developed that relied on parsing and manual pattern development for identifying the relations of interest (see, for example, (Aone et al., 1998)).",
        "An adaptive system (Miller et al., 1998), presented under the aegis of MUC, used lexicalized probabilistic context-free grammars augmented with semantic information to produce a semantic parse of text for detecting organization-location relations.",
        "Among other popular probabilistic formalisms for information extraction are Hidden Markov Models (HMM) (Bikel et al., 1999), Maximum Entropy Markov Models (MEMM) (McCallum et al., 2000) and Conditional Random Fields (CRF) (Lafferty et al., 2001).",
        "Online learning algorithms for learning linear models (e.g., Perceptron, Winnow) are becoming increasingly popular for NLP problems (Roth, 1999).",
        "The algorithms exhibit a number of attractive features such as incremental learning and scalability to a very large number of examples.",
        "Their recent applications to shallow parsing (Munoz et al., 1999) and information extraction (Roth and Yih, 2001) exhibit state-of-the-art performance.",
        "The linear models are, however, feature-based which imposes constraints on their exploiting long-range dependencies in text.",
        "In section 6, we compare the lA kernel function is a similarity function satisfying certain properties, see (Cristianini and Shawe-Taylor, 2000) for details.",
        "methods with our approach for the relation extraction problem.",
        "We next introduce a class of kernel machine learning methods and apply them to relation extraction."
      ]
    },
    {
      "heading": "3 Kernel-based Machine Learning",
      "text": [
        "Most learning algorithms rely on feature-based representation of objects.",
        "That is, an object is transformed into a collection features fl, ... , fN, thereby producing a N-dimensional vector.",
        "In many cases, data cannot be easily expressed via features.",
        "For example, in most NLP problems, feature based representations produce inherently local representations of objects, for it is computationally infeasible to generate features involving long-range dependencies.",
        "Kernel methods (Vapnik, 1998; Cristianini and Shawe-Taylor, 2000) are an attractive alternative to feature-based methods.",
        "Kernel methods retain the original representation of objects and use the objects in algorithms only via computing a kernel (or similarity) function between a pair of objects.",
        "In many cases, it may be possible to compute a similarity function in terms of certain features without enumerating all the features.",
        "An excellent example is that of subsequence kernels (Lodhi et al., 2002).",
        "In this case, the objects are strings of characters, and the similarity (kernel) function computes the number of common subsequences of characters in two strings.",
        "Despite the exponential number of features (subsequences), it is possible to compute the subsequence kernel in polytime.",
        "We therefore are able to take advantage of long-range features in strings without enumerating the features explicitly.",
        "In section 5, we will extend the subsequence kernel to operate on shallow parses for relation extraction.",
        "Another pertinent example is that of parse tree kernels(Collins and Duffy, 2001), where objects represent trees and the kernel function computes the number of common subtrees in two trees.",
        "The tree kernel used within the Voted Perceptron learning algorithm (Freund and Schapire, 1999) was shown to deliver excellent performance in improving Penn Treebank parsing.",
        "There are a number of learning algorithms that can operate only using kernels of examples.",
        "The models produced by the learning algorithms are also expressed using only ex-amples' kernels.",
        "The algorithms that process examples only via computing their kernels are sometimes called dual learning algorithms.",
        "The Support Vector Machine(SVM) (Cortes and Vapnik, 1995) is a learning algorithm that not only allows for a dual formulation, but also provides a rigorous rationale for resisting over-fitting (Vapnik, 1998).",
        "After discovery of the kernel methods, several existing learning algorithms were shown to have dual analogues.",
        "For instance, the Perceptron learning algorithm can be easily represented in the dual form (Cristianini and Shawe-Taylor, 2000).",
        "A variance-reducing improvement of Perceptron, Voted Perceptron (Freund and Schapire, 1999), is a robust and efficient learning algorithm that is very easy to implement.",
        "It has been shown to exhibit performance comparable to that of SVM.",
        "In section 6, we experimentally evaluate SVM and Voted Perceptron for relation extraction.",
        "We next show how to formalize relation extraction as a learning problem."
      ]
    },
    {
      "heading": "4 Problem Formalization",
      "text": [
        "Let us consider the sentence, \"John Smith is the chief scientist of the Hardcom Corp.\".",
        "The shallow parsing system produces the representation of the sentence shown in Figure 1.",
        "We convert the shallow parse tree into one or more examples for the person-affiliation relation.",
        "This type of relation holds between a person and an organization.",
        "There are three nodes in the shallow parse tree in Figure 1 referring to people, namely, the \"John Smith\" node with the type \"Person\", and the \"PNP\" nodes2.",
        "There is one \"Organization\" node in the tree that refers to an organization.",
        "We create an example for the person-affiliation relation by taking a person node and an organization Note that after the tree is produced, we do not know if the \"Person\" and the \"PNP\" nodes refer to the same person.",
        "com Corp.\".The types \"PNP\", \"Det\", \"Adj\", and \"Prep\" denote \"Personal Noun Phrase\", \"Determiner\", \"Adjective\", and \"Preposition\", respectively.",
        "node in the shallow parse tree and assigning attributes to the nodes specifying the role that a node plays in the person-affiliation relation.",
        "The person and organization under consideration will receive the member and affiliation roles, respectively.",
        "The rest of the nodes will receive none roles reflecting that they do not participate in the relation.",
        "We then attach a label to the example by asking the question whether the node with the role of member and the node with the role of affiliation are indeed (semantically) affiliated, according to the sentence.",
        "For the above sentence, we will then generate three positive examples, shown in Figure 2.",
        "Note that in generating the examples between the \"PNP\" and the \"Organization\" we eliminated the nodes that did not belong to the least common subtree of \"Organization\" and \"PNP\", thereby removing irrelevant subtrees.",
        "To summarize, a relation example is shallow parse, in which nodes are augmented with the role attribute, and each node of the shallow parse belongs to the least common subtree comprising the relation entities under consideration.",
        "We now formalize the notion of relation example.",
        "We first define the notion of the example node.",
        "We use p.a to denote the value of attribute with the name a in the node p, e.g., p.Type = Person and p.Role = member.",
        "generated from the shallow parse in Figure 1.",
        "The \"Label=+1\" means that the examples do express the relation.",
        "Definition 2 An (unlabeled) relation example is defined inductively as follows:",
        "• Let p be a node, then the pair P = (p, []) is a relation example, where by [] we denote an empty sequence.",
        "• Let p be a node, and [PI, P2,.",
        ", PI] be a sequence of relation examples.",
        "Then, the",
        "pair P = (p, [PI, P2,.",
        "Pl]) is a relation example.",
        "We say that p is the parent of PI, P2,.",
        ".. , PI, and Pi's are the children of p. We denote by Rp the first element of the example pair, by P.c the second element of the example pair, and use the shorthand Ra to refer to P.p.a, and P[i] to denote Pi.",
        "A labeled relation example is an unlabeled relation example augmented with a label l E {-1, +1}.",
        "An example is positive, if l = +1, and negative, otherwise.",
        "We now define kernels on relation examples that represent similarity of two shallow parse trees."
      ]
    },
    {
      "heading": "5 Kernels for Relation Extraction",
      "text": [
        "Kernels on parse trees were previously defined by (Collins and Duffy, 2001).",
        "The kernels enumerated (implicitly) all subtrees of two parse trees, and used the number of common subtrees, weighted appropriately, as the measure of similarity between two parse trees.",
        "Since we are operating with shallow parse trees, and the focus of our problem is relation extraction rather than parsing, we use a different definition of kernels.",
        "We first define a matching function t(•, •) E {0, 11 and a similarity function k(•, •) on nodes.",
        "The matching function defined on nodes determines whether the nodes are matchable or not.",
        "For example, the nodes may be matchable only if their types and roles match.",
        "That is, if two nodes have the same roles, and compatible types3, then their node matching function is equal to 1; otherwise, it is equal to 0.",
        "The similarity function on nodes is computed in terms of the nodes' attributes.",
        "Then, for two relation examples PI, P2, we define the similarity function K(PI, P2) in terms of similarity function of the parent nodes and the similarity function K, of the children.",
        "Formally (o.w.",
        "means \"otherwise\"), o, if t(P1-P,P2,P)=o k(P1.p,P2.P)+K,(P,.c,P2.c), o.w.",
        "Different definitions of the similarity function K, on children give rise to different K's.",
        "We now give a general definition of K, in terms of similarities of children subsequences.",
        "We first introduce some helpful notation (similar to (Lodhi et al., 2002)).",
        "We denote by i a sequence it < 12 < ... < in of indices, and we say that i E i, if i is one of the sequence indices.",
        "We also use d(i) for i,,, – it + 1, and l(i) for length of the sequence i.",
        "For a relation example P, we denote by P[i] the sequence of children [P[il], ... , P[i,,,]].",
        "For a similarity function K, we use K(Pi [i], P2 [j]) to denote �s=i K(Pi[4],P2[js]).",
        "Then, we define the similarity function Kc as follows",
        "where",
        "The formula (2) enumerates all subsequences of relation example children with matching parents, accumulates the similarity for each subsequence by adding the corresponding child exam-ples' similarities, and decreases the similarity by the factor of Ad(') Ad(') �0 < A < 1, reflecting how spread out the subsequences within children sequences.",
        "Finally, the similarity of two children sequences is the sum all matching subsequences similarities.",
        "The following theorem states that the formulas (1) and (2) define a kernel, under mild assumptions (the proof is omitted for lack of space) .",
        "Theorem 1 Let k(•, •) and t(•, •) be kernels over nodes.",
        "Then, K as defined by (1) and (2) is a kernel over relation examples.",
        "We first consider a special case of Kc, where the subsequences i and j are assumed to be contiguous and give a very efficient algorithm for computing Kc.",
        "In section 5.2, we address a more general case, when the subsequences are allowed to be sparse (non-contiguous)."
      ]
    },
    {
      "heading": "5.1 Contiguous Subtree Kernels",
      "text": [
        "For contiguous subtree kernels, the similarity function Kc enumerates only children contiguous subsequences, that is, for a subsequence i in",
        "this section by making A stand for A2 in formula (2).",
        "Hence, (2) becomes",
        "The core of the kernel computation resides in the formula (3).",
        "The formula enumerates all contiguous subsequences of two children sequences.",
        "We now give a fast algorithm for computing Kc between Pl and P2, which, given kernel values for children, runs in time O(mn), where m and n is the number of children of Pl and P2, respectively.",
        "Let C(i, j) be the Kc computed for suffixes of children sequences of Pl and P2, where every subsequence starts with indices i and j, respectively.",
        "That is,",
        "Let L(i, j) be the length of the longest sequence matching states in the children of Pl and P2 starting with indices i and j, respectively:",
        "The boundary conditions are:"
      ]
    },
    {
      "heading": "L(na+l,n+i)=o and C(na+l,n+l)=o",
      "text": [
        "The recurrence (5) follows from the observation that, if Pi [i] and P2 [A match, then every matching pair (CI, C2) of sequences that participated in computation of C(i + 1,J + 1) will be extended to the matching pair ([P1[i1,c11, [P2[j],c2]).",
        "Now we can easily com",
        "The time and space complexity of Kc computation is O(mn), given kernel values for children.",
        "Hence, for two relation examples the complexity of computing K(PI, P2) is the sum of computing Kc for the matching internal nodes (assuming that complexities of k(•, •) and t(•, •) are constant)."
      ]
    },
    {
      "heading": "5.2 Sparse Subtree Kernels",
      "text": [
        "For sparse subtree kernels, we use the general definition of similarity between children sequences as expressed by (2).",
        "As in the previous section, we give an efficient algorithm for computing K, between PI and P2.",
        "The algorithm runs in time 0(mn3), given kernel values for children, where m and n (m > n) is the number of children of PI and P2, respectively.",
        "Derivation of an efficient programming algorithm for sparse subtree computation will be presented in a full version of the paper, for lack of space.",
        "Below we list the recurrences for com",
        "As can be seen from the recurrences, the time complexity of the algorithm is 0(mn3) (assuming m > n).",
        "The space complexity is 0(mn2)."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": [
        "In this section, we apply kernel methods to extracting two types of relations from text: person-affiliation and organization-location.",
        "A person and an organization are part of the person-affiliation relation, if the person is a member of or employed by organization.",
        "A company founder, for example, is defined not to be affiliated with the company (unless, it is stated that (s)he also happens to be a company employee) .",
        "An organization and a location are part of the organization-location relation, if the organization's headquarters is at the location.",
        "Hence, if a single division of a company is located in a particular city, the company is not necessarily located in the city.",
        "The nuances in the above relation definitions make the extraction problem more difficult, but they also allow to make fine-grained distinctions between relationships that connect entities in text."
      ]
    },
    {
      "heading": "6.1 Experimental Methodology",
      "text": [
        "The (text) corpus for our experiments comprises 200 news articles from different news agencies and publications (Associated Press, Wall Street Journal, Washington Post, Los Angeles Times, Philadelphia Inquirer).",
        "We used the existing shallow parsing system to generate the shallow parses for the news articles.",
        "We generated relation examples from the shallow parses for both relations, as described in section 4.",
        "We retained only the examples, for which the shallow parsing system did not make major mistakes (90% of the generated examples).",
        "We then labeled the retained examples whether they expressed the relation of interest, whereby we obtained 3524 (1262 positive) examples for the person-affiliation relation and 1915 (506 positive) examples for the org-location relation.",
        "For each relation, we randomly split the set of examples into a training set (60% of the examples) and a testing set (40% of the examples).",
        "We obtained the models by running learning algorithms (with kernels, where appropriate) on the training set, testing the models on the test set, and computing performance measures.",
        "In order to get stable performance estimates, we averaged performance results over 10 random train/test splits.",
        "We report the standard performance estimates (precision, recall, and F-measure) for each experiment.",
        "We now describe the experimental setup of the algorithms used in evaluation."
      ]
    },
    {
      "heading": "6✺2 ❅e✯ne❈ Met✭od❀ ✼onfi✽✾✯ation",
      "text": [
        "We evaluated two kernel learning algorithms: Support Vector Machine (SVM) (Cortes and Vapnik, 1995) and Voted Perceptron (Freund and Schapire, 1999).",
        "For SVM, we used the SVM◗igh✣ (Joachims, 1998) implementation of the algorithm, with custom kernels incorporated therein.",
        "We implemented the Voted Perceptron algorithm as described in (Freund and Schapire, 1999).",
        "We implemented both contiguous and sparse subtree kernels and incorporated them in the kernel learning algorithms.",
        "For both kernels, ★ was set to 0.5.",
        "The only domain specific information in the two kernels was encapsulated by the following matching t(.,.)",
        "and similarity k(•, •) functions on nodes.4 �1, if C✌a✜✜ (Pi.",
        "Type) =C✌a✜✜ (P2.Type) and P1.Ro✌e=P2.Ro✌e",
        "We also normalized the computed kernels before their use within the algorithms.",
        "The normalization corresponds to the standard unit norm normalization of examples in the feature space corresponding to the kernel space (Cristianini and Shawe-Taylor, 2000):",
        "We evaluated two feature-based algorithms for learning linear discriminant functions: Naive-Bayes (Duda and Hart, 197❊) and Winnow (Lit-tlestone, 1987).",
        "Location, and Class(Type) = Type for other types.",
        "We implemented the two algorithms in the spirit of the SNOW system (Roth, 1999).",
        "The algorithms learn models that, given an example, produce a score for each label (+1 and -1), the predict the label corresponding to the larger score.",
        "Since the algorithms are feature-based, we designed features for the relation extraction problem.",
        "The features are conjunctions of conditions involving \"Text\", \"Type\", \"Role\" attributes for neighboring example nodes.",
        "We do not list features herein for lack of space."
      ]
    },
    {
      "heading": "6✺4 Expe✯i✱enta❈ Re❀✾❈t❀",
      "text": [
        "The performance results for relation extraction are shown in Table 1.",
        "The results indicate that kernel methods do exhibit excellent performance and fare better than feature-based algorithms in relation extraction.",
        "The results also highlights importance of kernels: algorithms with the sparse subtree kernels are always significantly better than their contiguous counterparts.",
        "The results show that performance of the Voted Perceptron is much less accurate, compared with other algorithms, for the organization-location relation than for the person-affiliation relation.",
        "This phenomenon can be probably attributed to the fact that the organization-location examples are more noisy, with more boundary cases present.",
        "For such a training set, regularization performed by SVM is crucial; it is more noise-tolerant than the Perceptron voting mechanism.",
        "Performance of Naive Bayes for organization-location relation is notable, since it performs as good as or better than more elaborate algorithms."
      ]
    },
    {
      "heading": "7 Conclusions and Further Work",
      "text": [
        "We presented an approach for using kernel-based machine learning methods for extracting relations from natural language sources.",
        "We defined kernels over shallow parse representations of text and designed efficient dynamic programming algorithms for computing the kernels.",
        "We applied SVM and Voted Perceptron learning algorithms with the kernels incorporated therein",
        "to the tasks of relation extraction.",
        "We also compared performance of the kernel-based methods with that of the feature methods, and concluded that kernels lead to superior performance.",
        "In the future, we intend to apply the kernel methodology to other sub-problems of information extraction.",
        "For example, the shallow parsing and entity extraction mechanism may also be learned, and, perhaps, combined in a seamless fashion with the relation extraction formalism presented herein.",
        "Furthermore, the real-world use of extraction results requires discourse resolution that collapses entities, noun phrases, and pronouns into a set of equivalence classes.",
        "We plan to apply kernel methods for discourse processing as well.",
        "8 Acknowledgements Our work was supported through the DARPA Evidence Extraction and Link Discovery Program."
      ]
    }
  ]
}
