{
  "info": {
    "authors": [
      "Jeffrey Heinz",
      "Colin de la Higuera",
      "Menno van Zaanen"
    ],
    "book": "Tutorial Abstracts of ACL 2011",
    "id": "acl-P11-5002",
    "title": "Formal and Empirical Grammatical Inference",
    "url": "https://aclweb.org/anthology/P11-5002",
    "year": 2011
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Outline of the tutorial I.",
        "Formal GI and learning theory (de la Higuera) II.",
        "Empirical approaches to regular and subregular natural language classes (Heinz) III.",
        "Empirical approaches to nonregular natural language",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns A simple definition Grammatical inference is about learning a grammar given information about a language Vocabulary Learning = building, inferring Grammar= finite representation of a possibly infinite set of strings, or trees, or graphs Information=you can learn from text, from an informant, by actively querying Language= possibly infinite set of strings, or trees, or graphs",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns A Dfa (Ack: Jeffrey Heinz) The (CV)* language representing licit sequences of sounds in many languages in the world.",
        "Consonants and vowels must alternate; words must begin with C and must end with V. States show the regular expression indicating its ?good tails?.",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns A context free grammar and a parse tree (de la Higuera 2010)",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns A finite state transducer (Ack: Jeffrey Heinz) A subsequential transducer illustrating a common phonological rule of palatalization ( k ??",
        ">tS / i).",
        "States are labelled with a number and then the output string given by the ?",
        "function for that state.",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Our definition Grammatical inference is about learning a grammar given information about a language"
      ]
    },
    {
      "heading": "Questions",
      "text": [
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Why not write ?learn a language??",
        "Because you always learn a representation of a language Paradox Take two learners learning a context-free language, one is learning a quadratic normal form and the other a Greibach normal form, they cannot agree that they have learnt the same thing (undecidable question).",
        "Worth thinking about.",
        ".",
        ".",
        "is it a paradox?",
        "Do two English speakers agree they speak the same language?",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Our definition Grammatical inference is about learning a grammar given information about a language How can a become the?",
        "Ask for the grammar to be the smallest, best (re a score).",
        "?",
        "Combinatorial characterisation The learning problem becomes an optimisation problem!",
        "Then we often have theorems saying that If our algorithm does solve the optimisation problem, what we have learnt is correct If we can prove that we can't solve the optimisation problem, then the class is not learnable",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Optimal with respect of some score Score should take into account:",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Moreover GI is not only about building a grammar from some data.",
        "It is concerned with saying something about: the quality of the result, the quality of the learning process, the properties of the process.",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Naive example Suppose you are building a random number generator.",
        "How are you convinced that it works?",
        "Because it follows sound principles as defined by number theory specialists?",
        "Because you have tested and the number 772356191 has been produced?",
        "Because you have proved that the series of numbers that will be produced is incompressible?"
      ]
    },
    {
      "heading": "Empirical approach Experimental approach Formal approach",
      "text": [
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Empirical approach: using good (safe?)",
        "ideas For example, genetic algorithms or neural networks Or some mathematical principle (Occam, Kolmogorov, MDL,.",
        ".",
        ". )",
        "Can become a principled approach Alternative point of view Empirical approach is about imitating what nature (or humans) do",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns"
      ]
    },
    {
      "heading": "Experimental approach Benchmarks Competitions",
      "text": [
        "Necessary but not sufficient How do we know that all the cases are covered?",
        "How do we know that we dont have a hidden bias?",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Formal approach: showing that the algorithm has converged Is impossible: Just one run Can't prove that 23 is random But we can say something about the algorithm: That in the near future, given some string, we can predict if this string belongs to the language or not; Choose between defining clearly ?near future?",
        "and accepting probable truths (or error bounds) or leaving it undefined and using identification.",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns What else would we like to say?",
        "That if the solution we have returned is not good, then that is because the initial data was bad (insufficient, biased) Idea: Blame the data, not the algorithm",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Suppose we cannot say anything of the sort?",
        "Then that means that we may be terribly wrong even in a favourable setting Thus there is a hidden bias Hidden bias: the learning algorithm is supposed to be able to learn anything inside class L1, but can really only learn things inside class L2, with L2 ?",
        "L1",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Saying something about the process itself Key idea: if there is something to learn and the data is not corrupt, then, given enough time, we will learn it Replace the notion of learning by that of identifying",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns In practise, does it make sense?",
        "No, because we never know if we are in the ideal conditions (something to learn + good data + enough of it) Yes, because at least we get to blame the data, not the algorithm",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Complexity issues Complexity theory should be used: the total or update runtime, the size of the data needed, the number of mind changes, the number and weight of errors.",
        ".",
        ".",
        ".",
        ".",
        ".",
        "should be measured and limited.",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns A linguistic criterion One argument appealing to linguists (we hope) is that if the criteria are not met for some class of languages that a human is supposed to know how to learn, something is wrong somewhere (preposterously, the maths can't be wrong.",
        ".",
        ". )",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Non probabilistic settings Identification in the limit Resource bounded identification in the limit Active learning (query learning)",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Identification in the limit Information is presented to the learner who updates its hypothesis after inspecting each piece of data At some point, always, the learner will have found the correct concept and not change from it",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns A presentation is a function ?",
        ": N ?",
        "X where X is some set, and such that ?",
        "is associated to a language L through a function Yields : Yields(?)",
        "= L If ?",
        "(N) = ?",
        "(N) then Yields(?)",
        "= Yields(?)",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns text presentation A text presentation of a language L ?",
        "??",
        "is a function ?",
        ": N ?",
        "??",
        "such that ?",
        "(N) = L ?",
        "is an infinite succession of all the elements of L (note : small technical difficulty with ?)",
        "informed presentation An informed presentation (or an informant) of L ?",
        "??",
        "is a function ?",
        ": N ?",
        "??",
        "?",
        "{?,+} such that",
        "?",
        "is an infinite succession of all the elements of ??",
        "labelled to indicate if they belong or not to L",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Example: presentations for {anbn : n ?",
        "N} Legal presentation from text: ?, a2b2, a7b7,.",
        ".",
        ".",
        "Illegal presentation from text: ab, ab, ab,.",
        ".",
        ".",
        "Legal presentation from informant : (?,+), (abab,?",
        "), (a2b2,+), (a7b7,+), (aab,?",
        "), (abab,?),.",
        ".",
        ".",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Example: presentation for Spanish Legal presentation from text: En un lugar de la Mancha.",
        ".",
        ".",
        "Illegal presentation from text: Goooool Legal presentation from informant : (en,+), (whatever,-), (un,+), (lugar,+), (lugor,-), (xwszrrzt,-),",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns What happens before convergence?",
        "On two occasions I have been asked [by members of Parliament], ?Pray, Mr. Babbage, if you put into the machine wrong figures, will the right answers come out??",
        "I am not able rightly to apprehend the kind of confusion of ideas that could provoke such a question."
      ]
    },
    {
      "heading": "Charles Babbage",
      "text": [
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Further definitions Given a presentation ?, ?n is the set of the first n elements in ?.",
        "A learning algorithm (learner) A is a function that takes as input a set ?n and returns a grammar of a language.",
        "Given a grammar G , L(G ) is the language generated/recognised/ represented by G .",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Consistency and conservatism We say that the learner A is consistent if ?n is consistent with A(?n) ?n A consistent learner is always consistent with the past Consistency and conservatism We say that the learner A is conservative if whenever ?",
        "(n + 1) is consistent with A(?n), we have A(?n) = A(?n+1) A conservative learner doesn't change his mind needlessly",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Learning from data A learner is order dependent if it learns something different depending on the order in which it receives the data.",
        "Usually an order independent learner is better.",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns What about efficiency?",
        "We can try to bound global time update time errors before converging (IPE) mind changes (MC) queries good examples needed (characteristic samples) (Pitt 1989, de la Higuera et al. 2008)",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Definition: polynomial number of implicit prediction errors Denote by G 6|= x if G is incorrect with respect to an element x of the presentation (i.e. the learner producing G has made an implicit prediction error.",
        "G is polynomially identifiable in the limit from Pres if there exists an identification learner A and a polynomial p() such that given any G in G, and given any presentation ?",
        "of L(G ),",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Definition: polynomial characteristic sample G has polynomial characteristic samples for identification learner A if there exists a polynomial p() such that: given any G in G, ?Y correct sample for G , such that whenever Y ?",
        "?n, A(?n) ?",
        "G and ?Y ?",
        "?",
        "p(?G?)",
        "As soon as the CS is in the data, the result is correct; The CS is small.",
        "number of queries made before halting with a correct grammar is polynomial in the size of the target, the size of the information received.",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Learning a language from sampling We have a distribution over ??",
        "We sample twice: once to learn, once to see how well we have learned The Pac setting: Les Valiant, Turing award 2010",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Pac-learning (Valiant 1984, Pitt 1989) L a class of languages G a class of grammars > 0 and ?",
        "> 0 m a maximal length over the strings n a maximal size of machines",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns"
      ]
    },
    {
      "heading": "Polynomial Pac learning",
      "text": [
        "There is a polynomial p(?, ?, ?, ?)",
        "such that in order to learn AC machines of size at most n with error at most ?",
        "we require at most p(m, n, 1?",
        ", 1? )",
        "data and time; we want the errors to be less than and bad luck to be less than ?.",
        "(French radio) Unless there is a surprise there should be no surprise French radio, (after the last primary elections, on 3rd of June 2008) First surprise is ?, second surprise is",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Alternatively Instead of learning classifiers in a probabilistic world, learn directly the distributions!",
        "Learn probabilistic finite automata (deterministic or not)",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns No error (Angluin 1988) This calls for identification in the limit with probability 1 Means that the probability of not converging is 0 Goal is to identify the structure and the probabilities Mainly a (nice) theoretic setting"
      ]
    },
    {
      "heading": "Results",
      "text": [
        "If probabilities are computable, we can learn with probability 1 finite state automata (Carrasco and Oncina, 1994) But not with bounded (polynomial) resources (de la Higuera and Oncina, 2004)",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns With error Pac definition applies But error should be measured by a distance between the target distribution and the hypothesis How do we measure the distance: L1, L2, L?, Kullback-Leibler?",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns"
      ]
    },
    {
      "heading": "Results",
      "text": [
        "Too easy to learn with L?",
        "Too hard to learn with L1 Both results hold for the same algorithm!",
        "(de la Higuera and Oncina, 2004) Nice algorithms for biased classes of distributions",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Open problems We conclude this section on ?what is language learning about?",
        "with some open questions: What is a good definition of polynomial identification?",
        "How do we deal with shifting targets?",
        "(robustness issues) Alternative views on learnability?",
        "Is being learnable a good indicator of being linguistically reasonable?",
        "Can we learn transducers?",
        "Probabilistic transducers?",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns II.",
        "GI of Regular Patterns Why regular?",
        "What are the general GI strategies?",
        "What are the main results?",
        "The main techniques?",
        "The main lessons?",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Why Begin with Regular?",
        "Insights obtained here can be (and have been) applied fruitfully to nonregular classes.",
        "Angluin 1982 showed a subclass of regular languages (the reversible languages) was identifiable in the limit from positive data by an incremental learner.",
        "Yokomori's (2004) Very Simple Languages are a subclass of the context-free languages, but draws on ideas from the reversible languages.",
        "Similarly, Clark and Eryaud's (2007) substitutable languages (also subclass of context-free) are also based on insights from this paper.",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns This tutorial: Finite State Automata",
        "determined.",
        "For example, there are no canonical (e.g. shortest) regular expressions for regular languages.",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Learning Frameworks: Main Choices Success required on which input data streams?",
        "All possible vs. some restricted set i.e. ?distribution-free?",
        "vs. ?non distribution-free?",
        "What kind of samples?",
        "Positive data vs. postive and negative data Other choices (e.g. query learning) are not discussed here.",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Only so much can be covered.",
        ".",
        ".",
        "It's impossible to be fair to all those who have contributed and to cover all the variants, even all the algorithms in a short tutorial.",
        "That's why there are books!",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Illustrative Example: Stress pattern of Pintupi a.",
        "pa?",
        "?a ?earth?",
        "??",
        "?",
        "b.",
        "tju?",
        "?aya ?many?",
        "??",
        "?",
        "?",
        "c.",
        "ma?",
        "?awa`na ?through from behind?",
        "??",
        "?",
        "?` ?",
        "d.",
        "pu?",
        "?iNka`latju ?we (sat) on the hill?",
        "??",
        "?",
        "?` ?",
        "?",
        "e. tja?mul`?mpatju`Nku ?our relation?",
        "??",
        "?",
        "?` ?",
        "?` ?",
        "f.",
        "???",
        "?ir`iNula`mpatju ?the fire for our benefit flared up?",
        "Generalization (Hayes (1995:62) citing Hansen and Hansen (1969:163)): Primary stress falls on the initial syllable Secondary stress falls on alternating nonfinal syllables",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Illustrative Example: Stress pattern of Pintupi Generalization (Hayes (1995:62) citing Hansen and Hansen (1969:163)): Primary stress falls on the initial syllable Secondary stress falls on alternating nonfinal syllables Minimal deterministic FSA for Pintupi Stress",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns State-merging Informally Eliminate redundant environments by state-merging.",
        "States are identified as equivalent and then merged.",
        "All transitions are preserved.",
        "This is one way in which generalizations may occur?because the post-merged machine accepts everything the pre-merged machine accepts, possibly more.",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Back to the Illustrative Example Results for stress patterns more generally Out of 109 distinct stress patterns in the world's languages (encoded as FSAs), this state-merging strategy works for only 44 of them If we merge states with the same paths up to length 5(!",
        "), only 81 are learned.",
        "This is the case even permitting very generous input samples.",
        "In other words, 44 attested stress patterns are Strictly 3-Local and 81 are Strictly 6-Local.",
        "28 are not Strictly 6-Local In fact those 28 are not Strictly k-Local for any k (Edlefsen et al. 2008).",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Other ways to merge states If the current structure is ?ill-formed?",
        "then merge states to",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns State-merging as inference rules Strictly k-Local languages (Garcia et al. 1990) merge states with same incoming paths of length k ?u, v ,w ?",
        "??",
        ": uv ,wv ,?",
        "Prefix(L) and |v |= k",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns State-merging as inference rules 0-Reversible languages (Angluin 1982) recursively eliminate reverse non-determinism ?u, v ,w , y ?",
        "??",
        ": uv ,wv , uy ?",
        "L ?",
        "wy ?",
        "L",
        "strategy (i.e. which distinctions will be maintained and which will be lost) Gleitman (1990:12): The trouble is that an observer who notices everything can learn nothing for there is no end of categories known and constructible to describe a situation [emphasis in original].",
        "2 are those which are recognized by subsequential transducers, which are determinstic on the input and which have an ?output?",
        "string associated with every state.",
        "3 have a canonical form.",
        "4 have been generalized to permit up to p outputs for each input (Mohri 1997).",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Learning larger classes of regular distributions More non-distribution-free with positive data The class of distributions describable with PDFA",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Have we put the cart before the horse?"
      ]
    },
    {
      "heading": "Research strategy Patterns ? Characterizations ? Learning algorithms",
      "text": [
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Conclusion to section 2 part 1 1 State-merging is a well-studied strategy for inferring automata, including acceptors, transducers, and weighted acceptors and transducers.",
        "2 It has yielded theoretical results in many learning frameworks including both distribution-free and non-distribution-free learning frameworks.",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns"
      ]
    },
    {
      "heading": "Empirical GI",
      "text": [
        "Try to identify language given samples E.g. sentences (syntax), words (morphology), .",
        ".",
        ".",
        "Underlying language class is unknown For algorithm we still need to make a choice If identification is impossible, provide approximation Evaluation of empirical GI is different from formal GI"
      ]
    },
    {
      "heading": "3 Output:",
      "text": [
        "Structure assigned to sentences ?",
        "extract grammar Extracted grammar rules (and probabilities) ?",
        "parse"
      ]
    },
    {
      "heading": "Issues",
      "text": [
        "Learning system has to deal with both flexibility in structure probabilities of structure",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Summarizing fixed versus flexible structure Fixed versus flexible is really a sliding scale Language modelling using n-grams Structure is very simple and very rigid Requires plain sequences as input Corresponds to k-testable languages (Garc?",
        "?a 1990) Language modelling using extracted grammar rules Structure is more flexible, but restricted by treebank Requires structured sequences as input Corresponds to e.g. (limited) context-free languages ?Learning structure?",
        "Structure is flexible, restricted by learning algorithm Requires plain sequences as input Corresponds to e.g. context-free languages",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Underlying approach Given a collection of plain sentences On what basis are we going to assign structure?",
        "Should structure be linguistically motivated?",
        "or similar to what linguists would assign?",
        "Perhaps we can use tests for constituency to find structure",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Learn grammar rules Terms that share (approximately) same context are clustered ?John?",
        "and ?Mary?",
        "are grouped together Occurrences of terms in cluster are replaced by new symbol Modified sequences may again contain terms/contexts Terms may consist of multiple words",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns"
      ]
    },
    {
      "heading": "MEX",
      "text": [
        "Compute probabilities depending on in-/out-degree of nodes",
        "PR describes path to the right similarly PL describes path to the left Significance is computed based on DR and DL wrt parameter Informally: find significant changes in number of paths Pick most significant pattern",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Constituent-Context Model (CCM) Consider all possible binary tree structures on POS sequences Define a probability distribution over the possible bracketings A bracketing is a particular structure on a sequence",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Dependency Model with Valence (DMV) DMV aims to learn dependency relations in contrast to CCM which learns context-free grammar rules Dependency parse links words in a head-dependent relation Model describes likelihood of left dependencies right dependencies stop condition (no more dependencies) Again, iterative EM is used to maximize likelihood of corpus",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Parsing Subtrees can be recombined into a larger tree Similar to context-free grammar rules Same parse may be created using different derivations Statistical model has to take this into account",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Underlying idea U-DOP works because span of subtrees reoccur in a corpus Likelihood of ?useful?",
        "spans increase Hence, likelihood of contexts (also subtrees) increase Essentially, U-DOP uses implied substitutability while system leans heavily on probabilities",
        "list the flights from baltimore to seattle that stop in minneapolis does this flight serve dinner the flight should arrive at eleven a.m. tomorrow what airline is this",
        "To what degree is the grammar context-sensitive?",
        "We may not need ?full?",
        "context-sensitiveness Grammar rules: ?A?",
        "?",
        "???",
        "Mildly context-sensitive grammars may be enough for NL (Huybrechts 1984, Shieber 1985) Perhaps the full power of context-freeness is not needed",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Relationship between empirical and formal GI Is there a relationship between empirical GI and formal GI?",
        "Example: consider the case of substitutability There are situations in which substitutability breaks:",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Learning NTS grammars It can be shown that NTS grammars are identifiable in the limit PAC learnable Unfortunately, natural language is not an NTS language Ultimate goal: Find family of languages that fits natural language and is learnable in the right learning setting",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns Formal GI and empirical GI Relation between formal GI and empirical GI Formal GI can show learnability Under certain conditions Emprical GI tries to learn structure from real data Practically shows possibilities and limitations Ultimate aim: Find family of languages that is learnable under different conditions fits natural languages",
        "Formal GI and learning theory GI of Regular Patterns Empirical GI and nonregular patterns"
      ]
    },
    {
      "heading": "CONCLUSIONS",
      "text": []
    }
  ]
}
