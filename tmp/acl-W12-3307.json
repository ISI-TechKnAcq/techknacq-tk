{
  "info": {
    "authors": [
      "Martin Riedl",
      "Chris Biemann"
    ],
    "book": "Proceedings of ACL 2012 Student Research Workshop",
    "id": "acl-W12-3307",
    "title": "TopicTiling: A Text Segmentation Algorithm based on LDA",
    "url": "https://aclweb.org/anthology/W12-3307",
    "year": 2012
  },
  "references": [
    "acl-A00-2004",
    "acl-H94-1020",
    "acl-J02-1002",
    "acl-N09-1040",
    "acl-N12-1064",
    "acl-P01-1064",
    "acl-P03-1071",
    "acl-P08-2068",
    "acl-P94-1002",
    "acl-W04-2214",
    "acl-W12-0703"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This work presents a Text Segmentation algorithm called TopicTiling.",
        "This algorithm is based on the well-known TextTiling algorithm, and segments documents using the Latent Dirichlet Allocation (LDA) topic model.",
        "We show that using the mode topic ID assigned during the inference method of LDA, used to annotate unseen documents, improves performance by stabilizing the obtained topics.",
        "We show significant improvements over state of the art segmentation algorithms on two standard datasets.",
        "As an additional benefit, TopicTiling performs the segmentation in linear time and thus is computationally less expensive than other LDA-based segmentation methods."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The task tackled in this paper is Text Segmentation (TS), which is to be understood as the segmentation of texts into topically similar units.",
        "This implies, viewing the text as a sequence of subtopics, that a subtopic change marks a new segment.",
        "The challenge for a text segmentation algorithm is to find the sub-topical structure of a text.",
        "In this work, this semantic information is gained from Topic Models (TMs).",
        "We introduce a newly developed TS algorithm called TopicTiling.",
        "The core algorithm is a simplified version of TextTiling (Hearst, 1994), where blocks of text are compared via bag-of-word vectors.",
        "TopicTiling uses topic IDs, obtained by the LDA inference method, instead of words.",
        "As some of the topic IDs obtained by the inference method tend to change for different runs, we recommend to use the most probable topic ID assigned during the inference.",
        "We denote this most probable topic ID as the mode (most frequent across all inference steps) of the topic assignment.",
        "These IDs are used to calculate the cosine similarity between two adjacent blocks of sentences, represented as two vectors, containing the frequency of each topic ID.",
        "Without parameter optimization we obtain state-of-the-art results based on the Choi dataset (Choi, 2000).",
        "We show that the mode assignment improves the results substantially and improves even more when parameterizing the size of sampled blocks using a window size parameter.",
        "Using these optimizations, we obtain significant improvements compared to other algorithms based on the Choi dataset and also on a more difficult Wall Street Journal (WSJ) corpus provided by Galley et al. (2003).",
        "Not only does TopicTiling deliver state-of-the-art segmentation results, it also performs the segmentation in linear time, as opposed to most other recent TS algorithms.",
        "The paper is organized as follows: The next section gives an overview of text segmentation algorithms.",
        "Section 3 introduces the TopicTiling TS algorithm.",
        "The Choi and the Galley datasets used to measure the performance of TopicTiling are described in Section 4.",
        "In the evaluation section, the results of TopicTiling are demonstrated on these datasets, followed by a conclusion and discussion."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "TS can be divided into two sub-fields: (i) linear TS and (ii) hierarchical TS.",
        "Whereas linear TS deals with the sequential analysis of topical changes,",
        "hierarchical segmentation is concerned with finding more fine grained subtopic structures in texts.",
        "One of the first unsupervised linear TS algorithms was introduced by Hearst (1994): TextTiling segments texts in linear time by calculating the similarity between two blocks of words based on the cosine similarity.",
        "The calculation is accomplished by two vectors containing the number of occurring terms of each block.",
        "LcSeg (Galley et al., 2003), a TextTiling-based algorithm, uses tf-idf term weights and improved TS results compared to TextTiling.",
        "Utiyama and Isahara (2001) introduced one of the first probabilistic approaches using Dynamic Programming (DP) called U00.",
        "Related to our work are the DP approaches described in Misra et al. (2009) and Sun et al. (2008): here, topic modeling is used to alleviate the sparsity of word vectors.",
        "This approach was extended by (Misra et al., 2009) and (Sun et al., 2008) using topic information achieved from the LDA topic model.",
        "The first hierarchical algorithm was proposed by Yaari (1997), using the cosine similarity and agglomerative clustering approaches.",
        "A hierarchical Bayesian algorithm based on LDA is introduced with Eisenstein (2009).",
        "In our work, however, we focus on linear TS.",
        "LDA was introduced by Blei et al. (2003) and is a generative model that discovers topics based on a training corpus.",
        "Model training estimates two distributions: A topic-word distribution and a topic-document distribution.",
        "As LDA is a generative probabilistic model, the creation process follows a generative story: First, for each document a topic distribution is sampled.",
        "Then, for each document, words are randomly chosen, following the previously sampled topic distribution.",
        "Using the Gibbs inference method, LDA is used to apply a trained model for unseen documents.",
        "Here, words are annotated by topic IDs by assigning a topic ID sampled by the document-word and word-topic distribution.",
        "Note that the inference procedure, in particular, marks the difference between LDA and earlier dimensionality reduction techniques such as Latent Semantic Analysis."
      ]
    },
    {
      "heading": "3 TopicTiling",
      "text": [
        "This section introduces the TopicTiling algorithm, first introduced in (Riedl and Biemann, 2012a).",
        "In contrast to the quite similar TextTiling algorithm, TopicTiling is not based on words, but on the last topic IDs assigned by the Bayesian Inference method of LDA.",
        "This increases sparsity since the word space is reduced to a topic space of much lower dimension.",
        "Therefore, the documents that are to be segmented have first to be annotated with topic IDs.",
        "For useful topic distinctions, however, the topic model must be trained on documents similar in content to the test documents.",
        "Preliminary experiments have shown that repeating the Bayesian inference, often leads to different topic distributions for a given sentence in several runs.",
        "Memorizing each topic ID assigned to a word in a document during each inference step can alleviate this instability, which is rooted in the probabilistic nature of LDA.",
        "After finishing the inference on the unseen documents, we select the most frequent topic ID for each word and assign it to the word.",
        "We call this method the mode of a topic assignment, denoted with d = true in the remainder (Riedl and Biemann, 2012b).",
        "Note that this is different from using the overall topic distribution as determined by the inference step, since this winner-takes-it-all approach reduces noise from random fluctuations.",
        "As this parameter stabilizes the topic IDs at low computational costs, we recommend using this option in all setups where subsequent steps rely on a single topic assignment.",
        "TopicTiling assumes a sentence si as the smallest basic unit.",
        "At each position p, located between two adjacent sentences, a coherence score cp is calculated.",
        "With w we introduce a so-called window parameter that specifies the number of sentences to the left and to the right of position p that define two blocks: sp?w, .",
        ".",
        ".",
        ", sp and sp+1, .",
        ".",
        ".",
        ", sp+w+1.",
        "In contrast to the mode topic assignment parameter d, we cannot state a recommended value for w, as this parameter is dependent on the number of sentences a segment should contain.",
        "This is conditioned on the corpus that is segmented.",
        "To calculate the coherence score, we exclusively use the topic IDs assigned to the words by inference: Assuming an LDA model with T topics, each block is represented as a T dimensional vector.",
        "The t-th element of each vector contains the frequency of the topic ID t obtained from the according block.",
        "The coherence score is calculated by the vector dot product, also referred to as cosine similarity.",
        "Val",
        "ues close to zero indicate marginal relatedness between two adjacent blocks, whereas values close to one denote a substantial connectivity.",
        "Next, the coherence scores are plotted to trace the local minima.",
        "These minima are utilized as possible segmentation boundaries.",
        "But rather using the cp values itself, a depth score dp is calculated for each minimum (cf. TextTiling, (Hearst, 1994)).",
        "In comparison to TopicTiling, TextTiling calculates the depth score for each position and than searches for maxima.",
        "The depth score measures the deepness of a minimum by looking at the highest coherence scores on the left and on the right and is calculated using following formula: dp = 1/2(hl(p)?",
        "cp + hr(p)?",
        "cp).",
        "The function hl(p) iterates to the left as long as the score increases and returns the highest coherence score value.",
        "The same is done, iterating in the other direction with the hr(p) function.",
        "If the number of segments n is given as input, the n highest depth scores are used as segment boundaries.",
        "Otherwise, a threshold is applied (cf. TextTiling).",
        "This threshold predicts a segment if the depth score is larger than ?",
        "?",
        "?/2, with ?",
        "being the mean and ?",
        "being the standard variation calculated on the depth scores.",
        "The algorithm runtime is linear in the number of possible segmentation points, i.e. the number of sentences: for each segmentation point, the two adjacent blocks are sampled separately and combined into the coherence score.",
        "This, and the parameters d and w, are the main differences to the dynamic programming approaches for TS described in (Utiyama and Isahara, 2001; Misra et al., 2009)."
      ]
    },
    {
      "heading": "4 Data Sets",
      "text": [
        "The performance of the introduced algorithm is demonstrated using two datasets: A dataset proposed by Choi and another more challenging one assembled by Galley."
      ]
    },
    {
      "heading": "4.1 Choi Dataset",
      "text": [
        "The Choi dataset (Choi, 2000) is commonly used in the field of TS (see e.g. (Misra et al., 2009; Sun et al., 2008; Galley et al., 2003)).",
        "It is a corpus, generated artificially from the Brown corpus and consists of 700 documents.",
        "For document generation, ten segments of 3-11 sentences each, taken from different documents, are combined forming one document.",
        "400 documents consist of segments with a sentence length of 3-11 sentences and there are 100 documents each with sentence lengths of 3-5, 6-8 and 9-11."
      ]
    },
    {
      "heading": "4.2 Galley Dataset",
      "text": [
        "Galley et al. (2003) present two corpora for written language, each having 500 documents, which are also generated artificially.",
        "In comparison to Choi's dataset, the segments in its ?documents?",
        "vary from 4 to 22 segments, and are composed by concatenating full source documents.",
        "One dataset is generated based on WSJ documents of the Penn Treebank (PTB) project (Marcus et al., 1994) and the other is based on Topic Detection Track (TDT) documents (Wayne, 1998).",
        "As the WSJ dataset seems to be harder (consistently higher error rates across several works), we use this dataset for experimentation."
      ]
    },
    {
      "heading": "5 Evaluation",
      "text": [
        "The performance of TopicTiling is evaluated using two measures, commonly used in the TS task: The Pk measure and the WindowDiff (WD) measure (Beeferman et al., 1999; Pevzner and Hearst, 2002).",
        "Besides the training corpus, the following parameters need to be specified for LDA: The number of topics T , the number of sample iterations for the model m and two hyperparameters ?",
        "and ?, specifying the sparseness of the topic-document and the topic-word distribution.",
        "For the inference method, the number of sampling iterations i is required.",
        "In line with Griffiths and Steyvers (2004), the following standard parameters are used: T = 100, ?",
        "= 50/T , ?",
        "= 0.01, m = 500, i = 100.",
        "We use the JGibbsLDA implementation described in Phan and Nguyen (2007)."
      ]
    },
    {
      "heading": "5.1 Evaluation of the Choi Dataset",
      "text": [
        "For the evaluation we use a 10-fold Cross Validation (CV): the full dataset of 700 documents is split into 630 documents for training the topic model and 70 documents that are segmented.",
        "These two steps are repeated ten times to have all 700 documents segmented.",
        "For this dataset, no part-of-speech based word filtering is necessary.",
        "The results for different parameter settings are listed in Table 1.",
        "When using only the window parameter without the mode (d=false), the results demonstrate a sig",
        "parameters.",
        "nificant error reduction when using a window of 2 sentences.",
        "An impairment is observed when using a too large window (w=5).",
        "This is expected, as the size of the segments is in a range of 3-11 sentences: A window of 5 sentences therefore leads to blocks that contain segment boundaries.",
        "We can also see that the mode method improves the results when using a window of one, except for the documents having small segments ranging from 3-5 sentences.",
        "The lowest error rates are obtained with the mode method and a window size of 2.",
        "As described above, the algorithm is also able to automatically estimate the number of segments using a threshold value (see Table 2).",
        "ber of segments as parameter.",
        "The results show that for small segments, the number of segments is not correctly estimated, as the error rates are much higher than with given segments.",
        "As the window parameter has a smoothing effect on the coherence score function, less possible boundary candidates are detected.",
        "We can also see that the usage of the mode parameter leads to worse results with w=1 compared to the results where the mode is deactivated for the documents containing segments of length 3-5.",
        "Especially, results on these documents suffer when not providing the number of segments.",
        "But for the other documents, results are much better.",
        "Some results (see segment lengths 6 8 and 3-11 with d=true and w=2) are even better than the results with segments provided (see Table 1).",
        "The threshold method can outperform the setup with given a number of segments, since not recognizing a segment produces less error in the measures than predicting a wrong segment.",
        "Table 3 presents a comparison of the performance of TopicTilig compared to different algorithms in the literature.",
        "ous algorithms in the literature with number of segments provided It is obvious that the results are far better than current state-of-the-art results.",
        "Using a one-sampled t-test with ?",
        "= 0.05 we can state significant improvements in comparison to all other algorithms.",
        "While we aim not using the same documents for training and testing by using a CV scheme, it is not guaranteed that all testing data is unseen, since the same source sentences can find their way in several artificially crafted ?documents?.",
        "We could detect reoccurring snippets in up to 10% of the documents provided by Choi.",
        "This problem, however, applies for all evaluations on this dataset that use any kind of training, be it LDA models in Misra et al. (2009) or tf-idf values in Fragkou et al. (2004) and Galley et al. (2003)."
      ]
    },
    {
      "heading": "5.2 Evaluation on Galley's WSJ Dataset",
      "text": [
        "For the evaluation on Galley's WSJ dataset, a topic model is created from the WSJ collection of the PTB project.",
        "The dataset for model estimation consists of 2499 WSJ articles, and is the same dataset Galley used as a source corpus.",
        "The evaluation generally leads to higher error rates than in the evaluation for the Choi dataset, as shown in Table 4.",
        "This table shows results of the WSJ data when using all words of the documents for training a topic model and assigning topic IDs to new documents and also filtered results, using only nouns (proper",
        "and common), verbs and adjectives1.",
        "Considering the unfiltered results we observe that results improve when using the mode assigned topic ID and a window of larger than one sentence.",
        "In case of the WSJ dataset, we find the optimal setting for w=5.",
        "As the test documents contain whole articles, which consist of at least 4 sentences, a larger window is advantageous here, yet a value of 10 is too large.",
        "Filtering the documents for parts of speech leads to ?",
        "1% absolute error rate reduction, as can be seen in the last two columns of Table 4.",
        "Again, we observe that the mode assignment always leads to better results, gaining at least 0.6%.",
        "Especially the window size of 5 helps TopicTiling to decrease the error rate to a third of the value observed with d=false and w=1.",
        "Similar to the previous findings, results decline when using a too large window.",
        "Table 5 shows the results we achieve with the threshold-based estimation of segment boundaries for the unfiltered and filtered data.",
        "In contrast to the results obtained with the Choi dataset (see Table 2) no decline is observed when the threshold approach is used in combination with the window approach.",
        "We attribute this due to the small segments and documents used in the Choi setting.",
        "Comparing the all-words data with pos-filtered data, an improvement is always observed.",
        "Also a continuous decreasing of both error rates, Pk and WD, is detected when using the mode and using a larger window size, even for w=10.",
        "The reason for this is that too many boundaries are detected when using small windows.",
        "As the window approach smoothes the similarity scores, this leads to less segmentation boundaries, which improve results.",
        "For comparison, we present the evaluation results of other algorithms, shown in Table 6, as published",
        "for C99, U00 and LCseg as stated in (Galley et al., 2003).",
        "Again, TopicTiling improves over the state of the art.",
        "The improvements with respect to LCseg are significant using a one-sample t-test with ?",
        "= 0.05."
      ]
    },
    {
      "heading": "6 Conclusion and Further Work",
      "text": [
        "We introduced TopicTiling, a new TS algorithm that outperforms other algorithms as shown on two datasets.",
        "The algorithm is based on TextTiling and uses the topic model LDA to find topical changes within documents.",
        "A general result with implications to other algorithms that use LDA topic IDs is that using the mode of topic assignments across the different inference steps is recommended to stabilize the topic assignments, which improves performance.",
        "As the inference method is relatively fast in comparison to building a model, this mechanism is a useful and simple improvement, not only restricted to the field of TS.",
        "Using more than a single sentence in inference blocks leads to further stability and less sparsity, which improves the results further.",
        "In contrast to other TS algorithms using topic models (Misra et al., 2009; Sun et al., 2008), the runtime of TopicTiling is linear in the number of sentences.",
        "This",
        "makes TopicTiling a fast algorithm with complexity of O(n) (n denoting the number of sentences) as opposed to O(n2) of the dynamic programming approach as discussed in Fragkou et al. (2004).",
        "Text segmentation benefits from the usage of topic models.",
        "As opposed to general-purpose lexical resources, topic models can also find fine-grained sub-topical changes, as shown with the segmentation results of the WSJ dataset.",
        "Here, most articles have financial content and the topic model can e.g. distinguish between commodity and stock trading.",
        "The topic model adapts to the subtopic distribution of the target collection, in contrast e.g. to static WordNet domain labels as in Bentivogli et al. (2004).",
        "For further work, we would like to devise a method to detect the optimal setting for the window parameter w automatically, especially in a setting where the number of target segments is not known in advance.",
        "This is an issue that is shared with the original TextTiling algorithm.",
        "Moreover, we will extend the usage of our algorithm to more realistic corpora.",
        "Another direction of research that is more generic for approaches based on topic models is the question of how to automatically select appropriate data for topic model estimation, given only a small target collection.",
        "Since topic model estimation is computationally expensive, and topic models for generic collections (think Wikipedia) might not suit the needs of a specialized domain (such as with the WSJ data), it is a promising direction to look at target-domain-driven automatic corpus synthesis."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work has been supported by the Hessian research excellence program ?Landes-Offensive zur Entwicklung Wissenschaftlich-konomischer Exzel-lenz?",
        "(LOEWE) as part of the research center ?Digital Humanities?."
      ]
    }
  ]
}
