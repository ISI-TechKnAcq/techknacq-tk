{
  "info": {
    "authors": [
      "Mengqiu Wang",
      "Yanxin Shi"
    ],
    "book": "SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-W06-0138",
    "title": "Using Part-Of-Speech Reranking to Improve Chinese Word Segmentation",
    "url": "https://aclweb.org/anthology/W06-0138",
    "year": 2006
  },
  "references": [
    "acl-C02-1145",
    "acl-H05-1094",
    "acl-W03-1025",
    "acl-W04-3236"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Chinese word segmentation and Part-of-Speech (POS) tagging have been commonly considered as two separated tasks.",
        "In this paper, we present a system that performs Chinese word segmentation and POS tagging simultaneously.",
        "We train a segmenter and a tagger model separately based on linear-chain Conditional Random Fields (CRF), using lexical, morphological and semantic features.",
        "We propose an approximated joint decoding method by reranking the N-best segmenter output, based POS tagging information.",
        "Experimental results on SIGHAN Bakeoff dataset and Penn Chinese Treebank show that our reranking method significantly improve both segmentation and POS tagging accuracies."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Word segmentation and Part-of-speeching (POS) tagging are the most fundamental tasks in Chinese natural language processing (NLP).",
        "Traditionally, these two tasks were treated as separate and independent processing steps chained together in a pipeline.",
        "In such pipeline systems, errors introduced at the early stage cannot be easily recovered in later steps, causing a cascade of errors and eventually harm overall performance.",
        "Intuitively, a correct segmentation of the input sentence is more likely to give rise to a correct POS tagging sequence than an incorrect segmentation.",
        "Hinging on this idea, one way to avoid error propagation in chaining subtasks such as segmentation and POS tagging is to exploit the learning transfer (Sutton and McCallum, 2005) among subtasks, typically through joint inference.",
        "Sutton et al.",
        "(2004) presented dynamic conditional random fields (DCRF), a generalization of the traditional linear-chain CRF that allow representation of interaction among labels.",
        "They used loopy belief propagation for inference approximation.",
        "Their empirical results on the joint task of POS tagging and NP-chunking suggested that DCRF gave superior performance over cascaded linear-chain CRF.",
        "Ng and Low (2004) and Luo (2003) also trained single joint models over the Chinese segmentation and POS tagging subtasks.",
        "In their work, they brought the two subtasks together by treating it as a single tagging problem, for which they trained a maximum entropy classifier to assign a combined word boundary and POS tag to each character.",
        "A major challenge, however, exists in doing joint inference for complex and large-scale NLP application.",
        "Sutton and McCallum (Sutton and McCallum, 2005) suggested that in many cases exact inference can be too expensive and thus formidable.",
        "They presented an alternative approach in which a linear-chain CRF is trained separately for each subtask at training time, but at decoding time they combined the learned weights from the CRF cascade into a single grid-shaped factorial CRF to perform joint decoding and make predictions for all subtasks.",
        "Similar to (Sutton and McCallum, 2005), in our system we also train a cascade of linear-chain CRF for the subtasks.",
        "But at decoding time, we experiment with an alternative approximation method to joint decoding, by taking the n-best hypotheses from the segmentation model and use the POS tagging model for reranking.",
        "We evaluated our system on the open tracks of SIGHAN Bakeoff 2006 dataset.",
        "Furthermore, to evaluate our reranking method’s impact on the POS tagging task, we also performed 10-fold cross-validation tests on the 250k Penn",
        "Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 205–208, Sydney, July 2006. c�2006 Association for Computational Linguistics Chinese Treebank (CTB) (Xue et al., 2002).",
        "Results from both evaluations suggest that our simple reranking method is very effective.",
        "We achieved a consistent performance gain on both segmentation and POS tagging tasks over linearly-cascaded CRF.",
        "Our official F-scores on the 2006 Bakeoff open tracks are 0.935 (UPUC), 0.964 (CityU), 0.952 (MSRA) and 0.949 (CKIP)."
      ]
    },
    {
      "heading": "2 Algorithm",
      "text": [
        "Given an observed Chinese character sequence X = {C1, C2,..., Cn}, let S and T denote a segmentation sequence and a POS tagging sequence over X.",
        "Our goal is to find a segmentation sequence S� and a POS tagging sequence T� that maximize the posterior probability :",
        "Applying chain rule, we can further derive from Equation 1 the following: <�S,T�> Since we have factorized the joint probability in Equation 1 into two terms, we can now model these two components using conditional random fields (Lafferty et al., 2001).",
        "Linear-chain CRF models define conditional probability, P(ZIX), by linear-chain Markov random fields.",
        "In our case, X is the sequence of characters or words, and Z is the segmentation labels for characters (START or NON-START, used to indicate word boundaries) or the POS tagging for words (NN, VV, JJ, etc.).",
        "The conditional probability is defined as:",
        "where N(X) is a normalization term to guarantee that the summation of the probability of all label sequences is one.",
        "fk(Z, X, t) is the kth local feature f unction at sequence position t. It maps a pair of X and Z and an index t to {0, 1}.",
        "(A1, ..., AK) is a weight vector to be learned from training set.",
        "A large positive value of Ai means that the ith feature function’s value is frequent to be 1, whereas a negative value of Ai means the ith feature function’s value is unlikely to be 1.",
        "At decoding time, we are interested in finding the segmentation sequence S� and POS tagging sequence T� that maximizes the probability defined in Equation 2.",
        "Instead of exhaustively searching the whole space of all possible segmentations, we restrict our searching to S = {S1, S2, ..., SN}, where S is the restricted search space consisting of N-best decoded segmentation sequences.",
        "This N-best list of segmentation sequences, S, can be obtained using modified Viterbi algorithm and A* search (Schwartz and Chow, 1990)."
      ]
    },
    {
      "heading": "3 Features",
      "text": []
    },
    {
      "heading": "3.1 Features for Segmentation",
      "text": [
        "We adopted the basic segmentation features used in (Ng and Low, 2004).",
        "These features are summarized in Table 1 ((1.1)-(1.7)).",
        "In these templates, C0 refers to the current character, and C – n, Cn refer to the characters n positions to the left and right of the current character, respectively.",
        "Pu(C0) indicates whether C0 is a punctuation.",
        "T(Cn) classifies the character Cn into four classes: numbers, dates (year, month, date), English letters and all other characters.",
        "LBegin(C0), LEnd(C0) and LMid (C0) represent the maximum length of words found in a lexicon1 that contain the current character as either the first, last or middle character, respectively.",
        "Single(C0) indicates whether the current character can be found as a single word in the lexicon.",
        "Besides the adopted basic features mentioned above, we also experimented with additional semantic features (Table 1 (1.8)).",
        "For (1.8), Sem0 refers to the semantic class of current character, and Sem_1, Sem1 represent the semantic class of characters one position to the left and right of the current character, respectively.",
        "We obtained a character’s semantic class from HowNet (Dong and Dong, 2006).",
        "Since many characters have multiple semantic classes defined by HowNet, it is a non-trivial task to choose among the different semantic classes.",
        "We performed contextual disambiguation of characters’ semantic classes by calculating semantic class similarities.",
        "For example, let us assume the current character is 8(look,read) in a word context of 8TR(read",
        "newspaper).",
        "The character *(look) has two semantic classes in HowNet, i.e. J(read) and N M(doctor).",
        "To determine which class is more appropriate, we check the example words illustrating the meanings of the two semantic classes, given by HowNet.",
        "For 1(read), the example word is * - 5(read book); for � �(doctor), the example word is * �(see a doctor).",
        "We then calculated the semantic class similarity scores between TR(newspaper) and - 5(book), and TR(newspaper) and �(illness), using HowNet’s built-in similarity measure function.",
        "Since TR(newspaper) and - 5(book) both have semantic class 3Z- 5(document), their maximum similarity score is 0.95, where the maximum similarity score between TR(newspaper) and >(illness) is 0.03478.",
        "Therefore, Sem0Sem1 =1(read),3t - 5(document).",
        "Similarly, we can figure out Sem – 1Sem0.",
        "For Sem0, we simply picked the top four semantic classes ranked by HowNet, and used ”‘NONE”’ for absent values."
      ]
    },
    {
      "heading": "3.2 Features for POS Tagging",
      "text": [
        "The bottom half of Table 1 summarizes the feature templates we employed for POS tagging.",
        "W0 denotes the current word.",
        "W – n and Wn refer to the words n positions to the left and right of the current word, respectively.",
        "Cn(W0) is the nth character in current word.",
        "If the number of characters in the word is less than 5, we use ”NONE” for absent characters.",
        "Len(W0) is the number of characters in the current word.",
        "We also used a group of binary features for each word, which are used to represent the morphological properties of current word, e.g. whether the current word is punctuation, number, foreign name, etc."
      ]
    },
    {
      "heading": "4 Experimental Results",
      "text": [
        "We evaluated our system’s segmentation results on the SIGHAN Bakeoff 2006 dataset.",
        "To evaluate our reranking method’s impact on the POS tagging part, we also performed 10-fold cross-validation tests on the 250k Penn Chinese Treebank (CTB 250k).",
        "The CRF model for POS tagging is trained on CTB 250k in all the experiments.",
        "We report recall (R), precision (P), and F1-score (F) for both word segmentation and POS tagging tasks.",
        "N value is chosen to be 20 for the N-best list reranking, based on cross validation.",
        "For CRF learning and decoding, we use the CRF++ toolkit2."
      ]
    },
    {
      "heading": "4.1 Results on Bakeoff 2006 Dataset",
      "text": [
        "We participated in the open tracks of the SIGHAN Bakeoff 2006, and we achieved F-scores of 0.935 (UPUC), 0.964 (CityU), 0.952 (MSRA) and 0.949 (CKIP).",
        "More detailed performances statistics including in-vocabulary recall (Riv) and out-of-vocabulary recall (Roov) are shown in Table More interesting to us is how much the N-best list reranking method using POS tagging helped to increase segmentation performance.",
        "For comparison, we ran a linear-cascade of segmentation and POS tagging CRFs without reranking as the baseline system, and the results are shown in Table 3.",
        "We can see that our reranking method consistently improved segmentation scores.",
        "In particular, there is a greater improvement gained in recall than precision across all four tracks.",
        "We observed the greatest improvement from the UPUC track.",
        "We think it is because our POS tagging model is trained on CTB 250k, which could be drawn from the same corpus as the UPUC training data, and therefore there is a closer mapping between segmentation standard of the POS tagging training data and the segmentation training data (at this"
      ]
    },
    {
      "heading": "4.2 Results on CTB Corpus",
      "text": [
        "To evaluate our reranking method’s impact on the POS tagging task, we also tested our systems on CTB 250k corpus using 10-fold cross-validation.",
        "Figure 1 summarizes the results of segmentation and POS tagging tasks on CTB 250k corpus.",
        "From figure 1 we can see that our reranking method improved both the segmentation and tagging accuracies across all 10 tests.",
        "We conducted pairwise t-tests and our reranking model was found to be statistically significantly better than the baseline model under significance level of 5.0-4 (p-value for segmentation) and 3.3-5 (p-value for POS tagging).",
        "Our system uses conditional random fields for performing Chinese word segmentation and POS tagging tasks simultaneously.",
        "In particular, we proposed an approximated joint decoding method by reranking the N-best segmenter output, based POS tagging information.",
        "Our experimental results on both SIGHAN Bakeoff 2006 datasets and Chinese Penn Treebank showed that our reranking method consistently increased both segmentation and POS tagging accuracies.",
        "It is worth noting that our reranking method can be applied not only to Chinese segmentation and POS tagging tasks, but also to many other sequential tasks that can benefit from learning transfer, such as POS tagging and NP-chunking."
      ]
    },
    {
      "heading": "Acknowledgment",
      "text": [
        "This work was supported in part by ARDA’s AQUAINT Program."
      ]
    }
  ]
}
