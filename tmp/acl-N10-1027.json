{
  "info": {
    "authors": [
      "Timothy Baldwin",
      "Marco Lui"
    ],
    "book": "Human Language Technologies: the 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics",
    "id": "acl-N10-1027",
    "title": "Language Identification: The Long and the Short of the Matter",
    "url": "https://aclweb.org/anthology/N10-1027",
    "year": 2010
  },
  "references": [
    "acl-D07-1016",
    "acl-E09-1099",
    "acl-W09-0307"
  ],
  "sections": [
    {
      "text": [
        "Timothy Baldwin and Marco Lui",
        "Language identification is the task of identifying the language a given document is written in.",
        "This paper describes a detailed examination of what models perform best under different conditions, based on experiments across three separate datasets and a range of tokeni-sation strategies.",
        "We demonstrate that the task becomes increasingly difficult as we increase the number of languages, reduce the amount of training data and reduce the length of documents.",
        "We also show that it is possible to perform language identification without having to perform explicit character encoding detection."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "With the growth of the worldwide web, ever-increasing numbers of documents have become available, in more and more languages.",
        "This growth has been a double-edged sword, however, in that content in a given language has become more prevalent but increasingly hard to find, due to the web's sheer size and diversity of content.",
        "While the majority of (X)HTML documents declare their character encoding, only a tiny minority specify what language they are written in, despite support for language declaration existing in the various (X)HTML standards.",
        "Additionally, a single encoding can generally be used to render a large number of languages such that the document encoding at best filters out a subset of languages which are incompatible with the given encoding, rather than disambiguates the source language.",
        "Given this, the need for automatic means to determine the source language of web documents is crucial for web aggregators of various types.",
        "There is widespread misconception of language identification being a \"solved task\", generally as a result of isolated experiments over homogeneous datasets with small numbers of languages (Hughes et al., 2006; Xia et al., 2009).",
        "Part of the motivation for this paper is to draw attention to the fact that, as a field, we are still a long way off perfect language identification of web documents, as evaluated under realistic conditions.",
        "In this paper we describe experiments on language identification of web documents, focusing on the broad question of what combination of tokenisation strategy and classification model achieves the best overall performance.",
        "We additionally evaluate the impact of the volume of training data and the test document length on the accuracy of language identification, and investigate the interaction between character encoding detection and language identification.",
        "One assumption we make in this research, following standard assumptions made in the field, is that all documents are monolingual.",
        "This is clearly an unrealistic assumption when dealing with general web documents (Hughes et al., 2006), and we plan to return to investigate language identification over multilingual documents in future work.",
        "Our contributions in this paper are: the demonstration that language identification is: (a) trivial over datasets with smaller numbers of languages and approximately even amounts of training data per language, but (b) considerably harder over datasets with larger numbers of languages with more skew in the amount of training data per language; byte-based tokenisation without character encoding detection is superior to codepoint-based tokenisation with character encoding detection; and simple cosine similarity-based nearest neighbour classification is equal to or better than models including support vector machines and naive Bayes over the language identification task.",
        "We also develop datasets to facilitate standardised evaluation of language identification."
      ]
    },
    {
      "heading": "2. Background Research",
      "text": [
        "Language identification was arguably established as a task by Gold (1967), who construed it as a closed class problem: given data in each of a predefined set of possible languages, human subjects were asked to classify the language of a given test document.",
        "It wasn't until the 1990s, however, that the task was popularised as a text categorisation task.",
        "The text categorisation approach to language identification applies a standard supervised classification framework to the task.",
        "Perhaps the best-known such model is that of Cavnar and Tren-kle (1994), as popularised in the textcat tool.The method uses a per-language character frequency model, and classifies documents via their relative \"out of place\" distance from each language (see Section 5.1).",
        "Variants on this basic method include Bayesian models for character sequence prediction (Dunning, 1994), dot products of word frequency vectors (Darnashek, 1995) and information-theoretic measures of document similarity (Aslam and Frost, 2003; Martins and Silva, 2005).",
        "More recently, support vector machines (SVMs) and kernel methods have been applied to the task of language identification task with success (Teytaud and Jalam, 2001; Lodhi et al., 2002; Kruengkrai et al., 2005), and Markov logic has been used for joint inferencing in contexts where there are multiple evidence sources (Xia et al., 2009).",
        "Language identification has also been carried out via linguistically motivated models.",
        "Johnson (1993) used a list of stop words from different languages to identify the language of a given document, choosing the language with the highest stop word overlap with the document.",
        "Grefenstette (1995) used word and part of speech (POS) correlation to determine if two text samples were from the same or different languages.",
        "Giguet (1995) developed a cross-language tokenisation model and used it to identify the language of a given document based on its tokenisation similarity with training data.",
        "Dueire Lins and Goncalves (2004) considered the use of syntactically-derived closed grammatical-class models, matching syntactic structure rather than words or character sequences.",
        "The observant reader will have noticed that some of the above approaches make use of notions such as \"word\", typically based on the naive assumption that the language uses white space to delimit words.",
        "These approaches are appropriate in contexts where there is a guarantee of a document being in one of a select set of languages where words are space-delimited, or where manual segmentation has been performed (e.g. interlinear glossed text).",
        "However, we are interested in language identification of web documents, which can be in any language, including languages that do not overtly mark word boundaries, such as Japanese, Chinese and Thai; while relatively few languages fall into this categories, they are among the most populous web languages and therefore an important consideration.",
        "Therefore, approaches that assume a language is space-delimited are clearly not suitable for our purposes.",
        "Equally, approaches which make assumptions about the availability of particular resources for each language to be identified (e.g. POS taggers, or the existence of precompiled stop word lists) cannot be used.",
        "Language identification has been applied in a number of contexts, the most immediate application being in multilingual text retrieval, where retrieval results are generally superior if the language of the query is known, and the search is restricted to only those documents predicted to be in that language (McNamee and Mayfield, 2004).",
        "It can also be used to \"word spot\" foreign language terms in multilingual documents, e.g. to improve parsing performance (Alex et al., 2007), or for linguistic corpus creation purposes (Baldwin et al., 2006; Xia et al.,",
        "In the experiments reported in this paper, we employ three novel datasets, with differing properties relevant to language identification research:",
        "Figure 1: Distribution of languages in the three datasets (vector of languages vs. the proportion of documents in that language) EuroGOV: longer documents, all in a single encoding, spread evenly across a relatively small number (10) of Western European languages; this dataset is comparable to the datasets conventionally used in language identification research.",
        "As the name would suggest, the documents were sourced from the Euro-GOV document collection, as used in the 2005 Web-",
        "CLEF task.",
        "T CL: a larger number of languages (60) across a wider range of language families, with shorter documents and a range of character encodings (12).",
        "The collection was manually sourced by the Thai Computational Linguistics Laboratory (TCL) in 2005 from online news sources.",
        "Wikipedia: a slightly larger number of languages again (67), a single encoding, and shorter documents; the distribution of languages is intended to approximate that of the actual web.",
        "This collection was automatically constructed by taking the dumps of all versions of Wikipedia with 1000 or more documents in non-constructed languages, and randomly selecting documents from them in a bias-preserving manner (i.e. preserving the document distribution in the full collection); this is intended to represent the document language bias observed on the web.",
        "All three corpora are available on request.",
        "We outline the characteristics of the three datasets in Table 1.",
        "We further detail the language distribution in Figure 1, using a constant vector of languages for all three datasets, based on the order of languages in the Wikipedia dataset (in descending order of documents per language).",
        "Of note are the contrasting language distributions between the three datasets, in terms of both the languages represented and the relative skew of documents per language.",
        "In the following sections, we provide details of the corpus compilation and document sampling method for each dataset."
      ]
    },
    {
      "heading": "4. Document Representation",
      "text": [
        "As we are interested in performing language identification over arbitrary web documents, we require a language-neutral document representation which does not make artificial assumptions about the source language of the document.",
        "Separately, there is the question of whether it is necessary to determine the character encoding of the document in order to extract out character sequences, or whether the raw byte stream is sufficient.",
        "To explore this question, we experiment with two document representations: (1) byte n-grams, and (2) codepoint n-grams.",
        "In both cases, a document is represented as a feature vector of token counts.",
        "Byte n-grams can be extracted directly without explicit encoding detection.",
        "Codepoint n-grams, on the other hand, require that we know the character encoding of the document in order to perform to-kenisation.",
        "Additionally, they should be based on a common encoding to prevent: (a) over-fragmenting the feature space (e.g. ending up with discrete feature spaces for euc-jp, s-jis and utf-8 in the case of Japanese); and (b) spurious matches between encodings (e.g. Japanese hiragana and Korean hangul mapping onto the same codepoint in euc-jp and euc-kr, respectively).",
        "We use unicode as the common encoding for all documents.",
        "Corpus",
        "Documents",
        "Languages",
        "Encodings",
        "Document Length (bytes)",
        "EuroGOV",
        "1500",
        "10",
        "1",
        "17460.5±39353.4",
        "TCL",
        "3174",
        "60",
        "12",
        "2623.2±3751.9",
        "Wikipedia",
        "4963",
        "67",
        "1",
        "1480.8±4063.9",
        "In practice, character encoding detection is an issue only for TCL, as the other two datasets are in a single encoding.",
        "Where a character encoding was provided for a document in TCL and it was possible to transcode the document to unicode based on that encoding, we used the encoding information.",
        "In cases where a unique encoding was not provided, we used an encoding detection library based on the Mozilla browser.",
        "Having disambiguated the encoding for each document, we transcoded it into unicode."
      ]
    },
    {
      "heading": "5. Models",
      "text": [
        "In our experiments we use a number of different language identification models, as outlined below.",
        "We first describe the nearest-neighbour and nearest-prototype models, and a selection of distance and similarity metrics combined with each.",
        "We then present three standalone text categorisation models.",
        "The 1-nearest-neighbour (1NN) model is a common classification technique, whereby a test document D is classified based on the language of the closest training document Di (with language l(Di)), as determined by a given distance or similarity metric.",
        "In nearest-neighbour models, each training document is represented as a single instance, meaning that the computational cost of classifying a test document is proportional to the number of training documents.",
        "A related model which aims to reduce this cost is nearest-prototype (AM), where each language is represented as a single instance, by merging all of the training instances for that language into a single centroid via the arithmetic mean.",
        "For both nearest-neighbour and nearest-prototype methods, we experimented with three similarity and distance measures in this research:",
        "Cosine similarity (Cos): the cosine of the angle between two feature vectors, as measured by the dot product of the two vectors, normalised to unit length.",
        "Skew divergence (Skew): a variant of Kullback-Leibler divergence, whereby the second distribution",
        "(y) is smoothed by linear interpolation with the first (x) using a smoothing factor a (Lee, 2001):",
        "In all our experiments, we set a to 0.99.",
        "Out-of-place (OOP): a ranklist-based distance metric, where the distance between two documents is calculated as (Cavnar and Trenkle, 1994):",
        "RD (t) is the rank of term t in document D, based on the descending order of frequency in document D; terms not occurring in document D are conventionally given the rank 1 + maxi RD (ti).",
        "Naive Bayes is a popular text classification model, due to it being lightweight, robust and easy to update.",
        "The language of test document D is predicted where L is the set of languages in the training data, NVD;tj is the frequency of the jth term in D, V is the set of all terms, and:",
        "In this research, we use the rainbow implementation of multinominal naive Bayes (McCallum, 1996).",
        "Support vector machines (SVMs) are one of the most popular methods for text classification, largely because they can automatically weight large numbers of features, capturing feature interactions in the process (Joachims, 1998; Manning et al., 2008).",
        "The basic principle underlying SVMs is to maximize the margin between training instances and the calculated decision boundary based on structural risk minimisation (Vapnik, 1995).",
        "In this work, we have made use of bsvm, an implementation of SVMs with multiclass classification support (Hsu et al., 2008).",
        "We only report results for multi-class bound-constrained support vector machines with linear kernels, as they were found to perform best over our data."
      ]
    },
    {
      "heading": "6. Experimental Methodology",
      "text": [
        "We carry out experiments over the crossproduct of the following options, as described above:",
        "model (x7): nearest-neighbour (Cos1nn, Skew1nn, OOP1NN), nearest-prototype (Cosam, Skewam), NB, SVM tokenisation (x2): byte, codepoint for a total of 42 distinct classifiers.",
        "Each classifier is run across the 3 datasets (EuroGOV, TCL and Wikipedia) based on 10-fold stratified cross-validation.",
        "We evaluate the models using micro-averaged precision recall (TRt) and F-score as well as macro-averaged precision (Pm), recall (RM) and F-score (FM).",
        "The micro-averaged scores indicate the average performance per document; as we always make a unique prediction per document, the micro-averaged precision, recall and F-score are always identical (as is the classification accuracy).",
        "The macro-averaged scores, on the other hand, indicate the average performance per language.",
        "In each case, we average the precision, recall and F-score across the 10 folds of cross validation.",
        "As a baseline, we use a majority class, or ZeroR, classifier (ZeroR), which assigns the language with highest prior in the training data to each of the test documents.",
        "Model_Token Pm Rm Fm Pm/Rm/Fm"
      ]
    },
    {
      "heading": "7. Results",
      "text": [
        "In our experiments, we first compare the different models for fixed n-gram order, then come back to vary the n-gram order.",
        "Subsequently, we examine the relative performance of the different models on test documents of differing lengths, and finally look into the impact of the amount of training data for a given language on the performance for that language.",
        "7.1 Results for the Different Models and Tokenisation Strategies",
        "First, we present the results for each of the classifiers in Tables 2^4, based on byte or codepoint tokenisation and bigrams.",
        "In each case, we present the best result in each column in bold.",
        "The relative performance over EUROGOV and TCL is roughly comparable for all methods barring Skew1nn, with near-perfect scores over all 6 evaluation metrics.",
        "SKEW1NN is near-perfect over EU-ROGOV and TCL, but drops to baseline levels over WIKIPEDIA; we return to discuss this effect in Section 7.2.",
        "In the case of EUROGOV, the near-perfect results are in line with our expectations for the dataset, based on its characteristics and results reported for comparable datasets.",
        "The results for WIKIPEDIA, however, fall off considerably, with the best model achieving an FM of .671 and Fß of .869, due to",
        "ZeroR",
        " – ",
        ".020 .084",
        ".032",
        ".100",
        "CosiNN",
        "byte",
        ".975 .978",
        ".976",
        ".975",
        "CosiNN",
        "codepoint",
        ".968 .973",
        ".970",
        ".971",
        "Cosam",
        "byte",
        ".922 .938",
        ".926",
        ".937",
        "CosAM",
        "codepoint",
        ".908 .930",
        ".913",
        ".931",
        "Skew1nn",
        "byte",
        ".979 .979",
        ".979",
        ".977",
        "Skew1nn",
        "codepoint",
        ".978 .978",
        ".978",
        ".976",
        "Skew am",
        "byte",
        ".974 .972",
        ".972",
        ".969",
        "Skew am",
        "codepoint",
        ".974 .972",
        ".973",
        ".970",
        "OOPiNN",
        "byte",
        ".953 .952",
        ".953",
        ".949",
        "OOPiNN",
        "codepoint",
        ".961 .960",
        ".960",
        ".957",
        "NB",
        "byte",
        ".975 .973",
        ".974",
        ".971",
        "NB",
        "codepoint",
        ".975 .973",
        ".974",
        ".971",
        "SVM",
        "byte",
        ".989 .985 .987",
        ".987",
        "SVM",
        "codepoint",
        ".988 .985",
        ".986",
        ".987",
        "Table 3: Results for byte vs. codepoint (bigram) tokeni- Table 4: Results for byte vs. codepoint (bigram) tokeni-sation over TCL sation over Wikipedia the larger number of languages, smaller documents, and skew in the amounts of training data per language.",
        "All models are roughly balanced in the relative scores they attain for Pm, RM and FM (i.e. there are no models that have notably higher Pm relative to RM, for example).",
        "The nearest-neighbour models outperform the corresponding nearest-prototype models to varying degrees, with the one exception of Skew1NN over Wikipedia.",
        "The nearest-prototype classifiers were certainly faster than the nearest-neighbour classifiers, by roughly an order of 10, but this is more than outweighed by the drop in classification performance.",
        "With the exception of Skew1NN over Wikipedia, all methods were well above the baselines for all three datasets.",
        "The two methods which perform consistently well at this point are Cos1NN and SVM, with Cos1NN holding up particularly well under micro-averaged F-score while NB drops away over Wikipedia, the most skewed dataset; this is due to the biasing effect of the prior in NB.",
        "Looking to the impact of byte vs. codepoint-tokenisation on classifier performance over the three datasets, we find that overall, bytes outperform codepoints.",
        "This is most notable for TCL and Wikipedia, and the Skew1NN and NB models.",
        "Given this result, we present only results for byte-based tokenisation in the remainder of this paper.",
        "The results for byte tokenisation of TCL are particularly noteworthy.",
        "The transcoding into unicode and use of codepoints, if anything, hurts performance, suggesting that implicit character encoding detection based on byte tokenisation is the best approach: it is both more accurate and simplifies the system, in removing the need to perform encoding detection prior to language identification.",
        "We present results with byte unigrams, bigrams and trigrams in Table 5 for Wikipedia.",
        "We omit results for the other two datasets, as the overall trend is the same as for Wikipedia, with lessened relative differences between n-gram orders due to the relative simplicity of the respective classification tasks.",
        "Sk ew1NN is markedly different to the other methods in achieving the best performance with uni-grams, moving from the worst-performing method by far to one of the best-performing methods.",
        "This is the result of the interaction between data sparseness and heavy-handed smoothing with the a constant.",
        "Rather than using a constant a value for all n-gram orders, it may be better to parameterise it using an exponential scale such as a = 1 – ßn (with Model n-gram VM TZM TM VßITlßITß",
        "Model",
        "Token",
        "pm rm fm p» /r»/f»",
        "Model",
        "Token",
        "pm rm fm p»/r»/f»",
        "ZeroR",
        " – ",
        ".003 .017 .005",
        ".173",
        "ZeroR",
        " – ",
        ".004 .013",
        ".007",
        ".328",
        "COSiNN",
        "byte",
        ".981 .975 .975",
        ".982",
        "COSiNN",
        "byte",
        ".740 .646 .671",
        ".869",
        "CosiNN",
        "codepoint",
        ".931 .930 .925",
        ".961",
        "CosiNN",
        "codepoint",
        ".685 .604",
        ".625",
        ".835",
        "Cosam",
        "byte",
        ".967 .975 .965",
        ".965",
        "Cosam",
        "byte",
        ".587 .634",
        ".573",
        ".776",
        "CosAM",
        "codepoint",
        ".979 .977 .974",
        ".964",
        "CosAM",
        "codepoint",
        ".486 .556",
        ".483",
        ".725",
        "Skew1nn",
        "byte",
        ".984 .974 .976",
        ".987",
        "Skew1nn",
        "byte",
        ".005 .013",
        ".008",
        ".304",
        "Skew1nn",
        "codepoint",
        ".910 .210 .320",
        ".337",
        "Skew1nn",
        "codepoint",
        ".006 .013",
        ".007",
        ".241",
        "Skew am",
        "byte",
        ".962 .959 .950",
        ".972",
        "Skew am",
        "byte",
        ".605 .617",
        ".588",
        ".844",
        "Skew am",
        "codepoint",
        ".968 .961 .957",
        ".967",
        "Skew am",
        "codepoint",
        ".552 .575",
        ".532",
        ".807",
        "OOPiNN",
        "byte",
        ".964 .945 .951",
        ".974",
        "OOPiNN",
        "byte",
        ".619 .518",
        ".548",
        ".831",
        "OOPiNN",
        "codepoint",
        ".901 .892 .893",
        ".933",
        "OOPiNN",
        "codepoint",
        ".598 .486",
        ".520",
        ".807",
        "NB",
        "byte",
        ".905 .905 .896",
        ".969",
        "NB",
        "byte",
        ".496 .454",
        ".442",
        ".851",
        "NB",
        "codepoint",
        ".722 .711 .696",
        ".845",
        "NB",
        "codepoint",
        ".426 .349",
        ".360",
        ".798",
        "SVM",
        "byte",
        ".981 .973 .977",
        ".984",
        "SVM",
        "byte",
        ".667 .545",
        ".577",
        ".845",
        "SVM",
        "codepoint",
        ".979 .970 .974",
        ".980",
        "SVM",
        "codepoint",
        ".634 .494",
        ".536",
        ".818",
        "ß = 0.01, e.g.), based on the n-gram order.",
        "We leave this for future research.",
        "For most methods, bigrams and trigrams are better than unigrams, with the one notable exception of SKEWinn.",
        "In general, there is little separating bigrams and trigrams, although the best result for is achieved slightly more often for bigrams than for tri-grams.",
        "For direct comparability with Cavnar and Tren-kle (1994), we additionally carried out a preliminary experiment with hybrid byte n-grams (all of 1 to 5-grams), combined with simple frequency-based feature selection of the top-1000 features for each n-gram order.",
        "The significance of this setting is that it is the strategy adopted by textcat, based on the original paper of Cavnar and Trenkle (1994) (with the one exception that we use 1000 features rather than 300, as all methods other than OOPiNN benefitted from more features).",
        "The results are shown in",
        "Table 6.",
        "Compared to the results in Table 5, Skew1nn and SkeWam both increase markedly to achieve the best overall results.",
        "OOP1NN, on the other hand, rises slightly, while the remaining three methods actually",
        "Table 6: Results for mixed n-grams (1-5) and feature selection over Wikipedia (a la Cavnar and Trenkle (1994)) drop back slightly.",
        "Clearly, there is considerably more experimentation to be done here with mixed n-gram models and different feature selection methods, but the results indicate that some methods certainly benefit from n-gram hybridisation and feature selection, and also that we have been able to surpass the results of Cavnar and Trenkle (1994) with Skew1nn in an otherwise identical framework.",
        "To better understand the impact of test document size on classification accuracy, we divided the test documents into 5 equal-size bins according to their length, measured by the number of tokens.",
        "We then computed Tu individually for each bin across the 10 folds of cross validation.",
        "We present the breakdown of results for Wikipedia in Figure 2.",
        "Wikipedia shows a pseudo-logarithmic growth in Tu (= Vu = 1Zu) as the test document size increases.",
        "This fits with our intuition, as the model has progressively more evidence to base the classification on.",
        "It also suggests that performance over shorter documents appears to be the dominating factor in the overall ranking of the different methods.",
        "In particular, Cos1nn and SVM appear to be able to classify shorter documents most reliably, leading to the overall result of them being the best-performing methods.",
        "While we do not show the graph for reasons of space, the equivalent graph for EuroGOV displays a curious effect: Tu drops off as the test documents get longer.",
        "Error analysis of the data indicates that this is due to longer documents being more likely to be \"contaminated\" with either data from a second language or extralinguistic data, such as large tables of numbers or chemical names.",
        "This suggests that all the models are brittle when the assump-",
        "Model",
        "Vm",
        "VMIRM ITit",
        "ZeroR",
        ".004",
        ".013",
        ".007",
        ".328",
        "COSiNN",
        ".735",
        ".664",
        ".682",
        ".865",
        "Cosam",
        ".592",
        ".626",
        ".580",
        ".766",
        "Skew1nn",
        ".789",
        ".708",
        ".729",
        ".902",
        "SKEWam",
        ".681",
        ".718",
        ".680",
        ".870",
        "OOPiNN",
        ".697",
        ".595",
        ".626",
        ".864",
        "SVM",
        ".669",
        ".500",
        ".544",
        ".832",
        "zeror",
        " – ",
        ".004",
        ".013",
        ".007",
        ".328",
        "CosiNN",
        "1",
        ".644",
        ".579",
        ".599",
        ".816",
        "CosiNN",
        "2",
        ".740",
        ".646",
        ".671",
        ".869",
        "CosiNN",
        "3",
        ".744",
        ".656",
        ".680",
        ".862",
        "CosAM",
        "1",
        ".526",
        ".543",
        ".487",
        ".654",
        "Cosam",
        "2",
        ".587",
        ".634",
        ".573",
        ".776",
        "Cosam",
        "3",
        ".553",
        ".632",
        ".545",
        ".761",
        "SKEWinn",
        "1",
        ".691",
        ".598",
        ".625",
        ".848",
        "Skew1nn",
        "2",
        ".005",
        ".013",
        ".008",
        ".304",
        "Skew1nn",
        "3",
        ".005",
        ".013",
        ".004",
        ".100",
        "Skew am",
        "1",
        ".552",
        ".569",
        ".532",
        ".740",
        "Skew am",
        "2",
        ".605",
        ".617",
        ".588",
        ".844",
        "Skew am",
        "3",
        ".551",
        ".631",
        ".554",
        ".825",
        "OOPiNN",
        "1",
        ".519",
        ".446",
        ".468",
        ".747",
        "OOPiNN",
        "2",
        ".619",
        ".518",
        ".548",
        ".831",
        "NB",
        "1",
        ".576",
        ".578",
        ".555",
        ".778",
        "NB",
        "2",
        ".496",
        ".454",
        ".442",
        ".851",
        "NB",
        "3",
        ".493",
        ".435",
        ".432",
        ".863",
        "SVM",
        "1",
        ".585",
        ".505",
        ".523",
        ".812",
        "SVM",
        "2",
        ".667",
        ".545",
        ".577",
        ".845",
        "SVM",
        "3",
        ".717",
        ".547",
        ".594",
        ".840",
        "Wikipedia EuroGov",
        "Training data (MB)",
        "tion of strict monolingualism is broken, or when the document is dominated by extralinguistic data.",
        "Clearly, this underlines our assumption of monolingual documents, and suggests multilingual language identification is a fertile research area even in terms of optimising performance over our \"monolingual\" datasets.",
        "As a final data point in our analysis, we calculated the TM for each language relative to the amount of training data available for that language, and present the results in the form of a combined scatter plot for the three datasets in Figure 3.",
        "The differing distributions of the three datasets are self-evident, with most languages in EuroGOV (the squares) both having reasonably large amounts of training data and achieving high TM values, but the majority of languages in Wikipedia (the crosses) having very little data (including a number of languages with no training data, as there is a singleton document in that language in the dataset).",
        "As an overall trend, we can observe that the greater the volume of training data, the higher the TM across all three datasets, but there is considerable variation between the languages in terms of their TM for a given training data size (the column of crosses for Wikipedia to the left of the graph is particularly striking)."
      ]
    },
    {
      "heading": "8. Conclusions",
      "text": [
        "We have carried out a thorough (re)examination of the task of language identification, that is predicting the language that a given document is written in, focusing on monolingual documents at present.",
        "We experimented with a total of 7 models, and tested each over two tokenisation strategies (bigrams vs. codepoints) and three token n-gram orders (un-igrams, bigrams and trigrams).",
        "At the same time as reproducing results from earlier research on how easy the task can be over small numbers of languages with longer documents, we demonstrated that the task becomes much harder for larger numbers of languages, shorter documents and greater class skew.",
        "We also found that explicit character encoding detection is not necessary in language detection, and that the most consistent model overall is either a simple 1-NN model with cosine similarity, or an SVM with a linear kernel, using a byte bigram or trigram document representation.",
        "We also confirmed that longer documents tend to be easier to classify, but also that multilingual documents cause problems for the standard model of language identification."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This research was supported by a Google Research Award."
      ]
    }
  ]
}
