{
  "info": {
    "authors": [
      "Xiao Li"
    ],
    "book": "EMNLP",
    "id": "acl-D09-1134",
    "title": "On the Use of Virtual Evidence in Conditional Random Fields",
    "url": "https://aclweb.org/anthology/D09-1134",
    "year": 2009
  },
  "references": [
    "acl-H05-1058",
    "acl-J04-3004",
    "acl-N06-1041",
    "acl-P05-1046",
    "acl-P06-1027",
    "acl-P07-1036",
    "acl-P08-1076",
    "acl-P08-1099",
    "acl-P95-1026"
  ],
  "sections": [
    {
      "text": [
        "Virtual evidence (VE), first introduced by (Pearl, 1988), provides a convenient way of incorporating prior knowledge into Bayesian networks.",
        "This work generalizes the use of VE to undirected graphical models and, in particular, to conditional random fields (CRFs).",
        "We show that VE can be naturally encoded into a CRF model as potential functions.",
        "More importantly, we propose a novel semi-supervised machine learning objective for estimating a CRF model integrated with VE.",
        "The objective can be optimized using the Expectation-Maximization algorithm while maintaining the discriminative nature of CRFs.",
        "When evaluated on the CLASSIFIEDS data, our approach significantly outperforms the best known solutions reported on this task."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Statistical approaches to sequential labeling problems rely on necessary training data to model the uncertainty of a sequence of events.",
        "Human's prior knowledge about the task, on the other hand, often requires minimum cognitive load to specify, and yet can provide information often complementary to that offered by a limited amount of training data.",
        "Whenever prior knowledge becomes available, it is desired that such information is integrated to a probabilistic model to improve learning.",
        "Virtual evidence (VE), first introduced by Pearl (1988), offers a principled and convenient way of incorporating external knowledge into Bayesian networks.",
        "In contrast to standard evidence (also known as observed variables), VE expresses a prior belief over values of random variables.",
        "It has been shown that VE can significantly extend the modeling power of Bayesian networks without complicating the fundamental inference methodology (Bilmes, 2004; Reynolds and Bilmes, 2005).",
        "This work extends the use of VE to undirected graphical models and, in particular, to conditional random fields (CRFs).",
        "We show that VE can be naturally encoded into an undirected graphical model as potential functions.",
        "More importantly, we discuss a semi-supervised machine learning setting for estimating CRFs with the presence of VE.",
        "As the conditional likelihood objective of CRFs is not directly maximizable with respect to unlabeled data, we propose a novel semi-supervised learning objective that can be optimized using the Expectation-Maximization (EM) algorithm while maintaining the discriminative nature of CRFs.",
        "We apply our model to the CLASSIFIEDS data (Grenager et al., 2005).",
        "Specifically, we use VE to incorporate into a CRF model two types of prior knowledge specified in previous works.",
        "The first is defined based on the notion of prototypes, i.e., example words for a given label; and the other assumes that adjacent tokens tend to have the same label.",
        "When unlabeled data becomes available, we further extend the sparse prototype information to other words based on distributional similarity.",
        "This results in so-called collocation lists, each consisting of a relatively large number of noisy \"prototypes\" for a label.",
        "Given the fact that these noisy prototypes are often located close to each other in an input sequence, we create a new type of VE based on word collocation to reduce ambiguity.",
        "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1289-1297, Singapore, 6-7 August 2009.",
        "©2009 ACL and AFNLP",
        "We compare our CRF model integrated with VE with two state-of-the-art models, i.e., constraint-driven learning (Chang et al., 2007) and generalized expectation criteria (Mann and McCallum, 2008).",
        "Experiments show that our approach leads to sequential labeling accuracies superior to the best results reported on this task in both supervised and semi-supervised learning."
      ]
    },
    {
      "heading": "2. Related work",
      "text": [
        "There have been various works that make use of prior knowledge in sequential labeling tasks.",
        "Grenager et al.",
        "(2005) explicitly constrain the transition matrix of a hidden Markov model (HMM) to favor self transitions, assuming that fields tend to consist of consecutive runs of the same label.",
        "Prototype-drive learning (Haghighi and Klein, 2006) specifies prior knowledge by providing a few prototypes (i.e., canonical example words) for each label.",
        "This sparse prototype information is then propagated to other words based on distributional similarity.",
        "The relation between words and their prototypes are then used as features in a Markov random field (MRF) model.",
        "Since an MRF model aims to optimize the joint probability p(x, y) of input and state sequences, it is possible to apply the EM algorithm for unsupervised/semi-supervised learning.",
        "Constraint-driven learning (Chang et al., 2007) expresses several kinds of constraints in a unified form.",
        "In inference, a new decision function is proposed to penalize the violation of the desired constraints as follows,",
        "Here A • F(x, y) is a linear decision function applicable to a number of sequential models, such as HMMs, MRFs and CRFs.",
        "Function d is implemented as the Hamming distance (or its approximation) between a hypothesis sequence and the space of state sequences that satisfy the constraint Ci.",
        "Due to the nature of the distance function, their work approximates EM training by finding the top K hypothesis sequences and using them as newly labeled instances to update the model.",
        "This process is repeated for a number of iterations in a self-training fashion (Yarowsky, 1995).",
        "Generalized expectation criteria (Mann and McCallum, 2008) represent prior knowledge as la beled features, and use such information to regularize semi-supervised learning for CRFs.",
        "Formally, their learning objective consists of the standard CRF training objective, plus a Gaussian prior on model parameters and an additional regularization term:",
        "In the last term, p and p\\ both refer to conditional distributions of labels given a feature.",
        "While the former is specified by prior knowledge, and the latter is estimated from unlabeled data.",
        "Our approach incorporates prior knowledge as virtual evidence to express preferences over the values of a set of random variables.",
        "The notion of VE was first introduced by Pearl (1998) and further developed by Bilmes (2004), both in the context of Bayesian networks.",
        "Different from constraint-driven learning, VE can be formally encoded as part of a graphical model.",
        "The fundamental inference methodology, therefore, does not need to be altered.",
        "Moreover, VE has the flexibility of representing various kinds of prior knowledge.",
        "For example, Reynolds and Bilmes (2005) use VE that explicitly favors self transitions in dynamic Bayesian networks.",
        "This work extends the use of VE to CRFs.",
        "In essence, VE herein can be viewed as probabilistic constraints in an undirected graph that allow exact inference.",
        "One of the biggest challenges of such a model lies in the semi-supervised machine learning setting.",
        "Since the entire state sequence of an unlabeled instance remains hidden, the conditional likelihood objective of CRFs is not directly opti-mizable.",
        "There have been a number of works that address this problem for conditional models.",
        "For example, minimum entropy regularization (Grand-valet and Bengio, 2004; Jiao et al., 2006), aims to maximize the conditional likelihood of labeled data while minimizing the conditional entropy of unlabeled data:",
        "This approach generally would result in \"sharper\" models which can be data-sensitive in practice.",
        "Another approach (Suzuki and Isozaki, 2008) embeds a joint probability model (HMM in their",
        "'We slightly modify the notation here to be consistent with the rest of the paper.",
        "case) into a CRF model as a new potential function.",
        "Semi-supervised learning is then conducted by iteratively (1) fixing the HMM and updating CRF parameters on labeled data and (2) fixing the CRF model and updating the HMM on unlabeled data.",
        "Additionally, when unlabeled instances have partial labeling information, it is possible to optimize a marginal distribution of the conditional likelihood, i.e., p\\(yo^\\x), on unlabeled data.",
        "Here Vo is a subvector of y W that denotes the set of observed state variables.",
        "The optimization can be done in a similar fashion as training a hidden-state CRF model (Quattoni et al., 2007).",
        "We consider the problem of extracting fields from free-text advertisements.",
        "We use the CLASSIFIEDS data (Grenager et al., 2005) which consists of 8767 ads for apartment rental.",
        "302 of the ads in the CLASSIFIEDS data have been manually-labeled with 12 fields, including size, rent, neighborhood and so on.",
        "The labeled data has been divided into train/dev/test sets with 102/100/100 ads respectively.",
        "The evaluation metric is the token-level accuracy where tokens include both words and punctuations.",
        "Our goal in this work is two folds: (1) leverage both the training data and the prior knowledge specified for this task for supervised learning, and (2) additionally use the unlabeled data for semi-supervised learning.",
        "We exploit two types of prior knowledge:",
        "• Kl: label consistency with prototypes;",
        "• K2: label consistency within a sentence.",
        "Kl involves a set of prototype lists.",
        "Each list is attached with a label and consists of a set of example words for that label.",
        "In this work, we use the prototype lists originally defined by Haghighi and Klein (2006) (HK06) and subsequently used by Chang et al.",
        "(2005) (CRR07) and Mann and McCallum (2008) (MM08).",
        "The labels as well as their prototypes are shown in the first two columns of Table 1.",
        "Our model is desired to be consistent with such prototype information.",
        "Secondly, K2 means that tokens tend to have consistent labels within a sentence.",
        "A similar type of prior knowledge is implemented by CRR07 as a constraint in inference."
      ]
    },
    {
      "heading": "4. Conditional Random Fields",
      "text": [
        "Conditional random fields are a probabilistic model that directly optimizes the conditional probability of a state (label) sequence given an input sequence (Lafferty et al., 2001).",
        "Formally, we let x = (x\\, X2, ■ ■ ■, xt) denote an input sequence of T tokens, and y = (yi, y2, ■ ■ ■, Vt) the corresponding state sequence.",
        "We further augment y with two special states, Start and End, represented by yo and yx+i respectively.",
        "A linear-chain CRF model is an undirected graphical model as depicted in Figure 1(a), with the conditional probability given by",
        "The partition function ZA(x) normalizes the exponential form to be a probability distribution, ip^ are a set of potential functions defined on the maximum cliques of the graph, i.e., (x, yt-\\,yt) in the case of a linear-chain CRF model.",
        "The potential functions are typically in the form of where A is a weight vector and / is a feature vector of arbitrary functions of the corresponding clique.",
        "Given a set of labeled examples {x^jW)}™!, we can estimate model parameters in a supervised machine learning setting.",
        "The objective is to estimate A that maximizes the conditional likelihood while regularizing the model size:",
        "In this work, we optimize C\\ using stochastic gradient descent and use the accuracy on the development set as the stopping criterion."
      ]
    },
    {
      "heading": "5. CRFs with Virtual Evidence",
      "text": [
        "A canonical way of using virtual evidence (VE) in Bayesian networks is to have a directed edge from a hidden variable h to a VE variable v. The variable v will always be observed with a particular value, e.g., v = 1, but the actual value itself does not matter.",
        "The prior knowledge about h is",
        "Start yi yi yr End",
        "Figure 1 : Graphical model representations of (a) a CRF model and (b) a CRF model integrated with virtual evidence.",
        "Solid and empty nodes denote observed and hidden variables respectively.",
        "expressed via the conditional probability p(v = l\\h).",
        "For example, by setting p(v = l\\h = a) > p(v = l\\h = b), we know that h = a is more likely a event than h = b.",
        "This conditional distribution is not learned from data, Instead, it is predefined in such a way that reflects a prior belief over the value of h.",
        "VE can be encoded in an undirected graphical model in a similar fashion.",
        "For our task, we modify the structure of a linear-chain CRF model as depicted in Figure 1(b) – we create a sequence of VE variables, denoted by v\\, v2, ■ ■ ■, vt+i, in parallel to the state variables.",
        "Each vt is assigned a constant 1 (one), and is connected with yt-\\ and yt, forming a new set of maximum cliques (yt-i,yt, vt), t = I,..., T + 1.",
        "We create cliques of size 3 because it is the minimum size required to represent the prior knowledge used in our task, as will be discussed shortly.",
        "However, it is possible to have a different graph structure to incorporate other types of prior knowledge, e.g., using large cliques to represent constraints that involve more variables.",
        "Next, in analogy to Equation (5), we define the corresponding potential functions as follows, s is a vector of VE feature functions and uj is the corresponding weight vector with predefined values.",
        "Given the new graphical model in Figure 1(b).",
        "It is natural to model the conditional probability of the state sequence given both the standard evidence and the VE as follows,",
        "Analogous to using p(v = l\\h)'m Bayesian networks, we can utilize (p^(yt-\\,yt, v = 1) to express preferences over state hypotheses in a CRF model.",
        "In general, the function form of (f>^ may or may not depend on the input x.",
        "Even when does depend on x, the relation is completely determined by external knowledge/systems (as opposed to by data).",
        "Thus we do not explicitly connect v with x in the graph.",
        "Now we show how to represent the prior knowledge introduced in Section 3 using the VE feature functions.",
        "Unless otherwise stated, we assume Vt = 1 for all i = 1,..., T and simply use Vt instead of vt = 1 in all equations.",
        "First, we define a VE function s\\ that represents Kl: label consistency with prototypes.",
        "We let Pi denote a prototype list associated with the label I.",
        "If xt belongs to Pi, we should prefer yt = I as opposed to other values.",
        "To this end, for cases where xt £ Pi, we set s i as",
        "On the other hand, if Xt is not a prototype, we will always have s\\(yt,vt,t) = 0 for all hypotheses of yt.",
        "The impact of this prior knowledge is controlled by the weight of s\\, denote by uo\\.",
        "At one extreme where uo\\ = 0, the prior knowledge is completely ignored in training.",
        "At the other extreme where uo\\ – > +oo, we constrain the values of state variables to agree with the prior knowledge.",
        "Note that although s\\ is implicitly related to x, we do not write S\\ as a function of x for consistency with the general definition of VE.",
        "To represent K2: label consistency within a sentence, we define a second VE feature function s2with weight uo2.",
        "Assume that we have an external system that detects sentence boundaries.",
        "If it is determined that xt is not the start of a sentence, we set s2 as",
        "It is easy to see that this would penalize state transitions within a sentence.",
        "On the other hand, if a;* is a sentence start, we set s2(yt-i, yt, vt, t) = 0 for all possible (yt-i,yt) pairs.",
        "In this work, we use a simple heuristics to detect sentence boundaries: we determine that xt is the start of a sentence if its previous token xt-\\ is a period (.",
        "), a semicolon (;) or an acclamation mark (!",
        "), and if xt is not a punctuation.",
        "When a large amount of unlabeled data is available, it is often helpful to leverage such data to improve learning.",
        "However, we cannot directly optimize p(y|x, v) since the correct state sequences of the unlabeled data are hidden.",
        "One heuristic approach is to adapt the self-training algorithm (Yarowsky, 1995) to our model.",
        "More specifically, for each input in the unlabeled dataset |x(t) }^=m+1, we decode the best state sequence,",
        "Then we use {(xW, yw ) }'i=m+l in addition to the labeled data to train a supervised CRF model.",
        "This approach, however, does not have a theoretical guarantee on optimality unless certain nontrivial conditions are satisfied (Abney, 2004).",
        "On the other hand, it is well known that unlabeled data can be naturally incorporated using a generative approach that models a joint probability (Nigam et al., 2000).",
        "This is achieved by maximizing a marginal distribution of the joint probability over hidden variables.",
        "Inspired by the generative approach, we propose to explicitly model p(y,v|x).",
        "In contrast to Equation (8), here we jointly model y and v but the probability is still conditioned on x.",
        "This \"joint\" distribution should be chosen such that it results in the same conditional distribution p(y|x, v) as defined in Equa-",
        "Here Z'x{x) is a normalization function obtained by summing the numerator over both y and v. By applying the Bayes rule, it is easy to see that p(y |x, v) is exactly equal to Equation (8).",
        "Given unlabeled data {xW}\"=m+1, we aim to optimize the following objective,",
        "This is essentially the marginal distribution of p(y,v|x) over hidden variables y.",
        "Here we ignore the labels of the dataset {(xW, y(i))}™ x, but we do use the label information in initializing the model which will described in Section 6.",
        "To optimize such an objective, we apply the EM algorithm in the same fashion as is used in a generative approach.",
        "In other words, we iteratively optimize Q(x) = EyPAf(y|x,v)logpA(y,v|x) where Aadenotes the model estimated from the previous iteration.",
        "The gradient of the Q function is straightforward to compute with the result given by",
        "We keep two sets of accumulators in running the Forward-Backward algorithm, one for computing p\\(yt-i,yt\\x, v) and the other for computing Px(yt-i,yt\\x).",
        "Loosely speaking, the model will converge to a local optimum if the difference between these two posterior probabilities becomes trivial.",
        "Prior knowledge represented by prototypes is typically sparse.",
        "This sparse information, however, can be propagated across all data based on distributional similarity (Haghighi and Klein, 2006).",
        "Following the same idea, we extend the prototype lists as follows.",
        "(1) We merge all prototypes in Pi into a single word type wi.",
        "(2) For each word",
        "Table 1: Field labels (except other) for the CLASSIFIEDS task, their respective prototype lists specified by prior knowledge, and collocation lists mined from unlabeled data.",
        "in the corpus, we collect a context vector of the counts of all words (excluding stop words) that occur within a window of size k in either direction, where the window is applied only within sentence boundaries.",
        "(3) Latent semantic analysis (Deerwester et al., 1990) is performed on the constructed context vectors.",
        "(4) In the resulting latent semantic space, all words (except stop words) that have a high enough dot product with wi will be grouped to form a new set, denoted as Ci, which is a superset of Pi.",
        "In this regard, Ci can be viewed as lists of noisy \"prototypes\".",
        "As observed in HK06, another consequence of this method is that many neighboring tokens will share the same prototypes.",
        "Differently from previous works, we use Ci directly as virtual evidence.",
        "We could apply s i in Equation (9) when xt G Ci (as opposed to when xt G Pi).",
        "This, however, would contaminate our model since Ci are often noisy.",
        "For example, \"water\" is found to be distribu-tionally similar to the prototypes of utilities.",
        "Although in most cases \"water\" indeed means utilities, it can mean features in the context of \"water front view\".",
        "To maximally reduce ambiguity, we propose to apply S\\ in Equation (9) if both of the following conditions hold,",
        "(2) There exists r s.t.",
        "|r – t\\ < k, and xT G Ci In other words, we will impose a non-uniform prior on yt if xt G Ci \"collocates\", within k tokens, with another word that belongs to Cj.",
        "Based on K2, it is reasonable to believe that neighboring tokens tend to share the same label.",
        "Therefore, knowing that two tokens close to each other both belong to Ci would strengthen our belief that either word is likely to have label I.",
        "We thus refer to this type of virtual evidence as collocation-based VE, and refer to Ci as collocation lists."
      ]
    },
    {
      "heading": "6. Evaluation",
      "text": [
        "We use the CLASSIFIEDS data provided by Grenager et al.",
        "(2005) and compare with results reported by CRR07 (Chang et al., 2007) and MM08 (Mann and McCallum, 2008) for both supervised and semi-supervised learning.",
        "Following all previous works conducted on this task, we tokenized both words and punctuations, and created a number of regular expression tokens for phone numbers, email addresses, URLs, dates, money amounts and so on.",
        "However, we did not tokenize newline breaks, as CRR07 did, which might be useful in determining sentence boundaries.",
        "Based on such tokenization, we extract n-grams, n = 1,2,3, from the corpus as features for CRFs.",
        "As described in Section 3, we integrate the prior knowledge Kl and K2 in our CRF model.",
        "The prototypes that represent K1 are given in Table 1.",
        "CRR07 used the same two kinds of prior knowledge in the form of constraints, and they implemented another constraint on the minimum number of words in a field chunk.",
        "MM08 used almost the same set of prototypes as labeled features, but they exploited two sets of 33 additional features for some experiments.",
        "In this regard, the comparison between CRR07, MM08 and the method presented here cannot be exact.",
        "However, we show that while our prior knowledge is no more than that used in previous works, our approach is able",
        "Label",
        "Prototype lists of HK06",
        "Collocation lists (top examples)",
        "ADDRESS",
        "address carlmont",
        "[4-digit] street [3-digit] streets",
        "AVAILABLE",
        "immediately begin cheaper",
        "available",
        "CONTACT",
        "[phone] call [time]",
        "[email] appointment email see today ...",
        "FEATURES",
        "kitchen laundry parking",
        "room new covered building garage ...",
        "NEIGHBORHOOD",
        "close near shopping",
        "transportation center located restaurants ...",
        "PHOTOS",
        "pictures image link",
        "[url] click view photos",
        "RENT",
        "$ month [amount]",
        "lease deposit security year agreement...",
        "RESTRICTIONS",
        "pets smoking dog",
        "ok sorry please allowed negotiable ...",
        "ROOMMATES",
        "roommate respectful drama",
        "SIZE",
        "[1-digit] br sq",
        "[4-digit] [3-digit] ft bath ba ...",
        "UTILITIES",
        "utilities pays electricity",
        "water included owner garbage paid",
        "Table 2: Token-level accuracy of supervised learning methods; \"+ VE\" refers to the cases where both kinds of prior knowledge, Kl and K2, are incorporated as VE in the CRF model.",
        "to achieve the state-of-art performance.",
        "Depending on whether VE is used at test time, we explore two decoding settings in all experiments:",
        "1.",
        "Find y that maximizes p\\ (y | x) as in standard CRF decoding, ignoring virtual evidence.",
        "2.",
        "Find y that maximizes p(y |x, v).",
        "We use \"+ VE in decoding\" to represent this setting.",
        "These two scenarios are analogous to those in CRR07 which conducted HMM decoding without/with constraints applied.",
        "We use \"+ constr.",
        "in decoding\" to represent the latter scenario of their work.",
        "MM08, on the other hand, found no accuracy improvement when adding constraints at test time.",
        "Note that in our second decoding setting, the weights for the VE feature functions, i.e., uo\\ and üü2, are tuned on the development set.",
        "This is done by a greedy search that first finds the best uo\\, and then finds the best W2 while fixing the value of uo\\, both with a step size 0.5.",
        "First, we experimented with a standard CRF model with VE applied neither in training nor in decoding.",
        "As shown in Table 2, our CRF implementation performed slightly worse than the implementation by MM08, probably due to slight difference in tokenization.",
        "Secondly, we used the same CRF model but additionally applied VE in decoding, corresponding to the second setting in Section 6.1.",
        "This method gave a significant boost to the tagging performance, yielding the best supervised learning results (shown as bolded in the",
        "Table 3 : Token-level accuracy of semi-supervised learning methods.",
        "\"+ Col-VE\" refers to cases where collocation-based VE is integrated in the CRF model in addition to the VE representing K1 and K2.",
        "table).",
        "This proves that the prior knowledge is indeed complementary to the information offered by the training data.",
        "Similar to the second decoding setting that incorporates VE, we can have a counterpart setting at training time.",
        "In other words, we can optimize p\\(y\\x., v) instead oîp\\(y\\x) during learning.",
        "In deciding uj = (wi,W2), it is possible to learn uj from data in the same way as how we learn A.",
        "This, however, might undermine the role of other useful features since we do not always have sufficient training data to reliably estimate the weight of prior knowledge.",
        "As shown in Table 2, we experimented with learning uj automatically (shown as \"auto weights\").",
        "While applying VE with such weights in both training and decoding worked reasonably well, applying VE only in training but not in decoding yielded very poor performance (probably due to excessively large estimates of uo\\ and LO2).",
        "Additionally, we repeated the above experiment with manually specified weights, but did not find further accuracy improvement over the best supervised learning results.",
        "One natural way of leveraging the unlabeled data (more than 8K examples) is to perform semi-supervised learning in a self-training fashion.",
        "To this end, we used our best supervised model in Table 2 to decode the unlabeled examples as well as the test-set examples (by treating them as unlabeled).",
        "Note that by doing this our comparison with CRR07 and MM08 cannot be exact as they sampled the unlabeled examples, with different rates, for semi-supervised learning, while we used as much data as possible.",
        "We applied the same uj that was used for the supervised model, and then combined the newly labeled examples, in addition to the manually labeled ones, as training data to learn a supervised CRF model.",
        "On this particular dataset, we did not find it helpful by selecting automatically labeled data based on a confidence threshold.",
        "We simply used all data available in self-training.",
        "This paradigm is referred to as \"CRF + VE (self-train)\" in Table 3.",
        "When no VE is applied at test time, this semi-supervised CRF model significantly outperformed the best model in Table 2.",
        "When applying VE at test time, however, the improvement over its supervised counterpart became trivial.",
        "Supervised model",
        "#lab<",
        "10",
        ";led ex",
        "25",
        "amples",
        "100",
        "CRR07: HMM",
        "+ Constr in decoding",
        "61.6 66.1",
        "70.0 73.7",
        "76.3 80.4",
        "MM08: CRF",
        "64.6",
        "72.9",
        "79.4",
        "CRF",
        "+ VE in decoding",
        "62.3 68.9",
        "71.4 74.6",
        "79.1 81.1",
        "CRF + VE (auto weights) + VE in decoding",
        "48.0 66.0",
        "54.8 72.5",
        "59.8 80.9",
        "Semi-supervised models",
        "#lab<",
        "10",
        ";led ex",
        "25",
        "amples",
        "100",
        "CRR07: HMM + Constr + Constr in decoding",
        "70.9 74.7",
        "74.8 78.5",
        "78.6 81.7",
        "MM08: CRF + GE",
        "72.6",
        "76.3",
        "80.1",
        "CRF + VE (Self-train) + VE in decoding",
        "69.0 69.1",
        "74.2 75.2",
        "81.4 81.2",
        "CRF + Col-VE (Self-train) + Col-VE in decoding",
        "73.1 75.7",
        "76.4 77.6",
        "81.8 82.9",
        "CRF + Col-VE (EM) + Col-VE in decoding",
        "78.3 78.8",
        "79.1 79.5",
        "82.7 82.9",
        "Next, following Section 5.3, we collected context vectors on the unlabeled data using a window size k = 3, and extracted the top 50 singular vectors therefrom.",
        "We created collocation lists that contain words close to the merged prototype words in the latent semantic space.",
        "Some examples are given in the last column of Table 1.",
        "We then augmented the prototype-based VE based on the following rules: lfxt belongs to any prototype list Pi, we directly apply S\\ in Equation (9); otherwise, we apply s\\ if xt and at least one neighbor (within 3 tokens from xt) belong to the same collocation list Ci.",
        "In our experiments, we let \"Col-VE\" represent such collocation-based VE.",
        "We conducted self-training using a CRF model integrated with Col-VE, where uj was tuned a priori by testing the same model on the development set.",
        "As shown in the table, \"CRF + Col-VE (self-train)\" gave significant accuracy improvement over \"CRF + VE\", while adding Col-VE at test time further boosted the performance.",
        "The accuracies were already on par with the best results previously reported on this task.",
        "Finally, we implemented the EM algorithm proposed in Section 5.2 that iteratively optimizes p(v|x) on all data.",
        "The model was initialized by the one obtained from \"CRF + Col-VE (self-train)\".",
        "After the model was initialized, we performed the EM algorithm until the model reached a maximum accuracy on the development set.",
        "Note that in some cases, we observed a development-set accuracy degradation after the first iteration of the EM, but the accuracy quickly recovered from the second iteration and kept increasing until a maximum accuracy was reached.As shown in the last two rows in Table 3, this method is clearly advantageous over self-training, leading to the best tagging accuracies in both decoding settings.",
        "Our model achieved 2.6% – 5.7% absolute accuracy increases in the three training settings compared with MM08 which had the best results without using any constraints in decoding.",
        "When applying VE at test time, our model was 1.2% - 4.1% better than CRR07 which had the best overall results.",
        "Additionally, when compared with supervised learning results, our best semi-supervised model trained on only 10 labeled examples performed almost as well as a standard supervised CRF model trained on 100 labeled examples."
      ]
    },
    {
      "heading": "7. Conclusions",
      "text": [
        "We have presented the use of virtual evidence as a principled way of incorporating prior knowledge into conditional random fields.",
        "A key contribution of our work is the introduction of a novel semi-supervised learning objective for training a CRF model integrated with VE.",
        "We also found it useful to create so-called collocation-based VE, assuming that tokens close to each other tend to have consistent labels.",
        "Our evaluation on the CLASSIFIEDS data showed that the learning objective presented here, combined with the use of collocation-based VE, yielded remarkably good accuracy performance.",
        "In the future, we would like to see the application of our approach to other tasks such as (Li et al., 2009)."
      ]
    }
  ]
}
