{
  "info": {
    "authors": [
      "Joohyun Kim",
      "Raymond Mooney"
    ],
    "book": "EMNLP",
    "id": "acl-D12-1040",
    "title": "Unsupervised PCFG Induction for Grounded Language Learning with Highly Ambiguous Supervision",
    "url": "https://aclweb.org/anthology/D12-1040",
    "year": 2012
  },
  "references": [
    "acl-C10-2062",
    "acl-D07-1071",
    "acl-D08-1082",
    "acl-D11-1131",
    "acl-P06-1115",
    "acl-P07-1121",
    "acl-P09-1010",
    "acl-P09-1110",
    "acl-P10-1083"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "?Grounded?",
        "language learning employs training data in the form of sentences paired with relevant but ambiguous perceptual contexts.",
        "Bo?rschinger et al2011) introduced an approach to grounded language learning based on unsupervised PCFG induction.",
        "Their approach works well when each sentence potentially refers to one of a small set of possible meanings, such as in the sportscasting task.",
        "However, it does not scale to problems with a large set of potential meanings for each sentence, such as the navigation instruction following task studied by Chen and Mooney (2011).",
        "This paper presents an enhancement of the PCFG approach that scales to such problems with highly-ambiguous supervision.",
        "Experimental results on the navigation task demonstrates the effectiveness of our approach."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The ultimate goal of ?grounded?",
        "language learning is to develop computational systems that can acquire language more like a human child.",
        "Given only supervision in the form of sentences paired with relevant but ambiguous perceptual contexts, a system should learn to interpret and/or generate language describing situations and events in the world.",
        "For example, systems have learned to commentate simulated robot soccer games by learning from sample sportscasts (Chen and Mooney, 2008; Liang et al. 2009; Bo?rschinger et al2011), or understand navigation instructions by learning from action traces produced when following the directions (Chen and Mooney, 2011; Tellex et al2011).",
        "Bo?rschinger et al2011) recently introduced an approach to grounded language learning using unsupervised induction of probabilistic context free grammars (PCFGs) to learn from ambiguous contextual supervision.",
        "Their approach first constructs a large set of production rules from sentences paired with descriptions of their ambiguous context, and then trains the parameters of this grammar using EM.",
        "Parsing a novel sentence with this grammar gives a parse tree which contains the formal meaning representation (MR) for this sentence.",
        "This approach works quite well on the sportscasting task originally introduced by Chen and Mooney (2008).",
        "In this task, each sentence in a natural-language commentary describing activity in a simulated robot soccer game is paired with the small set of actions observed within the past 5 seconds, one of which is usually described by the sentence.",
        "Even with this low level of ambiguity in a constrained domain, their method constructs a PCFG with about 33,000 productions.",
        "More fundamentally, their approach is restricted to a finite set of potential meaning representations, and the grammar size grows at least linearly with the number of possible MRs, which in turn is inevitably exponential in the number of objects and actions in the domain.",
        "The navigation task studied by Chen and Mooney (2011) provides much more ambiguous supervision.",
        "In this task, each instructional sentence is paired with a formal landmarks plan (represented as a large graph) that includes a full description of the observed actions and world-states that result when",
        "someone follows this instruction.",
        "An instruction generally refers to a subgraph of this large graph.",
        "Therefore, there are a combinatorial number of possible meanings to which a given sentence can refer.",
        "Chen and Mooney (2011) circumvent this combinatorial problem by never explicitly enumerating the exponential number of potential meanings for each sentence.",
        "Their system first induces a semantic lexicon that maps words and short phrases to formal representations of actions and objects in the world.",
        "This lexicon is learned by finding words and phrases whose occurrence highly correlates with specific observed actions and objects in the simulated environment when executing the corresponding instruction.",
        "This learned lexicon is then used to directly infer a formal MR for observed instructional sentences using a greedy covering algorithm.",
        "These inferred MRs are then used to train a supervised semantic parser capable of mapping novel sentences to their formal meanings.",
        "We present a novel enhancement of Bo?rschinger et al. PCFG approach that uses Chen and Mooney's lexicon learner to avoid a combinatorial explosion in the number of productions.",
        "The learned lexicon is first used to build a hierarchy of semantic lexemes (i.e. lexicon entries) called the Lexeme Hierarchy Graph (LHG) for each ambiguous landmarks plan in the training data.",
        "The intuition behind utilizing an LHG is that the MR for each lexeme constitutes a semantic concept that corresponds to some natural-language (NL) word or phrase.",
        "Therefore, the LHG represents how complex semantic concepts are composed of simpler semantic concepts and ultimately connected to NL words and phrases.",
        "Bo?rschinger et al. approach instead produces NL groundings at the level of atomic MR constituents, which causes an explosion in the number of PCFG productions for complex MR languages.",
        "We estimated that Bo?rschinger et al. approach would require more than 20!",
        "(> 1018) productions for our navigation problem.1 On the other hand, our method, which uses correspondences from the LHG at the semantic concept level, constructs a more focused PCFG of tractable size.",
        "It then extracts the MR for a novel 1The corpus contains quite a few examples with landmarks plans containing more than 20 actions.",
        "This results in at least 20!",
        "permutations representing possible alignments between actions and NL words.",
        "sentence from the most-probable parse tree for the resulting PCFG.",
        "Our approach can produce a large, combinatorial number of different MRs for a wide range of novel sentences by composing relevant MR components from the resulting parse tree, whereas Bo?rschinger et al. approach is only able to output MRs that are explicitly included as a nonterminals in the original learned PCFG.",
        "The remainder of the paper is organized as follows.",
        "Section 2 reviews Bo?rschinger et al. PCFG approach as well as the navigation task and data.",
        "Section 3 describes our enhanced PCFG approach and Section 4 presents an experimental evaluation of it.",
        "Then, Section 5 discusses the unique aspects of our approach and Section 6 describes additional related work.",
        "Finally, Section 7 presents future research directions and Section 8 gives our conclusions."
      ]
    },
    {
      "heading": "2 Background",
      "text": []
    },
    {
      "heading": "2.1 Existing PCFG Approach",
      "text": [
        "Our approach extends that of Bo?rschinger et al. (2011), which in turn was inspired by a series of previous techniques (Lu et al2008; Liang et al. 2009; Kim and Mooney, 2010) following the idea of constructing correspondences between NL and MR in a single probabilistic generative framework.",
        "Particularly, their approach automatically constructs a PCFG that generates NL sentences from MRs, which indicates how atomic MR constituents are probabilistically related to NL words.",
        "The nonterminals in the grammar correspond to complete MRs, MR constituents, and NL phrases.",
        "The nontermi-nal for a composite MR generates each of its MR constituents, and each atomic MR, x, generates an NL phrase, Phrasex.",
        "Each Phrasex then generates a sequence of Wordx's for describing x, and each Wordx can generate each possible word in the natural language.",
        "This allows the system to learn the words and phrases used to describe each atomic MR by properly weighting these rules.",
        "Figure 1 shows one possible derivation tree for a sample NL-MR pair and the PCFG rules that are constructed for it.",
        "Once a set of productions are assembled, their probabilities are learned using the Inside-Outside algorithm.",
        "Computing the most probable parse for a novel sentence with the trained PCFG provides its"
      ]
    },
    {
      "heading": "PINK GOALIE PASSES THE BALL TO PINK11 /",
      "text": [
        "pass(pink1, pink11).",
        "Left side shows PCFG rules that are added for each stage (full MR to atomic MRs, and atomic MRs to NL words ).",
        "preferred MR interpretation in the topmost nonter-minal.",
        "Unfortunately, as discussed earlier, this approach only works for finite MR languages, and the grammar becomes intractably large even for finite but complex MRs.",
        "It effectively assumes that MRs are fairly small and includes every possible MR constituent as a nonterminal in the PCFG.",
        "This is not tractable for more complex MRs.",
        "Therefore, our extension incorporates a learned lexicon to constrain the space of productions, thereby making the size of the PCFG tractable for complex MRs, and even giving it the ability to handle infinite MR languages.",
        "Moreover, when processing novel sentences, our approach can produce a large space of novel MRs that were not anticipated during training, which is not the case for Bo?rschinger et al. approach."
      ]
    },
    {
      "heading": "2.2 Navigation Task and Dataset",
      "text": [
        "We employ the task and data introduced by Chen and Mooney (2011) whose goal is to interpret and follow NL navigation instructions in a virtual world.",
        "Figure 2 shows a sample execution path in a particular virtual world.",
        "The challenge is learning to perform this task by simply observing humans following instructions.",
        "Formally, given training data of the form {(e1, a1, w1), .",
        ".",
        ".",
        ", (en, an, wn)}, where ei is an NL instruction, ai is an observed action sequence, and wi is the current world state (patterns of floors and walls, positions of any objects, etc.",
        "), we want to produce the correct actions aj for a novel (ej , wj).",
        "In order to learn, their system infers the intended formal plan pi (the MR for a sentence) which produced the action sequence ai from the instruction ei.",
        "However, there is a large space of possible plans for any given action sequence.",
        "Chen and Mooney first construct a formal landmarks plan, ci, for each ai, which is a graph representing the context of every action and the world-state encountered during the execution of the sequence.",
        "The correct plan MR, pi, is assumed to be a subgraph of ci, and this causes a combinatorial matching problem between ei and ci in order to learn the correct meaning of ei among all the possible subgraphs of ci.",
        "The landmarks and correct plans for a sample instruction are shown in Figure 3, illustrating the complexity of the MRs.",
        "Instead of directly solving the combinatorial correspondence problem, they first learn a semantic lex",
        "(2011)'s system.",
        "Our method replaces the plan refinement and semantic parser parts.",
        "icon that maps words and short phrases to small subgraphs representing their inferred meanings from the (ei, ci) pairs.",
        "The lexicon is learned by evaluating pairs of n-grams, wj , and MR graphs, mj , and scoring them based on how much more likely mj is a subgraph of the context ci when w occurs in the corresponding instruction ei.",
        "This process is similar to other ?cross-situational?",
        "approaches to learning word meanings (Siskind, 1996; Thompson and Mooney, 2003).",
        "Then, a plan refinement step estimates pi from ci by greedily selecting high-scoring lexemes of the form (wj ,mj) whose words and phrases (wj) cover the instruction ei and introduce components (mj) from the landmarks plan ci.",
        "The refined plans are used to construct supervised training data (ei, pi) for a supervised semantic-parser learner.",
        "The trained semantic parser can parse a novel instruction into a formal plan, which is finally executed for end-to-end evaluation.",
        "Figure 4 illustrates the overall system.",
        "As this figure indicates, our new PCFG method replaces the plan refinement and semantic parser components in their system with a unified model that both disambiguates the training data and learns a semantic parser.",
        "We use the landmarks plans and the learned lexicon produced by Chen and Mooney (2011) as inputs to our system.2"
      ]
    },
    {
      "heading": "3 Our PCFG Approach",
      "text": [
        "Like Bo?rschinger et al2011), our approach learns a semantic parser directly from ambiguous supervision, specifically NL instructions paired with their complete landmarks plans as context.",
        "Our method incorporates the semantic lexemes as building blocks to find correspondences between NL words and semantic concepts represented by the lexeme MRs, instead of building connections between NL words and every possible MR constituent as in Bo?rschinger et al. approach.",
        "Particularly, we utilize the hierarchical subgraph relationships between the MRs in the learned semantic lexicon to produce a smaller, more focused set of PCFG rules.",
        "The intuition behind our approach is analogous to the hierarchical relations between nonterminals in syntactic parsing, where higher-level categories such as S, VP, or NP are further divided into smaller categories such as V, N, or Det, thereby forming a hierarchical structure.",
        "Inspired by this idea, we introduce a directed acyclic graph called the Lexeme Hierarchy Graph (LHG) which represents the hierarchical relationships between lexeme MRs.",
        "Since complex lexeme MRs represent complicated semantic concepts while simple MRs represent simple concepts, it is natural to construct a hierarchy amongst them.",
        "The LHGs for all of the training examples are used to construct production rules for the PCFG, which are then parametrized using EM.",
        "Finally, a novel sentence is semantically parsed by computing its most-probable parse using the trained PCFG, and then its MR is extracted from the resulting parse tree."
      ]
    },
    {
      "heading": "3.1 Constructing a Lexeme Hierarchy Graph",
      "text": [
        "An LHG represents the hierarchy of lexical meanings relevant to a particular training instance by encoding the subgraph relations between the MRs of relevant lexemes.",
        "Algorithm 1 describes how an LHG is constructed for an ambiguous training pair of a sentence and its corresponding context, (ei, ci).",
        "First, we obtain all relevant lexemes (wij ,m i j) in the lexicon L, where the MR mij is a subgraph of the context ci (denoted as mij ?",
        "ci).",
        "These lexemes are 3The total number of PCFG rules constructed for our navigation training sets is about 18,000, while Bo?rschinger et al. method produces 33,000 rules for the much simpler sportscasting domain.",
        "Algorithm 1 LEXEME HIERARCHY GRAPH (LHG) Input: Training instance (ei, ci), Lexicon L Output: Lexeme hierarchy graph for (ei, ci) Find relevant lexemes (wi1,m",
        "s.t.",
        "mij ?",
        "ci Create a starting node T ; MR(T )?",
        "ci for all mij in the descending order of size do Create a node T ij ; MR(T",
        "if T ?",
        "was not placed under any child Tj then Add T ?",
        "as child of T",
        "sorted in descending order based on the number of nodes in their MRs mij .",
        "Then, after setting the context ci as the MR of the root node (MR(T ) ?",
        "ci), lexemes are inserted, in order, into the graph to create a hierarchy of MRs, where each child's MR is a subgraph of the MR of each of its parents.",
        "Figure 5 illustrates a sample construction of an LHG for the following landmarks plan (ci):",
        "The initial LHG may contain nodes with too many children.",
        "This is a problem, because when we subsequently extract PCFG rules, we need to add a production for every k-permutation of the children of each node (see Section 3.2).",
        "To reduce the branching factor in the LHG, we introduce pseudo-lexeme nodes by repeatedly combining the two most similar children of each node.",
        "Pseudocode for the process is shown in Algorithm 2.",
        "The MR for a pseudo-lexeme is the minimal graph, m?, that is a supergraph of both of the lexeme MRs that it combines.",
        "The pair of",
        "(a) All relevant lexemes are obtained for the training example and ordered by the number of nodes in their MR. (b) Lexeme MR [1] is added as a child of the top node.",
        "MR [2] is a subgraph of [1], so it is added as its child.",
        "(c) MR [3] is not a subgraph of [1] or [2], so it is added as a child of the root.",
        "MR [4] is added under [3], and MR [5] is recursively filtered down and added under [2].",
        "until There are no more pairs to combine for all non-leaf children Tk of T do",
        "most similar children, (mi,mj), is determined by measuring the fraction of the nodes in mi and mj that overlap with their minimum extension m?",
        "and is calculated as follows:",
        "where |m |is the number of nodes in the MR m. Adding pseudo-lexemes also has another advantage.",
        "They can be considered to be higher-level semantic concepts composed of two or more sub-concepts.",
        "These higher-level concepts will likely occur in other training examples as well, which allows for more flexible interpretations.",
        "For example, assuming the rule A ?",
        "BCD is constructed from an LHG, we will introduce a pseudo lexeme E and build two rules A?",
        "BE and E ?",
        "CD.",
        "It is likely that E also occurs in another rule constructed from other training examples such as E ?",
        "FG.",
        "This increases the model's expressive power by supporting additional derivations such as A??",
        "BFG, providing more flexibility when parsing novel NL sentences."
      ]
    },
    {
      "heading": "3.2 Composing PCFG Rules",
      "text": [
        "The next step composes PCFG rules from the LHGs and is summarized in Figure 6.",
        "We basically follow the scheme of Bo?rschinger et al2011), but instead of generating NL words from each atomic MR, words are generated from each lexeme MR,",
        "NLs refer to the set of NL words in the corpus.",
        "Lexeme rules come from the schemata of Bo?rschinger et al2011), and allow every lexeme MR to generate one or more NL words.",
        "Note that pseudo-lexeme nodes do not produce NL words.",
        "and smaller lexeme MRs are generated from more complex ones as given by the LHGs.",
        "A nonterminal Sm is generated for the MR, m, of each LHG node.",
        "Then, for every LHG node, T , with MR, m, we add rules of the form Sm ?",
        "Smi ...Smj , where the RHS is some k-permutation of the nonterminals for the MRs of the children of node T .",
        "Bo?rschinger et al. assume that every atomic MR generates at least one NL word.",
        "However, since we do not know which subgraph of the overall context (i.e. ci, the MR of the root node) conveys the intended plan and is therefore expressed in the NL instruction, we must allow each ordered subset of the children of a node (i.e. each k-permutation) to be a possible generation.",
        "The rest of the process more closely follows Bo?rschinger et al.",
        "Every MR, m, of a lexeme node4 generates a rule Sm ?",
        "Phrasem, and every Phrasem generates a sequence of NL words, including one or more ?content words?",
        "(Wordm) for expressing m and zero or more ?extraneous?",
        "words (Word?).",
        "While Bo?rschinger et alave Wordm generate all possible NL words (each of which are",
        "subsequently weighted by EM training), in our approach, each Wordm only produces the NL phrase associated with m in the lexicon, or individual words that appear in this phrase.",
        "The words not covered by Wordm also can be generated by Word?",
        "which has rules for every word.",
        "Phm and PhXm ensure that Phrasem produces at least one Wordm, where PhXm indicates that one or more Wordm's have already been generated, and Phm indicates that no Wordm has yet been generated."
      ]
    },
    {
      "heading": "3.3 Parsing Novel NL Sentences",
      "text": [
        "To learn the parameters of the resulting PCFG, we use the Inside-Outside algorithm.",
        "Then, the standard probabilistic CKY algorithm is used to produce the most probable parse for novel NL sentences (Ju-rafsky and Martin, 2000).",
        "Bo?rschinger et al2011) simply read the MR, m, for a sentence off the top Sm nonterminal of the most probable parse tree.",
        "However, in our approach, the correct MR is constructed by properly composing the appropriate subset of lexeme MRs from the most-probable parse tree.",
        "This allows the system to produce a wide variety of novel MRs for novel sentences, as long as the correct MR is a subgraph of the complete context (ci) for at least one of the training sentences.",
        "First, the parse tree is pruned to remove all sub-trees starting with Phrasex nodes.",
        "This leaves a tree consisting of the Root and a set of Sm nodes.",
        "The pruned subtrees only concern generating NL words and phrases from the selected MRs.",
        "The remaining tree shows which MR constituents were selected from the available context, from which the sentence is then generated.",
        "Each leaf in the pruned tree represents an MR constituent that was used to generate a phrase in the sentence.",
        "These are the constituents we want to assemble and compose into a final MR for the sentence.",
        "Algorithm 3 describes the procedure for extracting the final MR from the pruned parse tree.",
        "Figure 7 graphically depicts a sample trace of this algorithm.",
        "The algorithm recursively traverses the parse tree.",
        "When a leaf-node is reached, it marks all of the nodes in its MR. After traversing all of its children,",
        "Algorithm 3 CONSTRUCT PARSED MR RESULT Input: Parse tree T for input NL, e, with all Phrasex subtrees removed.",
        "Output: Semantic parse MR, m, for e",
        "for all children Ti of T do mi ?",
        "OBTAINPARSEDOUTPUT(Ti) Mark the nodes in MR(T ) corresponding to the marked nodes in mi end for if T is not the root then",
        "a node in the MR for the current parse-tree node is marked iff its corresponding node in any of the children's MRs were marked.",
        "The final output is the MR constructed by removing all of the unmarked nodes from the MR for the root node."
      ]
    },
    {
      "heading": "4 Experimental Evaluation",
      "text": [
        "For evaluation, we used the same data and methodology as Chen and Mooney (2011).",
        "Please see their paper for more details."
      ]
    },
    {
      "heading": "4.1 Data",
      "text": [
        "We used the English instructions and follower data collected by MacMahon et al2006).6 This data contains 706 route instructions for three virtual worlds.",
        "The instructions were produced by six instructors for 126 unique starting and ending location pairs spread evenly across the three worlds, and there were 1 to 15 human followers for each instruction who executed an average of 10.4 actions per instruction.",
        "Each instruction is a paragraph consisting of an average of 5.0 sentences, each containing an average of 7.8 words.",
        "Chen and Mooney constructed the additional single-sentence corpus by matching each sentence with the majority of human",
        "(a) Pruned parse tree showing only MRs for Sm nodes (b) Leaf nodes have all their elements marked (c) Upper level nodes are marked according to leaf-node markings (d) Removing all unmarked elements for the root node leads to the final MR output",
        "followers?",
        "actions.",
        "We use this single-sentence version for training, but use both the single-sentence and the original paragraph version for testing.",
        "Each sentence was manually annotated with a ?gold standard?",
        "execution plan, which is used for evaluation but not for training."
      ]
    },
    {
      "heading": "4.2 Methodology and Results",
      "text": [
        "Experiments were conducted using ?leave one environment out?",
        "cross-validation, training on two environments and testing on the third, averaging over all three test environments.",
        "We perform direct comparison to the best results of Chen and Mooney (2011) (referred to as CM).",
        "A Wilcoxon signed-rank test is performed for statistical significance, and ???",
        "denotes significant differences (p < .01) in the tables."
      ]
    },
    {
      "heading": "Semantic Parsing Results",
      "text": [
        "We first evaluated how well our system learns to map novel NL sentences for new test environments into their correct MRs.",
        "Partial semantic-parsing accuracy (Chen and Mooney, 2011) is calculated by",
        "???",
        "denotes difference is statistically significant.",
        "comparing the system's MR output to the hand-annotated gold standard.",
        "Accuracy is measured in terms of precision, recall, and F1 for individual MR constituents (thereby awarding partial credit for approximately correct MRs).",
        "Table 1 demonstrates that our method outperforms CM by 6 points in F1.",
        "Our PCFG-based approach is able to probabilistically disambiguate the training data as well as simultaneously learn a statistical semantic parser within a single framework.",
        "This results in better overall performance compared to CM, since they lose potentially useful information, particularly during the refinement stage, due to the separate disjoint components of the system."
      ]
    },
    {
      "heading": "Navigation Plan Execution Results",
      "text": [
        "Next, we test the end-to-end system by executing the parsed navigation plans for test instructions in novel environments to see if they reach the exact desired destinations in the environment.",
        "Table 2 shows the successful end-to-end navigation-task completion rate for both single-sentences and complete paragraph instructions.",
        "Again, our system outperforms CM's best results since more accurate semantic parsing produces more successful plans.",
        "However, the difference in performance is smaller than that observed for semantic parsing.",
        "This is because the redundancy in the human generated instructions allows an incorrect semantic parse to be successful, as long as the errors do not affect its ability to guide the system to the correct destination."
      ]
    },
    {
      "heading": "5 Discussion",
      "text": [
        "Our approach improves on Bo?rschinger et al. (2011)'s method in the following ways: ?",
        "The building blocks for associating NL and MR are semantic lexemes instead of atomic MR constituents.",
        "This prevents the number of constructed PCFG rules from becoming intractably large as happens with Bo?rschinger et al. approach.",
        "As previously mentioned, lexeme MRs are intuitively analogous to syntactic categories in that complex lexeme MRs represent complicated semantic concepts whereas higher-level syntactic categories such as S, VP, or NP represent complex syntactic structures.",
        "?",
        "Our approach has the ability to produce previously unseen MRs, whereas Bo?rschinger et alan only generate an MR if it is explicitly included in the PCFG rules constructed from the training data.",
        "Even though our MR parse is restricted to be a subgraph of some training context, ci, our model allows for exponentially many combinations.",
        "In addition, our approach can produce a wider range of MR outputs than Chen and Mooney (2011)'s even though we use their semantic lexicon as input.",
        "Their system deterministically builds a supervised training set by greedily selecting high-scoring lexemes, thus implicitly including only high-scoring lexemes during training.",
        "On the other hand, our probabilistic approach also considers relatively low-scoring but useful lexemes, thereby utilizing more semantic concepts in the lexicon.",
        "In particular, this explains why our approach obtains higher recall in the evaluation of semantic parsing.",
        "Even though we have demonstrated our approach on the specific task of following navigation instructions, it is straightforward to apply it to other language-grounding tasks where NL sentences potentially refer to some subset of states, events, or actions in the world, as long as this overall context can be represented as a semantic graph or logical form.",
        "Since the semantic lexicon is an input to our system, other approaches to lexicon learning are also easily incorporated."
      ]
    },
    {
      "heading": "6 Related Work",
      "text": [
        "Most work on learning semantic parsers that map natural-language sentences to formal representations of their meaning have relied upon totally supervised training data consisting of NL/MR pairs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Kate and Mooney, 2006; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Lu et al. 2008; Zettlemoyer and Collins, 2009).",
        "Several recent approaches have investigated grounded learning from ambiguous supervision extracted from perceptual context.",
        "A number of approaches (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al. 2010; Kim and Mooney, 2010; Bo?rschinger et al. 2011) assume training data consisting of a set of sentences each associated with a small set of MRs, one of which is usually the correct meaning of the sentence.",
        "Many of these approaches (Kate and Mooney, 2007; Chen and Mooney, 2008; Chen et al2010) disambiguate the data and match NL sentences to their correct MR by iteratively retraining a supervised semantic parser.",
        "Kim and Mooney (2010) proposed a generative semantic parsing model that first chooses which MRs to describe and then generates a hybrid tree structure (Lu et al2008) containing both the MR and NL sentence.",
        "They train",
        "this model on ambiguous data using EM.",
        "As previously discussed, Bo?rschinger et al2011) use a PCFG generative model and also train it on ambiguous data using EM.",
        "Liang et al2009) assume each sentence maps to one or more semantic records (i.e. MRs) and trains a hierarchical semi-Markov generative model using EM, and then finds a Viterbi alignment between NL words and records and their constituents.",
        "Several recent projects (Branavan et al. 2009; Vogel and Jurafsky, 2010) use NL instructions to guide reinforcement learning from independent exploration with delayed rewards.",
        "These systems do not even need the ambiguous supervision obtained from observing humans follow instructions; however, they do not learn semantic parsers that map sentences to complex, structural representations of their meaning.",
        "Interpreting and executing NL navigation instructions is our primary task, and several other recent projects have studied related problems.",
        "Shimizu and Haas (2009) present a system that parses natural language instructions into actions.",
        "However, they limit the number of possible actions to only 15 and treat the problem as a sequence labeling problem that is solved using a CRF with supervised training.",
        "Ma-tuszek et al2010) developed a system that learns to map NL instructions to executable commands for a robot navigating in an environment constructed by a laser range finder.",
        "However, their approach has limitations of ignoring any objects or other landmarks in the environment to which the instructions can refer.",
        "There are several recent projects (Vogel and Jurafsky, 2010; Kollar et al2010; Tellex et al2011) which learn to follow instructions in more linguistically complex environments.",
        "However, they assume predefined spatial words, direct matching between NL words and the names of objects and other landmarks in the MR, and/or an existing syntactic parser.",
        "By contrast, our work does not assume any prior linguistic knowledge, syntactic, lexical, or semantic, and must learn the mapping between NL words and phrases and the MR terms describing landmarks."
      ]
    },
    {
      "heading": "7 Future Work",
      "text": [
        "In the future, we would like to develop a better lexicon learner since our PCFG approach critically relies on the quality of the learned lexicon.",
        "Particularly, we would like to investigate how syntactic information (such as part-of-speech tags induced using unsupervised learning) could be used to improve semantic-lexicon learning.",
        "For example, some of the current lexicon entries violate the general constraint that nouns usually refer to objects and verbs to actions.",
        "Ideally, the lexicon learner would be able to induce and then utilize this sort of relationship between syntax and semantics.",
        "In addition, we want to investigate the use of discriminative reranking (Collins, 2000), which has proven effective in various other NLP tasks.",
        "We would expect the final MR output to improve if a discriminative model, which uses additional global features, is used to rerank the top-k parses produced by our generative PCFG model."
      ]
    },
    {
      "heading": "8 Conclusions",
      "text": [
        "We have presented a novel method for learning a semantic parser given only highly ambiguous supervision.",
        "Our model enhances Bo?rschinger et al.",
        "(2011)'s approach to reducing the problem of grounded learning of semantic parsers to PCFG induction.",
        "We use a learned semantic lexicon to aid the construction of a smaller and more focused set of PCFG productions.",
        "This allows the approach to scale to complex MR languages that define a large (potentially infinite) space of representations for capturing the meaning of sentences.",
        "By contrast, the previous PCFG approach requires a finite MR language and its grammar grows intractably large for even moderately complex MR languages.",
        "In addition, our algorithm for composing MRs from the final parse tree provides the flexibility to produce a wide range of novel MRs that were not seen during training.",
        "Evaluations on a previous corpus of navigational instructions for virtual environments has demonstrated the effectiveness of our method compared to a recent competing system."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank the anonymous reviewers and David Chen for useful comments that helped improve this paper.",
        "This work was funded by NSF grants IIS-0712907 and IIS-1016312.",
        "Experiments were performed on the Mastodon Cluster, provided by NSF grant EIA0303609."
      ]
    }
  ]
}
