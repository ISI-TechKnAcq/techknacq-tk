{
  "info": {
    "authors": [
      "Marc F. J. Drossaers"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C92-1021",
    "title": "Hopfield Models as Nondeterministic Finite-State Machines",
    "url": "https://aclweb.org/anthology/C92-1021",
    "year": 1992
  },
  "references": [
    "acl-C88-2097"
  ],
  "sections": [
    {
      "text": [
        "The Netherlands, email: mdrssrs4cs.utwente.nl."
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "The use of neural networks for integrated linguistic analysis may be profitable.",
        "This paper presents the first results of our research on that subject: a Hop field model for syntactical analysis.",
        "We construct a neural network as an implementation of a bounded push-down automaton, which can accept context-free languages with limited center-embedding.",
        "The network's behavior can be predicted a priori, so the presented theory can be tested.",
        "The operation of the network as an implementation of the acceptor is provably correct.",
        "Furthermore we found a solution to the problem of spurious states in Hopfield models: we use them as dynamically constructed representations of sets of states of the implemented acceptor.",
        "The so-called neural-network acceptor we propose, is fast but large."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Neural networks may be well suited for integrated linguistic analysis, as Waltz and Pollack [10] indicate.",
        "An integrated linguistic analysis is a parallel composition of several analyses, such as syntactical, semantical, and pragmatic analysis.",
        "When integrated, these analyses constrain each other interactively, and may thus suppress a combinatoric explosion of sentence structure and meaning representations.",
        "This paper presents the first results of our research into the use of neural networks for integrated linguistic analysis: a Hopfield model for syntactical analysis.",
        "Syntactical analysis in the context of integration with other analyses boils down to the decision whether a sentence is an element of a language.",
        "A parse tree is superfluous here as an intermediary representation, since it will not be finished before the complete integrated analysis is.",
        "This fact allows us to deal with the problem of a restricted length of the sentences a neural parser can handle, see e.g. [5],[7], a problem that could not be elegantly solved, see e.g. [6],[3].",
        "In this paper we propose a formal model that recognizes syntactically correct sentences (section 2), a Ilopfield model onto which we want to map this formal model (section 3), the parameters that makes the network operate as intended (section 4), and a way to map the formal model onto the Ilopfield model, including a correctness result for the latter (section 5).",
        "The theoretically predicted behavior of the so obtained network has been verified, and a simple example provides the taste of it (section 6).",
        "We also consider complexity aspects of the model (section 7).",
        "Section 8 consists of concluding remarks."
      ]
    },
    {
      "heading": "2 A Bounded Push-Down Automaton",
      "text": [
        "Although it is not an established fact, it is assumed here that natural languages are context-free, and consequently that sentences in a natural language can be recognized, by a push-down automaton (PDA).",
        "However, we are not interested in modeling the competence of natural language users, but in modeling their performance.",
        "The human performance in natural language use is also characterized by a very limited degree of center-embedding.",
        "In terms of PDAs this Ineans that there is a bound on theolumber of items on the stack of a PDA for a natural language.",
        "A bounded push-down automaton M = (Q,E, vs, Zs, is a PHA that has an upper limit k E IN on the number of items on its stack, i.e. ICI < k for every instantaneous description (ID) (q,t D,a) of M. The set of stack states of this PDA is defined to be: C sT {Cr I (q0, tu, Z0)114 (Q, e, a)).",
        "Q ST is finite: IQE/1 < (1r1)', therefore we may define a nondeterministic finite-state acceptor (NDA) M' that has C s'r as its set of states.",
        "The class of PDAs of which we would like to map bounded versions onto NDAs is constrained, among others to the class of E-free PDAs.",
        "By this constraint we anticipate the situation that grammars are stored in a neural network by self-organization.",
        "In that situation a neural network will store E-productions only if examples of applications of E-productions are repeatedly presented to it.",
        "This requires c to have a representation in the machine, in which case it fails to accommodate its definition.",
        "Another restriction we would like to introduce is to grammars in 2-standard form with a minimal number of quadratic productions: productions of the form 8. tr: see section 7; 9.",
        "UA'(1 a) 4 a).",
        "Note that there are an infinite number of parameter lists for each NDA.",
        "The mapping of an NDA onto a network starts by mapping basic entities of the NDA onto activity patterns, Such a pattern is called a code.",
        "Definition 5.2 Let M = (Q, E05, Q0,19 be an NDA, let P fa, A' , Ai, N, p,t, td,U) be a parameter list defined according to definition 5.1.",
        "The coding function c is given by:",
        "such that for q E Q, and x E E: 1 if b(q,x) c(q,x)10, where et is chosen at random from 10,1) with probability distribution"
      ]
    },
    {
      "heading": "2. undefined otherwise",
      "text": [
        "The set of codes is then partitioned: into sets of activity patterns corresponding to NDA states, and into sets of patterns corresponding to input symbols.",
        "Definition 5.3 Let M = (Q, E, b, Qv, F) be an NDA, let P , A', N, p, pm.",
        "t, td,U) be a parameter list defined according to definition 5.1.",
        "The set Pc, of activity patterns for q L Q is:",
        "where each J1 is an N x N matrix.",
        "This suffices to define a neural-network acceptor.",
        "Definition 5.5 Let M = (C), E05, Qv,be an NDA, let P (a, A'), N, p, td, U) be a parameter list defined according to definition 5.1.",
        "A neural-network acceptor (NNA) defined for an NDA M that takes its parameters from P is a quadruple H (T, f ,U, S), where:",
        "'lhe OR operation over a setof activity patterns is specified by: 1.",
        "OR({e'}, IC)) =; 2.",
        "014{(1), , {V)) OR({e ), (Mtn, ..., {C})) ; and 3.",
        "011.",
        "(1) = OR({0},{V}) if",
        "P. = {{0} , , V\")) At last a formal definition can be given of a temporal image as the set of all activity patterns for which there is a network input that makes such an activity pattern the network's next quasi-stable state.",
        "Definition 5.7 Let M(Q, E, 6, fjo, 61 be an NDA, let P = (a, A )re, N,to, td, t, (I) be a parameter list defined according to definition 5.1, and let H (T, f , U,be an NNA defined according to definition 5.5 that takes its parameters from P. A temporal image is a set: {c(q , x) input Olt(PC) for H implies rd(9.r) = 1a), a set P0, is a temporal image of a quasi-stable stair {S} c(q , x) of H if and only if J19,,,) is a transition of H. Now that we have a neural-network acceptor, we may also want to use it to judge the legality of strings against a given grammar with it.",
        "4.",
        "Not before a period td (= delay time) has passed, a new input arrives.",
        "The new temporal image is forwarded by the slow synapses during a period td +1,., starting when id ends.",
        "The slow synapses have forgotten the old temporal image while the network was in its td.",
        "The synapses modeled in the network collect the incoming activity over a period td + tr, and emit the time average over again a period td + t, after having waited a period td + t,..",
        "In the network this is modeled by computing a time average over prior neuronal activity, and then multiply it by the synaptic efficacy.",
        "The time average ranges over a period (2t0 + id + 34)/ N (In tr)/N.",
        "The first argument is the total time span in the network, covering two active periods and an intervening delay time, in chiding their transition times.",
        "The second argument is the current period of network is activity, activity that cannot directly interfere with the network's dynamics.",
        "More formally the network can be described as follows: al.",
        "[21:",
        "If athe pattern is biased.",
        "For Noo, 1/Na.",
        "The updating process is a Monte Carlo random walk.",
        "The Si are neuronal variables (.5: is a neuron in another network), hi is the total input on Si, U is a threshold value which is equal for all Si, ./ij is the synaptic efficacy of the synapse connecting Si to Si, and A is the relative magnitude of the synapses.",
        "The average at time t is expressed by 3}(t), where r (2t,, + td + 3t,)/N and 0 a (t + tr)/N.",
        "The function w(t) determines over which period activity is averaged.",
        "The input synapses are nonzero only in case i = j.",
        "These synapses carry a negative ground signal A`a, which is equivalent to an extra threshold generated by the input synapses.",
        "The activity patterns {e} ({V'} , ..... ek)1 ) are statistically independent, and satisfy the same probability distribution as the patterns in the model of Bulimann et Figure U The model for N = 3.",
        "Usually llopfield models consist of very many neurons.",
        "The arced arrows denote temporal synapses.",
        "The straight arrows denote input synapses."
      ]
    },
    {
      "heading": "4 Estimation of Parameters",
      "text": [
        "A number of system parameters need to be related in order to make the model work correctly.",
        "Timing is fairly important in this network.",
        "The time the network is active (t) should not exceed the delay time tr..",
        "If it does then td--1-tr > td+tr, and since no average is computed over a period td + t,.",
        "back in time, not the full time average of the previous activity need to be computed, consequently we choose In < td The choice for a transition time t,.",
        "depends on the probability with which one requires the network to get in the next stable activity state.",
        "This subject will be dealt with in section 7.",
        "In the estimates of A' and the storage capacity below, an expression of the temporal transition term in terms of the overlap parameter mo is used, which will be introduced here first.",
        "The overlap parameter m(I) measures the overlap of a network state {S} (Si, Sy, , Stir)T at time I with stored pattern lel, and is defined by:",
        "1. a E Ci/N where cl << N is a constant; 2.",
        "AC2N/Ci , where c2 is a small constant; 3.",
        "0 << A'a, i.e. 0 << c2; 4. p,,(a In 5.",
        "Noo; 6. tr/N.-o.",
        "The following lemma states that for neural-network acceptors that take their parameters from a list of large parameters both the probability that the network reaches the next stable state within relaxation time, and the probability that only the patterns that are temporally related to the previous activity pattern will become active, tend to unity.",
        "Essentially it means that such networks operate exactly as prescribed by the synapses.",
        "Such networks are intrinsically correct.",
        "Lemma 5.10 intrinsical correctness Let M = (Q,E,b,Q0,F) be an NDA, let p (a, A',N, p, Pm., to, td, tr, U) be a parameter list defined according to definitions 5.1 and 5.9, and let H -= (7', f ,U,S) be an NNA defined according to definition 5.5 that takes its parameters from P, then H is such that:",
        "1. for all neurons Si in H, P(Si is selected)1 during network evolution; and 2. for all activity patterns {}EUPy,yEQU E, pv, then P(:\" -=1) - 0, where",
        "Then the correctness of an NNA follows.",
        "Theorem 5.11 (correctness of the NNA) Let M = (Q,E,b,Q0,F) be an NBA, let p (a, A', N,P,Pm...,10,1d,tr.U) be a parameter list defined according to definitions 5.1 and 5.9, let H = (T, f ,U, be an NNA defined according to definition 5.5 that takes its parameters from P, and let w 9E+, then the probability that M accepts a string w if and only if 11 accepts w, tends to unity.",
        "The proof of the theorem is given in [4]."
      ]
    },
    {
      "heading": "6 Simulation Results",
      "text": [
        "As an.example we constructed an NNA that accepts the language generated by a grammar with productions:",
        "variables.",
        "Such a grammar can be seen as a minimal extension of a right-linear grammar.",
        "Within such grammars, quadratic productions provide for the center-embedding.",
        "Since such grammars have a minimal number of quadratic productions, acceptance by a PDA defined for such grammars requires a minimal use of (stack) memory, and thus generates a minimal Qs T. To maintain this minimal use of memory a restriction to one-state PDAs that accept by empty stack is also required: when a FDA is mapped onto an NDA, the information concerning its states is lost, unless it was stored on the stack.",
        "An E.-free, one-state PDA.",
        "that simulates a context free grammar in 2-standard form with a minimal number of quadratic productions (and that accepts by empty stack) satisfies all our criteria.",
        "For every such PDA we can define an NDA, for which we can prove [4] that it accepts the same language as the FDA does.",
        "Let M be an e-free one-state PDA with bound k, if M' is an NDA as defined in definition 2.1, then M accepts a string by empty stack if and only if M' accepts it by accepting state.",
        "In as far as a natural language is context-free, we claim that there is an instance of our acceptor that recognizes it."
      ]
    },
    {
      "heading": "3 An Input-Driven Sequencing Hopfield Model",
      "text": [
        "In this section a noiseless Hopfield model is proposed that is tailored to implement NDAs on.",
        "The model is based on the associative memory described by Buhmann et al.",
        "[2] and the theory of delayed synapses from [8].",
        "We chose the Hopfield model because of its analytical transparency, and its capability of sequence traversing, which agrees well with the sequential nature of language use at a phenomenological level.",
        "The Hopfield model proposed is a memory for temporal transitions extended with external-input synapses.",
        "Figure 1 shows the architecture involved.",
        "In this network only those neurons are active upon which a combined local field operates that transcends the threshold.",
        "The activity generated by such a local field is the active overlap of the temporal image of past activity provided by so-called temporal synapses, and the image of input external activity provided by so-called input synapses.",
        "By the temporal synapses, this activity will later generate another (subthreshold) temporal image, so network activity may be considered a transition mechanism that brings the network from one temporal image to another.",
        "Active overlaps are unique with high probability if the activity patterns are chosen at random arid represent low mean network activity.",
        "This uniqueness makes the selectivity of the network very plausible: if an external activity pattern is presented that does not match the current temporal image, then there will not be activity of any significance; the input is not recognized.",
        "When an NDA is mapped onto this network, pairs of NDA-state q and input-symbol a, such that 6(q, 0, are mapped onto activity patterns.",
        "Temporal relations in the network then serve to implement NDA transitions.",
        "Note that single NDA transitions are mapped onto single network transitions.",
        "This results in complex representations of the NDA states and the input symbols.",
        "An NDA state is represented by all activity patterns that represent a pair containing that state, and input patterns are represented by a component-wise OR over all activity patterns containing that input symbol.",
        "A consequence is that mixed temporal images, the subthreshold analogue of mixture states, are a very natural phenomenon in this network, because the temporal image of an active overlap comprises at least all activity patterns representing a successor state.",
        "But this is not all.",
        "Also the network will act as if it implements the deterministic equivalent of the NDA, i.e. it will trace all paths through state space the input allows for, concurrently.",
        "The representations of the states of this deterministic finite-state automaton (FSA) are dynamically constructed along the way; they are mixed temporal images.",
        "The concept of a \"dynamically constructed representation\" is borrowed from Touretzky [9], who, by the way, argued that they could not exist in the current generation of neural networks, such as llopfield models.",
        "A time cycle of the network can he described as follows: The network is allowed to evolve into a stable activity pattern that is the active overlap of a temporal image of past activity, and the input image of external input for a period t,.",
        "(= relaxation time), when an external activity pattern is presented to the network; 2 After some time the network reaches a state of stable activity and starts to construct a new temporal image.",
        "It is allowed to do this for a period (= active time); 3 Then the input is removed, and the network evolves towards inactivity.",
        "This takes again about a period 1,.; AcrEs DE COLING-92, N,thrms, 23-28 4oiir 1992114 PROC.",
        "OF COL1NG-92, Nmsms, AUG. 23-28, 1992 length n as a function of the length of the input sequence is thus (r 0)(n + I).",
        "The constant r also depends ou tr which is chosen to let the network satisfy a certain probability that it reaches the next state in relaxation.",
        "This probability is given by (1 where ft t,./N.",
        "The time complexity of the neural network acceptor is 0(n).",
        "The upper limit on the number p of stored temporal relations between single activity patterns is",
        "In this expression, p(crn)/v where vc is the number of activity patterns stored in the network, and en is the number of patterns that were supposed to be present in the mixture state.",
        "The probability g = 1 p, and n (i) is the number of patterns that can be constructed from the active neurons in the mix.",
        "S is the number of wrongly selected activity patterns for a given n. Pror(n) decreases with increasing N if the other parameters remain fixed.",
        "The space complexity of the network, expressed as the number of neurons, and as a function of the number of NI/A states is 0(1 Q 12).",
        "This is large because Q s\" I F Ifor some PDA M. However things could have been worse.",
        "Not using mixed temporal images to represent ESA states would necessitate the use of a number of temporal images of order 2140.",
        "So compared to a more conventional use of Hopfield models, this approach yields a reduction of the space complexity of the network."
      ]
    },
    {
      "heading": "8 Conclusions",
      "text": [
        "We proposed an acceptor for all context-free languages with limited center-embedding, and a suitable variant of the Ropfield model.",
        "The formal model was implemented on the Hopfield model, and a correctness theorem for the latter was given.",
        "Simulation results provided initial corroboration of our theory.",
        "The obtained neural-network acceptor is fast but large.",
        "Continuation of this research in the near future consists of the design of an adaptive variant of this model, one that learns a grammar from examples in an unsupervised fashion."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "I am very grateful for the indispensable support of Mantles Poe!",
        "and Anton Nijholt in making this paper.",
        "Special thanks go out to Bert Ilelthuis who wrote the very beautiful simulation program with which the theory was corroborated."
      ]
    },
    {
      "heading": "References",
      "text": [
        "Substituted in either po or p1 it results in pop, = This result is the same as obtained by Buh-V4ora.",
        "mann et al.",
        "[2], and they found a storage capacity a, r,s (a In a)' where a, -= The storage capacity is large for small a, so a will be chosen a << 0.5.",
        "A last remark concerns an initial input for the network.",
        "In case there has been no input for the network for some time, it does not contain a temporal image anymore, and consequently has to be restarted.",
        "This can be done by preceding a sequence by an extra strong first input, a kind of warning signal of magnitude e.g. A' + A'.",
        "This input creates a memory of previous activity and serves as a starting point for the temporal sequences stored in the network."
      ]
    },
    {
      "heading": "5 Neural-Network Acceptors",
      "text": [
        "In this section it is shown how NDAs from definition 2.1 can be mapped onto networks as described in sections 3 and 4.",
        "Such networks can only be used for cyclic recognition runs.",
        "Where \"cyclic\" indicates that both the initial and the accepting state of an NDA are mapped onto the same network state.",
        "If this were not done, the accepting state is not assigned an activity vector, since no transition departs from it, see definition 5.2 below.",
        "Cyclic recognition in its turn can only be correctly done for grammars that generate end-of-sentence markers.",
        "Any grammar can be extended to do this.",
        "An NDA is related to a network by a parameter list."
      ]
    }
  ]
}
