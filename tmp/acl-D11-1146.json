{
  "info": {
    "authors": [
      "Zhiyuan Liu",
      "Xinxiong Chen",
      "Maosong Sun"
    ],
    "book": "EMNLP",
    "id": "acl-D11-1146",
    "title": "A Simple Word Trigger Method for Social Tag Suggestion",
    "url": "https://aclweb.org/anthology/D11-1146",
    "year": 2011
  },
  "references": [
    "acl-C08-1093",
    "acl-C10-1148",
    "acl-C10-1149",
    "acl-D09-1027",
    "acl-D09-1051",
    "acl-J03-1002",
    "acl-J10-3002",
    "acl-J10-3010",
    "acl-J93-2003",
    "acl-P00-1041",
    "acl-P03-1003",
    "acl-P07-1059",
    "acl-P08-1082",
    "acl-P10-1085",
    "acl-W04-3219",
    "acl-W04-3252",
    "acl-W11-0316"
  ],
  "sections": [
    {
      "text": [
        "Zhiyuan Liu, Xinxiong Chen and Maosong Sun",
        "{lzy.thu, cxx.thuj@gmail.com, sms@tsinghua.edu.cn",
        "It is popular for users in Web 2.0 era to freely annotate online resources with tags.",
        "To ease the annotation process, it has been great interest in automatic tag suggestion.",
        "We propose a method to suggest tags according to the text description of a resource.",
        "By considering both the description and tags of a given resource as summaries to the resource written in two languages, we adopt word alignment models in statistical machine translation to bridge their vocabulary gap.",
        "Based on the translation probabilities between the words in descriptions and the tags estimated on a large set of description-tags pairs, we build a word trigger method (WTM) to suggest tags according to the words in a resource description.",
        "Experiments on real world datasets show that WTM is effective and robust compared with other methods.",
        "Moreover, WTM is relatively simple and efficient, which is practical for Web applications."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In Web 2.0, Web users often use tags to collect and share online resources such as Web pages, photos, videos, movies and books.",
        "Table 1 shows a book entry annotated with multiple tags by users.",
        "On the top of Table 1 we list the title and a short introduction of the novel \"The Count of Monte Cristo\".",
        "The bottom half of Table 1 shows the annotated tags, each of which is followed by a number in bracket, the total number of users who",
        "'The original record is obtained from the book review website Douban (www.douban.com) in Chinese.",
        "Here we translate it to English for comprehension.",
        "use the tag to annotate this book.",
        "Since the tags of a resource are annotated collaboratively by multiple users, we also name these tags as social tags.",
        "For a resource, we refer to the additional information, such as the title and introduction of a book, as description, and the user-annotated social tags as annotation.",
        "Description",
        "Title: The Count of Monte Cristo Intro: The Count of Monte Cristo is one of the most popular fictions by Alexandre Dumas.",
        "The writing of the work was completed in 1844.",
        "... Annotation",
        "Table 1 : An example of social tagging.",
        "The number in the bracket after each tag is the total count of users that annotate the tag on this book.",
        "Social tags concisely indicate the main content of the given resource, and potentially reflect user interests.",
        "Social tagging has thus been widely studied and successfully applied in recommender systems (Eck et al., 2007; Yanbe et al., 2007; Zhou et al., 2010), trend detection and tracking (Hotho et al., 2006), personalization (Wetzker et al., 2010), advertising (Mirizzi et al., 2010), etc.",
        "The task of automatic social tag suggestion is to automatically recommend tags for a user when he/she wants to annotate a resource.",
        "Social tag suggestion, as a crucial component for social tagging systems, can help users annotate resources.",
        "Moreover, social tag suggestion is usually considered as an equivalent problem to modeling social tagging behaviors, which is playing a more and more important role in social computing and information retrieval (Wang et al., 2007).",
        "Most online resources contain descriptions, which usually contain much resource information.",
        "For example, on a book review website, each book entry contains a title, the author(s) and an introduction of the book.",
        "Some researchers thus propose to automatically suggest tags based on resource descriptions, which are collectively known as the content-based approach.",
        "One may think to suggest tags by selecting important words from descriptions.",
        "This is far from enough because descriptions and annotations are using diverse vocabularies, usually referred to as a vocabulary gap problem.",
        "Take the book entry in Table 1 for instance, the word \"popular\" used in the description contrasts the tags \"classic\" and \"famous book\" in the annotation; the word \"novel\" is used in the description, while most users annotate with the tag \"fiction\".",
        "The vocabulary gap usually reflects in two main issues:",
        "• Some tags in the annotation do appear in the corresponding description, but they may not be statistically significant.",
        "• Some tags may even not appear in the description.",
        "It is not trivial to reduce the vocabulary gap and find the semantic correspondence between descriptions and annotations.",
        "By regarding both the description and the annotation as parallel summaries of a resource, we use word alignment models in statistical machine translation (SMT) (Brown et al., 1993) to estimate the translation probabilities between the words in descriptions and annotations.",
        "SMT has been successfully applied in many applications to bridge vocabulary gap.",
        "For detailed descriptions of related work, readers can refer to Section 2.2.",
        "In this paper, besides employing word alignment models to social tagging, we also propose a method to efficiently build description-annotation pairs for sufficient learning translation probabilities by word alignment models.",
        "Based on the learned translation probabilities between words in descriptions and annotations, we regard the tagging behavior as a word trigger process:",
        "1.",
        "A user reads the resource description to realize its substance by seeing some important words in the description.",
        "2.",
        "Triggered by these important words, the user translates them into the corresponding tags, and annotates the resource with these tags.",
        "Based on this perspective, we build a simple word trigger method (WTM) for social tag suggestion.",
        "In Fig. 1, we use a simple example to show the basic idea of using word trigger for social tag suggestion.",
        "In this figure, some words in the first sentence of the book description in Table 1 are triggered to the tags in annotation."
      ]
    },
    {
      "heading": "2. Related Work",
      "text": [
        "Previous work has been proposed to automatic social tag suggestion.",
        "Many researchers built tag suggestion systems based on collaborative filtering (CF) (Herlocker et al., 1999; Herlocker et al., 2004), a widely used technique in recommender systems (Resnick and Varian, 1997).",
        "These collaboration-based methods typically base their suggestions on the tagging history of the given resource and user, without considering resource descriptions.",
        "FolkRank (Jaschke et al., 2008) and Matrix Factorization (Rendle et al., 2009) are representative CF methods for social tag suggestion.",
        "Most of these methods suffer from the cold-start problem, i.e., they are not able to perform effective suggestions for resources that no one has annotated yet.",
        "The content-based approach for social tag suggestion remedies the cold-start problem of the",
        "Description Language",
        "The Count of Monte Cristo is one of the most DODular fictions bv Alexandre Dumas.",
        "/",
        "\\ T V\\ /",
        "Tran^jation /",
        "Annotation Language",
        "m ^ .........».",
        "r",
        "1 Dumas Count of Monte Cristo novel classic",
        "collaboration-based approach by suggesting tags according to resource descriptions.",
        "Therefore, the content-based approach plays an important role in social tag suggestion.",
        "Some researchers regarded social tag suggestion as a classification problem by considering each tag as a category label (Ohkura et al., 2006; Mishne,",
        "Fujimura et al., 2008; Heymann et al., 2008).",
        "Various classifiers such as Naive Bayes, kNN, SVM and neural networks have been explored to solve the social tag suggestion problem.",
        "There are two issues emerging from the classification-based methods:",
        "• The annotations provided by users are noisy, and the classification-based methods can not handle the issue well.",
        "• The training cost and classification cost of many classification-based methods are usually in proportion to the number of classification labels.",
        "These methods may thus be inefficient for a real-world social tagging system, where hundreds of thousands of unique tags should be considered as classification labels.",
        "Inspired by the popularity of latent topic models such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), various methods have been proposed to model tags using generative latent topic models.",
        "One intuitive approach is assuming that both tags and words are generated from the same set of latent topics.",
        "By representing both tags and descriptions as the distributions of latent topics, this approach suggests tags according to their likelihood given the description (Krestel et al., 2009; Si and Sun, 2009).",
        "Bundschus et al.",
        "(2009) proposed a joint latent topic model of users, words and tags.",
        "Iwata et al.",
        "(2009) proposed an LDA-based topic model, Content Relevance Model (CRM), which aimed at finding the content-related tags for suggestion.",
        "Empirical experiments showed that CRM outperformed both classification methods and Corr-LDA (Blei and Jordan, 2003), a generative topic model for contents and annotations.",
        "Most latent topic models have to pre-specify the number of topics before training.",
        "We can either use cross validation to determine the optimal number of topics or employ the infinite topic models, such as Hierarchical Dirichlet Process (HDP) (Teh et al., 2006) and nested Chinese Restaurant Process (Blei et al., 2010), to automatically adjust the number of topics during training.",
        "Both solutions are usually computationally complicated.",
        "What is more important, topic-based methods suggest tags by measuring the topical relevance of tags and resource descriptions.",
        "The latent topics are of concept-level which are usually too general to precisely suggest those specific tags such as named entities, e.g., the tags \"Dumas\" and \"Count of Monte Cristo\" in Table 1.",
        "To remedy the problem, Si et al.",
        "(2010) proposed a generative model, Tag Allocation Model (TAM), which considers the words in descriptions as the possible topics to generate tags.",
        "However, TAM assumes each tag can only have at most one word as its reason.",
        "This is against the fact that a tag may be annotated triggered by multiple words in the description.",
        "It should also be noted that social tag suggestion is different from automatic keyphrase extraction (Tur-ney, 2000; Frank et al., 1999; Liu et al., 2009a; Liu et al., 2010b; Liu et al., 2011).",
        "Keyphrase extraction aims at selecting terms from the given document to represent the main topics of the document.",
        "On the contrary, in social tag suggestion, the suggested tags do not necessarily appear in the given resource description.",
        "We can thus regard social tag suggestion as a task of selecting appropriate tags from a controlled tag vocabulary for the given resource description.",
        "SMT techniques have been successfully used in many tasks of information retrieval and natural language processing to bridge the vocabulary gap between two types ofobjects.",
        "Some typical tasks are document information retrieval (Berger and Laffer-",
        "Liu et al., 2010c), keyphrase extraction (Liu et al., 2011), sentiment analysis (Dalvi et al., 2009), computational advertising (Ravi et al., 2010), and image/video annotation and retrieval (Duygulu et al., 2002; Jeon et al., 2003)."
      ]
    },
    {
      "heading": "3. Word Trigger Method for Social Tag Suggestion",
      "text": [
        "We describe the word trigger method (WTM) for social tag suggestion as a 3-stage process:",
        "1.",
        "Preparing description-annotation pairs.",
        "Given a collection of annotated resources, we first prepare description-annotation pairs for learning translation probabilities using word alignment models.",
        "2.",
        "Learning a translation model.",
        "Given a collection of description-annotation pairs, we adopt IBM Model-1, a widely used word alignment model, to learn the translation probabilities between words in descriptions and tags in annotations.",
        "3.",
        "Suggesting tags given a resource description.",
        "After building translation probabilities between words and tags, given a resource description, we first compute the trigger power of each word in the description and then suggest tags according to their translation probabilities from the triggered words.",
        "Before introducing the method in details, we introduce the notations.",
        "In a social tagging system, a resource is denoted as r G R, where R is the set of all resources.",
        "Each resource contains a description and an annotation containing a set of tags.",
        "The description dr of resource r can be regarded as a bag of words wr = {(wi, e) where ei is the count of word wi and Nr is the number of unique words in r. The annotation ar of resource r is represented as tr = {(ti, ei)jM=r1, where ei is the count of tag tiand Mr is the number of unique tags for r.",
        "Learning translation probabilities requires a parallel training dataset consisting of a number of aligned sentence pairs.",
        "We assume the description and the annotation of a resource as being written in two distinct languages.",
        "We thus prepare our parallel training dataset by pairing descriptions with annotations.",
        "The annotation of a resource is a bag of tags with no position information.",
        "We thus select IBM Model-1 (Brown et al., 1993) for training, which does not take word position information into account on both sides for each aligned pair.",
        "In a social tagging system, the length of a resource description is usually limited to hundreds of words.",
        "Meanwhile, it is common that some popular resources are annotated by multiple users with thousands of tags.",
        "For example, the tag Dumas is annotated by 2, 748 users for the book in Table 1.",
        "We have to deal with the lengthunbalance between a resource description and its corresponding annotation for two reasons.",
        "• It is impossible to list all annotated tags on the annotation side of a description-annotation pair.",
        "The performance of word alignment models will also suffer from the unbalanced length of sentence pairs in the parallel training",
        "data set (Och and Ney, 2003).",
        "• Moreover, the annotated tags may have different importance for the resource.",
        "It would be unfair to treat these tags without distinction.",
        "Here we propose a sampling method to prepare length-balanced description-annotation pairs for word alignment.",
        "The basic idea is to sample a bag of tags from the annotation according to tag weights and make the generated bag of tags with comparable length with the description.",
        "We consider two parameters when sampling tags.",
        "First, we have to select a tag weighting type for sampling.",
        "In this paper, we investigate two straightforward sampling types, including tag frequency (TFt) within the annotation and tag-frequency inverse-resource-frequency (TF-IRFt).",
        "Given resource r, TFt and TF-IRFt of tag t are defined as TFt = et/ t et and TF-IRFt = et/ t et x log (lRl/^r€ß /et>oQ, where \\ *Er€R Jet>o| indicates the number of resources that have been annotated with tag t.",
        "Another parameter is the length ratio between the description and the sampled annotation.",
        "We denote the ratio as ö = \\wr|/|tr|, where |wr| is the number of words in the description and l trl is the number of tags in the annotation.",
        "3.3 Learning Translation Probabilities Using Word Alignment Models",
        "Suppose the source language is resource description and the target language is resource annotation.",
        "In IBM Model-1, the relationship of the source language w = w{ and the target language t = t[ is connected via a hidden variable describing an alignment mapping from source position j to target position a,j :",
        "The alignment a{ also contains empty-word alignments a,j = 0 which align source words to the an empty word.",
        "IBM Model-1 can be trained using Expectation-Maximization (EM) algorithm in an unsupervised fashion, and obtains the translation probabilities of two vocabularies, i.e., Pr(io|i), where t is a tag and w is a word.",
        "IBM Model-1 only produces one-to-many alignments from source language to target language.",
        "The learned model is thus asymmetric.",
        "We will learn translation models on two directions: one is regarding descriptions as the source language and annotations as the target language, and the other is in reverse direction of the pairs.",
        "We denote the first model as Prd2a and the latter as Pra2<i- We further define Pr(t\\w) as the harmonic mean of the two models:",
        "where A is the harmonic factor to combine the two models.",
        "When A = 1 or A = 0, it simply uses model Pïd2a or Pra2d correspondingly.",
        "3.4 Tag Suggestion Using Triggered Words and Translation Probabilities",
        "When given the description of a resource, we can rank tags by computing the scores:",
        "in which Pv(w\\d) is the trigger power of the word w in the description, which indicates the importance of the word.",
        "According to the ranking scores, we can suggest the top-ranked tags to users.",
        "Here we explore three methods to compute the trigger power of a word in a resource description: TF-IRFW, TextRank and their product.",
        "TF-IRFW and TextRank are two most widely adopted methods for keyword extraction.",
        "Similar to TF-IRFt mentioned in Section 3.2, TF-IRFW considers both the local importance (TFW) and global specification (IRFW).",
        "TextRank (Mihalcea and Tarau, 2004) is a graph-based method to compute term importance.",
        "Given a resource description, TextRank first builds a term graph by connecting the terms in the description according to their semantic relations, and then run PageRank algorithm (Page et al., 1998) to measure the importance of each term in the graph.",
        "Readers can refer to (Mihalcea and Tarau, 2004) for detailed information.",
        "We also use the product of TF-IRFW and TextRank to weight terms, which potentially takes both global information and term relations into account.",
        "Emphasize Tags Appearing In Description for WTM (EWTM) In some social tagging systems, the tags that appear in the resource description are more likely to be selected by users for annotation.",
        "Therefore, we propose to emphasize the tags in the description by ranking tags as follows where it (to) is an indicator function which gets value 1 when t = w and 0 when t / w; and 7 is the smooth factor with range 7 G [0.0,1.0].",
        "When 7 = 1.0, it suggests tags simply according to their trigger powers within the description, while when 7 = 0.0, it does not emphasize the tags appearing in the description and just suggests according to their translation probabilities.",
        "In Section 4.4, we will show the performance of EWTM."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "Datasets In our experiments, we select two real world datasets which are of diverse properties to evaluate our methods.",
        "In Table 2 we show the detailed statistical information of the two datasets.",
        "Table 2: Statistical information of two datasets.",
        "R, W, T, NVw and NVt are the number of resources, the vocabulary of descriptions, the vocabulary of tags, the average number of words in each description and the average number of tags in each resource, respectively.",
        "The first dataset, denoted as BOOK, is obtained from a popular Chinese book review website www.",
        "douban.",
        "com, which contains the descriptions of books and the tags collaboratively annotated by users.",
        "The second dataset, denoted as BIBTEX, is obtained from an English online bibliography website www.",
        "bibsonomy.",
        "org.",
        "The dataset contains the descriptions for academic papers (including the title and note for each paper) and the tags annotated by users.",
        "As shown in Table 2, the average length of descriptions in the BIBTEX dataset is much shorter than the BOOK dataset.",
        "Moreover, the BIBTEX dataset does not provide how many times each tag is annotated to a resource.",
        "Evaluation Metrics We use precision, recall and F-measure to evaluate the performance of tag suggestion methods.",
        "For a resource, we denote the original tags (gold standard) as Ta, the suggested tags as Ts, and the correctly suggested tags as Ts D Ta.",
        "Precision, recall and F-measure are defined as",
        "The final evaluation scores are computed by micro-averaging (i.e., averaging on resources of test set).",
        "We perform 5-fold cross validation for each method on all two datasets.",
        "In experiments, the number of suggested tags M ranges from 1 to 10.",
        "Baseline Methods We select four content-based algorithms as the baselines for comparison: Naive Bayes (NB) (Manning et al., 2008), k nearest neighbor algorithm (kNN) (Manning et al., 2008),",
        "Content Relevance (CRM) model (Iwata et al., 2010) .",
        "NB and kNN are two representative classification methods.",
        "NB is a simple generative model, which models the probability of each tag t given description d as",
        "Pr(t) is estimated by the frequency of the resources annotated with the tag t. Pr(w|t) is estimated by the frequency of the word w in the resource descriptions annotated with the tag t. kNN is a widely used classification method for tag suggestion, which recommends tags to a resource according to the annotated tags of similar resources measured using vector space models (Manning et al., 2008).",
        "CRM and TAM are selected to represent topic-based methods for tag suggestion.",
        "CRM is an LDA-based generative model.",
        "The number of latent topics K is the key parameter for CRM.",
        "In experiments, we evaluated the performance of CRM with different K values, and here we only show the best one obtained by setting K = 1,024.",
        "TAM is also a generative model which considers the words in descriptions as the topics to further generate tags for the resource.",
        "We set parameters for TAM as in (Si et al., 2010).",
        "For comparison, we denote our method as WTM.",
        "Complexity Analysis We compare the complexity of these methods.",
        "We denote the number of training iterations in CRM, TAM and WTM as I , and the number of topics in CRM as K. For the training phase, the complexity of NB is O(RNVwNVt), kNN is O(1), TAM is O(IRNw7Yt), CRM is O(IKRNVwNt), and WTM is O(IRNwNVt).",
        "When suggesting for a given resource description with length , the complexity of NB is O(NwT), kNN is O(RNwJVi), CRM is O(IKNwT), TAM is O(INwT) and WTM is O(NwT).",
        "From the analysis, we can see that WTM is a relatively simple method for both training and suggestion.",
        "This is especially valuable because WTM also shows good effectiveness for tag suggestion compared with other methods as we will shown later.",
        "Data",
        "R",
        "W",
        "T",
        "BOOK",
        "70,000",
        "174, 748",
        "46,150",
        "211.6",
        "3.5",
        "BIBTEX",
        "158, 924",
        "91, 277",
        "50, 847",
        "5.8",
        "2.7",
        "Parameter Settings We use GIZA++ (Och and",
        "Ney, 2003) as IBM Model-1 to learn translation probabilities using description-annotation pairs for WTM.",
        "The experimental results of WTM are obtained by setting parameters as follows: tag weighting type as TF-IRFt, length ratio ö = 1, harmonic factor A = 0.5 and the type of word trigger strength as TF-IRFw.",
        "The influence of parameters to WTM can be found in Section 4.3.",
        "Experiment Results and Analysis In Fig. 2 we show the precision-recall curves of NB, kNN, CRM and WTM on two datasets.",
        "Each point of a precision-recall curve represents different numbers of suggested tags from M = 1 (bottom right, with higher precision and lower recall) to M = 10 (upper left, with higher recall but lower precision) respectively.",
        "The closer the curve to the upper right, the better the overall performance of the method.",
        "From Fig. 2, we observe that:",
        "• WTM consistently performs the best on both datasets.",
        "This indicates that WTM is robust and effective for tag suggestion.",
        "• The advantage of WTM is more significant on",
        "the BOOK dataset.",
        "The reason is that WTM can take a good advantage of annotation count information of tags compared to other methods.",
        "• The average length of resource descriptions is short in the BIBTEX dataset, which makes it difficult to determine the trigger powers of words.",
        "But even on the BIBTEX dataset with no count information of tags, WTM still outperforms other methods especially when recommending first several tags.",
        "To further demonstrate the performance of WTM and other baseline methods, in Table 3 we show the",
        "Precision",
        "(b) BIBTEX",
        "kNN, CRM, TAM and WTM on two datasets.",
        "precision, recall and F-measure of NB, kNN, CRM, TAM and WTM on BOOK dataset when suggesting M = 3 tags.",
        "Due to the limit of space, we only show the variance of F-measure.",
        "In fact, WTM achieves its best performance when M = 2, where the F-measure of WTM is 0.370, outperforming both CRM (F = 0.263) and TAM (F = 0.277) by about 10%.",
        "An Example In Table 4 we show top 10 tags suggested by NB, CRM, TAM and WTM for the book in Table 1.",
        "The number in bracket after the name of each method is the count of correctly suggested tags.",
        "The correctly suggested tags are marked in bold face.",
        "We select not to show the results of kNN because the tags suggested by kNN are totally unrelated to the book due to the insufficient finding of nearest neighbors.",
        "WAM – B – NB – ■ – ",
        "kNN o",
        "CRM – • – ",
        "TAM A",
        "-",
        "WAM",
        "□",
        "NB",
        " – ■ – ",
        "kNN",
        " – e – ",
        "CRM",
        " – • – ",
        "TAM",
        " – A – ",
        "From Table 4, we observe that NB, CRM and TAM, as generative models, tend to suggest general tags such as \"novel\", \"literature\", \"classic\" and \"France\", and fail in suggesting specific tags such as \"Alexandre Dumas\" and \"Count of Monte Cristo\".",
        "On the contrary, WTM succeeds in suggesting both general and specific tags related to the book.",
        "NB (+6): novel, foreign literature, literature, history, Japan, classic, France, philosophy, America, biography CRM (+5): novel, foreign literature, literature, biography, philosophy, culture, France, British, comic, history TAM (+5): novel, sociology, finance, foreign literature, France, literature, biography, France literature, comic, China WTM (+7): novel, Alexandre Dumas, history, Count of Monte Cristo, foreign literature, biography, suspense, comic, America, France",
        "In Table 5, we list four important words (using TF-IRFw as weighting metric) of the description and their corresponding tags with the highest translation probabilities.",
        "The values in brackets are the probability of tag t given word w, Pr(t | w).",
        "For each word, we eliminated the tags with the probability less than 0.1.",
        "We can see that the translation probabilities can map the words in descriptions to their semantically corresponding tags in annotations.",
        "Count of Monte Cristo: Count of Monte Cristo (0.728), Alexandre Dumas (0.270), ... Alexandre Dumas: Alexandre Dumas (0.966), ... revenge: foreign literature (0.168), classic (0.130), martial arts (0.123), Alexandre Dumas (0.122), ... France: France (0.99), ...",
        "Table 5: Four important words (in bold face) in the book description in Table 1 and their corresponding tags with the highest translation probabilities.",
        "We explore the parameter influences to WTM for social tag suggestion.",
        "The parameters include harmonic factor, length ratio, tag weighting types, and types of word trigger strength.",
        "When investigating one parameter, we set other parameters to be the values inducing the best performance as mentioned in Section 4.2.",
        "Finally, we also investigate the influence of training data size for suggestion performance.",
        "In experiments we find that WTM reveals similar trends on both the BOOK dataset and the BIBTEX dataset.",
        "We thus only show the experimental results on the BOOK dataset for analysis.",
        "Harmonic Factor In Fig. 3 we investigate the influence of harmonic factor via the curves of F-measure of WTM versus the number of suggested tags on the BOOK dataset when harmonic factor A ranges from 0.0 to 1.0.",
        "As shown in Section 3.3, harmonic factor A controls the proportion between model Prd2a and Pra2d.",
        "From Fig. 3, we observe that neither single model Prd2a (A = 1.0) nor Pra2d (A = 0.0) achieves the best performance.",
        "When the two models are combined by harmonic mean, the performance is consistently better, especially when A ranges from 0.2 to 0.6.",
        "This is reasonable because IBM Model-1 constrains that only the term in source language can be aligned to multiple terms in target language, which makes the translation probability learned by a single model be asymmetric.",
        "Length Ratio Fig. 4 shows the influence of length ratios on the BOOK dataset.",
        "From the figure, we observe that the performance for tag suggestion is robust as the length ratio varies, except when the ratio breaks the default restriction of GIZA++ (i.e., 10).",
        "Method",
        "Precision",
        "Recall",
        "F-measure",
        "NB",
        "0.271",
        "0.302",
        "0.247 ± 0.004",
        "kNN",
        "0.280",
        "0.314",
        "0.258 ± 0.002",
        "CRM",
        "0.292",
        "0.323",
        "0.266 ± 0.004",
        "TAM",
        "0.310",
        "0.344",
        "0.283 ± 0.001",
        "WTM",
        "0.368",
        "0.452",
        "0.355 ± 0.002",
        "Figure 3: F-measure of WTM versus the number of suggested tags on the BOOK dataset when harmonic factor A ranges from 0.0 to 1.0.",
        "Number of Suggested Tags",
        "Figure 4: F-measure of WTM versus the number of suggested tags on the BOOK dataset when length ratio ö ranges from 10/1 to 1/5.",
        "Tag Weighting Types The influence of two weighting types, TFt and TF-IRFt, on social tag suggestion when M = 3 on the BOOK dataset is shown in Table 6.",
        "TF-IRFt tends to select the tags more specific to the resource while TFt tends to select the most popular tags, because the latter does not consider global information (the IRFt part).",
        "Methods for Computing Word Trigger Power",
        "In Table 7, we show the performance of social tag suggestions on the BOOK dataset with different methods for computing word trigger power.",
        "From the table, we can see that there is not significant difference between TF-IRFw and the product of TF-IRFw and TextRank, while TextRank itself performs the worst.",
        "This indicates that TextRank is less competitive to measure word trigger power since it does not take global information into consideration.",
        "Table 7: Evaluation results for different methods for computing word trigger powers when M = 3 on the",
        "BOOK dataset.",
        "Training Data Size We investigate the influence of training data size for social tag suggestion.",
        "As shown in Fig. 5, we increased the training data size from 8, 000 to 56, 000 step by 8, 000, and carried out evaluation on 4, 000 resources.",
        "The figure shows",
        "• When the training data size is small (e.g., 8, 000), WTM can still achieve good suggestion performance.",
        "• As the training data size increases, the performance of WTM improves, while the improvement speed declines.",
        "The observation indicates that WTM does not require huge-size dataset to achieve good performance.",
        "s.",
        "X = 0.0 – b – X = 0.2 – ■ – ",
        "X = 0.4 – e – ",
        "X = 0.5 – • – ",
        "X = 0.6 – » – ",
        "-",
        "/",
        "y",
        "X = X =",
        "0.8",
        "1.0 -",
        "^-",
        "Is.",
        "1",
        "Weighting",
        "Precision",
        "Recall",
        "F-measure",
        "0.356",
        "0.437",
        "0.342 ± 0.002",
        "TF-IRFt",
        "0.368",
        "0.452",
        "0.355 ± 0.002",
        "/",
        "s.",
        "n = 10/1 – b – n = 10/3 – ■ – n = 1o/5 – o – ",
        "-",
        "1",
        "N",
        "\\",
        "n = n =",
        "1/2",
        "1/5",
        "\\",
        "\\",
        ">",
        "N",
        "N",
        "»__",
        "Weighting",
        "Precision",
        "Recall",
        "F-measure",
        "TF-IRFw",
        "0.368",
        "0.452",
        "0.355 ± 0.002",
        "TextRank",
        "0.345",
        "0.424",
        "0.332 ± 0.002",
        "Product",
        "0.368",
        "0.451",
        "0.354 ± 0.002",
        "Precision",
        "Figure 5: Precision-recall curves when the training data size increases from 8, 000 thousand to 56, 000 thousand on the BOOK dataset.",
        "Conclusion By analyzing the influences of parameters on WTM, we find that WTM is robust to parameter variations.",
        "At the end of this section, we investigate the performance of EWTM for social tag suggestion.",
        "Here we simply set the smooth factor 7 = 0.5.",
        "As shown in Table 8, EWTM improves the performance of WTM (in Table 7) on the BOOK dataset when using TF-IRFw and the product as the methods for computing the word trigger powers, but decays when using TextRank.",
        "This verifies that TF-IRFw is the best method to measure word trigger powers for WTM.",
        "Table 8 indicates that emphasizing the tags appearing in the descriptions may enhance the suggestion power of the word trigger method.",
        "when M = 3 on the BOOK dataset.",
        "However, the performance of EWTM on the BIBTEX dataset decays much compared to WTM.",
        "The F-measure of EWTM is only F = 0.229 compared with WTM F = 0.267.",
        "The main reason of the decay is that: the resource descriptions in the BIBTEX dataset are usually too short to provide sufficient information to precisely emphasize tags.",
        "In this case, EWTM may emphasize wrong tags and drop correct tags.",
        "The experimental results on EWTM suggest that, the performance of EWTM is heavily influenced by the length of resource descriptions.",
        "Therefore, we have to analyze the characteristics of social tagging systems to decide whether to emphasize the tags that appear in the corresponding resource descriptions.",
        "As future work, we will investigate the influence of the smooth factor 7 to EWTM.",
        "It is also worth to investigate the problem when combining with collaboration-based methods for social tag suggestion."
      ]
    },
    {
      "heading": "5. Conclusion and Future Work",
      "text": [
        "In this paper, we present a new perspective to social tagging and propose the word trigger method for social tag suggestion based on word alignment in statistical machine translation.",
        "Experiments show that our method is effective and efficient for social tag suggestion compared to other baselines.",
        "There are still several open problems that should be further investigated:",
        "1.",
        "We can exploit other word alignment methods like log-linear models (Liu et al., 2010a) for social tag suggestion.",
        "2.",
        "We will ensemble WTM with other content-based and collaboration-based methods to build a practical social tag suggestion system.",
        "3.",
        "WTM and EWTM can only suggest the tags that have appeared in translation models.",
        "In future, we plan to incorporate keyphrase extraction in social tag suggestion to make it suggest more appropriate tags not only from translation models but also from the resource descriptions."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work is supported by the National Natural Science Foundation of China (NSFC) under Grant No.",
        "60873174.",
        "The authors would like to thank Peng Li for his insightful suggestions and thank the anonymous reviewers for their helpful comments.",
        "8,000 16,000",
        "24,000 32,000",
        "V",
        "40,000",
        "48,000",
        "56,000",
        "\\",
        "\\",
        "V",
        "X",
        "1",
        "\\^",
        "\\",
        "□",
        "Weighting",
        "Precision",
        "Recall",
        "F-measure",
        "TF-IRFw",
        "0.385",
        "0.472",
        "0.371 ± 0.001",
        "TextRank",
        "0.344",
        "0.423",
        "0.332 ± 0.002",
        "Product",
        "0.374",
        "0.457",
        "0.360 ± 0.001"
      ]
    }
  ]
}
