{
  "info": {
    "authors": [
      "Wenliang Chen",
      "Jun'ichi Kazama",
      "Yoshimasa Tsuruoka",
      "Kentaro Torisawa"
    ],
    "book": "COLING – POSTERS",
    "id": "acl-C10-2015",
    "title": "Improving Graph-based Dependency Parsing with Decision History",
    "url": "https://aclweb.org/anthology/C10-2015",
    "year": 2010
  },
  "references": [
    "acl-C08-1132",
    "acl-C96-1058",
    "acl-D07-1013",
    "acl-D07-1096",
    "acl-D07-1100",
    "acl-D07-1101",
    "acl-D08-1059",
    "acl-D09-1058",
    "acl-D09-1060",
    "acl-E06-1011",
    "acl-I08-1012",
    "acl-J93-2004",
    "acl-N06-1021",
    "acl-P04-1054",
    "acl-P07-1050",
    "acl-P08-1068",
    "acl-P08-1108",
    "acl-P09-1007",
    "acl-W04-2407",
    "acl-W06-2920",
    "acl-W96-0213"
  ],
  "sections": [
    {
      "text": [
        "Wenliang Client Jun'ichi Kazamat, Yoshimasa Tsuruoka^ and Kentaro Torisawat",
        "^Language Infrastructure Group, MASTAR Project, NICT",
        "This paper proposes an approach to improve graph-based dependency parsing by using decision history.",
        "We introduce a mechanism that considers short dependencies computed in the earlier stages of parsing to improve the accuracy of long dependencies in the later stages.",
        "This relies on the fact that short dependencies are generally more accurate than long dependencies in graph-based models and may be used as features to help parse long dependencies.",
        "The mechanism can easily be implemented by modifying a graph-based parsing model and introducing a set of new features.",
        "The experimental results show that our system achieves state-of-the-art accuracy on the standard PTB test set for English and the standard Penn Chinese Treebank (CTB) test set for Chinese."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Dependency parsing is an approach to syntactic analysis inspired by dependency grammar.",
        "In recent years, interest in this approach has surged due to its usefulness in such applications as machine translation (Nakazawa et al., 2006), information extraction (Culotta and Sorensen, 2004).",
        "Graph-based parsing models (McDonald and Pereira, 2006; Carreras, 2007) have achieved state-of-the-art accuracy for a wide range of languages as shown in recent CoNLL shared tasks (Buchholz et al., 2006; Nivre et al., 2007).",
        "However, to make parsing tractable, these models are forced to restrict features over a very limited history of parsing decisions (McDonald and Pereira, work showed that rich features over a wide range of decision history can lead to significant improvements in accuracy for transition-based models (Yamada and Matsumoto, 2003a; Nivre et al., 2004).",
        "In this paper, we propose an approach to improve graph-based dependency parsing by using decision history.",
        "Here, we make an assumption: the dependency relations between words with a short distance are more reliable than ones between words with a long distance.",
        "This is supported by the fact that the accuracy of short dependencies is in general greater than that of long dependencies as reported in McDonald and Nivre (2007) for graph-based models.",
        "Our idea is to use decision history, which is made in previous scans in a bottom-up procedure, to help parse other words in later scans.",
        "In the bottom-up procedure, short dependencies are parsed earlier than long dependencies.",
        "Thus, we introduce a mechanism in which we treat short dependencies built earlier as decision history to help parse long dependencies in later stages.",
        "It can easily be implemented by modifying a graph-based parsing model and designing a set of features for the decision history.",
        "To demonstrate the effectiveness of the proposed approach, we present experimental results on English and Chinese data.",
        "The results indicate that the approach greatly improves the accuracy and that richer history-based features indeed make large contributions.",
        "The experimental results show that our system achieves state-of-the-art accuracy on the data."
      ]
    },
    {
      "heading": "2. Motivation",
      "text": [
        "In this section, we present an example to show the idea of using decision history in a dependency parsing procedure.",
        "Suppose we have two sentences in Chinese, as shown in Figures 1 and 2, where the correct dependencies are represented by the directed links.",
        "For example, in Figure 1 the directed link from",
        "w3:5(bought) to 105:4?",
        "(books) mean that w3 is the head and w5 is the dependent.",
        "In Chinese, the relationship between clauses is often not made explicit and two clauses may simply be put together with only a comma (Li and Thompson, 1997).",
        "This makes it hard to parse Chinese sentences with several clauses.",
        "(Last year I bought some books and this year he also bought some books.)",
        "If we employ a graph-based parsing model, such as the model of (McDonald and Pereira, 2006; Carreras, 2007), it is difficult to assign the relations between w3 and wio in Example A and between w3 and w9 in Example B.",
        "For simplicity, we use wA to refer to of Example A and wB to refer to of Example B in what follows.",
        "The key point is whether the second clauses are independent in the sentences.",
        "The two sentences are similar except that the second clause of Example A is an independent clause but that of Example B is not.",
        "wA0 is the root of the second clause of Example A with subject wA, while wB is the root of the second clause of Example B, but the clause does not have a subject.",
        "These mean that the correct decisions are to assign wA0 as the head of wA and wB as the head of wB, as shown by the dash-dot-lines in Figures 1 and 2.",
        "However, the model can use very limited information.",
        "Figures 3-(a) and 4-(a) show the right dependency relation cases and Figures 3-(b) and 4-(b) show the left direction cases.",
        "For the right direction case of Example A, the model has the information about wA's rightmost child wA and wA0's leftmost child wA inside wA and wA0, but it does not have information about the other children (such as wA) of wA and wA0, which may be useful forjudging the relation between wA and w^j.",
        "The parsing model can not find the difference between the syntactic structures of two sentences for pairs (wA, wfo) and (wB, wB).",
        "If we can provide the information about the other children of w3A and w A0to the model, it becomes easier to find the correct direction between w3A and w A0.",
        "Next, we show how to use decision history to help parse w3A and w A0 of Example A.",
        "In a bottom up procedure, the relations between the words inside [wA, wA0] are built as follows before the decision for wA and wA0.",
        "In the first round, we build relations for neighboring words (word distance^), such as the relations between wA and wA and between wA and wA.",
        "In the second round, we build relations for words of distance 2, and then for longer distance words until all the possible relations between the inside words are built.",
        "Figure 5 shows all the possible relations inside [w3A, w A0] that we can build.",
        "To simplify, we use undirected links to refer to both directions of dependency relations between words in the figure.",
        "Then given those inside relations, we choose the inside structure with the highest score for each direction of the dependency relation between wand wA0.",
        "Figure 6 shows the chosen structures.",
        "Note that the chosen structures for two directions could either be identical or different.",
        "In Figure 6-(a) and -(b), they are different.",
        "Finally, we use the chosen structures as decision history to help parse wA and w^,.",
        "For example, the fact that w is a dependent of wA0 is a clue that suggests that the second clause may be independent.",
        "This results in w A0 being the head of",
        "wä.",
        "This simple example shows how to use the decision history to help parse the long distance dependencies."
      ]
    },
    {
      "heading": "3. Background: graph-based parsing models",
      "text": [
        "Before we describe our method, we briefly introduce the graph-based parsing models.",
        "We denote input sentence w by w = (w0, wi,wn), where w0 = ROOT is an artificial root token inserted at the beginning of the sentence and does not depend on any other token in w and wi refers to a word.",
        "We employ the second-order projective graph-based parsing model of Carreras (2007), which is an extension of the projective parsing algorithm of Eisner (1996).",
        "The parsing algorithms used in Carreras (2007) independently ind the left and right dependents of a word and then combine them later in a bottom-up style based on Eisner (1996).",
        "A subtree that spans the words in [s, t] (and roots at s or t) is represented by chart item [s, t, right/left, C/I], where right (left) indicates that the root of the subtree is s (t) and C means that the item is complete while I means that the item is incomplete (McDonald, 2006).",
        "Here, complete item in the right (left) direction means that the words other than s (t) cannot have dependents outside [s,t] and incomplete item in the right (left) direction, on the other hand, means that t (s) may have dependents outside [s, t].",
        "In addition, t (s) is the direct dependent of s (t) in the incomplete item with the right (left) direction.",
        "Larger chart items are created from pairs of smaller chart items by the bottom-up procedure.",
        "Figure 7 illustrates the cubic parsing actions ofthe Eisner's parsing algorithm (Eisner, 1996) in the right direction, where s, r, and t refer to the start and end indices of the chart items.",
        "In Figure 7-(a), all the items on the left side are complete and represented by triangles, where the triangle of [s, r] is complete item [s, r, – , C] and the triangle of [r + 1, t] is complete item [r + 1, t, , C].",
        "Then the algorithm creates incomplete item [s, t, – , I] (trapezoid on the right side of Figure 7-(a)) by combining the chart items on the left side.",
        "This action builds the dependency from s to t. In Figure 7-(b), the item of [s, r] is incomplete and the item of [r, t] is complete.",
        "Then the algorithm creates complete item [s, t, – , C].",
        "For the left direction case, the actions are similar.",
        "Note that only the actions of creating the incomplete chart items build new dependency relations between words, while the ones of creating the complete items merge the existing structures without building new relations.",
        "Once the parser has considered the dependency relations between words of distance 1, it goes on to dependency relations between words of distance 2, and so on by the parsing actions.",
        "For words of distance 2 and greater, it considers every possible partition of the structures into two parts and chooses the one with the highest score for each direction.",
        "The score is the sum of the feature weights of the chart items.",
        "The features are designed over edges of dependency trees and the weights are given by model parameters (McDonald and Pereira, 2006; Carreras, 2007).",
        "We store the obtained chart items in a table.",
        "The chart item includes the information on the optimal splitting point of itself.",
        "Thus, by looking up the table, we can obtain the best tree structure (with the highest score) of any chart item."
      ]
    },
    {
      "heading": "4. Parsing with decision history",
      "text": [
        "As mentioned above, the actions for creating the incomplete items build the relations between words.",
        "In this study, we only consider using history information when creating incomplete items.",
        "Suppose we are going to compute the scores of the relations between ws and wt.",
        "There are two possible directions for them.",
        "By using the bottom-up style algorithm, the scores of the structures between words with distance < |s – t| are computed in previous scans and the structures are stored in the table.",
        "We divide the decision history into two types: history-inside and history-outside.",
        "The history-inside type is the decision history made inside [s,t] and the history-outside type is the history made outside [s,t].",
        "We obtain the structure with the highest score for each direction of the dependency between wsand wt.",
        "Figure 8-(b) shows the best solution (with the highest score) of the left direction, where the structure is split into two parts, [s, v\\, – , C] and [r\\ + 1, t, – , C].",
        "Figure 8-(c) shows the best solution of the right case, where the structure is split into two parts, [s, r2, – , C] and [r2 + 1, t, – , C].",
        "By looking up the table, we have a subtree that roots at ws on the right side of ws and a subtree that roots at wt on the left side of wt.",
        "We use these structures as the information on history-inside.",
        "For history-outside, we try to obtain the subtree that roots at ws on the left side of ws and the one that roots at wt on the right side of wt.",
        "However, compared to history-inside, obtaining history-outside is more complicated because we do not know the boundaries and the proper structures of the subtrees.",
        "Here, we use an simple heuristic method to find a subtree whose root is at ws on the left side of ws and one whose root is at wt on the right side of wt.",
        "We introduce two assumptions: 1) The structure within a sub-sentence is more reliable than the one that goes across from sub-sentences.",
        "2) More context (more words) can result in a better solution for determining subtree structures.",
        "Algorithm 1 Searching for history-outside boundaries_",
        "Under these two assumptions, Algorithm 1 shows the procedure for searching for history-outside boundaries, where bs is the boundary for for the descendants on the left side of ws , btis the boundary for searching the descendants on the right side of wt, and isPunct is the function that checks if the word is a punctuation mark.",
        "bs should be in the same sub-sentence with s and | s – bs | should be less than 11 – s |.",
        "bt should be in the same sub-sentence with t and |bt – t| should be less than |t – s|.",
        "Next we try to find the subtree structures.",
        "First, we collect the part-of-speech (POS) tags of the heads of all the POS tags in training data and remove the tags that occur fewer than 10 times.",
        "Then, we determine the directions of the relations by looking up the collected list.",
        "For bs and s, we check if the POS tag of ws could be the head tag of the POS tag of wbs by looking up the list.",
        "If so, the direction d is – .",
        "Otherwise, we check if the POS tag of wbs could be the head tag of the POS tag of ws.",
        "If so, d is – , else d is – .",
        "Finally, we obtain the subtree of ws from chart item [bs, s, d, I].",
        "Similarly, we obtain the subtree of wt.",
        "Figure 9 shows the history-outside information for ws and wt, where the relation between wbs and wsand the relation between wbt and wt will be determined by the above method.",
        "We have subtree [rs,s, left, C] that roots at ws on the left side of ws and subtree [t,rt, right, C] that roots at wt on the right side of wt in Figure 9-(b) and (c).",
        "Then, we explain how to use these decision history in the parsing algorithm.",
        "We use Lst to represent the scores of basic features for the left direction and Rst for the right case.",
        "Then we design history-based features (described in Section 4.3) based on the history-inside and history-outside information, as mentioned above.",
        "Finally, we update the scores with the ones of the history-based features by the following equations:",
        "where L+ and R+ refer to the updated scores, LdJtand Rf{ refer to the scores of the history-based features.",
        "Algorithm 2 Parsing algorithm",
        "2: for k =1 to n do 5: % Create incomplete items 8: Calculate Lf{ and Rf ; 9: % Update the scores of incomplete chart items 12: % Create complete items",
        "Algorithm 2 is the parsing algorithm with the history-based features, where V[s, t, dir, I/C] refers to the score of chart item [s,t,dir,I/C], VI(r) is a function to search for the optimal sibling and grandchild nodes for the incomplete items (line 6 and 7) (Carreras, 2007) given the splitting point r and return the score of the structure, and V C(r) is a function to search for the optimal grandchild node for the complete items (line 13 and 14).",
        "Compared with the parsing algorithms of Carreras (2007), Algorithm 2 uses history information by adding line 8, 10, and 11.",
        "In Algorithm 2, it first creates chart items with distance 1, then goes on to chart items with distance 2, and so on.",
        "In each round, it searches for the structures with the highest scores for incomplete items shown at line 6 and 7 of Algorithm 2.",
        "Then we update the scores with the history-based features by Equation 1 and Equation 2 at line 10 and 11 of Algorithm 2.",
        "However, note that we can not guarantee to find the candidate with the highest score with Algorithm 2 because new features violate the assumptions of dynamic programming.",
        "In this section, we design features that capture the history information in the recorded decisions.",
        "For a dependency between two words, say s and t, there are four subtrees that root at s or t. We design the features by combining s, t with each child of s and t in the subtrees.",
        "The feature templates are shown as follows: (In the following, c means one of the children of s and t, and the nodes in the templates are expanded to their lexical form and POS tags to obtain actual features.",
        "):",
        "C+Dir this feature template is a 2-tuple consisting of (1) a c node and (2) the direction of the dependency.",
        "C+Dir+S/C+Dir+T this feature template is a 3-tuple consisting of (1) a c node, (2) the direction of the dependency, and (3) a s or t node.",
        "C+Dir+S+T this feature template is a 4-tuple consisting of (1) a c node, (2) the direction of the dependency, (3) a s node, and (4) a t node.",
        "r2 cso s csi ri n+1 cti t ct0 r3Figure 10: Structure of decision history We use SHI to represent the subtree of s in the history-inside, THI to represent the one of t in the history-inside, SHO to represent the one of s in the history-outside, and THO to represent the one of t in the history-outside.",
        "Based on the subtree types, the features are divided into four sets: F Shi , Fthi , FSHO , and FTHO refer to the features related to the children that are in subtrees SHI, THI, SHO, and THO respectively.",
        "Figure 10 shows the structure of decision history of a left dependency (between s and t) relation.",
        "For the right case, the structure is similar.",
        "In the figure, SHI is chart item [s,r\\, – ,C ], THI is chart item [ri + 1,t, – ,C], SHO is chart item [r2,s, – ,C], and THO is chart item [t, r3, – , C].",
        "We use csi, cti, cso, and cto to represent a child of s/t in subtrees SHI, THI, SHO, and THO respectively.",
        "The lexical form features of Fshi and FSHO are listed as examples in Table 1, where \"L\" refers to the left direction.",
        "We can also expand the nodes in the templates to the POS tags.",
        "Compared with the algorithm of Carreras (2007) that only considers the furthest children of s and t, Algorithm 2 considers all the children.",
        "In practice, we define several policies to use the history information for different word pairs as follows:",
        "• All: Use the history-based features for all the word pairs without any restriction.",
        "• Sub-sentences: use the history-based features only for the relation of two words from sub-sentences.",
        "Here, we use punctuation marks to split sentences into sub-sentences.",
        "• Distance: use the history-based features for the relation of two words within a predefined distance.",
        "We set the thresholds to 3, 5, and",
        "10.",
        "template",
        "Fshi",
        "Fsho",
        "C+DIR",
        "word-csi+L",
        "word-cso+L",
        "C+DIR+S",
        "word-csi+L+word-s",
        "word-cso +L+word-s",
        "C+DIR+T",
        "word-csi+L+word-t",
        "word-cso +L+word-t",
        "C+DIR +S+T",
        "word-csi+L +word-s+word-t",
        "word-cso+L +word-s+word-t"
      ]
    },
    {
      "heading": "5. Experimental results",
      "text": [
        "In order to evaluate the effectiveness of the history-based features, we conducted experiments on Chinese and English data.",
        "For English, we used the Penn Treebank (Marcus et al., 1993) in our experiments and the tool \"Penn2Malt\" to convert the data into dependency structures using a standard set of head rules (Ya-mada and Matsumoto, 2003a).",
        "To match previous work (McDonald and Pereira, 2006; Koo et al., 2008), we split the data into a training set (sections 2-21), a development set (Section 22), and a test set (section 23).",
        "Following the work of Koo et al.",
        "(2008), we used the MXPOST (Ratnaparkhi, 1996) tagger trained on training data to provide part-of-speech tags for the development and the test set, and we used 10-way jackknifing to generate tags for the training set.",
        "For Chinese, we used the Chinese Treebank (CTB) version 4.0 in the experiments.",
        "We also used the \"Penn2Malt\" tool to convert the data and created a data split: files 1-270 and files 400-931 for training, files 271-300 for testing, and files 301-325 for development.",
        "We used gold standard segmentation and part-of-speech tags in the CTB.",
        "The data partition and part-of-speech settings were chosen to match previous work (Chen et al., 2008; Yu et al., 2008).",
        "We measured the parser quality by the unla-beled attachment score (UAS), i.e., the percentage of tokens with the correct HEAD .",
        "And we also evaluated on complete dependency analysis.",
        "In our experiments, we implemented our systems on the MSTParser and extended with the parent-child-grandchild structures (McDonald and Pereira, 2006; Carreras, 2007).",
        "For the baseline systems, we used the first-and second-order (parent-sibling) features that were used in McDonald and Pereira (2006) and other second-order features (parent-child-grandchild) that were used in Carreras (2007).",
        "In the following sections, we call the second-order baseline systems Baseline and our new systems OURS.",
        "In this section, we test our systems with different settings on the development data.",
        "Table 2 shows the parsing results when we used different policies defined in Section 4.4 with all the types of features, where Dsub refers to applying the policy: sub-sentence, D1 refers to applying the policy: all, and D3|5|10 refers to applying the policy: distance with the predefined distance 3, 5, or 10.",
        "The results indicated that the accuracies of our systems decreased if we used the history information for short distance words.",
        "The system with Dsub performed the best.",
        "Then we investigated the effect of different types of the history-based features.",
        "Table 3 shows the results with policy Dsub.",
        "From the table, we found that FT Hi provided the largest improvement for Chinese and FTHO performed the best for English.",
        "In what follows, we used Dsub as the policy for all the languages, the features Fshi + Fthi + FSHO for Chinese, and the features Fshi + Fsho + Ftho for English.",
        "The main results are shown in the upper parts of Tables 4 and 5, where the improvements by OURS over the Baselines are shown in parentheses.",
        "The results show that OURS provided better performance over the Baselines by 1.02 points for Chinese and 0.29 points for English.",
        "The improvements of (OURS) were significant in McNemar's Test with p < 10-4 for Chinese and p < 10-3 for English.",
        "Chinese",
        "English",
        "Baseline",
        "89.04",
        "92.43",
        "Di",
        "88.73",
        "92.27",
        "D3",
        "88.90",
        "92.36",
        "D5",
        "89.10",
        "92.59",
        "Dio",
        "89.32",
        "92.57",
        "D sub",
        "89.57",
        "92.63",
        "Chinese",
        "English",
        "Baseline",
        "89.04",
        "92.43",
        "+Fshi",
        "89.14",
        "92.53",
        "+Fthi",
        "89.33",
        "92.35",
        "+Fsho",
        "89.25",
        "92.47",
        "+Ftho",
        "88.99",
        "92.54",
        "et al. (2009) that is the best reported result on this data, and STACK refers to our implementation of the combination parser of Nivre and McDonald (2008) using our baseline system and the MALTParser.",
        "The results indicated that OURS performed better than Zhao2009, Yu2008, and STACK, but worse than Chen2009 that used large-scale unlabeled data (Chen et al., 2009).",
        "We also implemented the combination system of OURS and the MALTParser, referred as OURS+STACK in Table 4.",
        "The new system achieved further improvement.",
        "In future work, we can combine our approach with the parser of Chen et al.",
        "(2009).",
        "Table 5 shows the comparative results for English, where Y&M2003 refers to the parser of Ya-mada and Matsumoto (2003b), CO2006 refers to the parser of Corston-Oliver et al.",
        "(2006), Z&C 2008 refers to the combination system of Zhang and Clark (2008), STACK refers to our implementation of the combination parser of Nivre and McDonald (2008), KOO2008 refers to the parser of",
        "Koo et al.",
        "(2008), Chen2009 refers to the parser of Chen et al.",
        "(2009), and Suzuki2009 refers to the parser of Suzuki et al.",
        "(2009) that is the best reported result for this data.",
        "The results shows that OURS outperformed the first two systems that were based on single models.",
        "Z&C 2008 and STACK were the combination systems of graphbased and transition-based models.",
        "OURS performed better than Z&C 2008, but worse than STACK.",
        "The last three systems that used large-scale unlabeled data performed better than OURS."
      ]
    },
    {
      "heading": "6. Related work",
      "text": [
        "There are several studies that tried to overcome the limited feature scope of graph-based dependency parsing models .",
        "Nakagawa (2007) proposed a method to deal with the intractable inference problem in a graph-based model by introducing the Gibbs sampling algorithm.",
        "Compared with their approach, our approach is much simpler yet effective.",
        "Hall (2007) used a re-ranking scheme to provide global features while we simply augment the features of an existing parser.",
        "Nivre and McDonald (2008) and Zhang and Clark (2008) proposed stacking methods to combine graph-based parsers with transition-based parsers.",
        "One parser uses dependency predictions made by another parser.",
        "Our results show that our approach can be used in the stacking frameworks to achieve higher accuracy."
      ]
    },
    {
      "heading": "7. Conclusions",
      "text": [
        "This paper proposes an approach for improving graph-based dependency parsing by using the decision history.",
        "For the graph-based model, we design a set of features over short dependencies computed in the earlier stages to improve the accuracy of long dependencies in the later stages.",
        "The results demonstrate that our proposed approach outperforms baseline systems by 1.02 points for Chinese and 0.29 points for English.",
        "UAS",
        "Complete",
        "Baseline",
        "88.41",
        "48.85",
        "OURS",
        "89.43(+1.02)",
        "50.86",
        "OURS+STACK",
        "89.53",
        "49.42",
        "Zhao2009",
        "87.0",
        "-",
        "Yu2008",
        "87.26",
        "-",
        "STACK",
        "88.95",
        "49.42",
        "Chen2009",
        "89.91",
        "48.56",
        "UAS",
        "Complete",
        "Baseline",
        "91.92",
        "44.28",
        "OURS",
        "92.21 (+0.29)",
        "45.24",
        "Y&M2003",
        "90.3",
        "38.4",
        "CO2006",
        "90.8",
        "37.6",
        "Z&C2008",
        "92.1",
        "45.4",
        "STACK",
        "92.53",
        "47.06",
        "KOO2008",
        "93.16",
        "-",
        "Chen2009",
        "93.16",
        "47.15",
        "Suzuki2009",
        "93.79",
        "-"
      ]
    }
  ]
}
