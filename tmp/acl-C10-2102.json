{
  "info": {
    "authors": [
      "Smaranda Muresan"
    ],
    "book": "COLING – POSTERS",
    "id": "acl-C10-2102",
    "title": "A Learnable Constraint-based Grammar Formalism",
    "url": "https://aclweb.org/anthology/C10-2102",
    "year": 2010
  },
  "references": [
    "acl-P07-1105",
    "acl-P07-1121",
    "acl-P09-1110",
    "acl-W05-0602",
    "acl-W08-2002"
  ],
  "sections": [
    {
      "text": [
        "Lexicalized Well-Founded Grammar (LWFG) is a recently developed syntactic-semantic grammar formalism for deep language understanding, which balances expressiveness with provable learnability results.",
        "The learnability result for LWFGs assumes that the semantic composition constraints are learnable.",
        "In this paper, we show what are the properties and principles the semantic representation and grammar formalism require, in order to be able to learn these constraints from examples, and give a learning algorithm.",
        "We also introduce a LWFG parser as a deductive system, used as an inference engine during LWFG induction.",
        "An example for learning a grammar for noun compounds is given."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Recently, several machine learning approaches have been proposed for mapping sentences to their formal meaning representations (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Muresan, 2008; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009).",
        "However, only few of them integrate the semantic representation with a grammar formalism: A-expressions and Combinatory Categorial Grammars (CCGs) (Steedman, 1996) are used by Zettlemoyer and Collins (2005;2009), and ontology-based representations and Lexical-ized Well-Founded Grammars (LWFGs) (Mure-san and Rambow, 2007) are used by Muresan (2008).",
        "An advantage of the LWFG formalism, compared to most constraint-based grammar formalisms developed for deep language understanding, is that it is accompanied by a learnability guarantee, the search space for LWFG induction being a complete grammar lattice (Muresan and Rambow, 2007).",
        "Like other constraint-based grammar formalisms, the semantic structures in LWFG are composed by constraint solving, semantic composition being realized through constraints at the grammar rule level.",
        "Moreover, semantic interpretation is also realized through constraints at the grammar rule level, providing access to meaning during parsing.",
        "However, the learnability result given by Mure-san and Rambow (2007) assumed that the grammar constraints were learnable.",
        "In this paper we present the properties and principles of the semantic representation and grammar formalism that allow us to learn the semantic composition constraints.",
        "These constraints are a simplified version of \"path equations\" (Shieber et al., 1983), and we present an algorithm for learning these constraints from examples (Section 5).",
        "We also present a LWFG parser as a deductive system (Shieber et al., 1995) (Section 3).",
        "The LWFG parser is used as an innate inference engine during LWFG learning, and we present an algorithm for learning LWFGs from examples (Section 4).",
        "A discussion and an example of learning a grammar for noun compounds are given is Section 6."
      ]
    },
    {
      "heading": "2. Lexicalized Well-Founded Grammars",
      "text": [
        "Lexicalized Well-Founded Grammar (LWFG) is a recently developed formalism that balances expressiveness with provable learnability results (Muresan and Rambow, 2007).",
        "LWFGs are a type of Definite Clause Grammars (Pereira and Warren, 1980) in which (1) the context-free backbone is extended by introducing a partial ordering relation among nonterminals, 2) grammar nonterminals are augmented with strings and their syntactic-semantic representations, called semantic molecules, and (3) grammar rules can have",
        "Figure 1: Syntagmas containing elementary semantic molecules (1) and a derived semantic molecule (2); A constraint grammar rule together with the semantic composition and ontology-based interpretation constraints, $c and $orato (3) two types of constraints, one for semantic composition and one for semantic interpretation.",
        "The first property allows LWFG learning from a small set of examples.",
        "The last two properties make LWFGs a type of syntactic-semantic grammars.",
        "Definition 1.",
        "A semantic molecule associated with a natural language string w, is a syntactic-semantic representation, w/ = (h), where h (head) encodes compositional information, while b (body) is the actual semantic representation of the string w.",
        "Grammar nonterminals are augmented with pairs of strings and their semantic molecules.",
        "These pairs are called syntagmas, and are denoted by a = (w,w') = (w, (h)).",
        "Examples of semantic molecules for the nouns laser and printer and the noun-noun compound laser printer are given in Figure 1.",
        "When associated with lexical items, semantic molecules are called elementary semantic molecules.",
        "When semantic molecules are built by the combination of others, they are called derived semantic molecules.",
        "Formally, the semantic molecule head, h, is a one-level feature structure (i.e., values are atomic), while the semantic molecule body, b, is a logical form built as a conjunction of atomic predicates (concept).",
        "(attr) = (concept), where variables are either concept or slot identifiers in an ontology.",
        "Muresan and Rambow (2007) formally defined LWFGs, and we present here a slight modification of their definition.",
        "Definition 2.",
        "A Lexicalized Well-Founded Grammar (LWFG) is a 7-tuple, G = (S, T!,NG, h ,PG,Pz,S), where:",
        "1.",
        "S is a finite set of terminal symbols.",
        "2.",
        "S/ is a finite set of elementary semantic molecules corresponding to the terminal symbols.",
        "3.",
        "NG is a finite set of nonterminal symbols.",
        "NGnS = 0.",
        "Wedenotepre(NG) ç NG, the set ofpre-terminals (a.k.a, parts ofspeech)",
        "4. h is a partial ordering relation among nonterminals.",
        "5.",
        "PG is the set of constraint grammar rules.",
        "A constraint grammar rule is written A(a) – Bi(ai),..., Bn(an) : where A, Bi G Ng, a = (a, a1,an) such that a = (w,w'),ai = (wi,wi/), 1 < i < n,w = w1 • • • wn, w/ = wl o • • • o w'n, and o is the composition operator for semantic molecules (more details about the composition operator are given in Section 5).",
        "For brevity, we denote a rule by A – ß : $, where A G NG,ß G Ps is the set of constraint grammar rules whose left-hand side are pre-terminals, A(a) – , A G pre(NG).",
        "is a flat ontology-based semantic representation.",
        "We use the notation A – a for this grammar rules.",
        "In LWFG due to partial ordering among nonterminals we can have ordered constraint grammar rules and non-ordered constraint grammar rules (both types can be recursive or non-recursive).",
        "A grammar rule A(a) – Bi(ai),... ,Bn(an): $(ä), is an ordered rule, if for all Bi, we have A h Bi.",
        "In LWFGs, each nonterminal symbol is a left-hand side in at least one ordered non-recursive rule and the empty string cannot be derived from any nonterminal symbol.",
        "6.",
        "S G NG is the start nonterminal symbol, and VA G NG ,S h A (we use the same notation for the reflexive, transitive closure of h).",
        "The partial ordering relation h makes the set of nonterminals well-founded , which allows the ordering of the grammar rules, as well as the ordering of the syntagmas generated by LWFGs.",
        "This ordering allow LWFG learning from a small set of representative examples (Muresan and Rambow, 2007) (Ps is not learned).",
        "An example of a LWFG rule is given in Figure 1(3).",
        "Nonterminals are augmented with syntagmas.",
        "Moreover, in LWFG the semantic composition and interpretation are realized via constraints at the grammar rule level ($(a) in Definition 2).",
        "More precisely, syntagma composition means string concatenation (w = w1w2) and semantic molecule composition ((h) = o (^) ) – where the bodies of semantic molecules are concatenated through logical conjunction (b = (b1} b2)v, where v is a variable substitution v = {X2/X,X3/X}), while the semantic molecules heads are composed through compositional constraints $c(h, h1 ,h2), which are a simplified version of \"path equations\" (Shieber et al., 1983) (see Figure 1(3)).",
        "During LWFG learning, compositional constraints $c are learned together with the grammar rules.",
        "Semantic interpretation, which is ontology-based in LWFG, is also encoded as constraints at the grammar rule level – $onto – providing access to meaning during parsing.",
        "$onto (b) constraints are applied to the body of the semantic molecule corresponding to the syntagma associated with the left-hand side nonterminal.",
        "The ontology-based constraints are not learned; rather, $onto is a general predicate that succeed or fail as a result of querying an ontology – when it succeeds, it instantiates the variables of the semantic representation with concepts/slots in the ontology (see the example in Figure 1(3)).",
        "The derivation in LWFG is called ground syntagma derivation, and it can be seen as the bottom up counterpart of the usual derivation.",
        "Given a LWFG, G, the ground syntagma derivation relation, is defined as: A – a (if a = (w, w/), w G S, w/ G S/, i.e., A G pre(NG, ),",
        "and Bi=fat, i=1,...,n, A(a)-s-Bi(ai),...,Bn(an) : $(a) Atta .",
        "S+, 3A G Ng,A a}.",
        "Given a LWFG G, Ea ç La(G) is called a sublanguage of G. Extending the notation, given a LWFG G, the set of syntagmas generated by a rule (A – ß : $) G PG a denotes the ground derivation A a obtained using the rule A – ß : $ in the last derivation step."
      ]
    },
    {
      "heading": "3. LWFG Parsing as Deduction",
      "text": [
        "Following Shieber (1995), we present the Lexicalized Well-Founded Grammar parser as a deductive proof system in Table 1.",
        "The items of the logic are of the form \\i,j,aij,A – a • ß$A], where A – aß : $A is a grammar rule, $A – the constraints corresponding to the grammar rule whose left-hand side nonterminal is A – can be true, • shows how much of the right-hand side of the rule has been recognized so far, i points to the parent node where the rule was invoked, and j points to the position in the input that the recognition has reached.",
        "We use the following notations:",
        "afj = (wfj, ) are syntagmas corresponding to the partially parsed right-hand side of a rule; aL = (wLj, (Jf)) are ground-derived syntagmas (i.e., they are augmenting the left-hand side non-",
        "The set ofall syntagmas generated by a grammar G is La(G) = [a\\a = (w,w/),w G",
        "l < i, j < n + l, A e Ng , aß e NG the <&A constraint can be true Inference Rules Prediction Completion",
        "o\"ik = o\"R ◦ ojk, where",
        "wik = wij Wjk , bik = bijbjk , hik = hij U hjk",
        "Constraint (<&A is satisflable )",
        "terminal of a LWFG rule).",
        "The goal items are of the form aL,A – where tl is ground-derived from the rule A – a : $A.",
        "Compared to the deductive system in (Shieber et al., 1995), the LWFG parser has the following characteristics: each item is augmented with a syntagma; the Constraint rule is a new inference rule, and the goal items are associated to every nonterminal in the grammar, not only to the start symbol (i.e., LWFG parser is a robust parser).",
        "The Constraint inference rule is the only one that obtains an inactive edge, from an active edge by executing the grammar constraint $A (the • is shifted across the constraint).",
        "By applying the Constraint rule as the last inference rule we obtain the ground-derived syntagmas tl .",
        "Thus, the goal items are obtained only after the Constraint rule is applied.",
        "During this inference rule we have that aij = ^(aij), where 0 is defined by: wL = , à = bfjVij, and hj = p(hfj).",
        "The substitution Vij and the function tp are implicitly contained in the grammar constraint (hfj, h|j ) (see Section 5 for details) Definition 3 (Robust parsing provability).",
        "Robust parsing provability corresponds to reaching the goal item: hrp A(tl) iff tl, A – a$A^].",
        "Thus, we can notice that the ground syntagma derivation is equivalent to robust parsing provability, i.e., A tiff G hrp A(t)."
      ]
    },
    {
      "heading": "4. Learning LWFGs",
      "text": [
        "The theoretical learning model for LWFG induction, Grammar Approximation by Representative Sublanguage (GARS), together with a learnability theorem was introduced in (Muresan and Ram-bow, 2007).",
        "LWFG's learning framework characterizes the \"importance\" of substructures in the model not simply by frequency, but rather linguistically, by defining a notion of \"representative examples\" that drives the acquisition process.",
        "Informally, representative examples are \"building blocks\" from which larger structures can be inferred via reference to a larger generalization corpus referred to as representative sublanguage in (Muresan and Rambow, 2007).",
        "The GARS model uses a polynomial algorithm for LWFG learning that take advantage of the building blocks nature of representative examples.",
        "The LWFG induction algorithm belongs to the class of Inductive Logic Programming methods (ILP), based on entailment (Muggleton, 1995; Dzeroski, 2007).",
        "At each step a new constraint grammar rule is learned from the current representative example, T .",
        "Then this rule is added to the grammar rule set.",
        "The process continues until all the representative examples are covered.",
        "We describe below the process of learning a grammar rule from the current representative example:",
        "1.",
        "Most Specific Grammar Rule Generation.",
        "In the first step, the most specific grammar rule is generated from the current representative example T .",
        "The category annotated",
        "u = (îu, . )",
        "- Current representative example",
        "(Muresan and Rambow, 2007)).",
        "The performance criterion in choosing the best grammar rule among these candidate hypotheses is the number of examples in the representative sublanguage Ea (generalization corpus) that can be parsed using the candidate grammar rule, rgi in the last ground derivation step, together with the previous learned rules, i.e., |E<jnL<j(rgi)|.",
        "In Figure 2 given the representative sublanguage Ea={ laser printer, laser printer manual, desktop laser printer} the learner will generalize to the recursive rule NC – NA NC : $7, since only this rule can parse all the examples in Ea.",
        "in the representative example gives the left-hand-side nonterminal, while a robust parser returns the minimum number of chunks covering the representative example.",
        "The categories of the chunks give the nonterminals of the right-hand side of the most specific rule.",
        "For example, in Figure 2, given the representative example laser printer annotated with its semantic molecule, and the background knowledge containing the already learned rules NA – Noun : $C1, NA – NANA : $C2, NC – Noun : $C3the robust parser generates the chunks corresponding to the noun laser and the noun printer: [NA(laser),Noun(laser)] and [NC(printer),Noun(printer)], respectively.",
        "The most specific rule is NC – Noun Noun : $C4, where the left-hand side nonterminal is given by the category of the representative example, in this case nc.",
        "Compositional constraints $C4are learned as well.",
        "In section 5 we give the algorithm for learning these constraints, and several properties and principles that are needed in order for these constraints to be learnable.",
        "2.",
        "Grammar Rule Generalization.",
        "In the second step, this most specific rule is generalized, obtaining a set of candidate grammar rules (the generalization step is the inverse of the derivation step used to define the complete grammar lattice search space in"
      ]
    },
    {
      "heading": "5. Learnable Composition Constraints",
      "text": [
        "In LWFG, the semantic structures are composed by constraint solving, rather than functional application (with lambda expressions and lambda reduction).",
        "This section presents the properties and principles that guarantee the learnability of the compositional constraints,^, and presents an algorithm to generate these constraints from examples, which is a key result for LWFG learnability.",
        "The information for semantic composition is encoded in the head of semantic molecules.",
        "There are three types of attributes that belong to the semantic molecule head h: category attributes Ach, variable attributes Ah, and feature attributes AJh.",
        "Thus, Ac = Ah U Ah U Afh and Ah, Ah, Afh are pairwise disjoint.",
        "For example, in Figure 1 for the noun-noun compound laser printer, we have that Ah = {cat}, Ah = {nr}, and Ah = {head}, while for the noun laser we have that ACh1 = {cat}, Ah = 0, and A\"hi = {head, mod} (nouns can be modifiers of other nouns, so their representation is similar to that of an adjective).",
        "We describe in turn each of these types of attributes and their corresponding principles.",
        "All principles, except the first and the last mirror principles in other constraint-based linguistic formalisms, such as HPSG (Pollard and Sag, 1994).",
        "The category attributes Ah are state attributes, and their value set gives the category of the semantic molecule.",
        "There is one attribute, cat G Ah, which is mandatory and whose value is the name of the category (e.g., h.cat = nc in Figure Principle 1 (Category Name Principle).1).",
        "The category of a semantic molecule can be given by: 1) the cat attribute alone, or 2) the cat attribute together with other state attributes in Ahwhich are syntactic-semantic markers.",
        "The category name h.cat of a syntagma a = (w, {^) ) is the same as the grammar nonterminal augmented with syntagma a.",
        "When learning a LWFG rule from an example a, the above principle allows us to determine the nonterminal in the left-hand side of the grammar rule.",
        "For example, when learning the LWFG rule from the syntagma corresponding to laser printer in Figure 2, the nonterminal in the left-hand side of the LWFG rule is NC since h.cat = nc.",
        "The variable attributes Ah are attributes whose values are logical variables and represent the semantic valence of the molecule, which allows the binding of the semantic representations.",
        "These logical variables appear in the semantic molecule body as well.",
        "For example, in Figure 1(2) for the noun-noun compound laser printer, the value of the variable attribute head G Ah is a variable X, which appears also in the body of the semantic molecule (X\\.isa = laser, X.Pi = X\\,X.isa = printer).",
        "It can be noticed that the semantic molecule body contains other variables as well (Xi,Pi).",
        "However, only the variables present in the semantic molecule head as well (X) will participate in further composition.",
        "Principle 2 (Semantic Representation Binding Principle).",
        "All the logical variables that the body b of a semantic molecule corresponding to a syntagma a = (w, ), share with other syntagmas, are at the same time values ofthe variable attributes (Ah) of the semantic molecule head.",
        "There is one variable attribute, head G Ah that represents the head of a syntagma, giving the following principle:",
        "Principle 3 (Semantic Head Principle).",
        "Given a syntagma a = (w, i^) ) ground derived from a grammar rule, r, there exists one and only one syntagma ai = (wi, ) corresponding to a nonterminal Bi in ruler's right-hand side, which has the same value ofthe attribute head, i.e., h.head = hi.head.",
        "The feature attributes Ah are the attributes whose values express the specific properties of the semantic molecules (e.g., number, person).",
        "Principle 4 (Feature Inheritance Principle).",
        "If ai = (wi, (h) ) is the semantic head of a ground-derived syntagma a = (w, h ), then all feature attributes ofa inherit the values ofthe corresponding attributes that belong to the semantic head ai.",
        "That is, if h.head = hi.head , then h.f = hi.f, V/ g Ah n Afhz.",
        "Besides this principle, the feature attributes are used for category agreement.",
        "The categories that enter in agreement are maximum projection categories.",
        "This linguistic knowledge about agreement is used in the form of the following principle:",
        "Principle 5 (Feature Agreement Principle).",
        "The agreeing categories and the agreement features are a-priori given based on linguistic knowledge, and are applied only at the semantic head level.",
        "Given all the above principles, we can now formulate the general Composition Principle:",
        "Principle 6 (Composition Principle).",
        "A syntagma a = (w,wr) corresponding to the left-hand side nonterminal of a grammar rule is obtained by string concatenation (w = w1.. .wn) and the composition ofsemantic molecules corresponding to the nonterminals from the rule right-hand side:",
        "The composition ofthe semantic molecule bodies is realized through conjunction after the application of a variable substitution v. The body variable specialization substitution v is the most general unifier (mgu) of b and bi,...,bn, s.t b = (bi,... ,bn)v. It is a particular form of the commonly used substitution (Lloyd, 2003), i.e., a finite set of the form {Xi/Yi,..., Xm/Ym}, where Xi,..., Xm, Yi,...,Ym are variables, and Xi , .",
        ".",
        ".",
        ", Xm are distinct.",
        "The composition of the semantic molecule heads is realized by a set of constraints ^c(h,hi...,hn), which is a system of equations similar to \"path equations\" (Shieber et al., 1983; van Noord, 1993), but applied to flat feature structures:",
        "hi.f = ctor } where vi eAhi,vj eAlj",
        "When learning a LWFG rule from a representative example a as in Figure 2, the robust parser returns the minimum number of chunks, n, covering a.",
        "The body variable substitution v is fully determined by the representative example as mgu of b and b\\,... ,bn, and the compositional constraints $c(h, hi,..., hn) are learned using Alg 1.",
        "For example, in Figure 2, when learning from the representative example corresponding to the string laser printer, we have that v = [Xi/B,X2/A,X3/A, Y/Pi}.",
        "In Alg 1 we use the notation ao = (w0, (h°) ) to denote the representative example a.",
        "Alg 1: LearnJConstraints(a0, ai,..., an)",
        "1 foreach 0 < i < n A c e Arhi do if hi.c = cl then 2 foreach 0 < i,j < n A i = j A X/Y e vA",
        "vi e A\\% A vj e A°hj do if hi.vi = X A hj .vj = Y then",
        "3 if hs.head = ho.head, l < s < n then foreach f e Ah0 n Als do",
        "if ho.f = cl A hsff = cl then if hs.cat = cs A 1 < i < n then",
        "foreach f e agrFeatures(cs, ci) do if hs.f = cl A hi.f = cl then",
        "In the first step, the constraints corresponding to category attributes are fully determined by the values of these attributes that appear in the semantic molecule heads of a0,... an.",
        "In Figure 2, when learning the most specific rule r from the representative example laser printer, the set of constraints {h.cat = nc, hi.cat = noun, h2 = noun} C $C4 are the constraints corresponding to category attributes.",
        "In the second step, the constraints corresponding to variable attributes are fully determined by the variables in the substitution v that also appear as values of variable attributes hi.Vi,hj.Vj, where 0 < i,j < n and i = j.",
        "In Figure 2, only {X2/A,X3/A} C v will be used, generating the set of constraints {h.head = hi.mod,h.head = h2.head} C $C4.",
        "In the third step, the values of the feature attributes which obey Principles 4 and 5 are generalized – agr(cs, ci) is the predicate which gives us the agreement between the categories cs and ci (e.g., the subject agrees with the verb), and agrFeatures(cs, ci) gives us the set of feature attributes that participate in agreement (e.g., nr, pers, case).",
        "In Figure 2, the set of constraints {h.nr = h2.nr} C $C4 represents the generalization of the feature attribute values for nr, using Principle 4 .",
        "For all features attributes besides the ones that obey the above two principles, the generated constraints keep the particular values ofthese attributes (step 4 of Alg 1)."
      ]
    },
    {
      "heading": "6. Examples",
      "text": [
        "The LWFG formalism allows us to learn grammars for deep language understanding from examples.",
        "Instead of writing syntactic-semantic grammar by hand (both rules and constraints), we need to provide only a small set of representative examples – strings and their semantic molecules.",
        "Qualitative experiments on learning LWFGs showed that complex linguistic constructions can be learned and covered, such as complex noun phrases, relative clauses and reduced relative clauses, finite and non-finite verbal constructions (including, tense, aspect, negation, and subject-verb agreement), and raising and control constructions (Muresan and Rambow, 2007).",
        "In Figure 3 we show an example of learning a LWFG grammar for noun-noun compounds.",
        "The first four examples (1-4) are representative examples, while the last four examples are used for gener-"
      ]
    },
    {
      "heading": "4. for all other f e",
      "text": []
    },
    {
      "heading": "1.. (laser,",
      "text": [
        "cat na head A mod B"
      ]
    },
    {
      "heading": "2.. (laser printer,",
      "text": []
    },
    {
      "heading": "3.. (printer,",
      "text": []
    },
    {
      "heading": "5.. (laser printer manual,",
      "text": []
    },
    {
      "heading": "6.. (desktop laser printer,",
      "text": []
    },
    {
      "heading": "7.. (laser printer manual,",
      "text": []
    },
    {
      "heading": "8.. (desktop laser printer,",
      "text": [
        "alization (5-8).",
        "The learned grammar rules, including the learned composition constraints are also shown.",
        "The first two LWFG rules ground derive syntagmas for noun adjuncts, while the last two rules ground derive syntagmas for noun compounds.",
        "For example, \"desktop laser printer\" can be either a fully-formed noun compound (category nc), or it can be further combined with the noun \"invoice\" to obtain \"desktop laser printer invoice\", case in which it is a noun adjunct (category na).",
        "The learned rule for noun adjuncts is both left and right recursive, accounting for both left and right-branching noun compounds.",
        "Even though we can obtain overgeneralization in syntax, the ontology-based interpretation constraint at the rule level will prune some erroneous parses.",
        "Preliminary results in the medical domain show that $orato can help remove erroneous parses even when using just a weak ontological model (semantic roles of verbs, prepositions, attributes of adjectives and adverbs, but no synonymy, or hierarchy of concepts or roles).",
        "However, more experiments need to be run for reporting quantitative results."
      ]
    },
    {
      "heading": "7. Conclusions",
      "text": [
        "We have presented the properties and principles that the semantic representation integrated in LWFG requires so that the semantic compositional constraints are learnable from examples.",
        "These properties together with Alg 1 give a theoretical result that in conjunction with the learn-ability result of Muresan and Rambow (2007) show that LWFG is a learnable constraint-based grammar formalism that can be used for deep language understanding.",
        "Instead of writing grammar rules and constraints by hand, one needs to provide only a small set of annotated examples."
      ]
    }
  ]
}
