{
  "info": {
    "authors": [
      "Travis Goodwin",
      "Bryan Rink",
      "Kirk Roberts",
      "Sanda Harabagiu"
    ],
    "book": "SemEval",
    "id": "acl-S12-1063",
    "title": "UTDHLT: COPACETIC System for Choosing Plausible Alternatives",
    "url": "https://aclweb.org/anthology/S12-1063",
    "year": 2012
  },
  "references": [
    "acl-L08-1018",
    "acl-N03-1033",
    "acl-P03-1054",
    "acl-P05-3021",
    "acl-P08-2045",
    "acl-P10-1040",
    "acl-S12-1052"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The Choice of Plausible Alternatives (COPA) task in SemEval-2012 presents a series of forced-choice questions wherein each question provides a premise and two viable cause or effect scenarios.",
        "The correct answer is the cause or effect that is the most plausible.",
        "This paper describes the COPACETIC system developed by the University of Texas at Dallas (UTD) for this task.",
        "We approach this task by casting it as a classification problem and using features derived from bigram co-occurrences, TimeML temporal links between events, single-word polarities from the Harvard General Inquirer, and causal syntactic dependency structures within the gigaword corpus.",
        "Additionally, we show that although each of these components improves our score for this evaluation, the difference in accuracy between using all of these features and using bigram co-occurrence information alone is not statistically significant."
      ]
    },
    {
      "heading": "1 The Problem",
      "text": [
        "?The surfer caught the wave.?",
        "This statement, although almost tautological for human understanding, requires a considerable depth of semantic reasoning.",
        "What is a surfer?",
        "What does it mean to ?catch a wave??",
        "How are these concepts related?",
        "What if we want to ascertain, given that the surfer caught the wave, whether the most likely next event is that ?the wave carried her to the shore?",
        "or that ?she paddled her board into the ocean??",
        "This type of causal and temporal reasoning requires a breadth of world-knowledge, often called commonsense understanding.",
        "Question 15 (Find the EFFECT) Premise: I poured water on my sleeping friend.",
        "ing an effect, and another targeting a cause.",
        "The seventh task of SemEval-2012 evaluates precisely this type of cogitation.",
        "COPA: Choice of Plausible Alternatives presents 1,0001 sets of two-choice questions (presented as a premise and two alternatives) provided in simple English sentences.",
        "The goal for each question is to choose the most plausible cause or effect entailed by the premise (the dataset provided an equal distribution of cause and effect targetting questions).",
        "Additionally, each question is labeled so as to describe whether the answer should be a cause or an effect, as indicated in Figure 1.",
        "The topics of these questions were drawn from two sources:",
        "1.",
        "Randomly selected accounts of personal stories taken from a collection of Internet weblogs (Gordon and Swanson, 2009).",
        "2.",
        "Randomly selected subject terms from the Library of Congress Thesaurus for Graphic Materials (of Congress.",
        "Prints et al., 1980).",
        "Additionally, the incorrect alternatives were authored 1This data set was split into a 500 question development (or training) set and a 500 question test set.",
        "with the intent of impeding ?purely associative methods?",
        "(Roemmele et al., 2011).",
        "The task aims to evaluate the state of commonsense causal reasoning (Roemmele et al., 2011)."
      ]
    },
    {
      "heading": "2 System Architecture",
      "text": [
        "Given a question, such as Question 15 (as shown in Figure 1), our system selects the most plausible alternative by using the output of an SVM classifier, trained on the 500 provided development questions and tested on the 500 provided test questions.",
        "The classifier operates with features describing information extracted from the processing of the question's premise and alternatives.",
        "As illustrated by Figure 2, the preprocessing involves part of speech (POS) tagging, and syntactic dependency parsing provided by the Stanford parser (Klein and Manning, 2003; Toutanova et al., 2003), multi-word expression detection using Wikipedia, automatic TimeML annotation using TARSQI (Verhagen et al., 2005; Pustejovsky et al., 2003), and Brown clustering as provided in (Turian, 2010).",
        "The architecture of the COPACETIC system is divided into offline (independent of any question) and online (question dependent) processing.",
        "The online aspect of our system inspects each question using an SVM and selects the most likely alternative.",
        "Our system's offline functions focus on preprocessing resources so that they may be used by components of the online aspect of our system.",
        "In the next section, we describe the offline processing upon which our system is built, and in the following section, the online manner in which we evaluate each question."
      ]
    },
    {
      "heading": "2.1 Offline Processing",
      "text": [
        "Because the questions presented in this task require a wealth of commonsense knowledge, we first extracted commonsense and temporal facts.",
        "This subsection describes the process of mining this information from the fourth edition of the English Gigaword corpus2 (Parker et al., 2009).",
        "We collected commonsense facts by extracting cause and effect pairs using twenty-four hand-crafted patterns.",
        "Rather than lexical patterns, we used patterns over syntactic dependency structures in order to capture the syntactic role each word plays.",
        "Figure 3 illuminates two examples of the dependency structures encoded by our causal patterns.",
        "Causal Pattern 1 captures all cases of causality indicated by the verb causes, while Causal Pattern 2 illustrates a more sophisticated pattern, in which the phrasal verb brought on indicates causality.",
        "In order to extract this information, we first parsed the syntactic dependence structure of each sentence using the Stanford parser (Klein and Manning, 2003).",
        "Next, we loaded each sentence's dependence tree",
        "the causal patterns: ?cause ?causes?",
        "?effect, and ?cause ?brought on?",
        "?effect.",
        "into the RDF3X (Neumann and Weikum, 2008) implementation of an RDF3 database.",
        "Then, we represented our dependency structures using in the SPARQL4query language and extracted cause and effect pairs by issuing SPARQL queries against the RDF3X database.",
        "We used SPARQL and RDF representations because they allowed us to easily represent and reason over graphical structures, such as those of our dependency trees.",
        "It has been shown that causality often manifests as a temporal relation (Bethard, 2008; Bethard and Martin, 2008).",
        "The questions presented in this task are no exception: many of the alternative-premise pairs necessitate temporal understanding.",
        "For example, consider question 63 provided in Figure 4.",
        "Question 63 (Find the EFFECT) Premise: The man removed his coat.",
        "WHERE clause for a SPARQL query associated with the brought on pattern from Figure 3 is provided below: { ?a <nsubj> ?cause ; <token> \"brought\" ; <prep> ?b .",
        "?b <token> \"on\" ; <pobj> ?effect . }",
        "In order to extract this temporal information, we automatically annotated our corpus with TimeML annotations using the TARSQI Toolkit (Verhagen et al., 2005).",
        "Unfortunately, the events represented in this corpus were too sparse to use directly.",
        "To mitigate this sparsity, we clustered events using the 3,200 Brown clusters5 described in (Turian, 2010).",
        "After all such offline processing has been completed, we incorporate the knowledge encoded by this processing in the online components of our system (online preprocessing, and feature extraction) as described in the following section."
      ]
    },
    {
      "heading": "2.2 Online Processing",
      "text": [
        "We cast the task of selecting the most plausible alternative as a classification problem, using a support vector machine (SVM) supervised classifier (using a linear kernel).",
        "To this end, we pre-process each question for lexical information.",
        "We extract parts of speech (POS) and syntactic dependencies using the Stanford CoreNLP parser (Klein and Manning, 2003; Toutanova et al., 2003).",
        "Stopwords are removed using a manually curated list of one hundred and one common stopwords; non-content words (defined as words whose POS is not a noun, verb, or adjective) are also discarded.",
        "Additionally, we extract multi-word expressions (noun collocations6 and phrasal verbs7).",
        "Finally, in order to utilize our offline TimeML annotations, we extract events using POS.",
        "Examples of the retained content words are underlined in Figures 5, 6, 7 and 8.",
        "After preprocessing each question, we convert it into two premise-alternative pairs (PREMISE-ALTERNATIVE1, and PREMISE-ALTERNATIVE2).",
        "For each of these pairs, we attempt to form a bridge from the causal sentence to the effect sentence, without distinction over whether the cause or effect originated from the premise or the alternative.",
        "This bridge is provided by four measures, or features, described in the following section."
      ]
    },
    {
      "heading": "3 The Features of the COPACETIC",
      "text": []
    },
    {
      "heading": "System",
      "text": [
        "In determining the causal relatedness between a cause and an effect sentence, we utilize four features.",
        "Each feature calculates a value indicating the perceived strength of the causal relationship between a cause and an effect using a different measure of causality.",
        "The four features used by our COPACETIC system are described in the following subsections."
      ]
    },
    {
      "heading": "3.1 Bigram Relatedness",
      "text": [
        "Our first feature measures the degree of relatedness between all pairs of bigrams (at the token level) in the cause and effect pair.",
        "We do this by calculating the point-wise mutual Information (PMI) (Fano, 1961) for all bigram combinations between the candidate alternative and its premise in the English Gigaword corpus (Parker et al., 2009) as shown in Equation 1.",
        "Under the assumption that distance words are unlikely to causally influence each other, we only consider co-occurrences within a window of one hundred tokens when calculating the joint probability of the PMI.",
        "Additionally, we allow for up to two tokens to occur within a single bigram's occurrence (e.g. the phrase pierced her ears would be considered a match for the bigram pierced ears ).",
        "Although these relaxations skew the values of our calculated PMIs by artificially lowering the joint probability, we are only concerned with how the values compare to each other.",
        "Note that because we employ no smoothing, the PMI of an unseen bigram is set to zero.",
        "The maximum PMI over all pairs of bigrams is retained as the value for this feature.",
        "Figure 5 illustrates this feature for Question 495."
      ]
    },
    {
      "heading": "3.2 Temporal Relatedness",
      "text": [
        "Although most of the questions in this task focus on causal relationships, for many questions, the nature of this causal relationship manifests instead as a temporal one (Bethard and Martin, 2008; Bethard, 2008).",
        "We use temporal link information from TimeML (Pustejovsky et al., 2005; Pustejovsky et al., 2003) annotations on our corpus to determine how temporally related a given cause and effect sentence are.",
        "PMI(wear earrings, pierced ears) = -10.928 PMI(wear earrings, tattoo) = -12.77 PMI(wanted wear, pierced ears) = -13.284 PMI(wanted wear, tattoo) = -14.284 PMI(girl wanted, pierced ears) = -13.437 PMI(girl wanted, tattoo) = -14.762",
        "(with content words underlined).",
        "Alternative 1 is correctly chosen as it has largest maxi mum PMI.",
        "This is accomplished by using the point-wise mutual information (PMI) between all pairs of events from the cause to the effect (see Equation 1).",
        "We define the relevant probabilities as follows: ?",
        "The joint probability (P (x, y)) of a cause and effect event is defined as the number of times the cause event participates in a temporal link ending with the effect event.",
        "?",
        "The probability of a cause event (P (x)) is defined as the number of times the cause event precipitates a temporal link to any event.",
        "?",
        "The probability of an effect event (P (y)) is defined as the number of times the effect event ends a temporal link begun by any event.",
        "We define the PMI to be zero for any unseen pair of events (and for any pairs involving an unseen event).",
        "The summation of all pairs of PMIs is used as the value of this feature.",
        "Figure 6 shows how this feature behaves.",
        "words underlined).",
        "Alternative 2 is correctly chosen as it has the highest summation."
      ]
    },
    {
      "heading": "3.3 Causal Dependency Structures",
      "text": [
        "We attempted to capture the degree of direct causal relatedness between a cause sentence and an effect sentence.",
        "To determine the strength of this relationship,",
        "we considered how often phrases from the cause and effect sentences occur within a causal dependency structure.",
        "We detect this through the use of twenty-four8 manually crafted causal patterns (described in Section 2.1).",
        "The alternative that has the maximum number of matched dependency structures with the premise is retained as the correct choice.",
        "Figure 7 illustrates this feature.",
        "tent words underlined).",
        "Alternative 1 is correctly selected because more patterns extracted ?won?",
        "causing ?rich?",
        "than ?won?",
        "causing ?owed?."
      ]
    },
    {
      "heading": "3.4 Polarity Comparison",
      "text": [
        "We observed that many of the questions involve the dilemma of determining whether a positive premise is more related to a positive or negative alternative (and vice-versa).",
        "This differs from sentiment analysis in that rather than determining if a sentence expresses a negative statement or view, we instead desire the overall sentimental connotation of a sentence (and thus of each word).",
        "For example, the premise from Question 494 (Figure 8) is ?the woman became famous.?",
        "Although this sentence makes no positive or negative claims about the woman, the word ?famous?",
        "?",
        "when considered on its own ?",
        "implies positive connotations.",
        "We capture this information using the Harvard General Inquirer (Stone et al., 1966).",
        "Originally developed in 1966, the Harvard General Inquirer provides a mapping from English words to their polarity (POSITIVE, or NEGATIVE).",
        "For example, it denotes the word ?abandon?",
        "as NEGATIVE, and the word ?abound?",
        "as POSITIVE.",
        "We use this information by summing the score for all words in a sentence (assigning POSITIVE words a score of 1.0, NEGATIVE words a score of -1.0, and NEUTRAL or unseen words a score of 0.0).",
        "The difference between 8Twenty-four patterns was deemed sufficient due to time constraints.",
        "these scores between the cause sentence and the effect sentence is used as the value of this feature.",
        "This feature is illustrated in Figure 8.",
        "words underlined).",
        "Alternative 1 is correctly chosen as it has the least difference from the score of the premise."
      ]
    },
    {
      "heading": "4 Results",
      "text": [
        "The COPA task of SemEval-2012 provided participants with 1,000 causal questions, divided into 500 questions for development or training, and 500 questions for testing.",
        "We submitted two systems to the COPA Evaluation for SemEval-2012, both of which are trained on the 500 development questions.",
        "Our first system uses only the bigram PMI feature and is denoted as bigram pmi.",
        "Our second system uses all four features and is denoted as svm combined.",
        "The accuracy of our two systems on the 500 provided test questions is provided in Table 1 (Gordon et al., 2012).",
        "On this task, accuracy is defined as the quotient of dividing the number of questions for which the correct alternative was chosen by the number of questions.",
        "Although multiple groups registered, ours were the only submitted results.",
        "Note that the difference in performance between our two systems is not statistically significant (p = 0.411) (Gordon et al., 2012).",
        "The primary hindrance to our approach is in combining each feature ?",
        "that is, determining the confidence of each feature's judgement.",
        "Because the questions vary significantly in their subject matter and the nature of the causal relationship between given causes and effects, a single approach is unlikely",
        "to satisfy all scenarios.",
        "Unfortunately, the problem of determining which feature best applies to a give question requires non-trivial reasoning over implicit semantics between the premise and alternatives."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "This evaluation has shown that although commonsense causal reasoning is trivial for humans, it belies deep semantic reasoning and necessitates a breadth of world knowledge.",
        "Additional progress towards capturing world knowledge by leveraging a large number of cross-domain knowledge resources is necessary.",
        "Moreover, distilling information not specific to any domain ?",
        "that is, a means of inferring basic and fundamental information about the world ?",
        "is not only necessary but paramount to the success of any future system desiring to build chains of commonsense or causal reasoning.",
        "At this point, we are merely approximating such possible distillation."
      ]
    },
    {
      "heading": "6 Acknowledgements",
      "text": [
        "We would like to thank the organizers of SemEval2012 task 7 for their work constructing the dataset and overseeing the task."
      ]
    }
  ]
}
