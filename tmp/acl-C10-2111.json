{
  "info": {
    "authors": [
      "Gaël Patin"
    ],
    "book": "COLING – POSTERS",
    "id": "acl-C10-2111",
    "title": "Incremental Chinese Lexicon Extraction with Minimal Resources on a Domain-Specific Corpus",
    "url": "https://aclweb.org/anthology/C10-2111",
    "year": 2010
  },
  "references": [
    "acl-C08-1130",
    "acl-I08-1002",
    "acl-J04-1004",
    "acl-P98-2206",
    "acl-W00-1207",
    "acl-W03-1719",
    "acl-W06-2403"
  ],
  "sections": [
    {
      "text": [
        "Gael Patin",
        "(1) Texts, Computer Science and Multilingualism Research Center (Ertim) National Institute of Oriental Languages and Civilizations (Inalco) (2) Arisem, Thales Company",
        "This article presents an original lexical unit extraction system for Chinese.",
        "The method is based on an incremental process driven by an association score featuring a minimal resources statistically aided linguistic approach.",
        "We also introduce a linguistics-based lexical unit definition and use it to describe an evaluation protocol dedicated to the task.",
        "The experimental results on a domain specific corpus show that the method performs better than other approaches.",
        "The extraction results, evaluated on a random sample of the working corpus, show a recall of 68.4 % and precision of 37.1 %."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Lexical resources are all the more fundamental to NLP systems since domain specific corpora are multiple and various.",
        "The performance of common tasks, such as Information Retrieval or Information Extraction, can be improved by comprehensive and updated domain specific lexicon (i.e. terminology).",
        "However the constitution of lexicons raises pragmatic issues, such as development cost or re-usability, which have a great importance in an industrial context ; and also theoretical issues, such as the definition of the lexical unit or evaluation protocol, which are crucial for the relevance of the results.",
        "In Chinese text processing context, lexicons are particularly important for dictionary-based word segmentation techniques in which out-of-vocabulary words are an important cause of errors (Sproat and Emerson, 2003).",
        "In this paper we consider the lexicon extraction task independent of the word segmentation, this position differs from Zhao and Kit's (2004) point of view.",
        "Generally speaking, word segmentation aims at delimiting units in a sequence of characters.",
        "The delimited units are usually morphological lexical units (i.e. words) and internal composition of the unit is not considered.",
        "The evaluation process checks whether each word occurrence is well delimited.",
        "On the opposite, lexicon extraction aims at extracting lexicon entries from a corpus.",
        "The extracted units are morphological or syntactic units and the internal components are also considered.",
        "The evaluation process checks the extracted candidates list considering the corpus global scope.",
        "Many approaches for Chinese lexicon extraction rely on a supervised word segmenter (Wu and Jiang, 2003; Li et al., 2004) or a morpho-syntactic tagger (Piao et al., 2006) to extract unknown words.",
        "These techniques perform well but suffer from a major drawback, they cannot be applied efficiently to corpora that cover different domains than the calibration corpus.",
        "Some approaches are nested in an unsupervised word segmentation process and aim at improving its effectiveness.",
        "Fung and Wu (1994) try to select segments using mutual information on bigram.",
        "Chang and Su (1997) present an iterative unsupervised lexicon extraction system driven by the quality of segmentation obtained with the discovered lexicon.",
        "This approach, although efficient, imposes an arbitrarily 4-character length restriction on candidates.",
        "Other works, like this approach, focus on the lexicon or terminology extraction as standalone task.",
        "Feng et al.",
        "(2004) introduce a lexicon extraction unsupervised method based on context variation with very convincing results.",
        "Yang et al.",
        "(2008) focus on terminology extraction using delimiters extracted from a training corpus with good results.",
        "This study proposes an original answer to the Chinese lexicon extraction task using an incremental minimal resources method to extract and rank lexical unit candidates.",
        "An annotated reference corpus is required to extract a common-word dictionary and to prepare the data.",
        "The method has the advantage of proposing structured candidates, which allow interactive candidate filtering.",
        "In addition the candidate maximum length is determined by the number of associations that allow the detection of the longer lexical units.",
        "We extend the association measure method introduced by Sun et al.",
        "(1998) for word segmentation without lexical resources.",
        "This paper starts with a linguistic definition of the lexical unit which drives the method.",
        "We also build on it to propose an improvement of the evaluation protocol for the Chinese lexicon extraction task."
      ]
    },
    {
      "heading": "2. Lexical Unit Definition",
      "text": [
        "Although defining the Chinese lexical unit is not a trivial task, we think that it is absolutely necessary for the understanding of the kind of linguistic phenomena we are dealing with.",
        "Without this knowledge we may miss important features and may not be able to efficiently evaluate the extraction process.",
        "We introduce two linguistic concepts to define the lexical units focusing on contemporary written Chinese: the morpho-syntactic unit and the lexical content.",
        "These definitions use concepts introduced by Polguere (2003) applied to the Chinese case by Nguyen (2008).",
        "A graphy is the Chinese minimal autonomous orthographic unit and it approximatively matches the glyph concept in computer science.",
        "The following glyphs are different Chinese graphies: I, fm, %, S5, S. A morph (noted | m |) is the smallest meaningful unit representable by a sequence of graphies.",
        "Morphs are atomic so that they cannot be representable by a smaller sequence of morphs.",
        "The following sequences of graphies are different the graphy S does not carry any meaning and is not a morph.",
        "A morpheme (noted ||M|) is a set of morphs sharing the same lexical content ignoring grammatical inflection or variants (Table 1).",
        "Chinese morphs cannot be inflected, unlike European languages, but some graphies have variants.",
        "Morpheme Morph",
        "r* |.. r* |..",
        "Table 1: Morphemes and related morphs",
        "A word-form (noted ( w )) is an autonomous and insepa able sequence of mo phs.",
        "Autonomy means that it can be enunciated individually and can take place in a syntactic pa adigm.",
        "Insepa-ability means that b eaking the sequence causes the loss of the elationship between elements.",
        "A lexeme (noted ( W ))) is a set of word-forms sharing the same lexical content ignoring inflection or variants (Table 2).",
        "Word-form",
        "Table 2: Lexemes and associated word-forms",
        "A phrase (noted [ s ]) is a syntactic combination of word-forms.",
        "The syntactic nature of the combination implies that the phrase components are relatively free.",
        "A locution (noted [ S ]) is a set of lexicalized phrases sharing the same lexical content ignoring inflection or variants (Table 3).",
        "Locution Phrase",
        "[ tfte] [(*)(&)][(*T)(te)] ...",
        "The morphs, word-forms and phrases are the morpho-syntactic units, they describe the composition of lexemes and locutions.",
        "The lexical units we look for are lexemes and locutions.",
        "Finding lexical units means identifying words-forms and phrases having a lexical content.",
        "We use two criteria to define the lexical content: the compositionality criterion and the referential-ity criterion (Table 4).",
        "Units which fulfill at least one of these criteria are said to have a lexical content.",
        "The compositionality criterion (or lexicalization criterion) is relative to the relationship between the sense of the unit and the sense of its components.",
        "The question is whether or not the sense of the unit can be deduced from the combination of its components.",
        "The referentiality criterion is related to the relationship between the unit and the referent concept or object.",
        "The question is whether or not the referent has specific properties for the speakers.",
        "This criterion is strongly dependent on human judgment and the working domain.",
        "Morph detection",
        "Table 4: Referential and Compositional units",
        "The Table 4 presents examples of four criterion combinations.",
        "Referentiality and compositionality criteria are always applied at the highest association level, thus [^Pè-^s] ]) is compositional, although {^ft) and {^^i) are not compositional.",
        "Word-forms are not necessarily compositional or referential, thus the unit IX-fc) does not refer to any concept and we can use the combination of its components to interpret it: (( -tft )) + || -fc ||.",
        "Referentiality does not imply lexicalization, thus the compositional unit is referential because it refers to the German car brands or characteristics in the automobile context.",
        "Segment detection Lexical unit candidate selection Max level reached ?",
        "Candidates reorganization 4 [ Presentation & user interaction J-* -",
        "Figure 1 : Method overview 3 Methodology",
        "The method (Figure 1) follows the linguistic intuitions developed in the previous section.",
        "We identify morpho-syntactic units and select those that are likely to have a lexical content to obtain lexical unit candidates (LUCs).",
        "The word-forms and phrases are respectively generated by associations of morphs or word-forms and association of word-forms or phrases.",
        "We consequently use an incremental process, which associates LUCs as they are selected.",
        "The incremental process is initiated by detecting every morph and spliting the corpus into segments.",
        "Then we enumerate all the morpho-syntactic unit couples and use lexical content criteria to select the couples to associate.",
        "This process is repeated until the maximum number of associations is reached.",
        "At the end, the LUCs are reorganized and submitted to the user.",
        "The user's answers are used to filter the remaining LUCs.",
        "As stated in Section 2.1, we consider that the morph is the minimal morpho-syntactic unit.",
        "Every glyph is considered as a morph unless it can be included in an ancient loanword morph",
        "(( äft ))) or a foreign transcription morph",
        "((;È*.#J)> {ê:ÏLK)).",
        "In an ambiguous case the longest possibility is accepted.",
        "Foreign transcriptions are phonetic interpretations of foreign words using the pronunciation of the Chinese graphies.",
        "The set of graphies used for transcription is well-",
        "Referential",
        "No-Referential",
        "Compositional",
        "„Chinese tood,.",
        "_ insurance company -„",
        "r African car ,",
        "Lexicalized",
        "„ disinfect „",
        "(«*)",
        "everyone",
        "_ dividend product _",
        "selling vinegar as wine",
        "known and closed.",
        "We trained a CRF taggerusing simple features based on current, next and previous graphies to extract foreign transcriptions (the training corpus is described in Section 4.1).",
        "Ancient loanwords importation process is not productive anymore, thus they are detected using a loanword list.",
        "The aim of the segment detection step is to split the corpus into segments (i.e. a succession of Chinese graphies).",
        "Chinese texts contain two kinds of delimiters which are not likely to be components of a lexical unit, delimiter-words and delimiter-expressions.",
        "Delimiter-words are enumerable with a common word dictionary and include prepositions (*t, ft), adverbs (#., -k, pronouns fßJL), interrogative pronouns C$1 conjunctions (i^Ü-, 'fa,",
        "discourse structure words (Sit, tonal particles (fî, p&) and tool-words (^).",
        "Delimiter-expressions include numerical expressions (^^^^, -2-^), temporal expressions (4^H&-i-, ^vAAl;^), circumstantial expressions (AL..fHê, ^...^) , which are easily describable using shallow context-free grammars.",
        "Delimiters are removed from the corpus and used to delimit the segments.",
        "The inflexions (T, Ü, Ji\"), which introduce inflectional variations, are also removed from the corpus but do not delimit the segments.",
        "The delimiters identification is controlled by rules.",
        "For instance tonal particles are removed only if they are the end of a segment, discourse structure words are removed only if they are the beginning of a segment.",
        "Delimiters and inflexions are not removed if they are inside a sequence of graphies which is present in a common-word dictionary.",
        "In this step, lexical unit candidates (LUCs) are extracted by selecting morpho-syntactic unit couples, which are likely to have a lexical content.",
        "The first assumption is that lexical units can always be decomposed into binary trees.",
        "Only a small number of lexical units do not satisfy this",
        "Sentence with delimiters noted {delimiter}:",
        "^.",
        "#*i&{f }^®{^}#n##.",
        "Obtained segments noted [segment]:",
        "Figure 2: Segment detection example assumption (e.g. ^-JLl,), in such case it is possible to select a non-linguistically motivated way to decompose the unit into binary associations.",
        "Thus, every couples of contiguous morpho-syntactic units are iteratively enumerated for each segment.",
        "The second assumption is that association measures are good statistical evidence to detect lexical content.",
        "Thereby, the association strength of morpho-syntactic couples is used as a main criterion to identify relevant candidates.",
        "Consider G the alphabet of all Chinese graphies, M = G+ the language describing the morpho-syntactic units, Sn a set of segments at step n, sn = m1,m2,...,mn the ith segment of Sn where Vm e sn | m e M and sn the set of all morpho-syntactic unit couples in Sn segments.",
        "Given the morpho-syntactic unit couple mi; m;+i e sn (denoted as m^+i), the lexical content criteria (LCC(m^+i)) matches if the following conditions are fulfilled:",
        "1.",
        "Neither mi nor mi+i has not been associated at the current step n.",
        "2.",
        "Nb(mi) = 1 or Nb(mi+i) = 1.",
        "3.",
        "AS(mi,i+i) > T.",
        "where Nb(x) is the number of occurrences of x, AS (x, y) returns the association score of the couple x, y computed with a given association measure, and T is the association threshold relative to the association measure (cf. 4.1).",
        "Let So the initial set of segments where Vs0 e So, s0 is a segment (cf. 3.2) such that Vm e s0, m is a morph (cf. 3.1).",
        "The LUC list is composed of morpho-syntactic couples produced by the association operator © to compute Smax (algorithm 1) with max the maximum number of iteration.",
        "with © the association operator whose result is a morpho-syntactic unit, Sn[mi © m2] the replacement of mi and m2 by the morpho-syntactic unit mi © m2 in the corresponding segment.",
        "See the Section 5 for more details about the maximum number of iteration setting.",
        "Once LUCs are extracted, we map every LUC to the couple of morpho-syntactic units it is composed of.",
        "These units are called components.",
        "Some LUCs are generated from two different couples at the candidate selection step.",
        "For instance, is discovered in two ways: 7k$f©?k3k or ^.^ik©;t\\ We always choose the most frequent option.",
        "When the \"LUC/couple\" map is created, we sort the LUCs by their corresponding couple association scores.",
        "Finally, if a LUC is ranked in the list before its components we move the components to the position just before it in the list and use the same rule to recursively check the moved components.",
        "The candidates list is expected to be ordered by likelihood deduced by an association measure and compositional order.",
        "The lexicon extraction task aims at submitting a ranked list of candidates to the user in order to help him produce lexical resources.",
        "The user is expected to check the list in this order and the method uses the user answers to discard not yet verified candidates.",
        "To do so, the user is asked to answer the following questions for each LUC according to the definition given in the Section 2:"
      ]
    },
    {
      "heading": "1.. Does the unit have a lexical content ?",
      "text": []
    },
    {
      "heading": "2.. Is the unit a part of a lexical unit ?",
      "text": [
        "If answers to both these questions are 'no' then all the candidates having this component are removed from the remaining list."
      ]
    },
    {
      "heading": "4. Evaluation",
      "text": [
        "Since the submitted candidates are progressively modified according to the user answers, the evaluated candidates are only the ones submitted to the user.",
        "We used three measures to evaluate the method: recall, precision and precision at rank n. Since producing large annotated corpora is costly, we perform the evaluation using a sample of texts from the evaluation corpus.",
        "Therefore the scores obtained are an estimation of the true scores.",
        "The inter-human variation is not considered here and should be integrated in further works.",
        "The morphs and the segment detection step use data from a reference corpus: The Lancaster Corpus of Mandarin Chinese (McEnery and Xiao, 2004).",
        "The corpus is composed of text samples choose in various domain and genre corpora, it contains two millions of glyphs and it is annotated according to the Beijing University annotation guideline.",
        "This corpus is mainly used to extract delimiter-words, to produce the grammar for delimiter-expressions and to extract a common-word dictionary.",
        "All foreign transcriptions are also annotated for the CRF tagger training (cf. 3.1).",
        "The lexical unit detection step is evaluated using four well-known association measures: Point-wise Mutual Information (PMI), Poisson-Striling (PS) (Quasthoff and Wolff, 2002), Log-likelihood (LL), Pointwise Mutual Information Cube (PMI) (Daille, 1994).",
        "These measures are detailed in table 5.",
        "The significant association threshold is intuitively given by the statistical interpretation of Variables",
        "x,y : words x : all words but x px : x probability fx : x frequency",
        "Table 5: Association score calculation the formulas for MI and PS.",
        "Thus, these measures are used for LCC's selection criterion 2 and T is set to 0 (cf. 3.3).",
        "A threshold can not be deduced from PS and PMI, therefore they are only used for LCC's comparison criteria 3 & 4.",
        "To prepare the evaluation we randomly selected twenty texts in an evaluation corpus and annotated lexical units according to the linguistic description given in Section 2.",
        "For each sample text, we obtained a set of lexical unit trees (Table 3) corresponding to all the encountered lexical units.",
        "N-trees are used for units which can not be transformed into binary tree.",
        "Two evaluation sets are defined, the shallow set which contains the root nodes of the lexical unit trees and the deep set which contains the inner nodes of the lexical unit trees.",
        "Given the four examples of Figure 3, the shallow set contains [fiîPè-Ha]], [4 and (f 4Sfc) ; and the deep",
        "(4$), (f 4Sfc) and (f 4S).",
        "Experiments with different parameters produce different candidate lists and an expert intervention is required to evaluate each candidate list.",
        "To avoid this problem, all the repeated sequences of non-inflectional graphies are generated from the annotated sample texts and intersected with the LUC list.",
        "The obtained list is a projection of the candidate list on the sample texts.",
        "This trick allows us to extract all LUCs appearing in the sam-",
        "Figure 3: Lexical unit trees ple texts and evaluate them automatically.",
        "5 Experiments",
        "The experiments are conducted on insurance domain corpus containing ten million graphies.",
        "This evaluation corpus is composed of news and articles collected automatically from Chinese insurance companies websites.",
        "The text fields are extracted with an xhtml parser.",
        "Several text fields, such as menus or buttons, are repeated and duplicates are removed to avoid noise.",
        "The presented method, referred as ILex (Incremental Lexicon Extractor), is applied using the previously mentioned 4 measures (cf. 4.1).",
        "The evaluation is based on couple of measures, the first measure is dedicated to candidates selection (LCC 2.)",
        "and the second to candidates comparison (LCC 3.",
        "& 4.).",
        "The comparison measure is also used to sort the candidates (cf. 3.4).",
        "The maximal number of iterations is set to 3 (for a maximal depth of 4), which is the maximum number of associations required to compose the majority of lexical units in the reference corpus.",
        "The precision and recall are computed on the deep set in order to consider all valid lexical units, the recall on the shallow set is given to see the results on wider lexical units (Table 6).",
        "The results show that PMI-LL couple performs better overall than the other measures.",
        "It can be noticed that the scores are relatively close (±1.8% for precision and ±7.0% for deep recall) meaning that the choice of the association measure has a low influence over the results.",
        "For the further experiments are conducted with PMI-LL, which achieves the best recall score.",
        "AM",
        "Formulas",
        "PMI",
        "log Pxy",
        "Px*P*y",
        "LL",
        "{x,x},{y,y} f",
        "2 E fij log f",
        "PS",
        "fc(log fc – log A – 1) log N",
        "PMI",
        ", Nfxy",
        "Jx*J*y",
        "M N H| |5|",
        "[4",
        "|4|li||*|V|",
        "(4",
        "Table 6: Measure combinations results",
        "The method extracted 585,794 LUCs from the whole corpus using the PMI-LL couple before applying the user interaction step.",
        "The candidate list projection (cf. 4.2) contains 4,539 LUCs.",
        "The user decisions are simulated with the lexical unit trees obtained from sample texts.",
        "In total 312 LUCs were removed in consequence of the user interaction (cf. 3.5), without this step the precision decreases to 33.7%.",
        "The 1,246 LUCs present in the common-word dictionary are ignored.",
        "Finally 1,886 invalid candidates and 1,105 valid lexical units are submitted to the user, the evaluation is based on these 3,059 LUCs.",
        "Table 7: Sample of extracted lexical units",
        "A sample of extracted lexical units is presented in Table 7.",
        "In this list, the lower number of occurrences is 1 and the longest unit length is 12 graphies.",
        "Most of the extracted lexical unit are terms, a significant number of people names, common words and larger named entities are extracted too.",
        "The most part of the very frequent lexical units are ranked at the top of the list but some low frequency LUCs are ranked over the high frequency candidates.",
        "The Figure 4 presents",
        "candidate list decile",
        "the results as a function of the LUC list deciles.",
        "The LL sorting is compared to frequency sorting for the precision at rank n. The LL sorting curve is above the frequency sorting curve, this fact shows that LL is more efficient at sorting valid LUCs.",
        "The majority of the missed candidates have a low number of occurrences (<3) and 57.8% of the longest lexical unit (>7) are also missed.",
        "Most of extraction errors have a low number of occurrences, 40.1% of the errors are caused by lexical unit composition errors",
        "The AccessVar method (Feng et al., 2004), an unsupervised lexicon extraction method having the best performance, was reimplemented and used as a reference.",
        "This method uses the corpus substrings' number of distinct contexts, noted AV (accessor variety), to extract candidates.",
        "Access-Var is configured by an accessors variety threshold (AVT), which is the minimal AV required to hold a candidate, the number of occurrences of candidates is consequently greater or equal to the AVT.",
        "For the experiments, the candidate maximal length is set to 7 graphies and AVT to 3.",
        "Similarly, ILex candidates appearing less than three times and having a length greater than 7 are discarded.",
        "The ILex user interaction is not applied for this comparison.",
        "In order unify the input data, AccessVar handles the segments detected by ILex",
        "Selection",
        "PMI",
        "PS",
        "Comparison",
        "LL",
        "PMI",
        "LL",
        "PMI",
        "Precision",
        "37.1",
        "38.9",
        "37.3",
        "38.1",
        "Deep recall",
        "68.4",
        "65.6",
        "62.3",
        "61.4",
        "Shallow recall",
        "75.1",
        "74.2",
        "70.6",
        "70.6",
        "precisi precis",
        "prec s m (sor ion (no",
        "n (sort ed by : user i sha d",
        "ed by '.",
        "requen iteracti low re eep re",
        "X)",
        "cy)",
        "on)",
        "call call",
        "_",
        "■*.....",
        "A -",
        "r - ' '",
        "k- -",
        ".....\"^\"-i",
        "-._t",
        "-",
        "!..........1",
        "--",
        "__",
        "■",
        "P-",
        "f------,",
        "f ' !",
        "Lexical unit",
        "Rank",
        "Nb.",
        "155",
        "1798",
        "Tai Kang Life Insurance",
        "453",
        "1,854",
        "(( a-STa ))",
        "1,048",
        "4,999",
        "... Nan Kai University _,,",
        "2,828",
        "111",
        "_ Los Angeles tourism professionals",
        "9,647",
        "3",
        "11,647",
        "871",
        "Wang Enshao (person).",
        "( iI« 1",
        "14,617",
        "2",
        "compensated use",
        "34,596",
        "8",
        "Taihu Lake Basin",
        "102,612",
        "2",
        "wait an opportunity",
        "(( ))",
        "126,044",
        "31",
        "_ The People's Republic of China labor contract law",
        "387,235",
        "1",
        "ILex-MI3 Precision (no user interaction) ILex-MI3 Recall",
        "AccessVar (with ILex segments) precision AccessVar (with ILex segments) recall candidate list projection rank",
        "Figure 5: ILex & AccessVar results instead of the corpus full text.",
        "AccessVar and ILex extract respectively 125,467 and 116,412 LUCs and the candidate list projection contains 2,190 and 1,876 LUCs.",
        "The results are computed on the deep set (figure 5).",
        "AccessVar and ILex achieve respectively recall of units extracted are common to both methods, 161 lexical units are extracted exclusively by ILex and 74 lexical units are extracted exclusively by AccessVar.",
        "This means that both methods have close covering capacities.",
        "From rank 100 to rank 700, the results are close but the curves begin to diverge after this rank, this trend means that the performance are similar for the 700 best candidates.",
        "However, ILex achieves 44.4% precision which is 10.6% higher than AccessVar (33.8%), this difference, in view of the close recall score, shows that ILex generates less invalid candidates.",
        "The errors specific to AccessVar are association errors (e.g. *|@|©|&|, *(xjt)©(|fcii))).",
        "ILex avoids these errors because of three mechanisms.",
        "First, the statistical likelihood between the couple components is tested (e.g. *|S|©|&| PMI score is negative).",
        "Second, the method checks association likelihood of the contexts before associating two morpho-syntactic units, score in [tltsxita*^]).",
        "Third, the incremental association process determine smaller unit before trying associating bigger couples"
      ]
    },
    {
      "heading": "6. Conclusion and Further Works",
      "text": [
        "The presented method features incremental lexical unit extraction with interactive candidate filtering capability.",
        "The maximal candidate length is not imposed directly, but instead is determined by the maximal number of associations.",
        "The lexical resources required are reusable and non-domain specific, which significantly reduce their cost for long-term deployment.",
        "The method achieves decent performance and improves the reference method's precision for this task.",
        "Furthermore, the extracted results include low-frequency and long candidates which are known to be difficult to extract.",
        "Finally, the binary association process allows us to sort the candidates by association measure, which is more relevant than frequency.",
        "This paper also introduced the beginning of a linguistically consistent lexical unit definition.",
        "This definition draws the outlines of a corpus annotation guide dedicated to the lexicon extraction task.",
        "The evaluation process is improved by the lexical unit trees annotations and a candidate list projection technique, which allows full-automatic estimation of extraction system performance.",
        "The first upcoming objective is the development of a robust evaluation protocol for the lexical extraction task.",
        "This is crucial for further improvements and means that the variation between annotators of the evaluation corpus, and the stability of the method over different corpora need to be considered.",
        "Finally we will try to solve the not yet managed lexicon extraction issues, Latin characters tokens which cause the method miss some extractions (e.g. ( M&A ))), and the discontinuous or [ fltft ] in ^3 (ft tft ))."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "Our sincere thanks to the anonymous reviewers.",
        "Special thanks to Pierre Zweigenbaum, to all my colleagues from Arisem and Ertim and to the corpus annotators without which this work would not be possible."
      ]
    }
  ]
}
