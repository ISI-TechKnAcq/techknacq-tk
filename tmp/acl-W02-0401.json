{
  "info": {
    "authors": [
      "Miles Osborne"
    ],
    "book": "Workshop on Automatic Summarization",
    "id": "acl-W02-0401",
    "title": "Using Maximum Entropy for Sentence Extraction",
    "url": "https://aclweb.org/anthology/W02-0401",
    "year": 2002
  },
  "references": [
    "acl-C00-1012",
    "acl-J96-1002",
    "acl-W96-0213",
    "acl-W97-0710"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "A maximum entropy classifier can be used to extract sentences from documents.",
        "Experiments using technical documents show that such a classifier tends to treat features in a categorical manner.",
        "This results in performance that is worse than when extracting sentences using a naive Bayes classifier.",
        "Addition of an optimised prior to the maximum entropy classifier improves performance over and above that of naive Bayes (even when naive Bayes is also extended with a similar prior).",
        "Further experiments show that, should we have at our disposal extremely informative features, then maximum entropy is able to yield excellent results.",
        "Naive Bayes, in contrast, cannot exploit these features and so fundamentally limits sentence extraction performance."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Sentence extraction the recovery of a given set of sentences from some document is useful for tasks such as document summarisation (where the extracted sentences can form the basis of a summary) or question-answering (where the extracted sentences can form the basis of an answer).",
        "In this paper, we concentrate upon extraction of sentences for inclusion into a summary.",
        "From a machine learning perspective, sentence extraction is interesting because typically, the number of sentences to be extracted is a very small fraction of the total number of sentences in the document.",
        "Furthermore, those clues which determine whether a sentence should be extracted or not tend to be either extremely specific, or very weak, and furthermore interact together in non-obvious ways.",
        "From a linguistic perspective, the task is challenging since success hinges upon the ability to integrate together diverse levels of linguistic description.",
        "Frequently (see section 6 for examples), sentence extraction systems are based around simple algorithms which assume independence between those features used to encode the task.",
        "A consequence of this assumption is that such approaches are fundamentally unable to exploit dependencies which presumably exist in the features that would be present in an ideal sentence extraction system.",
        "This situation may be acceptable when the features used to model sentence extraction are simple.",
        "However, it will rapidly become unacceptable when more sophisticated heuristics, with complicated interactions, are brought to bear upon the problem.",
        "For example, Boguraev and Neff (2000a) argue that the quality of summarisation can be increased if lexical cohesion factors (rhetorical devices which help achieve cohesion between related document utterances) are modelled by a sentence extraction system.",
        "Clearly such devices (for example, lexical repetition, ellipsis, co-reference and so on) all contribute towards the general discourse structure of some text and furthermore are related to each other in non-obvious ways.",
        "Maximum entropy (log-linear) models, on the other hand, do not make unnecessary independence assumptions.",
        "Within the maximum entropy framework, we are able to optimally integrate together whatever sources of knowledge we believe potentially to be useful for the task.",
        "Should we use features that are beneficial, then the model will be able to exploit this fact.",
        "Should we use features that are irrelevant, then again, the model will be able to notice this, and effectively ignore them.",
        "Models based on maximum entropy are therefore well suited to the sentence extraction task, and furthermore, yield competitive results on a variety of language tasks (Ratnaparkhi, 1996; Berger et al., 1996; Charniak, 1999; Nigam et al., 1999).",
        "In this paper, we outline a conditional maximum entropy classification model for sentence extraction.",
        "Our model works incrementally, and does not always need to process the entire document before assigning classification.'",
        "It discriminates between those sentences which should and should not be extracted.",
        "This contrasts with ranking approaches which need to process the entire document before extracting sentences.",
        "Because we model whether a sentence should be extracted or not in terms of features that are extracted from the sentence (and its context in the document), we do not need to specify the size of the summary.",
        "Again, this contrasts with ranking approaches which need to specify a priori the summary size.",
        "Our maximum entropy approach for sentence extraction does not come without problems.",
        "Using reasonably standard features, and when extracting sentences from technical papers, we find that precision levels are high, but recall is very low.",
        "This arises from the fact that those features which predict whether a sentence should be extracted tend to be very specific and occur infrequently.",
        "Features for sentences that should not be extracted tend to be much more abundant, and so more likely to be seen in the future.",
        "A simple prior probability is shown to help counteract this tendency.",
        "Using our prior, we find that the maximum entropy approach is able to yield results that are better than a naive Bayes classifier.",
        "Our final set of experiments looks more closely at the differences between maximum entropy and naive Bayes.",
        "We show that when we have access to an oracle that is able to tell us when to extract a sentence, then in the situation when that information is encoded in dependent features, maximum entropy easily outperforms naive Bayes.",
        "Furthermore, we also show that even when that information is encoded in terms of independent features, naive Bayes can be incapable of fully utilising this information, and so produces worse results than maximum entropy.2 Incremental classification means that a document is processed from start-to-finish and decisions are made as soon as sentences are encountered.",
        "Some of our features (in particular, those which encode sentence position in a document) do require processing the entire document.",
        "Using such features prevents true incremental processing.",
        "However, it is trivial to remove such features and so ensure true incrementality.",
        "2 A a reviewer commented, under certain circumstances, naive Bayes can do well even when there are strong dependencies within features (Domingos and Paz-zani, 1997).",
        "For example, when the sample size is small, naive Bayes can be competitive with more sophisticated approaches such as maximum entropy.",
        "Given this, a fuller comparison of naive Bayes and maximum entropy for sentence extraction requires considering sample size in addition to the choice of features.",
        "The rest of this paper is as follows.",
        "Section 2 outlines the general framework for sentence extraction using maximum entropy modelling.",
        "Section 3 presents our naive Bayes classifier (which is used as a comparison with maximum entropy).",
        "We then show in section 4 how both our maximum entropy and naive Bayes classifiers can be extended with an (optimised) prior.",
        "The issue of summary size is touched upon in section 5.",
        "Section 6 discusses related work.",
        "We then present our main results (section 7).",
        "Finally, section 8 discusses our results and considers future work."
      ]
    },
    {
      "heading": "2 Maximum Entropy for Sentence Extraction",
      "text": []
    },
    {
      "heading": "2.1 Conditional Maximum Entropy",
      "text": [
        "The parametric form for a conditional maximum entropy model is as follows (Nigam et al., 1999):",
        "Here, c is a label (from the set of labels C) and s is the item we are interested in labelling (from the set of items S).",
        "In our domain, C simply consists of two labels: one indicating that a sentence should be in the summary (`keep'), and another label indicating that the sentence should not be in the summary (`re-ject').",
        "S consists of a training set of sentences, linked to their originating documents.",
        "This means that we can recover the position of any given sentence in any given document.",
        "Within maximum entropy models, the training set is viewed in terms of a set of features.",
        "Each feature expresses some characteristic of the domain.",
        "For example, a feature might capture the idea that abstract-worthy sentences contain the words in this paper.",
        "In equation 1, fi(c, s) is a feature.",
        "In this paper we restrict ourselves to integer-valued functions.",
        "An example feature might be as follows: if s contains the phrase in this paper and c is the label keep otherwise Features are related to each other through weights (as can be seen in equation 1, where some feature fi has a weight Ai).",
        "Weights are real-valued numbers.",
        "When a closed form solution cannot be found,",
        "they are determined by numerical optimisation techniques.",
        "In this paper, we use conjugate gradient descent to find the optimal set of weights.",
        "Conjugate Gradient descent converges faster than Improved Iterative Scaling (Lafferty et al., 1997), and empirically we find that it is numerically more stable."
      ]
    },
    {
      "heading": "2.2 Maximum Entropy Classification",
      "text": [
        "When classifying sentences with maximum entropy, we use the equation:",
        "In practice, we are not interested in the probability of a label given a sentence.",
        "Instead we use the unnormalised score:",
        "Note that this maximum entropy classifier assumes a uniform prior.",
        "Section 4 shows how a non-uniform prior is used in place of this uniform prior.",
        "We now present our basic naive Bayes classifier.",
        "Afterwards, we extend this classifier with a nonuniform prior."
      ]
    },
    {
      "heading": "3 Naive Bayes Classification",
      "text": [
        "As an alternative to maximum entropy, we also investigated a naive Bayes classifier.",
        "Unlike maximum entropy, naive Bayes assumes features are conditionally independent of each other.",
        "So, comparing the two together will give an indication of the level of statistical dependencies which exist between features in the sentence extraction domain.",
        "For our experiments, we used a variant of the multivariate Bernoulli event model (McCallum and Nigam, 1998).",
        "In particular, we did not consider features that are absent in some example.",
        "This allows us to avoid summing over all features in the model for each example.",
        "Note that our maximum entropy model also did not consider absent features.",
        "Within our naive Bayes approach, the probability of a label given the sentence is as follows:",
        "As before, s is some sentence, c the label, and gi is some active feature describing sentence s. Naive Bayes models can be estimated in a closed form by simple counting.",
        "For features which have zero counts, we use add-k smoothing (where k is a small number less than one).",
        "Since the probability of the data (P(s)) is constant: P(c 1 s) cc P(c) P(gi I c) (7) If we assume a uniform prior (in which case P(c) is a constant for all c), this can be further simplified to:",
        "As with the maximum entropy classifier, we later replace the uniform prior with a non-uniform prior."
      ]
    },
    {
      "heading": "4 Maximum a Posteriori Classification",
      "text": [
        "In this section, we show how our classifiers can be extended with a non-uniform prior.",
        "We also describe how such a prior can be optimised."
      ]
    },
    {
      "heading": "4.1 Adding a non-uniform prior",
      "text": [
        "Now, the two classifiers mentioned previously (equations 9 and 5) are both based on maximum likelihood estimation.",
        "However, as we describe later, for sentence extraction, the maximum entropy classifier tends to over-select labels.",
        "In particular, it tends to reject too many sentences for inclusion into the summary.",
        "So, it it useful to extend the two previous classifiers with a non-uniform prior.",
        "For the naive Bayes classifier, we have:",
        "Here, P(c) is our prior.",
        "The probability of the data (P(s)) is constant and so can be dropped.",
        "For the maximum entropy case, we are not interested in the actual probability:",
        "F(c) is a function equivalent to the prior when using the unnormalised classifier.",
        "When this prior distribution (or equivalent function) is uniform, classification is as before (namely as outlined in sections 2 and 3), and depends upon the maximum entropy or naive Bayes component.",
        "When the prior is nonuniform, the classifier behaviour will change.",
        "This prior therefore allows us to affect the performance of our system.",
        "In particular, we can change the precision-recall balance."
      ]
    },
    {
      "heading": "4.2 Optimising the prior",
      "text": [
        "We treat the problem of selecting a prior as an optimisation task: select some P(c) (or F(c)) such that performance, as measured by some objective function of the overall classifier, is maximised.",
        "Since the choice of objective function is up to us, we can easily optimise the classifier in any way we decide.",
        "For example, we could optimise for recall by using as our objective function an f-measure that weighted recall more highly than precision.",
        "In this paper, we optimise the prior using as an objective function the f2 score of the classifier (section 7 details this score).",
        "Our prior therefore does not reflect relative frequencies of labels (as found in some corpus).",
        "We now need to optimise our prior.",
        "Brent's one dimensional function minimisation method is well suited to this task (Press et al., 1993), since for a random variable taking two values, the probability of one value can be defined in terms of the other value.",
        "Section 7 describes the held-out optimisation strategy used in our experiments.",
        "Should we decide to use a more elaborate prior (for example, one which was also sensitive to properties of documents) then we would need to use a multidimensional function minimisation method.",
        "Note that we have not simultaneously optimised the likelihood and prior probabilities.",
        "This means that we do not necessarily find the optimal maximum a posteriori (MAP) solution.",
        "It is possible to integrate into maximum entropy estimation (simple) conjugate priors that do allow MAP solutions to be found (Chen and Rosenfeld, 1999).",
        "Although it is an open question whether more complex priors can be directly integrated, future work ought to consider the efficacy of such approaches in the context of summarisation."
      ]
    },
    {
      "heading": "5 Summary size",
      "text": [
        "Determining the size of the summary is an important consideration for summarisation.",
        "Frequently, this is carried out dynamically, and specified by the user.",
        "For example, when there is limited opportunity to display long summaries a user might want a terse summary.",
        "Alternatively, when recall is important, a user might prefer a longer summary.",
        "Usually, systems rank all sentences in terms of how abstract-worthy that are, and then take the top n most highly ranked sentences.",
        "This always requires the size of summary to be specified.",
        "In our classification framework, sentences are processed (largely) independently of each other, and so there is no direct way of controlling the size of the summary.",
        "Altering the prior will indirectly influence the summary size.",
        "For more direct control over summary size, we can rank sentences using our classifiers (we not only label but can also assign label probabilities) and select the top n most highly ranked sentences.",
        "Within our classification approach, the optimised prior plays a similar role to the user-defined number of sentences that a ranking approach might return.",
        "Experiments (not reported here) showed that ranking sentences using our maximum entropy classifier, and then selecting the top n most highly ranked sentences produced slightly worse results than when selecting sentences in terms of classification."
      ]
    },
    {
      "heading": "6 Related Work",
      "text": [
        "The summarisation literature is large.",
        "Here we consider only a representative sample.",
        "Kupiec et al.",
        "(1995) used Naive Bayes for sentence extraction.",
        "They did not consider the role of the prior, nor did they use Naive Bayes for classification.",
        "Instead, they used it to rank sentences and selected the top n sentences.",
        "The TEXTRACT system included a sentence extraction component that is frequency-based (Boguraev and Neff, 2000b).",
        "Whilst the system uses a wide variety of linguistic cues when scoring sentences, it does not combine these scores in an optimal manner.",
        "Also, it does not consider interactions between the linguistic cues.",
        "Goldstein et al.",
        "(1999) used a centroid similarity measure to score sentences.",
        "They do not appear to have optimised their metric, nor do they deal with statistical dependencies between their features."
      ]
    },
    {
      "heading": "7 Experiments",
      "text": [
        "Summarisation evaluation is a hard task, principally because the notion of an objective summary is ill-defined.",
        "That aside, in order to compare our various systems, we used an intrinsic evaluation approach.",
        "Our summaries were evaluated using the standard f 2 score:"
      ]
    },
    {
      "heading": "2 r",
      "text": [
        "r=m p=� f2= p p+r where: r Recall p Precision i Number of correct sentences in summary k Number of sentences in summary In Number of correct sentences in the document A sentence being `correct' means that it was marked as being somehow important (abstract-worthy) by a human and labelled `keep' by one of our classifiers.",
        "Summaries produced by our systems will therefore attempt to mimic the process of selecting what it means for a sentence to be important in a document.",
        "Naturally this premise that an annotator can decide a priori whether a sentence is abstract-worthy or not is open to question.",
        "That aside, in other sentence extraction scenarios, it may well be the case that sentences can be reliably annotated.",
        "The f2 score treats recall and precision equally.",
        "This is a sensible metric to use as we have no a priori reason to believe in some other non-equal ratio of the two components.",
        "Our evaluation results are based on the following approach:",
        "1.",
        "Split the set of documents into two disjoint sets (T1 and T2), with 70 documents in T1 and 10 documents in T2.",
        "2.",
        "Further split T1 into two disjoint sets T3 and T4.",
        "T3 is used to train a model, and T4 is a held-out set.",
        "The prior is estimated using Brent's line minimisation method, when training using T3 and evaluating on T4.",
        "T3 consisted of 60 documents and T4 consisted of 10 documents.",
        "3.",
        "Results are then presented using a model trained on T1, with the prior just found, and evaluated using T2.",
        "T1 is therefore the training set and T2 is the testing set.",
        "Results are also presented using a flat prior.",
        "4.",
        "The whole process is then repeated after randomising the documents.",
        "The final results are then averaged over these n runs.",
        "We set n to 40."
      ]
    },
    {
      "heading": "7.1 Document set",
      "text": [
        "For data, we used the same documents that Teufel (2001) used in her experiments.3 In brief, these were 80 conference papers, taken from the Comp-lang preprint archive, and semi-automatically converted from LAT�,Xto XML.",
        "The XML annotated documents were then additionally manually marked-up with tags indicating the status of various sentences.",
        "This document set is modest in size.",
        "On the other hand, the actual documents are longer than newswire messages typically used for summarisation tasks.",
        "Also, the documents show variation in style.",
        "For example, some documents are written by nonnative speakers, some by students, some by multiple authors and so on.",
        "Summarisation is therefore hard.",
        "Here are some properties of the documents.",
        "On average, each document contained 8 sentences that were marked as being abstract-worthy (standard deviation of 3.1).",
        "The documents on average each contained in total 174 sentences (standard deviation 50.7).",
        "Here, a `sentence' is either any sequence of words that happened to be in a title, or else any sequence of words in the rest of the document.",
        "As can be seen, the summaries are not uniformly long.",
        "Also, the documents vary considerably in length.",
        "Summary size is therefore not constant.",
        "7.2 Features We used the following, fairly standard features when describing all sentences in the documents:",
        "• Word pairs.",
        "Word pairs are consecutive words as found in a sentence.",
        "A word pair feature simply indicates whether a particular word pair is present.",
        "All words were reduced: truncated to be at most 10 characters long.",
        "Stemming (as for example carried out by the Porter stemmer) produced worse results.",
        "We extracted all word pairs found in all sentences, and for any given sentence, found the set of (reduced) word pairs.",
        "• Sentence length.",
        "We encoded in three binary features whether a sentence was less than 6 words in length, whether it was greater than 20 words in length, or whether it was in between these two ranges.",
        "We also used a feature which encoded whether a previous sentence was less than 5 words or longer.",
        "This captured the idea that summary sentences tend to follow headings (which are short).",
        "• Sentence position.",
        "Summary sentences tend to occur either at the start, or the end of a document.",
        "We used three features: whether a given sentence was within the first 8 paragraphs of a document, whether a sentence was in the last 3 paragraphs, or whether the sentence was in a paragraph between these two ranges to encode sentence position.",
        "Note that this feature requires the whole document to be processed before classification can take place.",
        ".",
        "(Limited) discourse features.",
        "Our features described whether a sentence immediately followed typical headings such as conclusion or introduction, whether a sentence was at the start of a paragraph, or whether a sentence followed some generic heading.",
        "Our features are not exhaustive, and are not designed to maximise performance.",
        "Instead, they are designed to be typical of those found in sentence extraction systems.",
        "Note that some of our features exploit the fact that the documents are annotated with structural information (such as headers etc).",
        "Experiments with removing stop words from documents resulted in decreased performance.",
        "We conjecture that this is because our word pairs are extremely crude syntax approximations.",
        "Removing stop words from sentences and then creating word pairs makes these pairs even worse syntax approximations.",
        "However, using stop words increased the number of features in our model, and so again reduced performance.",
        "We therefore compromised between these two positions, and mapped all stop words to the same symbol prior to creation of word pair features.",
        "We also found it useful to remove word pairs which consisted solely of stop words.",
        "Finally, for maximum entropy, we deleted any feature that occurred less than 4 times.",
        "Naive Bayes did not benefit from a frequency-based cutoff."
      ]
    },
    {
      "heading": "7.3 Classifier comparison",
      "text": [
        "Here we report on our classifiers.",
        "As a baseline model, we simply extracted the first n sentences from a given document.",
        "Figure 1 summarises our results as n varies.",
        "In this table, as in all subsequent tables, P and R are averaged precision and recall values, whilst F2 is the f2 score of these averaged values.",
        "feature instances in the model, the vast majority are deeded irrelevant by maximum entropy, and assigned a zero weight.",
        "Only 7086 features (roughly 10% in total) had non zero weights.",
        "Performance using the optimised prior shows more balanced results, with an increase in F2 score.",
        "Clearly optimising the prior has helped counter the categorical behaviour of features in our maximum entropy classifier.",
        "Figure 3 shows the results we obtained when using a naive Bayes classifier.",
        "As before, the results show performance with and without the addition of the optimised prior.",
        "Naive Bayes outperforms maximum entropy when both classifiers do not use a prior.",
        "Performance with and without the prior however, is worse than the performance of our maximum entropy classifier with the prior.",
        "Evidently, even our relatively simple features interact with each other, and so approaches such as maximum entropy are required to fully exploit them.",
        "Figure 2 shows our results for maximum entropy, both with and without the prior.",
        "Prior optimisation was with respect to the f2 score.",
        "As in subsequent tables, we show system performance when adding more and more features.",
        "Performance without the prior is heavily skewed towards precision.",
        "This is because our features are largely acting categorically: the sheer presence of some feature is sufficient to influence labelling choice.",
        "Further evidence for this analysis is supported by inspecting one of the models produced when using the full set of all feature types.",
        "We see that of the 85883"
      ]
    },
    {
      "heading": "7.4 Using informative features",
      "text": [
        "Our previous results showed that maximum entropy could outperform naive Bayes.",
        "However, the differences, though present, were not large.",
        "Clearly, our feature set was imperfect.4 It is therefore instructive to see what happens if we had access to an oracle who always told us the true status of some unseen sentence.",
        "To make things more interesting, we",
        "encoded this information in terms of dependent features.",
        "We simulated this oracle by using two features which were active whenever a sentence should not be in the summary; for sentences that should be included in the summary, we let either one of those two features be active, but on a random basis.",
        "Our features therefore are only informative when the learner is capable of noting that there are dependencies.",
        "We then repeated our previous maximum entropy and naive Bayes experiments.",
        "Figure 4 summarise our results.",
        "Unsurprisingly, we see that when features are highly dependent upon each other, maximum entropy easily outperforms naive Bayes.",
        "Even when we have access to features that are independent of each other, naive Bayes can still do worse than maximum entropy.",
        "To demonstrate this, we used a feature that was active whenever a sentence should be in the summary.",
        "This feature was not active on sentences that should not be in the summary.",
        "Figure 5 summarises our results.",
        "As can be seen (figure 5), even when naive Bayes has access to a perfectly reliable informative feature, the fact that the other features are not suitably discounted means that performance is worse than that of maximum entropy.",
        "Maximum entropy can discount the other features, and so can take advantage of reliable features."
      ]
    },
    {
      "heading": "8 Comments and Future Work",
      "text": [
        "We showed how maximum entropy could be used for sentence extraction, and in particular, that adding a prior could deal with the categorical nature of the features.",
        "Maximum entropy, with an optimised prior, did yield marginally better results than naive Bayes (with and without a similarly optimised prior).",
        "However, the differences were not that great.",
        "Our further experiments with informative features showed that this lack of difference was probably due (at least in part) to the actual features used, and not due to the technique itself.",
        "Our oracle results are an idealisation.",
        "A fuller comparison should use more sophisticated features, along with more data.",
        "As a result of this, we conjecture that should we use a much more sophisticated feature set, we would expect that the differences between maximum entropy and naive Bayes would become greater.",
        "Our approach treated sentences largely independently of each other.",
        "However, abstract-worthy sentences tend to bunch together, particularly at the beginning and end of a document.",
        "We intend capturing this idea by making our approach sequence-based: future decisions should also be conditioned on previous choices.",
        "A problem with supervised approaches (such as ours) is that we need annotated material (Marcu, 1999).",
        "This is costly to produce.",
        "Future work will consider weakly supervised approaches (for example cotraining) as a way of bootstrapping labelled material from unlabelled documents (Blum and Mitchell, 1998).",
        "Note that there is a close connection between multi-document summarisation (where many alternative documents all consider similar issues) and the concept of a view in cotraining.",
        "We expect that this redundancy could be exploited as a means of providing more annotated training material, and so yield better results.",
        "In summary, maximum entropy can be beneficially used in sentence extraction.",
        "However, one needs to guard against categorial features.",
        "An optimised prior can provide such help."
      ]
    },
    {
      "heading": "Acknowledgement",
      "text": [
        "We would like to thank Rob Malouf for supplying the excellent log-linear estimation code, Simone Teufel for providing the annotated data, Karen Spark Jones for a discussion about summarisation, Steve Clark for spotting textual bugs and the anonymous reviewers for useful comments.",
        "Features Word pairs and sent length and sent position and discourse Features Word pairs and sent length and sent position and discourse"
      ]
    }
  ]
}
