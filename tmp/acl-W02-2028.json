{
  "info": {
    "authors": [
      "Hiroya Takamura",
      "Yuji Matsumoto"
    ],
    "book": "Conference on Computational Natural Language Learning CoNLL",
    "id": "acl-W02-2028",
    "title": "Two-Dimensional Clustering for Text Categorization",
    "url": "https://aclweb.org/anthology/W02-2028",
    "year": 2002
  },
  "references": [
    "acl-J90-1003",
    "acl-J92-4003",
    "acl-P98-2124"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We propose a new method to improve the accuracy of Text Categorization using two-dimensional clustering.",
        "In a number of previous probabilistic approaches, texts in the same category are implicitly assumed to be generated from an identical distribution.",
        "We empirically show that this assumption is not accurate, and propose a new framework based on two-dimensional clustering to alleviate this problem.",
        "In our method, training texts are clustered so that the assumption is more likely to be true, and at the same time, features are also clustered in order to tackle the data sparseness problem.",
        "We conduct some experiments to validate the proposed two-dimensional clustering method."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Text Categorization is the task of classifying texts into their most plausible category.",
        "One problem in most previous probabilistic approaches to Text Categorization is that texts in the same category are assumed to be generated by an identical distribution (we call it the i.d.",
        "assumption, in this paper).",
        "However, categories are manually defined and there is no predefined probabilistic structure behind them, as discussed in the next section.",
        "Another problem with Text Categorization is the data-sparseness problem caused by the high dimensionality of the feature space.",
        "The frequency of each word is usually so small that it is difficult to estimate reliable statistics.",
        "In order to tackle these problems, we propose a new framework based on two-dimensional clustering.",
        "We first cluster training texts into several clusters whose elements can be thought as being generated from an identical distribution before estimating the probability model of each category.",
        "The data-sparseness problem is more critical, if the number of parameters is larger as in the text clustering approach we are adopt.",
        "So we alleviate this problem by clustering features (words).",
        "That is to say, we cluster both texts and features simultaneously.",
        "Through experiments, we show that our approach works well with probabilistic classifiers.",
        "In Natural Language Processing, several clustering applications have been proposed, for example in (Brown, 1992; Li and Abe, 1998).",
        "Among those applications, (Baker et al., 1998) applied the class-distributional clustering to Text Categorization.",
        "They theoretically proved the optimality of their clustering method in terms of Naive Bayes Score, and validated it empirically.",
        "In class-distributional clustering, occurrences of categories given a word are regarded as a probability distribution, and words are clustered according to this distribution.",
        "In (Slonim and Tishby, 2001), they use the Information Bottleneck Method (Tishby et al., 1999) for Text Categorization.",
        "Both (Baker et al., 1998) and (Slonim and Tishby, 2001), however, deal only with word clustering.",
        "Unlike those methods, our method adopts a two-dimensional clustering of words and texts.",
        "The idea of clustering words and texts simultaneously is also pursued in (Slonim and Tishby, 2000; Dhillon, 2001).",
        "However, those are concerned exclusively with clustering and do not propose any framework applicable to Text Categorization.",
        "This paper is organized as follows.",
        "In Section 2, we investigate the i.d.",
        "assumption in Text Categorization.",
        "In Section 3, we describe our clustering methods.",
        "In Section 4, we explain our categorization methods.",
        "Section 5 presents the experiments and results with discussions.",
        "Finally, in Section 6, we summarize our research.",
        "Given a set of co-occurrence samples of words and texts: 2.",
        "Merge two text clusters or two word clusters with the least likelihood decrease repeatedly, while the stopping criterion is not satisfied.",
        "where N(x) denotes the frequency of x.",
        "3.☆ ✫lusterin✯ ✰�✯sr��hms In the algorithm described in (Li and Abe, 1998), given two positive integers k and l, merging for the first dimension is performed k times, followed by l merges for the second dimension.",
        "We propose two di✻erent clustering algorithms .",
        "In both algorithms, a pair of words or texts are chosen and merged at each step, based on the model described in Section 3.1.",
        "The di✻erence is the way to choose the pair of words or texts to be merged.",
        "One is what we call text✽✾rst clustering, in which text clustering is conducted first, followed by word clustering.",
        "The other is ❀reedy clustering, in which, at each step, the pair with the least likelihood decrease is searched from the word pairs and the text pairs, and merged :",
        "❂ Text-first Clustering 1.",
        "Initialize 2.",
        "Merge two text clusters with the least likelihood decrease repeatedly, while the stopping criterion is not satisfied.",
        "3.",
        "Merge two word clusters with the least likelihood decrease repeatedly, while the stopping criterion is not satisfied.",
        "❂ Greedy Clustering 1.",
        "Initialize",
        "We set the constraints that only texts in the same category can be merged (we call the cate❀ory✽constraint), and that only words with the same part-of-speech can be merged (❅os✽ constraint).",
        "The category-constraint is indispensable in our method, because of our categorization method which is explained later.",
        "Both of these constraints decrease the computational time needed for clustering.",
        "The text-first clustering has the advantage that word clustering can be conducted using the information given by class-distribution ☎.",
        "Class-distributional clustering is a special case of text-first clustering.",
        "If the stopping criterion of the text clustering step is set as \"no two clusters can be merged without violating the category-constraint\", then text-first clustering is identical to the class-distributional clustering."
      ]
    },
    {
      "heading": "3.3 The Relation to Jensen■❑hannon",
      "text": [
        "Pi◗er✯ence Here we show that using the criterion of the least likelihood decrease is equivalent to selecting the closest pair of clusters in terms of a certain distance as a probability distribution.",
        "Let ❙L denote the decrease of the log-likelihood (3) caused by merging word-clusters V and W. Let I✁I denote the number of the whole training examples.",
        "Using P(CYZ, Cd) = P(CY, Cd) +P(CZ, Cd), ❙L divided by I✁I is transformed as :",
        "❥Mo�e precisely, the information used in the text♠ ♦rst clusterin♣ is direrent from the information ♣iten by class♠distribution, but as the clusterin♣ proceeds, these two types of information become more similar.",
        "where DKL(�IIq) is the KL-divergence between the probability distribution p and q.",
        "The last line of (7) is the Jensen-Shannon divergence, which is also known as \"KL divergence to the mean\".",
        "That is, in our method, the closest pair of clusters in terms of the Jensen-Shannon divergence is merged at each step.",
        "In other words, the clustering method used in (Baker et al., 1998) is valid in terms of the likelihood."
      ]
    },
    {
      "heading": "3.4 AIC-based Stopping Criterion",
      "text": [
        "As the stopping criterion in the clustering algorithm, we adopt AIC (Akaike Information Criterion) (Akaike, 1974).",
        "In (Li and Abe, 1998), they use MDL (Minimum Description Length) Principle (Rissanen, 1987).",
        "We do not use MDL Principle, because it tends to predict too small numbers of clusters in preliminary experiments (for text clustering, it predicted a smaller number of clusters than the number of categories, which is not suitable for our method because of the category-constraints).",
        "AIC is realized as follows.",
        "The decrease of the number of parameters caused by merging a pair of clusters is :",
        "where, IC(D)I_Number of clusters of words, IC(W)I _ Number of clusters of texts.",
        "According to AIC, the stopping criterion should be",
        "The first term AL denotes the decrease of log-likelihood (3) caused by merging.",
        "Note that, in the text-first clustering, there are two possible points where AIC is applied.",
        "One is the point when the text clustering is finished, and the other is when the word clustering is finished."
      ]
    },
    {
      "heading": "4 Categorization",
      "text": [
        "Probabilistic classifiers are expected to yield good results combined with our clustering method, but the performance of non-probabilistic classifiers with our method is unpredictable.",
        "We evaluate our clustering method using NB (Naive Bayes) classifiers (Mitchell, 1997), which is a probabilistic classifier, and SVMs (Support Vector Machines) (Vapnik, 1995), which is a non-probabilistic classifier.",
        "In our method, the texts are clustered beforehand.",
        "So we first categorize the test texts and predict which cluster each test text belongs to.",
        "Then, we assign to each text the category that the predicted cluster belongs to (in our clustering method, all the training texts in each cluster are supposed to have the same category tag).",
        "For the NB classifier, we use the Multinomial Model (McCallum and Nigam, 1998), but ignore the concern of document length.",
        "SVM is a binary classifier based on Structural Risk Minimization (Vapnik, 1995).",
        "It has a high generalization performance and has been successfully applied to Text Categorization, for example in (Joachims, 1998).",
        "In order to apply SVMs to multi-class classification, we use the one-versus-rest method.",
        "However, when constructing a hyperplane for one cluster, the training texts belonging to the other clusters in the same category are removed from the training set."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": []
    },
    {
      "heading": "5.1 Experimental Settings",
      "text": [
        "The data corpus used in this research is Reuters215783.",
        "We removed the texts whose body was meaningless, after applying ModApte-split, which is a standard way to split the corpus into training texts and test texts.",
        "This procedure yielded 8815 training texts, 3023 test texts and 116 categories.",
        "We used as features only nouns, verbs, proper nouns, adjectives and adverbs that occur five times or more in the whole training data.",
        "Stemming was also done using TreeTagger (Schmid, 1994).",
        "Some of the texts in Reuters-21578 have multiple category-tags.",
        "In the clustering phase, we introduced multiple copies of those texts and label each text with one of its tags, so that every",
        "gether with the actual best compression rates and their accuracies.",
        "Table 2 shows the performance of NB classification combined with greedy clustering.",
        "In the case of greedy clustering, it is necessary to display both word compression rates and text compression rates, so we didn't include the results of the greedy clustering into Figure 2.",
        "In Table 2, the compression rates predicted by AIC, 9.7% for words and 6.8% for texts, are also displayed."
      ]
    },
    {
      "heading": "5.3 Discussion",
      "text": [
        "At the point of 100% word compression rate (i.e. no comression) in Figure 2, text-first clustering performs better than class-distributional clustering, although the difference is small (at this point, texts have been clustered in the text-first clustering).",
        "As the word compression rate decreases, the difference of the accuracy increases.",
        "This means that the combination of text clustering and word clustering works well.",
        "Figure 3 shows that, also for SVMs, text-first clustering outperforms class-distributional clustering.",
        "However, the performance is worse for smaller compression rates.",
        "This means, in terms of accuracy, word-clustering is not effective for SVMs.",
        "The clustering of texts is still effective.",
        "Predicted compression rates in Table 1 are not close to the actual best compression rates, although the corresponding accuracy is not so different for the text-first clustering.",
        "The difference of two AIC-predicted accuracies is significant in the sign-test (with 1% significance-level).",
        "The difference of the AIC-predicted accuracy of our method and the accuracy without clustering is also siginificant in the same test with 5% siginificance-level.",
        "Table 2 shows that the greedy-clustering does not work well.",
        "The reason would be that word-clustering in the early stage cannot use the information of class-distribution."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We proposed a new method to improve the accuracy of Text Categorization using the two-dimensional clustering.",
        "In our method, both training texts and features are clustered before estimating the probability model.",
        "Our approach is motivated by the fact that, in most previous probabilistic approaches, one category is assumed to have one identical probabilistic distribution, but this assumption is not always true, as discussed in this paper.",
        "Our two-dimensional clustering approach alleviates this problem, and at the same time, it can avoid the data-sparseness problem.",
        "Through experiments, we showed that two-dimensional clustering worked well with Naive Bayes Classifiers and that, for the SVMs, two-dimensional clustering outperformed class-distributional clustering.",
        "Future work includes the following.",
        "First, in this research, we conducted experiments with only one data set.",
        "It would be desirable to confirm our conclusions with further experiments using different data sets.",
        "We used AIC as a stopping criterion of the text clustering step in the text-first clustering.",
        "But we haven't investigated whether AIC was valid as the turning criterion, because it needs experiments over two-dimensional parameter space.",
        "This point has to be investigated.",
        "As a stopping criterion, AIC does not always work well enough.",
        "Better criteria should be pursued.",
        "In our framework, AIC is actually targetting the joint probability of words and texts.",
        "But, in order to obtain a better stopping criterion, AIC should be incorporated in a more sophisticated way, such that it aims at the categorization.",
        "We used an agglomerative clustering, but a divisive clustering method might be better in terms of computational time.",
        "One of the possible extensions of this model is the soft version, as discussed in (Hofmann, 1998), in which the Expectation-Maximization algorithm is used with the soft version of this model."
      ]
    }
  ]
}
