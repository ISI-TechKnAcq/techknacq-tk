{
  "info": {
    "authors": [
      "Prashant Mathur",
      "Cettolo Mauro",
      "Marcello Federico"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W13-2237",
    "title": "Online Learning Approaches in Computer Assisted Translation",
    "url": "https://aclweb.org/anthology/W13-2237",
    "year": 2013
  },
  "references": [
    "acl-C04-1072",
    "acl-C08-1064",
    "acl-C96-2141",
    "acl-D09-1079",
    "acl-D11-1125",
    "acl-J03-1002",
    "acl-J93-2003",
    "acl-N09-1069",
    "acl-N10-1062",
    "acl-P03-1021",
    "acl-P05-1032",
    "acl-P06-1096",
    "acl-P07-2045",
    "acl-P11-2031",
    "acl-P96-1041",
    "acl-W02-1001",
    "acl-W08-0509"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present a novel online learning approach for statistical machine translation tailored to the computer assisted translation scenario.",
        "With the introduction of a simple online feature, we are able to adapt the translation model on the fly to the corrections made by the translators.",
        "Additionally, we do online adaption of the feature weights with a large margin algorithm.",
        "Our results show that our online adaptation technique outperforms the static phrase based statistical machine translation system by 6 BLEU points absolute, and a standard incremental adaptation approach by 2 BLEU points absolute."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The growing needs of the localization and translation industry have recently boosted research around computer assisted translation (CAT) technology.",
        "The purpose of CAT is to increase the productivity of a human translator.",
        "A CAT tool comes as a package of a Translation Memory (TM), built-in spell checkers, a dictionary, a terminology list etc.",
        "which help the translator while translating a sentence.",
        "Recent research has led to the integration of CAT tools with statistical machine translation (SMT) engines.",
        "SMT makes use of a large available parallel corpus to generate statistical models for translation.",
        "Due to their generalization capability, SMT systems are a good fit in this scenario and a seamless integration of SMT engines in CAT have shown to increase translator's productivity (Federico et al., 2012).",
        "Although automatic systems generate reliable translations they are not accurate enough to be used directly and need postedition by human translators.",
        "In state-of-the-art CAT tools, the SMT systems are static in nature and so they cannot adapt to these corrections.",
        "When a SMT system keeps repeating the same error, productivity of translators as well as their trust in SMT technology are negatively affected.",
        "As an example, technical documentation typically contains a lot of repetitions due to the employed writing style and pervasive use of terminology.",
        "Hence, in order to provide useful hints, SMT systems are expected to behave consistently regarding the translation of domain-specific terms.",
        "However, if the user edits the translation of a technical term in the target text, most current SMT systems are incapable to learn from those corrections.",
        "Online learning is a machine learning task where a predictor iteratively: (1) receives an input and outputs a label, (2) receives the correct label from a human and if the two labels do not match, it learns from the mistake.",
        "The task of learning from user corrections at the sentence level fits well the online learning scenario, and its expected usefulness is clearly related to the amount of repetitions occurring in the text.",
        "The higher the number of repetititions in a document the more the SMT system has chances to translate consistently through the use of online learning.",
        "In this paper, we implemented two online learning methods through which a phrase-based SMT system evolves over time, sentence after sentence, by taking advantage of the post-edition or translation of the previous sentence by the user.1 In the first approach, we focus on the translation model aspect of SMT which is represented by five conventional features, namely lexical and phrase translation probabilities in both directed and inverted directions, plus a phrase penalty score.",
        "Translation, language and reordering models are combined in a linear fashion to obtain a score for",
        "the translation hypothesis as shown in Equation 1.",
        "score(e?, f) = ?i?ihi(e?, f) (1) where hi(?)",
        "are the feature functions representing the models and ?i are the linear weights.",
        "The highest scored translation is the best hypothesis e?",
        "output by the system.",
        "We extend the translation model with a new feature which provides extra phrase-pair scores changing according to the user feedback.",
        "The scores of the new feature are adapted in a discriminative fashion, by rewarding phrase-pairs observed in the search space and in the reference, and penalizing phrase-pairs observed in the search space but not in the reference.",
        "In the second approach, we also adapt the model weights of the linear combination after each test sentence by using a margin infused relaxed algorithm (MIRA).",
        "For assessing the robustness of our methods, we performed experiments on two datasets from different domains and language pairs (?6).",
        "Moreover, our online learning approaches are compared against a static baseline system and against the incremental adaptation approach proposed by Lev-enberg et.",
        "al.",
        "(2010) (?5)."
      ]
    },
    {
      "heading": "2 Related Works",
      "text": [
        "Several online adaptation strategies have been proposed in the past, only a few deal with adaptation of post-edited/evaluation data while most works are on adaptation over development data during tuning of parameters (Och and Ney, 2003)."
      ]
    },
    {
      "heading": "2.1 Online Adaptation during Tuning",
      "text": [
        "Liang et.",
        "al.",
        "(2006) improved SMT performance by online adaptation of scaling factors (?",
        "in (1)) using averaged perceptron algorithm (Collins, 2002).",
        "They presented different strategies to update the SMT models towards reference or oracle translation: (1) aggressively updating towards reference, bold update; (2) update towards the oracle translation in N-Best list, local update; (3) a hybrid approach in which a bold update is performed when the reference is reachable, otherwise a local update is performed.",
        "Liang and Klein (2009) compared two online EM algorithms, stepwise online EM (Sato and Ishii, 2000; Cappe?",
        "and Moulines, 2007) and incremental EM (Neal and Hinton, 1998) which they use to update the alignment models (the generative component of SMT) on the fly.",
        "However, stepwise EM is prone to failure if mini-batch size and stepsize parameters are not chosen correctly, while incremental EM requires substantial storage costs because it has to store sufficient statistics for each sample.",
        "Other works on online minimum error rate training in SMT (Och and Ney, 2003) that deserve mentioning are (Hopkins and May, 2011; Hasler et al., 2011)."
      ]
    },
    {
      "heading": "2.2 Online Adaptation during Decoding",
      "text": [
        "Cesa-Bianchi et.",
        "al.",
        "(2008) proposed an online learning approach during decoding.",
        "They construct a layer of online weights over the regular feature weights and update these weights at sentence level using margin infused relaxed algorithm (Crammer and Singer, 2003); to our knowledge, this is the first work on online adaptation during decoding.",
        "Mart?",
        "?nez-Go?mez et.",
        "al.",
        "(2011; 2012) presented a comparison of online adaptation techniques in post editing scenario.",
        "They compared different adaptation strategies on scaling factors and feature functions (respectively, ?",
        "and h(?)",
        "in (1)).",
        "However, they modified the feature values during adaptation without any normalization, which disregards the initial assumption of the feature values being probabilities.",
        "In our approach, the value of the additional online feature can be modified during decoding without changing other feature values (probabilities) and thus preserving their probability distribution."
      ]
    },
    {
      "heading": "3 Feature Adaptation",
      "text": [
        "In the CAT scenario, the user receives a translation suggestion for each source segment, post-edits it and finally approves it.",
        "From the SMT point of view, for each source segment the decoder explores a search space of possible translations and finally returns the best scoring one (bestHyp) to the user.",
        "The user possibly corrects this suggestion thus generating the final translation (postedit).",
        "Our online learning procedure is based on the following idea.",
        "For each N-best translation (candidate) in the search space, we compute a similarity score against the postedit using the sentence-level BLEU metric (Lin and Och, 2004), a smoothed variant of the popular BLEU metric (Papineni et al., 2001).",
        "We hence compare the similarity score of each candidate against the similarity score achieved by the bestHyp, that was also computed against the postedit.",
        "If the candidate",
        "scores better than the bestHyp, then we promote the building blocks, i.e. phrase-pairs, of candidate that were not used in bestHyp and demote the phrase-pairs used in bestHyp that were not used for candidate.",
        "On the contrary, if the candidate scores worse than the bestHyp, we promote the building blocks of bestHyp that are not in candidate and demote those of candidate that are not in bestHyp.",
        "Our promotion/demotion mechanism could be implemented by updating the features values of the phrase pairs used in the candidate and bestHyp translations.",
        "However, features in the translation models are conditional probabilities and perturbing a subset of them by also preserving their normalization constraints can be computationally expensive.",
        "Instead, we propose to introduce an additional online feature which represents a goodness score of each phrase-pair in the test set.",
        "We call the set of phrase pairs used to generate a candidate as candidatePP and the set of phrase pairs used to generate the bestHyp as bestPP .",
        "The online feature value of each phrase-pair is initialized to a constant and is updated according to the perceptron update (Rosenblatt, 1958) method.",
        "In particular, the amount by which a current feature value is rewarded or penalized depends on a learning rate ?",
        "and on the difference between the model scores (i.e. h ?w) of candidate and bestHyp as calculated by the MT system.",
        "A sketch of our online learning procedure is shown in Algorithm 1.",
        "In Algorithm 1, ?h ?",
        "w is the above mentioned score difference as computed by the decoder; multiplied by ?, it is the margin, that is the value with which the online feature score (f ) of the phrase pair under processing is modified.",
        "We can observe that the feature scores are unbounded and could lead to instability of the algorithm; therefore, we normalise the scores through the sigmoid function:"
      ]
    },
    {
      "heading": "4 Weight Adaptation",
      "text": [
        "In addition to adapting the online feature values, we can also apply online adaptation on the feature weights of the linear combination (eq.",
        "1).",
        "In particular, after translating each sentence we can adapt the parameters depending on how good the last translation was.",
        "A commonly used algorithm in this online paradigm for tuning of parameters is the Margin Infused Relaxed Algorithm (MIRA).",
        "MIRA is an online large margin algorithm that updates the parameter w?",
        "of a given model according to the loss that is occurred due to incorrect classification.",
        "In the case of SMT this margin can be coupled with the loss function, which in this case is the complement of the sentence level BLEU(sBLEU).",
        "Thus, the loss function can be formulated as:",
        "where y?",
        "is the oracle (closest translation to the reference) and y?",
        "is the candidate being processed.",
        "Ideally, this loss should correspond to the difference between the model scores: ?h ?",
        "w?",
        "= score(y?)?",
        "score(y?)",
        "(4) MIRA is an ultraconservative algorithm, meaning that the update of the current weight vector is the smallest possible value satisfying the constraint that the variation incurred by the objective function must not be larger than the variation incurred by the model (plus a non-negative slack variable ?).",
        "Formally, weight update at ith iteration is defined as:",
        "where j ranges over all candidates in the Nbest list, lj is the loss between oracle and the candidate j, and ?hj ?",
        "w is the corresponding difference in the model scores.",
        "C is an aggressive parameter which controls the size of the update, ?",
        "is the learning rate of the algorithm and ?",
        "is usually a very small value (in our experiments we kept it as 0.0001).",
        "After partial differentiation and lin-earizing the loss, equation 5 can be rewritten as:",
        "We solve equation 5, by computing ?",
        "with the optimizer integrated in the Moses toolkit by (Hasler et al., 2011).",
        "Algorithm 2 gives an overview of the online margin infused relaxed algorithm we implemented in Moses.",
        "In the following section we overview a stream based adaptation method with which we experimentally compared our two online learning approaches as it well fits the framework we are working in."
      ]
    },
    {
      "heading": "5 Stream based adaptation",
      "text": [
        "Continuously updating an SMT system to an incoming stream of parallel data comes under stream based adaptation.",
        "Levenberg et.",
        "al.",
        "(2010) proposed an incremental adaptation technique for the core generative component of the SMT system, word alignments and language models (Levenberg and Osborne, 2009).",
        "To get the word alignments on the new data they use a Stepwise online EM algorithm, where old counts (from previous alignment models) are interpolated with the new counts.",
        "Since we work at the sentence level, on-the-fly computation of probabilities of translation and reordering models is expensive in terms of both computational and memory requirements.",
        "To save these costs, we prefer using dynamic suffix array approach described in (Levenberg et al., 2010; Callison-Burch et al., 2005; Lopez, 2008).",
        "They are used to efficiently store the source and the target corpus and alignments in efficient data structure, namely the suffix array.",
        "When a phrase translation is asked by the decoder, the corpus is searched, the counts are collected and its probabilities are computed on the fly.",
        "However, the current implementation in Moses of the stream based MT relying on the suffix arrays is severely limited as it allows the computation of only three translation features, namely the two direct translation probabilities and the phrase penalty.",
        "This results in a significant degradation of performance."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": []
    },
    {
      "heading": "6.1 Datasets",
      "text": [
        "We compared our online learning approaches (Sections 3 and 4) and the stream based adaptation method (Section 5) on two datasets from different domains, namely Information Technology (IT) and TED talks, and two different language pairs.",
        "The IT domain dataset is proprietary, it involves the translation of technical documents from English to Italian and has been used in the field test carried out under the MateCat project2.",
        "Experiments are also conducted on English to French TED talks dataset (Cettolo et al., 2012) to assess the robustness of the proposed approaches in a different scenario and to provide results on a publicly available dataset for the sake of reproducibility.",
        "The training, development (dev2010) and evaluation (tst20103) sets are the same as used in the last IWSLT last evaluation campaigns.",
        "In experiments on TED data, we considered the human reference translations as post edits, even if they were",
        "actually generated from scratch.",
        "In our experiments, the extent of usefulness of online learning highly depends on the amount of repetition of text.",
        "A reasonable way to measure the quantity of repetition in each document is through the repetition rate (Bertoldi et al., 2013).",
        "It computes the rate of non-singleton n-grams, n=1...4, averaging the values over sub-samples S of thousand words from the text, and then combining the rate of each n-gram to a single score by using the geometric mean.",
        "Equation 7 shows the formula for calculating the repetition rate of a document, where dict(n) represents the total number of different n-grams and nr is the number of different n-grams occurring exactly r times:",
        "Statistics of the parallel sets and their repetition rate on both sides are reported in Table 1.",
        "the corresponding repetition rate (RR).",
        "It can be noted that the repetition rates of IT and TED sets are significantly different, particularly high in IT documents, much lower in the TED talks."
      ]
    },
    {
      "heading": "6.2 Systems",
      "text": [
        "The SMT systems were built using the Moses toolkit (Koehn et al., 2007).",
        "Training data in each domain was used to create translation and lexical reordering models.",
        "We created a 5-gram LM for TED talks and a 6-gram LM for the IT domain using IRSTLM (Federico et al., 2008) with improved Kneser-Ney smoothing (Chen and Goodman, 1996) on the target side of the training parallel corpora.",
        "The log linear weights for the baseline systems are optimized using MERT (Och, 2003) provided in the Moses toolkit.",
        "To counter the instability of MERT, we averaged the weights of three MERT runs in each case.",
        "Performance is measured in terms of BLEU and TER (Snover et al., 2006) computed using the MultEval script (Clark et al., 2011).",
        "Since the implementations of standard Giza and of incremental Giza combined with dynamic suffix arrays are not comparable, we constructed two baselines, a standard phrase based SMT system and an incremental Giza baseline (?5).",
        "Details on experimental SMT systems we built follow.",
        "Baseline This system was built on the parallel training data for each domain.",
        "We run 5 iterations of model 1, 5 of HMM (Vogel et al., 1996), 3 of model 3, 3 of model 4 (Brown et al., 1993) using MGiza (Gao and Vogel, 2008) toolkit to align the parallel corpus at word level.",
        "Translation and reordering models were built using Moses, while log-linear weights were optimized with MERT on the corresponding development sets.",
        "The same IT baseline system was used in the field test of MateCat and the references in the IT data are actual postedits of its translation.",
        "IncGiza Baseline We trained alignment models with incGiza++4 with 5 iterations of model 1 and 10 iterations of the HMM model.",
        "To build incremental Giza baselines, we used dynamic suffix arrays as implemented in Moses which allow the addition of new parallel data during decoding.",
        "In the incremental Giza baseline, once a sentence of the test set is translated, the sentence pair (source and target post-edit/reference) along with the alignment provided by incGiza are added to the models.",
        "Online learning systems We developed several online systems on top of the two aforementioned baseline systems: (1) +O employ the additional online feature (Section 3) updated with Algorithm 1; (2) +O+NS as (1) but with the online feature normalized with the sigmoid function; (3) +W weights updated (Section 4) with Algorithm 2; (4) +O+W combination of online feature and weight update; (5) +O+NS+W as system (4) with normalized online feature score.",
        "In the online learning system we have three additional parameters: a weight for the online feature, a learning rate for features (used in the perceptron update), and a learning rate for feature weights used by MIRA.",
        "These additional parameters were optimized by maximizing the BLEU",
        "score on the devset and on top of already optimized feature weights.",
        "For practical reasons, optimization of the parameters was run with the Simplex algorithm (Nelder and Mead, 1965)."
      ]
    },
    {
      "heading": "7 Results and Discussion",
      "text": [
        "Tables 2 and 3 collect results by the systems described in Section 6.2 on the IT and TED translation tasks, respectively.",
        "In Table 2, the online system (1st block ?+O+NS+W?",
        "system with 10 iterations of online learning) shows significant improvements, over 6 BLEU points absolute above the baseline.",
        "In this case the online feature can clearly take advantage of the high repetition rates observed in the IT dev and test sets (Table 1).",
        "Similarly, in the second block, the online system (2nd block ?+O+NS+W?",
        "with 10 iterations of online learning) outperforms IncGiza baseline, too.",
        "It is interesting to note that by continuously updating the baseline system after each translation step, even the plain translation models are capable to learn from the correction in the post-edited text.",
        "Figure 1 depicts learning curve of Baseline system, ?+O+NS?",
        "(referred as +online feature) and ?+O+NS+W?",
        "(referred as +MIRA).",
        "We plotted incremental BLEU scores after translation of each sentence, thereby the last point on the plot shows the corpus level BLEU on the whole test set.",
        "In Table 3, from the first block we can observe that online learning systems perform only slightly better than the baseline systems, the main reason being the low repetition rate observed in the evaluation set (as shown in Table 1).",
        "The positive results observed in the second block (?+O+W?",
        "with 10 iterations) are probably due to the larger room for improvement available for translation models implemented with dynamic suffix arrays, as they only incorporate 3 features instead of 5.",
        "Sometimes, online learning systems show worse results with higher numbers of iterations, which seems due to overfitting.",
        "It is also interesting to notice that after optimization the weight value of the online feature was 0.509 for the IT task and 0.072 for the TED talk task.",
        "This confirms the different use and potential assigned to the online feature by the SMT systems in the two tasks."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "We have shown a new way to update the translation model on the fly without changing the original probability distribution.",
        "We empirically proved that this method is robust and works for different domain datasets be it Information Technology or TED talks.",
        "In addition, if the repetition rate is high in the text, online learning works much better than if the rate is low.",
        "We tested both with an unbounded and a bounded range on the online feature and found out that bounded values produce more stable and consistent results.",
        "From previous works, it has been proven that MIRA works well with sparse features too, so, as for the future plan we would like to treat each phrase pair as a sparse feature and tune the sparse weights using MIRA.",
        "From the results, it is evident that we have not used any sort of stopping criterion for online learning; a random of 1, 5 and 10 iterations were chosen in a naive way.",
        "Our future plan will extend to working on finding a stopping criterion for online learning process."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This work was supported by the MateCat project, which is funded by the EC under the 7th Framework Programme."
      ]
    }
  ]
}
