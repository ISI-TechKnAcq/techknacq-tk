{
  "info": {
    "authors": [
      "Maria Holmqvist",
      "Sara Stymne",
      "Jody Foo",
      "Lars Ahrenberg"
    ],
    "book": "Proceedings of the Fourth Workshop on Statistical Machine Translation",
    "id": "acl-W09-0421",
    "title": "Improving Alignment for SMT by Reordering and Augmenting the Training Corpus",
    "url": "https://aclweb.org/anthology/W09-0421",
    "year": 2009
  },
  "references": [
    "acl-E03-1076",
    "acl-J03-1002",
    "acl-J07-3002",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P04-1023",
    "acl-P06-1097",
    "acl-P07-1039",
    "acl-P07-2045",
    "acl-P08-1112",
    "acl-W07-0733",
    "acl-W08-0312",
    "acl-W08-0317"
  ],
  "sections": [
    {
      "text": [
        "Improving alignment for SMT by reordering and augmenting the training corpus",
        "Maria Holmqvist, Sara Stymne, Jody Foo and Lars Ahrenberg",
        "Linköping University, Sweden {marho,sarst,jodfo,lah}@ida.liu.se",
        "We describe the LIU systems for English-German and German-English translation in the WMT09 shared task.",
        "We focus on two methods to improve the word alignment: (i) by applying Giza++ in a second phase to a reordered training corpus, where reordering is based on the alignments from the first phase, and (ii) by adding lexical data obtained as high-precision alignments from a different word aligner.",
        "These methods were studied in the context of a system that uses compound processing, a morphological sequence model for German, and a part-of-speech sequence model for English.",
        "Both methods gave some improvements to translation quality as measured by Bleu and Meteor scores, though not consistently.",
        "All systems used both out-of-domain and in-domain data as the mixed corpus had better scores in the baseline configuration."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "It is an open question whether improved word alignment actually improves statistical MT.",
        "Fraser and Marcu (2007) found that improved alignments as measured by AER will not necessarily improve translation quality, whereas Ganchev et al.",
        "(2008) did improve translation quality on several language pairs by extending the alignment algorithm.",
        "For this year's shared task we therefore studied the effects of improving word alignment in the context of our system for the WMT09 shared task.",
        "Two methods were tried: (i) applying Giza++ in a second phase to a reordered training corpus, where reordering is based on the alignments from the first phase, and (ii) adding lexical data obtained as high-precision alignments from a different word aligner.",
        "The submitted system includes the first method in addition to the processing of compounds and additional sequence models used by Stymne et al.",
        "(2008).",
        "Heuristics were used to generate true-cased versions of the translations that were submitted, as reported in section 6.",
        "In this paper we report case-insensitive Bleu scores (Papineni et al., 2002), unless otherwise stated, calculated with the NIST tool, and case-insensitive Meteor-ranking scores, without WordNet (Agarwal and Lavie, 2008)."
      ]
    },
    {
      "heading": "2. Baseline system",
      "text": [
        "Our baseline system uses compound splitting, compound merging and part-of-speech/morphological sequence models (Stymne et al., 2008).",
        "Except for these additions it is similar to the baseline system of the workshop .",
        "The translation system is a factored phrase-based translation system that uses the Moses toolkit (Koehn et al., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models.",
        "Minimum error rate training was used to tune the model feature weights (Och, 2003).",
        "Tuning was performed on the news-dev2009a set with 1025 sentences.",
        "All development testing was performed on the news-dev2009b set with 1026 sentences.",
        "The translation models were factored with one additional output factor.",
        "For English we used part-of-speech tags obtained with TreeTagger (Schmid, 1994).",
        "For German we enriched the tags from TreeTagger with morphological information, such as case or tense, that we get from a commercial",
        "http://www.statmt.org/wmt0 9/baseline.",
        "dependency parser.",
        "We used the extra factor in an additional sequence model which can improve agreement between words, and word order.",
        "For German this factor was also used for compound merging.",
        "Prior to training and translation, compound processing was performed using an empirical method based on (Koehn and Knight, 2003; Stymne, 2008).",
        "Words were split if they could be split into parts that occur in a monolingual corpus.",
        "We chose the split with the highest arithmetic mean of the corpus frequencies of compound parts.",
        "We split nouns, adjectives and verbs into parts that were content words or particles.",
        "A part had to be at least 3 characters in length and a stop list was used to avoid parts that often lead to errors, such as arische (Aryan) in konsularische (consular).",
        "Compound parts sometimes have special compound suffixes, which could be additions or truncations of letters, or combinations of these.",
        "We used the top 10 suffixes from a corpus study of Langer (1998), and we also treated hyphens as suffixes of compound parts.",
        "Compound parts were given a special part-of-speech tag that matched the head word.",
        "For translation into German, compound parts were merged to form compounds, both during test and tuning.",
        "The merging is based on the special part-of-speech tag used for compound parts (Stymne, 2009).",
        "A token with this POS-tag is merged with the next token, either if the POS-tags match, or if it results in a known word."
      ]
    },
    {
      "heading": "3. Domain adaptation",
      "text": [
        "This year three training corpora were available, a small bilingual news commentary corpus, a reasonably large Europarl corpus, and a very large monolingual news corpus, see Table 1 for details.",
        "The bilingual data was filtered to remove sentences longer than 60 words.",
        "Because the German news training corpus contained a number of English sentences, this corpus was cleaned by removing sentences containing a number of common English words.",
        "Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data",
        "connexor.eu",
        "from the news domain.",
        "We used the possibility to include several translation models in the Moses decoder by using multiple alternative decoding paths.",
        "We first trained systems on either bilingual news data or Europarl.",
        "Then we trained a mixed system, with two translation models one from each corpus, a language model from the bilingual news data, and a Europarl reordering model.",
        "The mixed system was slightly better than the Europarl only system.",
        "All sequence models used 5-grams for surface form and 7-grams for part-of-speech.",
        "All scores are shown in Table 2.",
        "We wanted to train sequence models on the large monolingual corpora, but due to limited computer resources, we had to use a lower order for this, than on the small corpus.",
        "Thus our sequence models on this data has lower order than those trained on bilingual news or Europarl, with 4-grams for surface form and 6-grams for part-of-speech.",
        "We also used the entropy-based pruning included in the SRILM toolkit, with 10-8 as a threshold.",
        "Using these sequence models in the mixed model, called mixed+, improved the results drastically, as shown in Table 2.",
        "The other experiments reported in this paper are based on the mixed+ system."
      ]
    },
    {
      "heading": "4. Improved alignment by reordering",
      "text": [
        "Word alignment with Giza++ has been shown to improve from making the source and target language more similar, e.g., in terms of segmentation (Ma et al., 2007) or word order.",
        "We used the following simple procedure to improve alignment of the training corpus by reordering the words in one of the texts according to the word order in the other language:",
        "Corpus",
        "German English",
        "news-commentary09",
        "Europarl",
        "news-train08",
        "81,141 1,331,262 9,619,406 21,215,311",
        "Corpus",
        "En^De",
        "De^En",
        "Bleu",
        "Meteor",
        "Bleu",
        "Meteor",
        "News com.",
        "12.13",
        "47.01",
        "17.21",
        "36.08",
        "Europarl",
        "12.92",
        "47.27",
        "18.53",
        "37.65",
        "Mixed",
        "12.91",
        "47.96",
        "18.76",
        "37.69",
        "Mixed+",
        "14.62",
        "49.48",
        "19.92",
        "38.18",
        "1.",
        "Word align the corpus with Giza++.",
        "2.",
        "Reorder the German words according to the order of the English words they are aligned to.",
        "(This is a common step in approaches that extract reordering rules for translation.",
        "However, this is not what we use it for here.)",
        "3.",
        "Word align the reordered German and original English corpus with Giza++.",
        "4.",
        "Put the reordered German words back into their original position and adjust the alignments so that the improved alignment is preserved.",
        "After this step we will have a possibly improved alignment compared to the original Giza++ alignment.",
        "A phrase table was extracted from the alignment and training was performed as usual.",
        "The reordering procedure was carried out on both source (Re-Src) and target data (Re-Trg) and the results of translating devtest data using these alignments are shown in Table 3.",
        "Compared with our baseline (mixed+), Bleu and Meteor increased for the translation direction German-English.",
        "Both source reordering and target reordering resulted in a 0.6 increase in Bleu.",
        "For translation into German, source reordering resulted in a somewhat higher Meteor score, but overall did not seem to improve translation.",
        "Target reordering in this direction resulted in lower scores.",
        "It is not clear why reordering improved translation for German-English and not for English-German.",
        "In all experiments, the heuristic sym-metrization of directed Giza++ alignments was performed in the intended translation direction ."
      ]
    },
    {
      "heading": "5. Augmenting the corpus with an extracted dictionary",
      "text": [
        "Previous research (Callison-Burch et al., 2004; Fraser and Marcu, 2006) has shown that including word aligned data during training can improve translation results.",
        "In our case we included a dictionary extracted from the news-commentary corpus during the word alignment.",
        "Using a method originally developed for term extraction (Merkel and Foo, 2007), the news-commentary09 corpus was grammatically annotated and aligned using a heuristic word aligner.",
        "Candidate dictionary entries were extracted from the alignments.",
        "In order to optimize the quality of the dictionary, dictionary entry candidates were ranked according to their Q-value, a metric specifically designed for aligned data (Merkel and Foo, 2007).",
        "The Q-value is based on the following statistics:",
        "• Type Pair Frequencies (TPF), i.e. the number of times where the source and target types are aligned.",
        "• Target types per Source type (TpS), i.e. the number of target types a specific source type has been aligned to.",
        "• Source types per Target type (SpT), i.e. the number of source types a specific target type has been aligned to.",
        "The Q-value is calculated as Q – value= Tps+spT.",
        "A high Q-value indicates a dictionary candidate pair with a relatively low number of translation variations.",
        "The candidates were filtered using a Q-value threshold of 0.333, resulting in a dictionary containing 67287 entries.",
        "For the experiments, the extracted dictionary was inserted 200 times into the corpus used during word alignment.",
        "The added dictionary entries were removed before phrase extraction.",
        "Experiments using the extracted dictionary as an additional phrase table were also run, but did not result in any improvement of translation quality.",
        "The results can be seen in Table 4.",
        "There was no evident pattern how the inclusion of the dictionary during alignment (DictAl) affected the translation quality.",
        "The inclusion of the dictionary produced both higher and lower Bleu scores than the baseline system depending on the translation direction.",
        "Meteor scores were however consistently lower than the baseline system.",
        "Corpus",
        "En^De",
        "De^En",
        "Bleu",
        "Meteor",
        "Bleu",
        "Meteor",
        "Mixed+",
        "14.62",
        "49.48",
        "19.92",
        "38.18",
        "Re-Src",
        "14.63",
        "49.80",
        "20.54",
        "38.86",
        "Re-Trg",
        "14.51",
        "48.62",
        "20.48",
        "38.73"
      ]
    },
    {
      "heading": "6. Post processing of out-of-vocabulary words",
      "text": [
        "In the standard systems all out-of-vocabulary words are transferred as is from the translation input to the translation output.",
        "Many of these words are proper names, which do not get capitalized properly, or numbers, which have different formatting in German and English.",
        "We used postprocessing to improve this.",
        "For all unknown words we capitalized either the first letter, or all letters, if they occur in that form in the translation input.",
        "For unknown numbers we switched between the German decimal comma and the English decimal point for decimal numbers.",
        "For large numbers, English has a comma to separate thousands, and German has a period.",
        "These were also switched.",
        "This improved case-sensitive Bleu scores in both translation directions, see Table 5."
      ]
    },
    {
      "heading": "7. Submitted system",
      "text": [
        "For both translation directions De-En and En-De we submitted a system with two translation models trained on bilingual news and Europarl.",
        "The alignment was improved by using the reordering techniques described in section 4.",
        "The systems also use all features described in this paper except for the lexical augmentation (section 5) which did not result in significant improvement.",
        "The results of the submitted systems on devtest data are boldfaced in Table 3.",
        "Table 6: Bleu scores for the reordered systems on two sections of development set news-dev2009b.",
        "NIST scores show the same distribution."
      ]
    },
    {
      "heading": "8. Results on two sections of devtest data",
      "text": [
        "Comparisons of translation output with reference translations on devtest data showed some surprising differences, which could be attributed to corresponding differences between source and reference data.",
        "The differences were not evenly distributed but especially frequent in those sections where the original language was something other than English or German.",
        "To check the homogeneity of the devtest data we divided it into two sections, one for documents of English or German origin, and the other for the remainder.",
        "It turned out that scores were dramatically different for the two sections, as shown in Table 6.",
        "The reason for the difference is likely to be that only the En-De set contains source texts and translations, while the other section contains parallel translations from the same source.",
        "This suggests that it would be interesting to study the effects of splitting the training corpus in the same way before training."
      ]
    },
    {
      "heading": "9. Conclusion",
      "text": [
        "The results of augmenting the training corpus with an extracted lexicon were inconclusive.",
        "However, the alignment reordering improved translation quality, especially in the De-En direction.",
        "The result of these reordering experiments indicates that better word alignment quality will improve SMT.",
        "The reordering method described in this paper also has the advantage of only requiring two runs of Giza++, no additional resources or training is necessary to get an improved alignment.",
        "Corpus",
        "En^De Bleu Meteor",
        "De^En Bleu Meteor",
        "Mixed+ DictAl",
        "14.62 49.48 14.73 49.39",
        "19.92 38.18",
        "18.93 37.71",
        "Corpus",
        "En^De De^En",
        "All",
        "En-De orig.",
        "Other set",
        "14.63 20.54 19.93 26.82 11.66 16.17",
        "Corpus",
        "En^De De^En",
        "Mixed+ with OOV",
        "13.31 17.47 13.74 17.96"
      ]
    }
  ]
}
