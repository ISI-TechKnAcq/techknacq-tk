{
  "info": {
    "authors": [
      "Prodromos Malakasiotis",
      "Ion Androutsopoulos"
    ],
    "book": "ACL-PASCAL Workshop on Textual Entailment and Paraphrasing",
    "id": "acl-W07-1407",
    "title": "Learning Textual Entailment using SVMs and String Similarity Measures",
    "url": "https://aclweb.org/anthology/W07-1407",
    "year": 2007
  },
  "references": [
    "acl-A00-1021",
    "acl-W01-0509"
  ],
  "sections": [
    {
      "text": [
        "We present the system that we submitted to the 3rd Pascal Recognizing Textual Entailment Challenge.",
        "It uses four Support Vector Machines, one for each subtask of the challenge, with features that correspond to string similarity measures operating at the lexical and shallow syntactic level."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Textual Entailment is desirable in many natural language processing areas, such as question answering, information extraction, information retrieval, and multi-document summarization.",
        "In the Pascal Recognizing Textual Entailment Challenge (rte), it is defined as the task of deciding whether or not the meaning of a hypothesis text (H) can be inferred from the meaning of another text (T)} For instance:",
        "T: The drugs that slow down or halt Alzheimer's disease work best the earlier you administer them.",
        "H: Alzheimer's disease is treated using drugs.",
        "is a correct entailment pair, but the following is not:",
        "T: Drew Walker, nhs Tayside's public health director, said: \"It is important to stress that this is not a confirmed case of rabies.\"",
        "H: A case of rabies was confirmed.",
        "In previous rte challenges (Dagan et al., 2006; Bar-Haim et al., 2006), several machine-learning approaches appeared, but their results showed that significant improvements were still necessary.",
        "In this paper, we present the system we used in the third",
        "!See http://www.pascal-network.org/.",
        "rte challenge.",
        "The latter had four different development and test sets (qa, ir, ie, sum), intended to evaluate textual entailment recognition in the four natural language processing areas mentioned above."
      ]
    },
    {
      "heading": "2. System overview",
      "text": [
        "Our system uses svms (Vapnik, 1998) to determine whether each TH pair constitutes a correct textual entailment or not.",
        "In particular, it employs four svms, each trained on the development dataset of the corresponding rte subtask (qa, ir, ie, sum) and used on the corresponding test dataset.",
        "Preliminary experiments indicated that training a single svm on all four subsets leads to worse results, despite the increased size of the training set, presumably because of differences in how the pairs were constructed in each subtask, which do not allow a single svm to generalize well over all four.",
        "The system is based on the assumption that string similarity at the lexical and shallow syntactic level can be used to identify textual entailment reasonably well, at least in question answering, the main area we are interested in.",
        "We, therefore, try to capture different kinds of similarity by employing 10 different string similarity measures, to be discussed below.",
        "In each TH case, every measure is applied to the following 8 pairs of strings, producing a total of 80 measurements:",
        "pair 1: two strings with the original words of T and H, respectively; although we refer to 'words', this and the following string pairs also contain non-word tokens, such as punctuation.",
        "pair 2: two strings containing the corresponding stems of the words of T and H, respectively;",
        "pair 3: two strings containing the part-of-speech (pos) tags of the words of T and H;",
        "pair 4: two strings containing the chunk tags (see below) of the words of T and H;",
        "pair 5: two strings containing only the nouns of T and H, as identified by a POS-tagger;",
        "pair 6: two strings containing only the stems of the nouns of T and H;",
        "pair 7: two strings containing only the verbs of T and H, as identified by a POS-tagger;",
        "pair 8: two strings containing only the stems of the verbs of T and H.",
        "Chunk tags are of the form B-x, ix or O, were B and I indicate the initial and other words of the chunks, respectively, whereas O indicates words outside all chunks; x can be NP, VP, or PP, for noun phrase, verb phrase, and prepositional phrase chunks.",
        "Partial matches: When applying the string similarity measures, one problem is that T may be much longer than H, or vice versa.",
        "Consider, for example, the following TH pair.",
        "The difference in the lengths of T and H may mislead many similarity measures to indicate that the two texts are very dissimilar, even though H is included verbatim in T.",
        "T: Charles de Gaulle died in 1970 at the age of eighty.",
        "He was thus fifty years old when, as an unknown officer recently promoted to the (temporary) rank of brigadier general, he made his famous broadcast from London rejecting the capitulation of France to the Nazis after the debacle of May-June 1940.",
        "H: Charles de Gaulle died in 1970.",
        "To address this problem, when we consider a pair of strings (si;s2), if si is longer than s2, we also compute the ten values /i(si, s2), where / (1 < i < 10) are the string similarity measures, for every si that is a substring of s1 of the same length as s2.",
        "We then locate the si with the best average similarity to s2, shown below as si*:",
        "and we keep the ten /i(s/i*, s2) values and their average as 11 additional measurements.",
        "Similarly, if s2 is longer than si, we keep the ten /i(si, s2*) values and their average.",
        "This process could be applied to all pairs 1-8 above, but the system we submitted applied it only to pairs 1-4; hence, there is a total of 44 additional measurements in each TH case.",
        "The 124 measurements discussed above provide 124 candidate numeric features that can be used by the SVMs.",
        "To those, we add the following four:",
        "Negation: Two Boolean features, showing if T or H, respectively, contain negation, identified by looking for words like \"not\", \"won't\", etc.",
        "Length ratio: This is min(/LT>Lh), were LT and LH are the lengths, in words, of T and H.",
        "Text length: Binary feature showing if the markup of the dataset flags T as 'long' or 'short'.",
        "Hence, there are 128 candidate features in total.",
        "From those, we select a different subset for the SVM of each subtask, as will be discussed in following sections.",
        "Note that similarity measures have also been used in previous RTE systems as features in machine learning algorithms; see, for example, Kozareva and Montoyo (2006), Newman et al.",
        "(2006).",
        "However, the results of those systems indicate that improvements are still necessary, and we believe that one possible improvement is the use of more and different similarity measures.",
        "We did not use similarity measures that operate on parse trees or semantic representations, as we are interested in RTE methods that can also be applied to less spoken languages, where reliable parsers, fact extractors, etc.",
        "are often difficult to obtain.",
        "We now describe the ten string similarity measures that we use.",
        "The reader is reminded that the measures are applied to string pairs (si;s2), where siand s2 derive from T and H, respectively.",
        "Levenshtein distance: This is the minimum number of operations (edit distance) needed to transform one string (in our case, si) into the other one (s2), where an operation is an insertion, deletion, or substitution of a single character.",
        "In pairs of strings that contain POS or chunk tags, it would be better to consider operations that insert, delete, or substitute entire tags, instead of characters, but the system we submitted did not do this; we addressed this issue in subsequent work, as will be discussed below.",
        "Jaro-Winkler distance: The Jaro-Winkler distance (Winkler, 1999) is a variation of the Jaro distance (Jaro, 1995), which we describe first.",
        "The Jaro distance dj of si and s2 is defined as:",
        "where 1i and 12 are the lengths (in characters) of siand s2, respectively.",
        "The value m is the number of characters of si that match characters of s2.",
        "Two characters from si and s2, respectively, are taken to match if they are identical and the difference in their positions does not exceed max21'l2) – 1.",
        "Finally, to compute t ('transpositions'), we remove from si and s2 all characters that do not have matching characters in the other string, and we count the number of positions in the resulting two strings that do not contain the same character; t is half that number.",
        "The Jaro-Winkler distance dw emphasizes prefix similarity between the two strings.",
        "It is defined as:",
        "where l is the length of the longest common prefix of si and s2, and p is a constant scaling factor that also controls the emphasis placed on prefix similarity.",
        "The implementation we used considers prefixes up to 6 characters long, and sets p = 0.1.",
        "Again, in pairs of strings (si; s2) that contain POS tags or chunk tags, it would be better to apply this measure to the corresponding lists of tags in si and s2, instead of treating si and s2 as strings of characters, but the system we submitted did not do this; this issue was also addressed in subsequent work.",
        "Soundex: Soundex is an algorithm intended to map each English name to an alphanumeric code, so that names whose pronunciations are the same are mapped to the same code, despite spelling differences.",
        "Although Soundex is intended to be used on names, and in effect considers only the first letter and the first few consonants of each name, we applied it to si and s2, in an attempt to capture similarity at the beginnings of the two strings; the strings were first stripped of all white spaces and non-letter characters.",
        "We then computed similarity between the two resulting codes using the Jaro-Winkler distance.",
        "A better approach would be to apply Soundex to all words in T and H, forming a 9th pair (si; s2), on which other distance measures would then be applied; we did this in subsequent work.",
        "Manhattan distance: Also known as City Block distance or Li, this is defined for any two vectors x = (xi,..., x„) and y = (yi,..., y.n) in an n-dimensional vector space as:",
        "Li(x,y) =J2 |xi – yi|.",
        "In our case, n is the number of distinct words (or tags) that occur in si and s2 (in any of the two); and xi, yi show how many times each one of these distinct words occurs in si and s2, respectively.",
        "Euclidean distance: This is defined as follows:",
        "J2(xi – yi)2.",
        "In our case, x and y correspond to si and s2, respectively, as in the previous measure.",
        "Cosine similarity: The definition follows:",
        "In our system yx and yy are as above, except that they are binary, i.e., xi and yi are 1 or 0, depending on whether or not the corresponding word (or tag) occurs in si or s2, respectively.",
        "N-gram distance: This is the same as Li, but instead of words we use all the (distinct) character n-grams in si and s2; we used n = 3.",
        "Matching coefficient: This is |X n Y|, where X and Y are the sets of (unique) words (or tags) of siand s2, respectively; i.e., it counts how many common words si and s2 have.",
        "Dice coefficient: This is the following quantity; in our case, X and Y are as in the previous measure.",
        "Jaccard coefficient: This is defined as |; again X and Y are as in the matching coefficient.",
        "As already noted, we employed four svms, one for each subtask of the challenge (ir, ie, qa, sum).In each subtask, feature selection was performed as follows.",
        "We started with a set of 20 features, which correspond to the ten similarity measures applied to both words and stems (string pairs 1 and 2 ofsection 1); see table 1.",
        "We then added the 10 features that correspond to the ten similarity measures applied to pos tags (string pair 3).",
        "In ie and ir, this addition led to improved leave-one-out cross-validation results on the corresponding development sets, and we kept the additional features (denoted by 'X' in table 1).",
        "In contrast, in qa and sum the additional 10 features were discarded, because they led to no improvement in the cross-validation.",
        "We then added the 10 features that corresponded to the ten similarity measures applied to chunk tags (string pair 4), which were retained only in the ie svm, and so on.",
        "The order in which we considered the various extensions of the feature sets is the same as the order of the rows of table 1, and it reflects the order in which it occurred to us to consider the corresponding additional features while preparing for the challenge.",
        "We hope to investigate additional feature selection schemes in further work; for instance, start with all 128 features and explore if pruning any groups of features improves the cross-validation results.",
        "With each feature set that we considered, we actually performed multiple leave-one-out cross-validations on the development dataset, for different values of the parameters of the svm and kernel, using a grid-search utility.",
        "Each feature set was evaluated by considering its best cross-validation result.",
        "The best cross-validation results for the final feature sets of the four svms are shown in table 2.",
        "Table 2: Best cross-validation results of our system on the development datasets.",
        "Results with subsequent improvements are shown in brackets.",
        "Table 3: Official results of our system.",
        "Results with subsequent improvements are shown in brackets."
      ]
    },
    {
      "heading": "3. Official results and discussion",
      "text": [
        "We submitted only one run to the third rte challenge.",
        "The official results of our system are shown in table 3.",
        "They are worse than the best results we had obtained in the cross-validations on the development datasets (cf. table 2), but this was expected to a large extent, since the svms were tuned on the development datasets; to some extent, the lower official results may also be due to different types of entailment being present in the test datasets, which had not been encountered in the training sets.",
        "As in the cross-validation results, our system performed best in the qa subtask; the second and third best results of our system were obtained in ir and sum, while the worst results were obtained in ie.",
        "Although a more thorough investigation is necessary to account fully for these results, it appears that they support our initial assumption that string similarity at the lexical and shallow syntactic level can be used to identify textual entailment reasonably well in question answering systems.",
        "Some further reflections on the results of our system follow.",
        "In the qa subtask of the challenge, it appears that each T was a snippet returned by a question answering system for a particular question.",
        "We are not aware of exactly how the Ts were selected by the systems used, but qa systems typically return Ts that contain the expected answer type of the input question; for instance, if the question is \"When did Charles de Gaulle die?",
        "\", T will typically contain a temporal expression.",
        "Furthermore, qa systems typically prefer T s that contain many words of the question, preferably in the same order, etc.",
        "(Radev et al., 2000; Ng et al., 2001; Harabagiu et al., 2003).",
        "Hence, if the answers are sought in a document collection with high redundancy (e.g., the Web), i.e., a collection where each answer can be found with many different phrasings, the Ts (or parts of them) that most qa systems return are often very similar, in terms of phrasings, to the questions, provided that the required answers exist in the collection.",
        "Subtask",
        "Accuracy (%)",
        "qa",
        "86.50 (90.00)",
        "ir",
        "80.00 (75.50)",
        "sum",
        "73.00 (72.50)",
        "ie",
        "62.00 (61.50)",
        "all",
        "75.38 (74.88)",
        "Subtask",
        "Accuracy (Vo)",
        "Average Precision (Vo)",
        "qa",
        "73.50 (76.00)",
        "81.03 (81.08)",
        "ir",
        "64.50 (63.50)",
        "63.61 (67.28)",
        "sum",
        "57.00 (60.50)",
        "60.88 (61.58)",
        "ie",
        "52.00 (49.50)",
        "58.16 (51.57)",
        "all",
        "61.75 (62.38)",
        "68.08 (68.28)",
        "In the qa datasets of the challenge, for each T, which was a snippet returned by a qa system for a question (e.g., \"When did Charle de Gaulle die?",
        "\"), an H was formed by \"plugging into\" the question an expression of the expected answer type from T. In effect, this converted all questions to propositions (e.g., \"Charle de Gaulle died in 1970.\")",
        "that require a \"yes\" or \"no\" answer.",
        "Note that this plugging in does not always produce a true proposition; T may contain multiple expressions of the expected answer type (e.g., \"Charle de Gaulle died in 1970.",
        "In 1990, a monument was erected.",
        ".",
        ". \")",
        "and the wrong one may be plugged into the question (H = \"Charle de",
        "Gaulle died in 1990.",
        "\").",
        "Let us first consider the case where the proposition (H) is true.",
        "Assuming that the document collection is redundant and that the answer to the question exists in the collection, T (or part of it) will often be very similar to H, since it will be very similar to the question that H was derived from.",
        "In fact, the similarity between T and H may be greater than between T and the question, since an expression from T has been plugged into the question to form H. Being very similar, T will very often entail H, and, hence, the (affirmative) responses of our system, which are based on similarity, will be correct.",
        "Let us now consider the case where H is false.",
        "Although the same arguments apply, and, hence, one might again expect T to be very similar to H, this is actually less likely now, because H is false and, hence, it is more difficult to find a very similarly phrased T in the presumed trustful document collection.",
        "The reduced similarity between T and H will lead the similarity measures to suggest that the TH entailment does not hold; and in most cases, this is a correct decision, because H is false and, thus, it cannot be entailed by a (true) T that has been extracted from a trustful document collection.",
        "Similar arguments apply to their subtask, where our system achieved its second best results.",
        "Our results in this subtask were lower than in the qa subtask, presumably because the Ts were no longer filtered by the additional requirement that they must contain an expression of the expected answer type.",
        "Feature sets",
        "features",
        "ie",
        "ir",
        "qa",
        "sum",
        "similarity measures on words",
        "10",
        "X",
        "X",
        "X",
        "X",
        "similarity measures on stems",
        "10",
        "X",
        "X",
        "X",
        "X",
        "+ similarity measures on POS tags",
        "+10",
        "X",
        "X",
        "+ similarity measures on chunk tags",
        "+10",
        "X",
        "X",
        "+ average of sim.",
        "measures on words of best partial match",
        "+1",
        "X",
        "+ average of sim.",
        "measures on stems of best partial match",
        "+1",
        "X",
        "X",
        "+ average of sim.",
        "measures on POS tags of best partial match",
        "+1",
        "X",
        "X",
        "+ average of sim.",
        "measures on chunk tags of best partial match",
        "+1",
        "X",
        "X",
        "+ similarity measures on words of best partial match",
        "+10",
        "+ similarity measures on stems of best partial match",
        "+10",
        "X",
        "+ similarity measures on POS tags of best partial match",
        "+10",
        "X",
        "+ similarity measures on chunk tags of best partial match",
        "+10",
        "+ negation",
        "+2",
        "X",
        "+ length ratio",
        "+1",
        "X",
        "+ similarity measures on nouns",
        "+10",
        "X",
        "+ similarity measures on noun stems",
        "+10",
        "+ similarity measures on verbs",
        "+10",
        "X",
        "+ similarity measures on verb stems",
        "+10",
        "+ short/long T",
        "+1",
        "X",
        "X",
        "Total",
        "128",
        "64",
        "31",
        "23",
        "54",
        "We attribute the further deterioration of our results in the sum subtask to the fact that, according to the challenge's documentation, all the TH pairs of that subtask, both true and false entailments, were chosen to have high lexical similarity, which does not allow the similarity measures of our system to distinguish well between the two cases.",
        "Finally, the lower results obtained in the ie subtask may be due to the fact that the TH pairs of that subtask were intended to reflect entailments identified by information extraction systems, which specialize on identifying particular semantic relations by employing more complicated machinery (e.g., named entity recognizers and matchers, fact extractors, etc.)",
        "than simple string similarity measures; the results may also be partly due to the four different ways that were used to construct the TH pairs of that sub-task.",
        "It is interesting to note (see table 1) that the feature sets were larger in the subtasks where our system scored worse, which may be an indication of the difficulties the corresponding svms encountered."
      ]
    },
    {
      "heading": "4. Conclusions and further work",
      "text": [
        "We presented a textual entailment recognition system that relies on svms whose features correspond to string similarity measures applied to the lexical and shallow syntactic level.",
        "Experimental results indicate that the system performs reasonably well in question answering (qa), which was our main target, with results deteriorating as we move to information retrieval (ir), multi-document summarization (sum), and information extraction (ie).",
        "In work carried out after the official submission of our system, we incorporated two of the possible improvements that were mentioned in previous sections: we treated strings containing pos or chunk tags as lists of tags; and we applied Soundex to each word of T and H, forming a 9th pair of strings, on which all other similarity measures were applied; feature selection was then repeated anew.",
        "The corresponding results are shown in brackets in tables 2 and 3.",
        "There was an overall improvement in all tasks (qa, ir, sum), except for ie, where textual entailment is more difficult to capture via textual similarity, as commented above.",
        "We have suggested two additional possible improvements: applying partial matching to all of the string pairs that we consider, and investigating other feature selection schemes.",
        "In future work, we also plan to exploit WordNet to capture synonyms, hypernyms, etc."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This work was funded by the Greek pened 2003 programme, which is co-funded by the European Union (75%), and the Greek General Secretariat for Research and Technology (25%)."
      ]
    }
  ]
}
