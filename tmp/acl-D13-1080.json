{
  "info": {
    "authors": [
      "Matt Gardner",
      "Partha Pratim Talukdar",
      "Bryan Kisiel",
      "Tom Mitchell"
    ],
    "book": "EMNLP",
    "id": "acl-D13-1080",
    "title": "Improving Learning and Inference in a Large Knowledge-Base using Latent Syntactic Cues",
    "url": "https://aclweb.org/anthology/D13-1080",
    "year": 2013
  },
  "references": [
    "acl-C92-2082",
    "acl-D11-1049",
    "acl-D11-1132",
    "acl-D12-1093",
    "acl-N13-1008",
    "acl-P02-1006",
    "acl-P03-1009",
    "acl-P06-1101"
  ],
  "sections": [
    {
      "heading": "Abstract Automatically constructed Knowledge Bases",
      "text": [
        "(KBs) are often incomplete and there is a genuine need to improve their coverage.",
        "Path Ranking Algorithm (PRA) is a recently proposed method which aims to improve KB coverage by performing inference directly over the KB graph.",
        "For the first time, we demonstrate that addition of edges labeled with latent features mined from a large dependency parsed corpus of 500 million Web documents can significantly outperform previous PRA-based approaches on the KB inference task.",
        "We present extensive experimental results validating this finding.",
        "The resources presented in this paper are publicly available."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Over the last few years, several large scale Knowledge Bases (KBs) such as Freebase (Bollacker et al., 2008), NELL (Carlson et al., 2010), and YAGO (Suchanek et al., 2007) have been developed.",
        "Each such KB consists of millions of facts (e.g., (Tiger Woods, playsSport, Golf )) spanning over multiple relations.",
        "Unfortunately, these KBs are often incomplete and there is a need to increase their coverage of facts to make them useful in practical applications.",
        "A strategy to increase coverage might be to perform inference directly over the KB represented as a graph.",
        "For example, if the KB contained the following facts, (Tiger Woods, participatesIn, PGA Tour)) and (Golf, sportOfTournament, PGA Tour), then by putting these two facts together, we could potentially infer that (Tiger Woods, playsSport, Golf ).",
        "The",
        "tent labels can improve inference performance by reducing data sparsity.",
        "See Section 1.1 for details.",
        "recently proposed Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) performs such inference by automatically learning semantic inference rules over the KB (Lao et al., 2011).",
        "PRA uses features based off of sequences of edge types, e.g., ?playsSport, sportOfTournament?, to predict missing facts in the KB.",
        "PRA was extended by (Lao et al., 2012) to perform inference over a KB augmented with dependency parsed sentences.",
        "While this opens up the possibility of learning syntactic-semantic inference rules, the set of syntactic edge labels used are just the unlexicalized dependency role labels (e.g., nobj, dobj, etc., without the corresponding words), thereby limiting overall expressitivity of the learned inference rules.",
        "To overcome this limitation, in this paper we augment the KB graph by adding edges with more expressive lexicalized syntactic labels (where the labels are words instead of dependen",
        "cies).",
        "These additional edges, e.g., (Alex Rodriguez, ?plays for?, NY Yankees), are mined by extracting 600 million Subject-Verb-Object (SVO) triples from a large corpus of 500m dependency parsed documents, which would have been prohibitively expensive to add directly as in (Lao et al., 2012).",
        "In order to overcome the explosion of path features and data sparsity, we derive edge labels by learning latent embeddings of the lexicalized edges.",
        "Through extensive experiments on real world datasets, we demonstrate effectiveness of the proposed approach."
      ]
    },
    {
      "heading": "1.1 Motivating Example",
      "text": [
        "In Figure 1, the KB graph (only solid edges) is disconnected, thereby making it impossible for PRA to discover any relationship between Alex Rodriguez and World Series.",
        "However, addition of the two edges with SVO-based lexicalized syntactic edges (e.g., (Alex Rodriguez, plays for, NY Yankees)) restores this inference possibility.",
        "For example, PRA might use the edge sequence ?",
        "?plays for?, team-PlaysIn?",
        "as evidence for predicting the relation instance (Alex Rodriguez, athleteWonChampionship, World Series).",
        "Unfortunately, such na?",
        "?ve addition of lexicalized edges may result in significant data sparsity, which can be overcome by mapping lexicalized edge labels to some latent embedding (e.g., (Alex Rodriguez, LatentFeat#5, NY Yankees) and running PRA over this augmented graph.",
        "Using latent embeddings, PRA could then use the following edge sequence as a feature in its prediction models: ?LatentFeat#5, teamPlaysIn?.",
        "We find this strategy to be very effective as described in Section 4."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "There is a long history of methods using suface-level lexical patterns for extracting relational facts from text corpora (Hearst, 1992; Brin, 1999; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002; Etzioni et al., 2004).",
        "Syntactic information in the form of dependency paths have been explored in (Snow et al., 2006; Suchanek et al., 2006).",
        "A method of latent embedding of relation instances for sentence-level relation extraction was shown in (Wang et al., 2011).",
        "However, none of this prior work makes explicit use of the background KBs as we explore in this paper.",
        "Path Ranking Algorithm (PRA) (Lao and Cohen, 2010) has been used previously to perform inference over graph-structured KBs (Lao et al., 2011), and to learn formation of online communities (Settles and Dow, 2013).",
        "In (Lao et al., 2012), PRA is extended to perform inference over a KB using syntactic information from parsed text.",
        "In contrast to these previous PRA-based approaches where all edge labels are either KB labels or at surface-level, in this paper we explore using latent edge labels in addition to surface-level labels in the graph over which PRA is applied.",
        "In particular, we focus on the problem of performing inference over a large KB and learn latent edge labels by mining dependency syntax statistics from a large text corpus.",
        "Though we use Principal Components Analysis (PCA) for dimensionality reduction for the experiments in this paper, this is by no means the only choice.",
        "Various other dimensionality reduction techniques, and in particular, other verb clustering techniques (Korhonen et al., 2003), may also be used.",
        "OpenIE systems such as Reverb (Etzioni et al., 2011) also extract verb-anchored dependency triples from large text corpus.",
        "In contrast to such approaches, we focus on how latent embedding of verbs in such triples can be combined with explicit background knowledge to improve coverage of existing KBs.",
        "This has the added capability of inferring facts which are not explicitly mentioned in text.",
        "The recently proposed Universal Schema (Riedel et al., 2013) also demonstrates the benefit of using latent features for increasing coverage of KBs.",
        "Key differences between that approach and ours include our use of syntactic information as opposed to surface-level patterns in theirs, and also the ability of the proposed PRA-based method to generate useful inference rules which is beyond the capability of the matrix factorization approach in (Riedel et al., 2013)."
      ]
    },
    {
      "heading": "3 Method",
      "text": []
    },
    {
      "heading": "3.1 Path Ranking Algorithm (PRA)",
      "text": [
        "In this section, we present a brief overview of the Path Ranking Algorithm (PRA) (Lao and Cohen, 2010), building on the notations in (Lao et al., 2012).",
        "Let G = (V,E, T ) be the graph, where V is the set of vertices, E is the set of edges, and T is the set of edge types.",
        "For each edge (v1, t, v2) ?",
        "E, we have",
        "v1, v2 ?",
        "V and t ?",
        "T .",
        "LetR ?",
        "T be the set of types predicted by PRA.",
        "R could in principal equal T , but in this paper we restrict prediction to KB relations, while T also includes types derived from surface text and latent embeddings.",
        "Let pi = ?t1, t2, .",
        ".",
        ".",
        ", tw?",
        "be a path type of length w over graph G, where ti ?",
        "T is the type of the ith edge in the path.",
        "Each such path type is also a feature in the PRA model.",
        "For a given source and target node pair s, t ?",
        "V , let P (s ?",
        "t;pi) be the value of the feature pi specifying the probability of reaching node t starting from node s and following a path constrained by path type pi.",
        "We approximate these probabilities using random walks.",
        "A value of 0 indicates unreachability from s to t using path type pi.",
        "Let B = {pi1, .",
        ".",
        ".",
        ", pim} be the set of all features (path types).",
        "The score that relation r holds between node s and node t is given by the following function:",
        "where ?rpi is the weight of feature pi in class r ?",
        "R. Feature Selection: The set B of possible path types grows exponentially in the length of the paths that are considered.",
        "In order to have a manageable set of features to compute, we first perform a feature selection step.",
        "The goal of this step is to select for computation only those path types that commonly connect sources and targets of relation r. We perform this feature selection by doing length-bounded random walks from a given list of source and target nodes, keeping track of how frequently each path type leads from a source node to a target node.",
        "The most common m path types are selected for the set B.",
        "Training: We perform standard logistic regression with L2 regularization to learn the weights ?rpi.",
        "We follow the strategy in (Lao and Cohen, 2010) to generate positive and negative training instances."
      ]
    },
    {
      "heading": "3.2 PRAsyntactic",
      "text": [
        "In this section, we shall extend the knowledge graph",
        ", with the set of vertices unchanged.",
        "In order to get the edges in E ?",
        "?",
        "E, we first collect a set of Subject-Verb-Object (SVO) triples D = {(s, v, o, c)} from a large dependency parsed text corpus, with c ?",
        "R+ denoting the frequency of this triple in the corpus.",
        "The additional edge set is then defined as Esyntactic = E",
        "= T ?",
        "S. In other words, for each pair of directly connected nodes in the KB graph G, we add an additional edge between those two nodes for each verb which takes the NPs represented by two nodes as subjects and objects (or vice versa) as observed in a text corpus.",
        "In Figure 1, (Alex Rodriguez, ?plays for?, NY Yankees) is an example of such an edge.",
        "PRA is then applied over this augmented graph",
        ", over the same set of prediction types R as before.",
        "We shall refer to this version of PRA as PRAsyntactic.",
        "For the experiments in this paper, we collected |D |= 600 million SVO triples1 from the entire ClueWeb corpus (Callan et al., 2009), parsed using the Malt parser (Nivre et al., 2007) by the Hazy project (Kumar et al., 2013)."
      ]
    },
    {
      "heading": "3.3 PRAlatent",
      "text": [
        "In this section we construct G",
        "another syntactic-information-induced extension of the knowledge graph G, but instead of using the surface forms of verbs in S (see previous section) as edge types, we derive those edges types T ??",
        "based on latent embeddings of those verbs.",
        "We note that",
        "In order to learn the latent or low dimensional embeddings of the verbs in S, we first define QS = {(s, o) |?",
        "(s, v, o, c) ?",
        "D, v ?",
        "S}, the set of subject-object tuples in D which are connected by at least one verb in S. We now construct a matrix X|S|?|QS |whose entry Xv,q = c, where v ?",
        "S, q = (s, o) ?",
        "QS , and (s, v, o, c) ?",
        "D. After row normalizing and centering matrix X , we apply PCA on this matrix.",
        "Let A|S|?d with d << |QS |be the low dimensional embeddings of the verbs in S as induced by PCA.",
        "We use two strategies to derive mappings for verbs from matrix A. ?",
        "PRAlatentc : The verb is mapped to concatenation of the k2 most positive columns in the row in A that corresponds to the verb.",
        "Similarly, for the most negative k2 columns.",
        "of PRA micro averaged across 15 NELL relations.",
        "We find that use of latent edge labels, in particular the proposed approach PRAlatentd , significantly outperforms other approaches.",
        "This is our main result.",
        "(See Section 4) ?",
        "PRAlatentd : The verb is mapped to disjunction of top-k most positive and negative columns in the row in A that corresponds to the verb."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "We compared the various methods using 15 NELL relations.",
        "For each relation, we split NELL's known relation instances into 90% training and 10% testing.",
        "For each method, we then selected 750 path features and trained the model, as described in Section 3, using GraphChi (Kyrola et al., 2012) to perform the random walk graph computations.",
        "To evaluate the model, we took all source nodes in the testing data and used the model to predict target nodes.",
        "We report the precision and recall (on the set of known target nodes) of the set of predictions for each model that are above a certain confidence threshold.",
        "Because we used strong regularization, we picked for our threshold a model score of 0.405, corresponding to 60% probability of the relation instance being true; values higher than this left many relations without any predictions.",
        "Table 1 contains the results.",
        "As can be seen in the table, PRAsyntactic on average performs slightly worse than PRA.",
        "While the extra syntactic features are very informative for some relations, they also introduce a lot of sparsity, which makes the model perform worse on other relations.",
        "When using latent factorization methods to reduce the sparsity of the syntactic features, we see a significant improvement in performance.",
        "PRAlatentc has a 45% reduction in precision errors vs. PRA while maintaining the same recall, and PRAlatentd reduces precision errors by 35% while improving recall by 27%.",
        "Section 4.1 contains some qualitative analysis of how sparsity is reduced with the latent methods.",
        "As a piece quanti",
        "proach which exploits latent edge labels, outperforms other alternatives.",
        "tative analysis, there were 908 possible path types found in the feature selection step with PRA on the relation cityLiesOnRiver (of which we then selected 750).",
        "For PRAsyntactic, there were 73,820, while PRAlatentc had 47,554 and PRAlatentd had 58,414.",
        "Table 2 shows F1 scores for each model on each relation, and Figure 2 shows representative Precision-Recall plots for two NELL relations.",
        "In both cases, we find that PRAlatentd significantly outperforms other baselines."
      ]
    },
    {
      "heading": "4.1 Discussion",
      "text": [
        "While examining the model weights for each of the methods, we saw a few occasions where surface relations and NELL relations combined to form interpretable path types.",
        "For example, in athletePlaysForTeam, some highly weighted features took the form of ?athletePlaysSport, ?",
        "(sport) played by (team)??.",
        "A high weight on this feature would bias the prediction towards teams that are known to play the same sport as the athlete.",
        "For PRA, the top features for the best performing relations are path types that contain a single edge",
        "which is a supertype or subtype of the relation being predicted.",
        "For instance, for the relation athletePlaysForTeam (shown in Figure 2), the highest-weighted features in PRA are athleteLedSportsTeam (more specific than athletePlaysForTeam) and personBelongsToOrganization (more general than athletePlaysForTeam).",
        "For the same relation, PRAsyntactic has features like ?scored for?, ?signed?, ?have?, and ?led?.",
        "When using a latent embedding of these verb phrases, ?signed?, ?have?, and ?led?",
        "all have the same representation in the latent space, and so it seems clear that PRAlatent gains a lot by reducing the sparsity inherent in using surface verb forms.",
        "For cityLiesOnRiver, where PRA does not perform as well, there is no NELL relation that is an immediate supertype or subtype, and so PRA does not have as much evidence to use.",
        "It finds features that, e.g., are analogous to the statement ?cities in the same state probably lie on the same river?.",
        "Adding lexical labels gives the model edges to use like ?lies on?, ?runs through?, ?flows through?, ?starts in?",
        "and ?reaches?, and these features give a significant boost in performance to PRAsyntactic.",
        "Once again, almost all of those verb phrases share the same latent embedding, and so PRAlatent gains another significant boost in performance by combining them into a single feature."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "In this paper, we introduced the use of latent lexical edge labels for PRA-based inference over knowledge bases.",
        "We obtained such latent edge labels by mining a large dependency parsed corpus of 500 million web documents and performing PCA on the result.",
        "Through extensive experiments on real datasets, we demonstrated that the proposed approach significantly outperforms previous state-of-the-art baselines."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank William Cohen (CMU) for enlightening conversations on topics discussed in this paper.",
        "We thank the ClueWeb project (CMU) and the Hazy Research Group (http://hazy.cs.wisc.edu/hazy/) for their generous help with data sets; and to the anonymous reviewers for their constructive comments.",
        "This research has been supported in part by DARPA (under contract number FA8750-13-2-0005), and Google.",
        "Any opinions, findings, conclusions and recommendations expressed in this paper are the authors?",
        "and do not necessarily reflect those of the sponsors."
      ]
    }
  ]
}
