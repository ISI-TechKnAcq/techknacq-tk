{
  "info": {
    "authors": [
      "Paul McNamee",
      "James Mayfield",
      "Tim Finin",
      "Tim Oates",
      "Dawn Lawrie",
      "Tan Xu",
      "Douglas Oard"
    ],
    "book": "NAACL",
    "id": "acl-N13-3008",
    "title": "KELVIN: a tool for automated knowledge base construction",
    "url": "https://aclweb.org/anthology/N13-3008",
    "year": 2013
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Proceedings of the NAACL HLT 2013 Demonstration Session, pages 32?35, Atlanta, Georgia, 10-12 June 2013. c?2013 Association for Computational Linguistics KELVIN: a tool for automated knowledge base construction"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "We present KELVIN, an automated system for processing a large text corpus and distilling a knowledge base about persons, organizations, and locations.",
        "We have tested the KELVIN system on several corpora, including: (a) the TAC KBP 2012 Cold Start corpus which consists of public Web pages from the University of Pennsylvania, and (b) a subset of 26k news articles taken from English Gigaword 5th edition.",
        "Our NAACL HLT 2013 demonstration permits a user to interact with a set of searchable HTML pages, which are automatically generated from the knowledge base.",
        "Each page contains information analogous to the semi-structured details about an entity that are present in Wikipedia Infoboxes, along with hyperlink citations to supporting text."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The Text Analysis Conference (TAC) Knowledge Base Population (KBP) Cold Start task1 requires systems to take set of documents and produce a comprehensive set of <Subject, Predicate, Object> triples that encode relationships between and attributes of the named-entities that are mentioned in the corpus.",
        "Systems are evaluated based on the fidelity of the constructed knowledge base.",
        "For the 2012 evaluation, a fixed schema of 42 relations (or slots), and their logical inverses was provided, for example:",
        "?",
        "X:Person has-job-title title ?",
        "X:Organization headquartered-in Y:Location Multiple layers of NLP software are required for this undertaking, including at the least: detection of named-entities, intra-document co-reference resolution, relation extraction, and entity disambiguation.",
        "To help prevent a bias towards learning about prominent entities at the expense of generality, KELVIN refrains from mining facts from sources such as documents obtained through Web search, Wikipedia2, or DBpedia.3 Only facts that are asserted in and gleaned from the source documents are posited.",
        "Other systems that create large-scale knowledge bases from general text include the Never-Ending"
      ]
    },
    {
      "heading": "2 Washington Post KB",
      "text": [
        "No gold-standard KBs were available to us to assist during the development of KELVIN, so we relied on qualitative assessment to gauge the effectiveness of our extracted relations ?",
        "by manually examining ten random samples for each relations, we ascertained that most relations were between 30-80% accurate.",
        "Although the TAC KBP 2012 Cold Start task was a pilot evaluation of a new task using a novel evaluation methodology, the KELVIN system did attain the highest reported F1 scores.4",
        "During our initial development we worked with a 26,143 document collection of 2010 Washington Post articles and the system discovered 194,059 relations about 57,847 named entities.",
        "KELVIN learns some interesting, but rather dubious relations from",
        "the Washington Post articles5 ?",
        "Sen. Harry Reid is an employee of the ?Republican Party.?",
        "Sen. Reid is also an employee of the ?Democratic Party.?",
        "?",
        "Big Foot is an employee of Starbucks.",
        "?",
        "MacBook Air is a subsidiary of Apple Inc. ?",
        "Jill Biden is married to Jill Biden.",
        "However, KELVIN also learns quite a number of correct facts, including: ?",
        "Warren Buffett owns shares of Berkshire Hathaway, Burlington Northern Santa Fe, the Washington Post Co., and four other stocks.",
        "?",
        "Jared Fogle is an employee of Subway.",
        "?",
        "Freeman Hrabowski works for UMBC, founded the Meyerhoff Scholars Program, and graduated from Hampton University and the University of Illinois.",
        "?",
        "Supreme Court Justice Elena Kagan attended Oxford, Harvard, and Princeton.",
        "?",
        "Southwest Airlines is headquartered in Texas.",
        "?",
        "Ian Soboroff is a computer scientist6 employed by NIST.7"
      ]
    },
    {
      "heading": "3 Pipeline Components",
      "text": []
    },
    {
      "heading": "3.1 SERIF",
      "text": [
        "BBN's SERIF tool8 (Boschee et al., 2005) provides a considerable suite of document annotations that are an excellent basis for building a knowledge base.",
        "The functions SERIF can provide are based largely",
        "the Washington Post texts.",
        "on the NIST ACE specification,9 and include: (a) identifying named-entities and classifying them by type and subtype; (b) performing intra-document co-reference analysis, including named mentions, as well as co-referential nominal and pronominal mentions; (c) parsing sentences and extracting intra-sentential relations between entities; and, (d) detecting certain types of events.",
        "In Table 1 we list the most common slots SERIF extracts from the Washington Post articles."
      ]
    },
    {
      "heading": "3.2 FACETS",
      "text": [
        "FACETS, another BBN tool, is an add-on package that takes SERIF output and produces role and argument annotations about person noun phrases.",
        "FACETS is implemented using a conditional",
        "have), as well as role-specific attributes, such as medical specialty for physicians, or academic institution for someone associated with an university.",
        "In Table 2 we report the most prevalent slots FACETS extracts from the Washington Post.10"
      ]
    },
    {
      "heading": "3.3 CUNY toolkit",
      "text": [
        "To increase our coverage of relations we also integrated the KBP Slot Filling Toolkit (Chen et al., 2011) developed at the CUNY BLENDER Lab.",
        "Given that the KBP toolkit was designed for the traditional slot filling task at TAC, this primarily involved creating the queries that the tool expected as input and parallelizing the toolkit to handle the vast number of queries issued in the cold start scenarios.",
        "To informally gauge the accuracy of slots extracted from the CUNY tool, some coarse assessment was done over a small collection of 807 New York Times articles that include the string ?University of Kansas.?",
        "From this collection, 4264 slots were identified.",
        "Nine different types of slots were filled in order of frequency: per:title (37%), per:employee of (23%), per:cities of residence (17%), per:stateorprovinces of residence (6%), 10Note FACETS can independently extract some slots that SERIF is capable of discovering (e.g., employment relations).",
        "org:top members/employees (6%), org:member of (6%), per:countries of residence (2%), per:spouse (2%), and per:member of (1%).",
        "We randomly sampled 10 slot-fills of each type, and found accuracy to vary from 20-70%."
      ]
    },
    {
      "heading": "3.4 Coreference",
      "text": [
        "We used two methods for entity coreference.",
        "Under the theory that name ambiguity may not be a huge problem, we adopted a baseline approach of merging entities across different documents if their canonical mentions were an exact string match after some basic normalizations, such as removing punctuation and conversion to lower-case characters.",
        "However we also used the JHU HLTCOE CALE system (Stoyanov et al., 2012), which maps named-entity mentions to the TAC-KBP reference KB, which was derived from a 2008 snapshot of English Wikipedia.",
        "For entities that are not found in the KB, we reverted to exact string match.",
        "CALE entity linking proved to be the more effective approach for the Cold Start task."
      ]
    },
    {
      "heading": "3.5 Timex2 Normalization",
      "text": [
        "SERIF recognizes, but does not normalize, temporal expressions, so we used the Stanford SUTime package, to normalize date values.",
        "following hyperlinks."
      ]
    },
    {
      "heading": "3.6 Lightweight Inference",
      "text": [
        "We performed a small amount of light inference to fill some slots.",
        "For example, if we identified that a person P worked for organization O, and we also extracted a job title T for P, and if T matched a set of titles such as president or minister we asserted that the tuple <O, org:top members employees, P> relation also held."
      ]
    },
    {
      "heading": "4 Ongoing Work",
      "text": [
        "There are a number of improvements that we are undertaking, including: scaling to much larger corpora, detecting contradictions, expanding the use of inference, exploiting the confidence of extracted information, and applying KELVIN to various genres of text."
      ]
    },
    {
      "heading": "5 Script Outline",
      "text": [
        "The KB generated by KELVIN is best explored using a Wikipedia metaphor.",
        "Thus our demonstration consists of a web browser that starts with a list of moderately prominent named-entities that the user can choose to examine (e.g., investor Warren Buffett, Supreme Court Justice Elena Kagan, Southwest Airlines Co., the state of Florida).",
        "Selecting any entity takes one to a page displaying its known attributes and relations, with links to documents that serve as provenance for each assertion.",
        "On every page, each entity is hyperlinked to its own canonical page; therefore the user is able to browse the KB much as one browses Wikipedia by simply following links.",
        "A sample generated page is shown in Figure 1 and text that supports some of the learned assertions in the figure is shown in Figure 2.",
        "We also provide a search interface to support jumping to a desired entity and can demonstrate accessing the data encoded in the semantic web language RDF (World Wide Web Consortium, 2013), which supports ontology browsing and executing complex SPARQL queries (Prud?Hommeaux and Seaborne, 2008) such as ?List the employers of people living in Nebraska or Kansas who are older than 40.?"
      ]
    }
  ]
}
