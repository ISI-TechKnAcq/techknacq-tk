{
  "info": {
    "authors": [
      "Tsutomu Hirao",
      "Jun Suzuki",
      "Hideki Isozaki",
      "Eisaku Maeda"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C04-1064",
    "title": "Dependency-Based Sentence Alignment for Multiple Document Summarization",
    "url": "https://aclweb.org/anthology/C04-1064",
    "year": 2004
  },
  "references": [
    "acl-J97-1003",
    "acl-P02-1040",
    "acl-P03-1005",
    "acl-W02-2016",
    "acl-W03-0507",
    "acl-W03-1004",
    "acl-W99-0625"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we describe a method of automatic sentence alignment for building extracts from abstracts in automatic summarization research.",
        "Our method is based on two steps.",
        "First, we introduce the “dependency tree path” (DTP).",
        "Next, we calculate the similarity between DTPs based on the ESK (Extended String Subsequence Kernel), which considers sequential patterns.",
        "By using these procedures, we can derive one-to-many or many-to-one correspondences among sentences.",
        "Experiments using different similarity measures show that DTP consistently improves the alignment accuracy and that ESK gives the best performance."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Many researchers who study automatic summarization want to create systems that generate abstracts of documents rather than extracts.",
        "We can generate an abstract by utilizing various methods, such as sentence compaction, sentence combination, and paraphrasing.",
        "In order to implement and evaluate these techniques, we need large-scale corpora in which the original sentences are aligned with summary sentences.",
        "These corpora are useful for training and evaluating sentence extraction systems.",
        "However, it is costly to create these corpora.",
        "Figure 1 shows an example of summary sentences and original sentences from TSC-2 (Text Summarization Challenge 2) multiple document summarization data (Okumura et al., 2003).",
        "From this example, we can see many-to-many correspondences.",
        "For instance, summary sentence (A) consists of a part of source sentence (A).",
        "Summary sentence (B) consists of parts of source sentences (A), (B), and (C).",
        "It is clear that the correspondence among the sentences is very complex.",
        "Therefore, robust and accurate alignment is essential.",
        "In order to achieve such alignment, we need not only syntactic information but also semantic information.",
        "Therefore, we combine two methods.",
        "First, we introduce the “dependency tree path” (DTP) for",
        "First, we stop the new investment of 64-Mega bit memory from competitive companies, such as in Korea or Taiwan, and we begin the investment for development of valuable system-on-chip or",
        "On a long-term target, we plan to reduce the rate of general-purpose semiconductor enterprises that produce DRAM for personal computers.",
        "From now on, we will be supplied with DRAM from Taiwan.",
        "We stopped the new investment of 64-Mega bit DRAM.",
        "We begin the investment for valuable development and will be supplied with general-purpose DRAMs for personal computers from Taiwan in the long run.",
        "syntactic information.",
        "Second, we introduce the “Extended String Subsequence Kernel” (ESK) for semantic information.",
        "Experimental results using different similarity measures show that DTP consistently improves alignment accuracy and ESK enhances the performance."
      ]
    },
    {
      "heading": "2 Related Work",
      "text": [
        "Several methods have been proposed to realize automatic alignment between abstracts and sentences in source documents.",
        "Banko et al.",
        "(1999) proposed a method based on sentence similarity using bag-of-words (BOW) representation.",
        "For each sentence in the given abstract, the corresponding source sentence is determined by combing the similarity score and heuristic rules.",
        "However, it is known that bag-of-words representation is not optimal for short texts like single sentences (Suzuki et al., 2003).",
        "Marcu (1999) regards a sentence as a set of “units” that correspond to clauses and defines similarity between units based on BOW representation.",
        "Next, the best source sentences are extracted in terms of “unit” similarity.",
        "Jing et al.",
        "(Jing and McKeown, 1999) proposed bigram-based similarity using the Hidden Markov Model.",
        "Barzilay (Barzilay and Elhadad, 2003) combines edit distance and context information around sentences.",
        "However, these three methods tend to be strongly influenced by word order.",
        "When the summary sentence and the source sentences disagree in terms of word order, the methods fail to work well.",
        "The supervised learning-based method called SimFinder was proposed by Hatzivassiloglou et al.",
        "(Hatzivassiloglou et al., 1999; Hatzivassiloglou et al., 2001).",
        "They translate a sentence into a feature vector based on word counts and proper nouns, and so on, and then sentence pairs are classified into “similar” or not.",
        "Their approach is effective when a lot of training data is available.",
        "However, the human cost of making this training data cannot be disregarded."
      ]
    },
    {
      "heading": "3 An Alignment Method based on Syntax",
      "text": []
    },
    {
      "heading": "and Semantics",
      "text": [
        "For example, Figure 2 shows two sentences that have different word order but the same meaning.",
        "The English translation is “I took the lost article to the neighborhood police.”",
        "Since conventional techniques other than BOW are strongly influenced by word order, they are fragile when word order is damaged."
      ]
    },
    {
      "heading": "3.1 Dependency Tree Path (DTP)",
      "text": [
        "When we unify two sentences, some elements become longer, and word order may be changed to improve readability.",
        "When we rephrase sentences, the dependency structure does not change in many cases, even if word order changes.",
        "For example, the two sentences in Figure 2 share the same dependence structure shown in Figure 3.",
        "Therefore, we transform a sentence into its dependency structure.",
        "This allows us to consider a sentence as a set of dependency tree paths from a leaf to the root node of the tree.",
        "For instance, the two sentences in Figure 2 can be transformed into the following DTPs.",
        "• fb'h' P (t L (I took) watashi ga todoke ta • 3R – Pfr 0) WN9, 4 P (t L (took to the neighborhood police) kinjo no keisatsu ni todoke ta • L 11 P (t L (took the lost article) otoshimono wo todoke ta."
      ]
    },
    {
      "heading": "3.2 An Alignment Algorithm using DTPs",
      "text": [
        "In this section, we describe a method that aligns source sentences with the summary sentences in an abstract.",
        "Our algorithm is very simple.",
        "We take the corresponding sentence to be the one whose DTP is most similar to that of the summary sentence.",
        "The algorithm consists of the following steps: Step 0 Transform all source sentences into DTPs.",
        "Step 1 For each sentence “a” in the abstract, apply Step 2 and Step 3.",
        "Step 2 Transform “a” into a DTP set.",
        "Here, F(a) denotes cg’s DTP set.",
        "F(s2) denotes the DTP set of the i-th source sentences.",
        "Step 3 For each P, (E F(a)), we align an optimal source sentence as follows: We define sim(P,, si) def max sim(P,, P).",
        "Here, P E F(si), where, for P,, we align a source sentence that satisfies argmax,iCSOUrcc Sim (P\" si).",
        "The above procedure allows us to derive many-to-many correspondences."
      ]
    },
    {
      "heading": "3.3 Similarity Metrics",
      "text": [
        "We need a similarity metric to rank DTP similarity.",
        "The following cosine measure (Hearst, 1997) is used in many NLP tasks.",
        "rIt wt,01wt,02 Et wt 01 Et wt 02 Here, wt,01I wt,021 denote the weight of term t in texts 01, 02, respectively.",
        "Note that syntactic and semantic information is lost in the BOW representation.",
        "In order to solve this problem, we use similarity measures based on word co-occurrences.",
        "As an example of it s application, N-gram co-occurrence is used for evaluating machine translations (Papineni et al., 2002).",
        "String Subsequence Kernel (SSK) (Lodhi et al., 2002) and Word Sequence Kernel (WSK) (Cancedda et al., 2003) are extensions of n-gram-based measures used for text categorization.",
        "In this paper, we compare WSK to its extension, the Extended String Subsequence Kernel (ESK).",
        "First, we describe WSK.",
        "WSK receives two sequences of words as input and maps each of them into a high-dimensional vector space.",
        "WSK’s value is just the inner product of the two vectors.",
        "subsequence abaca abbab abb 0 1+2A2",
        "For instance, the WSK value for ‘abaca’ and ‘abbab’ is determined as follows.",
        "A subsequence whose length is three or less is shown in Table 1.",
        "Here, A is a decay parameter for the number of skipped words.",
        "For example, subsequence ‘aba’ appears in ‘abaca’ once without skips.",
        "In addition, it appears again with two skips, i.e., ‘ab**a.’ Therefore, abaca’s vector has “1+A 2” in the component corresponding to ‘aba.’ From Table 1, we can calculate the WSK value as follows:",
        "+ 3x2+1x3.",
        "(2) In this way, we can measure the similarity between two texts.",
        "However, WSK disregards synonyms, hyponyms, and hypernyms.",
        "Therefore, we introduce ESK, an extension of WSK and a simplification of HDAG Kernel (Suzuki et al., 2003).",
        "ESK allows us to add word senses to each word.",
        "Here, we do not try to disambiguate word senses, but use all possible senses listed in a dictionary.",
        "Figure 4 shows an example of subsequences and their values.",
        "The use of word sense yields flexible matching even when paraphrasing is used for summary sentences.",
        "Formally, ESK is defined as follows.",
        "Here, K'„ , (ti , u j) is defined as follows.",
        "ti and u j are nodes of T and U, respectively.",
        "The function val(s, t) returns the number of attributes common to given nodes s and t."
      ]
    },
    {
      "heading": "4 Evaluation Settings",
      "text": []
    },
    {
      "heading": "4.1 Corpus",
      "text": [
        "We used the TSC2 corpus which includes both single and multiple document summarization data.",
        "Table 2 shows its statistics.",
        "For each data set, each of three experts made short abstracts and long abstracts.",
        "For each data, summary sentences were aligned with source sentences.",
        "Table 3 shows the distribution of the numbers of aligned original sentences for each summary sentence.",
        "The values in brackets are percentages.",
        "Table 4 shows the distribution of the number of aligned summary sentences for each original sentence.",
        "These tables show that sentences are often split and reconstructed.",
        "In particular, multiple document summarization data exhibit",
        "very complex correspondence because various summarization techniques such as sentence compaction, sentence combination, and sentence integration are used."
      ]
    },
    {
      "heading": "4.2 Comparison of Alignment Methods",
      "text": [
        "We compared the proposed methods with a baseline algorithm using various similarity measures."
      ]
    },
    {
      "heading": "Baseline",
      "text": [
        "This is a simple algorithm that compares sentences to sentences.",
        "Each summary sentence is compared with all source sentences and the top n sentences that have a similarity score over a certain threshold value T are aligned."
      ]
    },
    {
      "heading": "DTP-based Method",
      "text": [
        "This method was described in Section 3.2.",
        "In order to obtain DTPs, we used the Japanese morphological analyzer ChaSen and the dependency structure analyzer CaboCha (Kudo and Matsumoto, 2002).",
        "We utilized the following similarity measures.",
        "BOW BOW is defined by equation (1).",
        "Here, we use only nouns and verbs.",
        "N-gram This is a simple extension of BOW.",
        "We add n-gram sequences to BOW.",
        "We examined “2-gram” (unigram + bigram) and “3- gram ,”(unigram + bigram + trigram).",
        "TREE The Tree Kernel (Collins and Duffy, 2001) is a similarity measure based on the number of common subtrees.",
        "We regard a sentence as a dependency structure tree.",
        "WSK We examined d = 2, 3, and 4, and A 0.05, 0.1, 0.15 1.",
        "ESK We used the Japanese lexicon Goi-Taikei (Ikehara et al., 1997), to obtain word senses.",
        "The parameters, d and A, were changed on the same Conditions as above."
      ]
    },
    {
      "heading": "4.3 Evaluation Metric",
      "text": [
        "Each system’s alignment output was scored by the average F-measure.",
        "For each summary sentence, the following F-measure was calculated.",
        "Here, Precision = bl a and Recall = bl c, where a is the number of source sentences aligned by a system for the summary sentence.",
        "b is the number of correct source sentences in the output.",
        "c is the number of source sentences aligned by the human expert.",
        "We set y to 1, so this F-measure was averaged over all summary sentences."
      ]
    },
    {
      "heading": "5 Results and Discussion",
      "text": []
    },
    {
      "heading": "5.1 Single Document Summarization Data",
      "text": [
        "Table 5 shows the results of the baseline method (i.e., without DTPs) with the best T; Table 6 shows",
        "the results of using DTPs with the best d and A, which are shown in brackets.",
        "From the results, we can see the effectiveness of DTPs because Table 6 shows better performance than Table 5 in most cases.",
        "Table 7 shows the difference between Tables 5 and 6.",
        "DTPs improved the results of BOW by about five points.",
        "The best result is DTP with ESK.",
        "However, we have to admit that the improvements are relatively small for single document data.",
        "On the other hand Tree Kernel did not work well since it is too sensitive to slight differences.",
        "This is known as a weak point of Tree Kernel (Suzuki et al., 2003).",
        "According to the tables, BOW is outperformed by the other methods except Tree Kernel.",
        "These results show that word co-occurrence is important.",
        "Moreover, we see that sequential patterns are better than consequential patterns, such as the N-gram.",
        "Without DPTs, ESK is worse than WSK.",
        "However, ESK becomes better than WSK when we use DTPs.",
        "This result implies that word senses are disambiguated by syntactic information, but more examination is needed."
      ]
    },
    {
      "heading": "5.2 Multiple Document Summarization Data",
      "text": [
        "Table 8 shows the results of the baseline method with the best T for multiple document data while Table 9 shows the result of using DTPs with the best d and A, (in brackets).",
        "Compared with the single document summarization results, the F-measures are low.",
        "This means that the sentence alignment task is more difficult in multiple document summarization than in single document summarization.",
        "This is because sentence compaction, combination, and integration are common.",
        "Although the results show the same tendency as the single document summarization case, more improvements are noticed.",
        "Table 10 shows the difference between Tables 8 and 9.",
        "We see improvements in 10 points in ESK, WSK, and BOW.",
        "In multiple document summarization, sentences are often reorganized.",
        "Therefore, it is more effective to decompose a sentence into DTP sets and to compute similarity between the DTPs.",
        "Moreover, DTP(ESK) is once again superior to DTP(WSK)."
      ]
    },
    {
      "heading": "5.3 Parameter Tuning",
      "text": [
        "For ESK and WSK, we have to choose parameters, d and A.",
        "However, we do not know an easy way of finding the best combination of d and A.",
        "Therefore, we tuned these parameters for a development set.",
        "The experimental results show that the best d is 2 or 3.",
        "However, we could not find a consistently optimal value of A.",
        "Figure 5 shows the F-measure with various A for d = 2.",
        "The results shows that the F-measure does not change very much in the middle range A, [0.4,0.6] which suggests that good results are possible by using a middle range A."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "This paper introduced an automatic sentence alignment method that integrates syntax and semantic information.",
        "Our method transforms a sentence into a DTP set and calculates the similarity between the DTPs by using ESK.",
        "Experiments on the TSC (Text Summarization Challenge) corpus, which has complex correspondence, showed that the introduction of DTP consistently improves alignment accuracy and that ESK gave the best results."
      ]
    }
  ]
}
