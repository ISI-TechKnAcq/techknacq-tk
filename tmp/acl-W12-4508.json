{
  "info": {
    "authors": [
      "Xinxin Li",
      "Xuan Wang",
      "Xingwei Liao"
    ],
    "book": "Joint Conference on EMNLP and CoNLL â€“ Shared Task",
    "id": "acl-W12-4508",
    "title": "Simple Maximum Entropy Models for Multilingual Coreference Resolution",
    "url": "https://aclweb.org/anthology/W12-4508",
    "year": 2012
  },
  "references": [
    "acl-H05-1013",
    "acl-J01-4004",
    "acl-N06-1025",
    "acl-P02-1014",
    "acl-P05-1020",
    "acl-P06-1006",
    "acl-P07-1068",
    "acl-W02-1040",
    "acl-W11-1901",
    "acl-W11-1905",
    "acl-W12-4501"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper describes our system participating in the CoNLL-2012 shared task: Mod-eling Multilingual Unrestricted Coreference in Ontonotes.",
        "Maximum entropy models are used for our system as classifiers to determine the coreference relationship between every two mentions (usually noun phrases and pronouns) in each document.",
        "We exploit rich lexical, syntactic and semantic features for the system, and the final features are selected using a greedy forward and backward strategy from an initial feature set.",
        "Our system participated in the closed track for both English and Chinese languages."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In this paper, we present our system for the CoNLL2012 shared task which aims to model coreference resolution for multiple languages.",
        "The task of coref-erence resolution is to group different mentions in a document into coreference equivalent classes (Prad-han et al., 2012).",
        "Plenty of machine learning algorithms such as Decision tree (Ng and Cardie, 2002), maximum entropy model, logistic regression (Bjo?rkelund and Nugues, 2011), Support Vector Machines, have been used to solve this problem.",
        "Meanwhile, the CoNLL-2011 shared task on English language show that a well-designed rule-based approach can achieve a comparable performance as a statistical one (Pradhan et al., 2011).",
        "Our system treats coreference resolution problem as classification problem by determining whether every two mentions in a document has a coreference relationship or not.",
        "We use maximum entropy (ME) models to train the classifiers.",
        "Previous work reveal that features play an important role on coreference resolution problem, and many different kinds of features has been exploited.",
        "In this paper, we use many different lexical, syntactic and semantic features as candidate features, and use a greedy forward and backward approach for feature selection for ME models."
      ]
    },
    {
      "heading": "2 System Description",
      "text": [
        "The framework of our system is shown in figure 1.",
        "It includes four components: candidate mention selection, training example generation, model generation, and decoding algorithm for test data.",
        "The details of each component as described below."
      ]
    },
    {
      "heading": "2.1 Candidate Mention Selection",
      "text": [
        "In both training and test sets, our system only consider all noun phrases (NP) and pronouns (PRP, PRP$) as candidate mentions for both English and Chinese.",
        "The mentions in each sentence are obtained from given syntactic tree by their syntactic label.",
        "Other phrases in the syntactic tree are omitted due to their small proportion.",
        "For example, in the English training dataset, our candidate mentions includes about 91% of golden mentions."
      ]
    },
    {
      "heading": "2.2 Training Example Generation",
      "text": [
        "There are many different training example generation algorithms, e.g., McCarthy and Lehnert's method, Soon et als method, Ng and Cardies method (Ng, 2005).",
        "For our baseline system, we choose Soon et al's method because it is easily understandable, implemented and popularly used.",
        "It",
        "selects pairs of two coreferent mentions as positive examples, and pairs between mentions among the two mentions and the last mention as negative examples."
      ]
    },
    {
      "heading": "2.3 Feature Selection",
      "text": [
        "Rich and meaningful features are important for coreference resolution.",
        "Our system starts with Soon's 12 features as baseline features (Soon et al., 2001), and exploits many lexical, syntactic, and semantic features as candidate features.",
        "Totally 71 features are considered in our system, and summarized",
        "below: Distance features: sentence distance, distance in phrases, whether it's a first mention (Strube et al., 2002) Lexical features: string match, partial match, apposition, proper name match, head word match, partial head word match, minimum edit distance (Daume?",
        "III and Marcu, 2005) Grammatical features: pronoun, demonstrative noun phrase, embedded noun, gender agreement, number agreement (Soon et al., 2001) Syntactic features: same head, maximal NP, syntactic path (Yang et al., 2006) Semantic features: semantic class agreement, governing verb and its grammatical role, predicate (Ponzetto and Strube, 2006)",
        "For English, the number agreement and gender agreement features can be obtained through the gender corpus provided.",
        "However, there is no corpus for Chinese.",
        "Our system obtains this information by collecting dictionaries for number and gender information from training dataset.",
        "For example, the Algorithm 1 Greedy forward and backward feature selection Initialization: all candidate features in set C Choose initial feature set Compute F1 with features c while forward jj backward: while forward: for each feature f in Cc Compute F1 with features c+f if best(F1) increases: backward = true, c=c+f, continue forward else forward = false while backward: for each feature f in in c Compute F1 with features c-f if best(F1) increases: forward = true, c=c-f continue backward else backward = false pronoun ???",
        "(he) denotes a male mention, and the noun phrase ?s??",
        "(girlfriend) represents a female mention.",
        "Similarly for number information, e.g., the mentions containing ???",
        "(and), ???",
        "(group) are plural.",
        "We use these words to build number and gender dictionaries, and determine the number and gender information of a new mention by checking whether one of the words in the dictionaries is in the mention.",
        "For semantic class agreement feature in English, the relation between two mentions is extracted from WordNet 3.0 (Ng, 2007),(Miller, 1995).",
        "There is no corresponding dictionary for Chinese, so we keep it blank.",
        "The head word for each mention is selected by its dependency head, which can be extracted throught the conversion head rules ( English 1 and Chinese 2).",
        "Maximum Entropy modeling is used to train the classifier for our system 3.",
        "We employ a greedy forward and backward procedure for feature selection.",
        "The procedure is shown in Algorithm 1.",
        "The algorithm will iterate forward and backward procedures until the performance does not improve.",
        "We use two initial feature sets: a blank set and Soon's baseline feature set.",
        "Both feature sets start",
        "with a forward procedure."
      ]
    },
    {
      "heading": "2.4 Decoding",
      "text": [
        "For every candidate mention pair, to determine their coreference relationship is simple because the probability whether they are coreferent can be obtained by our maximum entropy model.",
        "We can just set a threshold = 0:5 and select the pairs with probability larger than .",
        "But usually it is hard for multiple mentions.",
        "Suppose there are three mentions A, B, C where the probability between A and B, A and C is larger than , but B and C is small.",
        "Thus choosing an appropriate decoding algorithm is necessary.",
        "We use best-first clustering method for our system which for each candidate mention in a document, chooses the mention before it with best probability larger than threshold .",
        "The difference between English and Chinese is that we consider the coreference relationship of two mentions nested in Chinese, but not in English."
      ]
    },
    {
      "heading": "3 Experiments",
      "text": []
    },
    {
      "heading": "3.1 Setting",
      "text": [
        "Our system participates in the English and Chinese closed tracks with auto mentions.",
        "For both the English and Chinese datasets, we use gold annotated training data for training, and a portion of auto annotated development data for feature selection.",
        "Only part of development data is chosen because the evaluation procedure takes lot of time.",
        "To simplify, We only select one or two file in each directory as our development data.",
        "The performance of the system is evaluated on MUC, B-CUBED, CEAF(M), CEAF(E), BLANC metrics.",
        "The official metric is calculated as"
      ]
    },
    {
      "heading": "3.2 Development set",
      "text": [
        "Figures 2 and 3 show the performance on the English and Chinese development datasets using feature selection starting from a empty feature set and Soon's baseline feature set.",
        "The x-axis means the number of iterations with either forward or backward selection.",
        "The performance on Soon's baseline feature set for both languages are shown on 1st iteration.",
        "The performance from empty feature set starts on 2nd iteration.",
        "From these figures, we can see that",
        "However the performance of our system is improved only on a few iteration.",
        "The best system for English stops at the 4th iteration with total 10 features left, which starts from Soon's baseline feature set.",
        "Similarly, the system for Chinese achieves its best performance at the 4th iteration with only 8 features.",
        "The phenomenon reveals that most of the features left for our system are still from Soon's baseline features, and our newly exploited lexical, syntactic, and semantic features are not well utilized.",
        "Then we evaluate our model on the entire development data.",
        "The results are shown on Table 1.",
        "Comparing Figures 2, 3 and Table 1, we can observe that the performance on entire development data is lower than part one, about 1% decrease."
      ]
    },
    {
      "heading": "3.3 Test",
      "text": [
        "For test data, we retrain our model on both gold training data and development data using the selected features.",
        "The final results for English and Chinese are shown in Table 2.",
        "Comparing tables 2 and 1, we can observe that the performance for the Chinese test data is similar as the development data.",
        "The result seems reasonable because the model for testing use additional development data which is much smaller than training data.",
        "However, the result on English test data seem a little odd.",
        "The performance is about 1.4% less than that on the development data.",
        "The result needs further analysis."
      ]
    },
    {
      "heading": "4 Conclusion",
      "text": [
        "In this paper, we presented our coreference resolution system which uses maximum entropy model to determine the coreference relationship between two mentions.",
        "Our system exploits many lexical, syntactic and semantic features.",
        "However, using greedy forward and backward feature selection strategy for ME model, these rich features are not well utilized.",
        "In future work we will analyze the reason for this phenomenon and extend these features to other machine learning algorithms."
      ]
    }
  ]
}
