{
  "info": {
    "authors": [
      "Stephan Gouws"
    ],
    "book": "NAACL",
    "id": "acl-N12-2009",
    "title": "Deep Unsupervised Feature Learning for Natural Language Processing",
    "url": "https://aclweb.org/anthology/N12-2009",
    "year": 2012
  },
  "references": [
    "acl-J02-3001",
    "acl-J92-4003",
    "acl-P10-1040"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Statistical natural language processing (NLP) builds models of language based on statistical features extracted from the input text.",
        "We investigate deep learning methods for unsupervised feature learning for NLP tasks.",
        "Recent results indicate that features learned using deep learning methods are not a silver bullet and do not always lead to improved results.",
        "In this work we hypothesise that this is the result of a disjoint training protocol which results in mismatched word representations and classifiers.",
        "We also hypothesise that modelling long-range dependencies in the input and (separately) in the output layers would further improve performance.",
        "We suggest methods for overcoming these limitations, which will form part of our final thesis work."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Natural language processing (NLP) can be seen as building models h : X ?",
        "Y for mapping an input encoding x ?",
        "X representing a natural language (NL) fragment, to an output encoding y ?",
        "Y representing some construct or formalism used in the particular NLP task of interest, e.g. part-of-speech (POS) tags, begin-, inside-, outside (BIO) tags for information extraction, semantic role labels, etc.",
        "Since the 90s, the predominant approach has been statistical NLP, where one models the problem as learning a predictive function h for mapping from h : X ?",
        "Y using machine learning techniques.",
        "Machine learning consists of a hypothesis function which learns this mapping based on latent or explicit features extracted from the input data.",
        "In this framework, h is usually trained in a supervised setting from labelled training pairs (xi, yi) ?",
        "X ?",
        "Y .",
        "Additionally, the discriminant function h typically operates on a transformed representation of the data to a common feature space encoded as a feature vector ?",
        "(x), and then learns a mapping from feature space to the output space, h : ?",
        "(x) ?",
        "y.",
        "In supervised learning, the idea",
        "?the cat sits on the mat?.",
        "X represents the words in the input space, Y represents labels in the output space.",
        "?",
        "(~x) is a feature representation for the input text ~x and the bottom row represents the output named entity tags in a more standard form.",
        "is generally that features represent strong discriminating characteristics of the problem gained through manual engineering and domain-specific insight.",
        "As a concrete example, consider the task of syntactic chunking, also called ?shallow parsing?, (Gildea and Ju-rafsky, 2002): Given an input string, e.g. ?the cat sits on the mat?, the chunking problem consists of labelling segments of a sentence with syntactic constituents such as noun or verb phrases (NPs or VPs).",
        "Each word is assigned one unique tag often encoded using the BIO encoding1.",
        "We represent the input text as a vector of words xi ?",
        "~x, and each word's corresponding label is represented by yi ?",
        "~y (see Table 1).",
        "Given a feature generating function ?",
        "(xi) and a set of labelled training pairs (xi, yi) ?",
        "X ?",
        "Y , the task then reduces to learning a suitable mapping h : ?",
        "(X ) ?",
        "Y .",
        "Most previous works have focused on manually engineered features and simpler, linear models, including ?shallow?",
        "model architectures, like the perceptron (Rosenblatt, 1957), linear SVM (Cortes and Vapnik, 1995) and linear-chain conditional random fields (CRFs) (Lafferty, 2001).",
        "However, a shallow learning architecture is only as good as its input features.",
        "Due to the complex nature of NL, deeper architectures may be re-1E.g.",
        "B-NP means ?begin NP?, I-NP means ?inside NP?, and O means other/outside.",
        "quired to learn data representations which contain the appropriate level of information for the task at hand.",
        "Prior to 2006, it was computationally infeasible to perform inference in hierarchical (?deep?",
        "), non-linear models such as multilayer perceptrons with more than one hidden layer.",
        "However, Hinton (2006) proposed an efficient, layer-wise greedy method for learning the model parameters in these architectures, which spurred a renewed interest in deep learning research.",
        "Still, creating annotated training data is labour-intensive and costly, and manually designing and extracting discriminating features from the data to be used in the learning process is a costly procedure requiring significant levels of domain expertise.",
        "Over the last two decades, the growth of available unlabeled data x ?",
        "X and the ubiquity of scalable computing power has shifted research focus to unsupervised approaches for automatically learning appropriate feature representations ?",
        "(x) from large collections of unlabeled text.",
        "Several methods have been proposed for unsupervised feature learning, including simple k-means clustering (Lloyd, 1982), Brown clustering (Brown et al., 1992), mutual information (Shannon and Weaver, 1962), principal components analysis (PCA) (Jolliffe, 2002), and independent component analysis (ICA) (Hyva?rinen et al., 2001).",
        "However, natural language has complex mappings from text to meaning, arguably involving higher-order correlations between words which these simpler methods struggle to model adequately.",
        "Advances in the ?deep learning?",
        "community allow us to perform efficient unsupervised feature learning in highly complex and high-dimensional input feature spaces, making it an attractive method for learning features in e.g. vision or language (Bengio, 2009).",
        "The standard deep learning approach is to learn lower-dimensional embeddings from the raw high-dimensional2 input space X to lower dimensional (e.g. 50-dimensional) feature spaces in an unsupervised manner, via repeated, layer-wise, non-linear transformation of the input features, e.g. y?",
        "= f (k)(?",
        "?",
        "?",
        "f (2)(f (1)(~x)) ?",
        "?",
        "?",
        "), where f (i)(x) is some non-linear function (typically tanh) for which the parameters are learned by back propagating error gradients.",
        "This configuration is referred to as a ?deep?",
        "architecture with k layers (see Figure 1 for an example).",
        "For feature generation, we present a trained network with a new vector ~x representing the input data on its 2E.g.",
        "a ?one-hot?",
        "50,000-dimensional vector of input words, with a ?1?",
        "indicating the presence of the word at that index, and a ?0?",
        "everywhere else.",
        "formed into the hidden representation, here denoted as h1, using an affine transformation W and a non-linearity.",
        "Each subsequent hidden layer hk takes as input the output of its preceding layer h(k?1) (Bengio, 2009).",
        "input layer.",
        "After performing one iteration of forward-propagation through the network, we can then view the activation values in the hidden layers as dense, so-called ?distributed representations?",
        "(features) of the input data.",
        "These features can in turn be passed to an output classifier layer to produce some tagging task of interest.",
        "Recent work in deep learning show state-of-the-art results in part-of-speech parsing, chunking and named-entity tagging (Collobert, 2011), however performance in more complex NLP tasks like entity and event disambiguation and semantic role labelling are still trailing behind.",
        "In this work we focus specifically on extending current state of the art deep neural models to improve their performance on these more difficult tasks.",
        "In the following section we briefly review and discuss the merits and limitations of three of the current state of the art deep learning models for NLP.",
        "We then identify our primary research questions and introduce our proposed future work roadmap."
      ]
    },
    {
      "heading": "2 Current State-of-the-Art and",
      "text": []
    },
    {
      "heading": "Limitations",
      "text": [
        "Most work builds on the idea of a neural probabilistic language model (NPLM) where words are represented by learned real-valued embeddings, and a neural network combines word embeddings to predict the most likely next word.",
        "The first successful NPLM was introduced by Bengio et al. in 2003 (Bengio et al., 2003).",
        "Historically, training and testing these models were slow, scaling linearly in the vocabulary size.",
        "However, several recent approaches have been proposed which overcome these limitations (Morin and Bengio, 2005), including the work by Collobert and Weston (2008) and Mnih and Hinton (2009) discussed next."
      ]
    },
    {
      "heading": "2.1 Collobert & Weston (2008)",
      "text": [
        "Collobert and Weston (2008) present a discriminative, non-probabilistic, non-linear neural language model that can be scaled to train over billions of words since each training iteration only computes a loss gradient over a small stochastic sample of the training data.",
        "All K-dimensional word embeddings are initially set to a random state.",
        "During each training iteration, an n-gram is read from the training data and each word is mapped to its respective embedding.",
        "All embeddings are then concatenated to form a nK-length positive training vector.",
        "A corrupted n-gram is also created by replacing the n'th (last) word by some word uniformly chosen from the vocabulary.",
        "The training criterion is that the network must predict positive training vectors with a score at least some margin higher than the score predicted for corrupted n-grams.",
        "Model parameters are trained simultaneously with word embeddings via gradient descent."
      ]
    },
    {
      "heading": "2.2 The Hierarchical Log Bilinear (HLBL) Model",
      "text": [
        "Mnih and Hinton (2007) proposed a simple probabilistic linear neural language model called the log bilinear (LBL) model.",
        "For an n-gram context window, the LBL model concatenates the first (n?",
        "1)K-dimensional word embeddings and then learns a linear mapping from R(n?1)K to RK for predicting the embedding of the nth word.",
        "For predicting the next word, the model outputs a probability distribution over the entire vocabulary by computing the dot product between the predicted embedding and the embedding for each word in the vocabulary in an output softmax layer.",
        "This softmax computation is linear in the length of the vocabulary for each prediction, and is therefore the performance bottleneck.",
        "In follow-up work, Mnih and Hinton (2009) speed up training and testing time by extending the LBL model to predict the next word by hierarchically decomposing the search through the vocabulary by traversing a binary tree constructed over the vocabulary.",
        "This speeds up training and testing exponentially, but initially reduces model performance as the construction of the binary partitioning has a strong effect on the model's performance.",
        "They introduce a method for bootstrapping the construction of the tree by initially using a random binary tree to learn word embeddings, and then rebuilding the tree based on a clustering of the learned embeddings.",
        "Their final results are superior to the standard LBL in both model perplexity and model testing time."
      ]
    },
    {
      "heading": "2.3 Recursive Neural Networks (RNNs)",
      "text": [
        "Socher (2010; 2011) introduces a recursive neural network (RNN) framework for parsing natural language.",
        "Previous approaches dealt with variable-length sentences by either i using a window approach (shifting a window of n words over the input, processing each fixed-size window at a time), or ii by using a convolutional layer where each word is convolved with its neighbours within some sentence-or window-boundary.",
        "RNNs operate by recursively applying the same neural network to segments of its input, thereby allowing RNNs to naturally operate on variable-length inputs.",
        "Each pair of neighbouring words is scored by the network to reflect how likely these two words are considered to form part of a phrase.",
        "Each such operation takes two K-dimensional word vectors and outputs another K-dimensional vector and a score.",
        "Socher (2010) proposes several strategies (ranging from local and greedy to global and optimal) for choosing which pairs of words to collapse into a new K-dimensional vector representing the phrase comprised of the two words.",
        "By viewing these collapsing operations as branch merging decisions, one can construct a binary parse tree over the words in a bottom-up fashion."
      ]
    },
    {
      "heading": "2.4 Discussion of Limitations",
      "text": [
        "Neural language models are appealing since they can more easily deal with missing data (unknown word combinations) due to their inherent continuous-space representation, whereas n-gram language models (Manning et al., 1999) need to employ (sometimes ad hoc) methods for smoothing unseen and hence zero probability word combinations.",
        "The original NPLM performs well in terms of model perplexity on held-out data; however, its training and testing time is very slow.",
        "Furthermore, it provides no support for handling multiple word senses, the property that any word can have more than one meaning, since each word is assigned an embedding based on its literal string representation (i.e. from a lookup table).",
        "The Collobert & Weston model still provides no mechanism for handling word senses, but improves on the NPLM by adding several non-linear layers which increase its modelling capacity, and a convolutional layer for modelling longer range dependencies between words.",
        "Recursive neural nets (RNNs) directly address the problem of longer-range dependencies by allowing neighbour words to be combined into their phrasal equivalents in a bottom-up process.",
        "The LBL model, despite its very simple linear structure, provides very good performance in terms of model",
        "perplexity, but shares the problem of slow training and testing times and the inability to handle word senses or dependencies between words (outside its n-gram context).",
        "In the HLBL model, Mnih and Hinton address the slow testing performance of the LBL model by using a hierarchical search tree over the vocabulary to exponentially speed up testing time, analogous to the concept of class-based language models (Brown et al., 1992).",
        "The HLBL model can also handle multiple word senses, but in their evaluation they show that in practice the model learns multiple senses (codes) for infrequently observed words instead of words with more than one meaning (Mnih and Hinton, 2009).",
        "The performance is strongly dependent on the initialisation of the tree, for which they present an iterative but non-optimal bootstrap-and-train procedure.",
        "Despite being non-optimal, it is shown to outperform the standard LBL model in terms of perplexity."
      ]
    },
    {
      "heading": "3 Mismatched Word Representations and",
      "text": []
    },
    {
      "heading": "Classifiers",
      "text": [
        "The deep learning ideal is to train deep, non-linear models over large collections of unlabeled data, and then use these models to automatically extract information-rich, higher-level features3 to integrate into standard NLP or image processing systems as added features to improve performance.",
        "However, several recent papers report surprising and seemingly contradicting results for this ideal.",
        "In the most direct comparison for NLP, Turian (2010) compares features extracted using Brown clustering (a hierarchical clustering technique for clustering words based on their observed co-occurrence patterns), the hierarchical log-bilinear (HLBL) embeddings (Mnih and Hinton, 2007) and Collobert and Weston (C+W) embeddings (Collobert and Weston, 2008), by integrating these as additional features in standard supervised conditional random field (CRF) classification systems for NLP.",
        "Somewhat surprisingly, they find that using the more complex C+W and HLBL features do not improve significantly over Brown features.",
        "Indeed, under several conditions the Brown features give the best results.",
        "These results are important for several reasons (we highlight these results in Table 2).",
        "The goal was to improve classification performance in structured prediction tasks in natural language by integrating features learned in a deep, unsupervised approach within a standard linear classification framework.",
        "Yet these complex, deep methods are outperformed by simpler unsupervised feature extraction methods.",
        "3?Higher-level?",
        "features simply mean combining simpler features extracted from a text to produce conceptually more abstract indicators, e.g. combining word-indicators for ?attack?, ?soldier?, etc.",
        "to form an indicator for WAR, even though ?war?",
        "is not mentioned anywhere in the text.",
        "In a sense, these seem to be negative results for the utility of deep learning in NLP.",
        "However, in this work we argue that these seemingly anomalous results stem from a mismatch between the feature learning function and the classifier that was used in the classification (and hence evaluation) process.",
        "We consider the learning problem h : X ?",
        "Y to decompose into h = h?(?",
        "(X )), where ?",
        "is the feature learning function and h?",
        "is a standard supervised classifier.",
        "?",
        "reads input from X and outputs encodings in feature space ?(x).",
        "h reads input in feature space ?",
        "(x) and outputs encodings in the output label space Y .",
        "Note that this easily extends to deep feature learning models by simply replacing ?",
        "(X ) with ?(k)(?",
        "?",
        "??(2)(?",
        "(1)(X )) ?",
        "?",
        "?",
        "), for a k-layer architecture, where the first layer reads input inX and each subsequent layer reads the output of the previous layer.",
        "Within this view of the deep learning process, we can see that unsupervised feature learning does not happen in isolation.",
        "Instead, the learned features only make sense within some learning framework, since the output of the feature learning function ?",
        "(and each deep layer ?",
        "(k?1)) maps to a region in feature code space which becomes in turn the input to the output classifier h?",
        "(or subsequent layer ?",
        "(k)) .",
        "We therefore argue that in a semi-supervised or unsupervised classification problem, the feature learning function ?",
        "should be strongly dependent on the classifier h?",
        "that interprets those features, and vice versa.",
        "This notion ties in with the standard deep-learning training protocol of unsupervised pre-training followed by joint supervised fine-tuning (Hinton et al., 2006) of the top classification layer and the deeper feature extraction layers.",
        "We conjecture that jointly training a deep feature extraction model with a linear output classifier leads to better linearly separable feature vectors ?",
        "(x) than training both independently.",
        "Note that this is in contrast to how Turian (2010) integrated the unsupervised features into existing NLP systems via disjoint training."
      ]
    },
    {
      "heading": "4 Proposed Work and Research Questions",
      "text": [
        "For simpler sequence tagging tasks such as part-of-speech tagging and noun phrase chunking, the state-of-the-art models introduced in Section 2 perform adequately.",
        "However, in order to make use of the increased",
        "modelling capacity of deep neural models, and to successfully model more complex semantic tasks such as anaphora resolution and semantic role labelling, we hypothesise that the model needs to avoid modelling purely local lexical semantics and needs to efficiently handle multiple word senses and long-range dependencies between input words (or phrases) and output labels.",
        "We propose to overcome the limitations of previous models with regard to these design goals, by focusing on the following key areas: Input language representation: Neural models rely on vector representations of their input (as opposed to discrete representations as in, for instance, HMMs).",
        "In NLP, sentences are therefore encoded as real-valued embedding vectors.",
        "These vectors are learned in either a task-specific setting (as in the C+W model) or as part of a language model (as in the LBL model), where the goal is to predict the next word given the learned representations of the previous words.",
        "In order to maximise the information available to the model, we need to provide information-rich representations to the model.",
        "Current approaches represent each word in a sentence using a distinct word vector based on its literal string representation.",
        "However, as noted earlier, in NL the same words can have different senses based on the context in which it appears (polysemy).",
        "We propose to extend the hierarchical log-bilinear (HLBL) language model (see Section 2.2) in two important ways.",
        "We choose the HLBL model for its simplicity and good performance compared to more complex models.",
        "Firstly, we propose to replace the iterative bootstrap-and-train process for learning the hierarchical tree structure over the vocabulary with a modified self-balancing binary tree.",
        "The tree rebalances itself from an initial random tree to leave most frequently accessed words near the root (for shorter codes and faster access times), while moving words between clusters to maximise overall model perplexity.",
        "Secondly, we propose to add a word sense disambiguation layer capable of modelling long-range dependencies between input words.",
        "For this layer we will compare a modified RNN layer to a convolutional layer.",
        "The modified RNN will embed each focus word with its nmost discriminative neighbour words (in a sentence context window) into a new K-dimensional, sense-disambiguated embedding vector for the focus word.",
        "We will evaluate and optimise the final model's learned representations by evaluating language model perplexity on held out data.",
        "Model architecture and internal representation: Deep models derive their modelling power from their hierarchical structure.",
        "Each layer transforms the output representation of its previous layer, allowing the model to learn more general and abstract feature combinations in the higher layers which are relevant for the current task.",
        "The representations on the hidden layers serve as transformed feature representations of the input data for the output classifier.",
        "Enforcing sparsity on the hidden layers has been shown to produce stronger features for certain tasks in vision (Coates et al., 2010).",
        "Additionally, individual nodes might be highly correlated, which can also reduce the performance of certain classifiers which make strong independence assumptions (for instance naive Bayes).",
        "We propose to study the effect that enforcing sparsity in the learned feature representations has on task performance in NLP.",
        "Additionally, we propose to evaluate the effect that an even stronger training objective ?",
        "one that encourages statistical independence between hidden nodes by learning factorial code representations (Hochreiter and Schmidhuber, 1999) ?",
        "has on model performance.",
        "Modelling structure in the output space: Tasks in NLP mostly involve predicting labels which exhibit highly regular structure.",
        "For instance, in part-of-speech tagging, two determiners have a very low likelihood of following directly on one another, e.g. ?the the?.",
        "In order to successfully model this phenomenon, a model must take into account previous (and potentially future) predictions when making the current prediction, e.g. as in hidden Markov models and conditional random fields.",
        "We propose to include sequential dependencies in the output labels and to compare this with including a convolutional layer below the output layer, for predicting output labels in complex NLP tasks such as coreference resolution and event structure detection."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "Deep learning methods offer an attractive unsupervised approach for extracting higher-level features from large quantities of text data to be used for NLP tasks.",
        "However current attempts at integrating these features into existing NLP systems do not produce the desired performance improvements.",
        "We conjecture that this is due to a mismatch between the learned word representations and the classifiers used as a result of disjoint training schemes, and our thesis roadmap suggests three key areas for overcoming these limitations."
      ]
    }
  ]
}
