{
  "info": {
    "authors": [
      "Andras Kornai",
      "Gerald Penn",
      "James Rogers",
      "Anssi Yli-Jyr√§"
    ],
    "book": "ACL",
    "id": "acl-P13-5007",
    "title": "The mathematics of language learning",
    "url": "https://aclweb.org/anthology/P13-5007",
    "year": 2013
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 11?13, Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational Linguistics The mathematics of language learning"
      ]
    },
    {
      "heading": "Andra's Kornai",
      "text": [
        "Over the past decade, attention has gradually shifted from the estimation of parameters to the learning of linguistic structure (for a survey see Smith 2011).",
        "The Mathematics of Language (MOL) SIG put together this tutorial, composed of three lectures, to highlight some alternative learning paradigms in speech, syntax, and semantics in the hopes of accelerating this trend.",
        "Compounding the enormous variety of formal models one may consider is the bewildering range of ML techniques one may bring to bear.",
        "In addition to the surprisingly useful classical techniques inherited from multivariate statistics such as Principal Component Analysis (PCA, Pearson 1901) and Linear Discriminant Analysis (LDA, Fisher 1936), computational linguists have experimented with a broad range of neural net, nearest neighbor, maxent, genetic/evolutionary, decision tree, max margin, boost, simulated annealing, and graphical model learners.",
        "While many of these learners became standard in various domains of ML, within CL the basic HMM approach proved surprisingly resilient, and it is only very recently that deep learning techniques from neural computing are becoming competitive not just in speech, but also in OCR, paraphrase, sentiment analysis, parsing and vector-based semantic representations.",
        "The first lecture will provide a mathematical introduction to some of the fundamental techniques that lie beneath these linguistic applications of neural networks, such as: BFGS optimization, finite difference approximations of Hessians and Hessian-free optimization, contrastive divergence and variational inference.",
        "Lecture 1: The mathematics of neural computing ?",
        "Penn Recent results in acoustic modeling, OCR, paraphrase, sentiment analysis, parsing and vector-based semantic representations have shown that natural language processing, like so many other corners of artificial intelligence, needs to pay more attention to neural computing."
      ]
    },
    {
      "heading": "I Gaussian Mixture Models",
      "text": [
        "?",
        "Lagrange's theorem ?",
        "Stochastic gradient descent ?",
        "typical acoustic models using GMMs and HMMs II Optimization theory ?",
        "Hessian matrices ?",
        "Broyden-Fletcher-Goldfarb-Shanno theory ?",
        "finite difference approximations of Hessians ?",
        "Hessian-free optimization ?",
        "Krylov methods III Application: Product models ?",
        "products of Gaussians vs. GMMs ?",
        "products of ?experts?",
        "?",
        "Gibbs sampling and Markov-chain Monte Carlo ?",
        "contrastive divergence IV Experimentation: Deep NNs for acoustic modeling ?",
        "intersecting product models with Boltzmann machines ?",
        "?generative pre-training?",
        "?",
        "acoustic modeling with Deep Belief Networks ?",
        "why DBNs work well",
        "V Variational inference ?",
        "variational Bayes for HMMs In spite of the enormous progress brought by ML techniques, there remains a rather significant range of tasks where automated learners cannot yet get near human performance.",
        "One such is the unsupervised learning of word structure addressed by MorphoChallenge, another is the textual entailment task addressed by RTE.",
        "The second lecture recasts these and similar",
        "problems in terms of learning weighted edges in a sparse graph, and presents learning techniques that seem to have some potential to better find spare finite state and near-FS models than EM.",
        "We will provide a mathematical introduction to the Minimum Description Length (MDL) paradigm and",
        "spectral learning, and relate these to the better-known techniques based on (convex) optimization and (data-oriented) memorization.",
        "Lecture 2: Lexical structure detection ?",
        "Kornai While modern syntactic theory focuses almost entirely on productive, rule-like regularities with compositional semantics, the vast bulk of the information conveyed by natural language, over 85%, is encoded by improductive, irregular, and non-compositional means, primarily by lexical meaning.",
        "Morphology and the lexicon provide a rich testing ground for comparing structure learning techniques, especially as inferences need to be based on very few examples, often just one.",
        "?",
        "Case frames and valency ?",
        "Spectral learning as data cleaning (Ng 2001) ?",
        "Brew and Schulte im Walde 2002 (German), Nemeskey et al(Hungarian) ?",
        "Optionality in case frames V Models with zeros ?",
        "Relating ungrammaticality and low probability (Pereira 2000, Stefanowitsch 2006) ?",
        "Estimation errors, language distances (Kornai 1998, 2011) ?",
        "Quantization error VI Minimum description length ?",
        "Kolmogorov complexity and universal grammar (Clark 1994) ?",
        "MDL in morphology (Goldsmith 2000, Creutz and Lagus 2002, 2005,...) ?",
        "MDL for weighted languages ?",
        "Ambiguity ?",
        "Discarding data ?",
        "yes, you can!",
        "?",
        "Collapsing categories VII New directions ?",
        "Spectral learning of HMMs (Hsu et al2009, 2012) ?",
        "of weighted automata (Balle and Mohri 2012) ?",
        "Feature selection, LASSO (Pajkossy 2013) ?",
        "Long Short-Term Memory (Monner and Reg-gia 2012) ?",
        "Representation learning (Bengio et al2013)",
        "Given the broad range of competing formal models such as templates in speech, PCFGs and various MCS models in syntax, logic-based and association-based models in semantics, it is somewhat surprising that the bulk of the applied work is still performed by HMMs.",
        "A particularly significant case in point is provided by PCFGs, which have not proved competitive with straight tri-gram models.",
        "Undergirding the practical failure of PCFGs is a more subtle theoretical problem, that the nonterminals in better PCFGs cannot be identified with the kind of nonterminal labels that grammarians assume, and conversely, PCFGs embodying some form of grammatical knowledge tend not to outperform flatly initialized models that make no use of such knowledge.",
        "A natural response to this outcome is to retrench and use less powerful formal models, and the last lecture will be spent in the subregular space of formal models even less powerful than finite state automata."
      ]
    },
    {
      "heading": "Lecture 3: Subregular Languages and Their Linguistic Relevance ? Rogers and Yli-Jyra?",
      "text": [
        "The difficulty of learning a regular or context-free language in the limit from positive data gives a motivation for studying non-Chomskyan language classes.",
        "The lecture gives an overview of the taxonomy of the most important subregular classes of languages and motivate their linguistic relevance in phonology and syntax.",
        "?",
        "Notions of (parameterized) locality in syntax.",
        "The relevance of some parameterized subregular language classes is shown through machine learning and typological arguments.",
        "Typological results on a large set of languages (Heinz 2007, Heinz et al. 2011) relate language types to the theory of subregular language classes.",
        "There are finite-state approaches to syntax showing subregular properties.",
        "Although structure-assigning syntax differs from phonotac-tical constraints, the inadequacy of right-linear grammars does not generalize to all finite-state representations of syntax.",
        "The linguistic relevance and descriptive adequacy are discussed, in particular, in the context of intersection parsing and conjunctive representations of syntax."
      ]
    },
    {
      "heading": "Instructors",
      "text": []
    }
  ]
}
