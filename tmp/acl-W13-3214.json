{
  "info": {
    "authors": [
      "Nal Kalchbrenner",
      "Phil Blunsom"
    ],
    "book": "CVSC",
    "id": "acl-W13-3214",
    "title": "Recurrent Convolutional Neural Networks for Discourse Compositionality",
    "url": "https://aclweb.org/anthology/W13-3214",
    "year": 2013
  },
  "references": [
    "acl-D10-1115",
    "acl-D12-1050",
    "acl-D12-1110",
    "acl-J00-3003",
    "acl-P12-2018",
    "acl-P13-1088",
    "acl-W11-0114",
    "acl-W12-1812"
  ],
  "sections": [
    {
      "text": [
        "Manuscripts must be in two-column format.",
        "Exceptions to the two-column format include the title, authors?",
        "names and complete addresses, which must be centered at the top of the first page, and any full-widt figures or tables (see the guidelines in Subsectio 6.5).",
        "Type single-spaced.",
        "Start all pag s directly under the top margin.",
        "S e the guidelines later regarding formatting the first pag .",
        "The manuscript should be printed si gle-sided and its len th should not xceed the maximum pag limit described in Section 8.",
        "Do not number the pages."
      ]
    },
    {
      "heading": "6.1 Electronically-availabl resources",
      "text": [
        "ACL 2013 provides this description in LATEX2e(acl2013.tex) and PDF format (acl2013.pdf), along with the LATEX2e style file used to format",
        "work f r sentential compositionality.",
        "The bottom layer represents a single feature across all the word vectors in the sentence.",
        "The top layer is the value for that feature in the r ulting sent nc vector.",
        "Lines represent single weights and color coded lines indicate sharing of weights.",
        "The parameter k indicates the size of the convolution kernel at the corresponding layer.",
        "We experiment with the discourse model coupled to the sentence model on the task of recognizing dialogue acts of utterances within a conversation.",
        "The dataset is given by 1134 transcribed and annotated telephone conversations amounting to about 200K utterances from the Switchboard Dialogue Act Corpus (Calhoun et al., 2010).1 The model is trained in a supervised setting without previous pretraining; word vectors are also randomly initialised.",
        "The model learns a probability distribution over the dialogue acts at step i given the sequence of utterances up to step i, the sequence of acts up to the previous step i?1 and the binary sequence of agents up to the current step i.",
        "Predicting the sequence of dialogue acts is performed in a greedy fashion.2 We proceed as follows.",
        "In Sect.",
        "2 we give the motivation and the definition for the HCNN sentence model.",
        "In Sect.",
        "3 we do the same for the RCNN discourse model.",
        "In Sect.",
        "4 we describe the dialogue act classification experiment and the training procedure.",
        "We also inspect the discourse vector representations produced by the model.",
        "We conclude in Sect.",
        "5.",
        "(RCNN) discourse model based on a RNN architecture.",
        "At each step the RCNN takes as input the current sentence vector si generated through the HCNN sentence model and the previous label xi?1 to predict a probability distribution over the current label P (xi).",
        "The recurrent weights Hi?1 are conditioned on the previous agent ai?1 and the output weights are conditioned on the current agent ai.",
        "Note also the sentence matrix Ms of the sentence model and the hierarchy of convolutions applied to each feature that is a row in Ms to produce the corresponding feature in si."
      ]
    },
    {
      "heading": "2 Sentence Model",
      "text": [
        "The general aim of the sentence model is to compute a vector for a sentence s given the sequence of words in s and a vector for each of the words.",
        "The computation captures certain general considerations regarding sentential compositionality.",
        "We first relate such considerations and we then proceed to give a definition of the model."
      ]
    },
    {
      "heading": "2.1 Sentential compositionality",
      "text": [
        "There are three main aspects of sentential compositionality that the model aims at capturing.",
        "To relate these, it is useful to note the following basic property of the model: a sentence s is paired to the matrix Ms whose columns are given sequentially by the vectors of the words in s. A row in Ms corresponds to the values of the corresponding feature across all the word vectors.",
        "The first layer of the network in Fig. 1 represents one such row of Ms, whereas the whole matrix Ms is depicted in Fig. 2.",
        "The three considerations are as follows.",
        "First, at the initial stage of the composition, the value of a feature in the sentence vector is a function of the values of the same feature in the word vectors.",
        "That is, the m-th value in the sentence vector of s is a function of the m-throw of Ms.",
        "This aspect is preserved in the additive and multiplicative models where the composition operations are, respectively, addition + and component-wise multiplication .",
        "The current model preserves the aspect up to the computation of the sentence vector s by adopting one-dimensional, feature-wise convolution operations.",
        "Subsequently, the discourse model that uses the sentence vector s includes transformations across the features of s (the transformation S in Fig. 2).",
        "The second consideration concerns the hierarchical aspect of the composition operation.",
        "We take the compositionality of meaning to initially yield local effects across neighbouring words and then yield increasingly more global effects across all the words in the sentence.",
        "Composition operations like those in the structured models that are guided by the syntactic parse tree of the sentence capture this trait.",
        "The sentence model preserves this aspect not by way of syntactic structure, but by adopting convolution kernels of gradually increasing sizes that span an increasing number of words and ultimately the entire sentence.",
        "The third aspect concerns the dependence of the composition operation.",
        "The operation is taken to depend on the different features, but not on the different words.",
        "Word specific parameters are introduced only by way of the learnt word vectors, but no word specific operations are learnt.",
        "We achieve this by using a single convolution kernel across a feature, and by utilizing different convolution kernels for different features.",
        "Given these three aspects of sentential compositionality, we now proceed to describe the sentence model in detail."
      ]
    },
    {
      "heading": "2.2 Hierarchical Convolutional Neural",
      "text": [
        "Network The sentence model is taken to be a CNN where the convolution operation is applied one dimensionally across a single feature and in a hierarchical manner.",
        "To describe it in more detail, we first recall the convolution operation that is central to the model.",
        "Then we describe how we compute the sequence of kernel sizes and how we determine the hierarchy of layers in the network.",
        "Convolution Given a sentence s and its paired matrix Ms, let m be a feature that is a row in Ms. Before defining kernels and the convolution operation, let us consider the underlying operation of local weighted addition.",
        "Let w1, ..., wk be a sequence of k weights; given the feature m, local weighted addition over the first k values of m gives:",
        "Then, a kernel simply defines the value of k by specifying the sequence of weights w1, ..., wk and the one-dimensional convolution applies local weighted addition with the k weights to each subsequence of values of m. More precisely, let a one-dimensional kernel k be a vector of weights and assume |k |?",
        "|m|, where |?",
        "|is the number of elements in a vector.",
        "Then we define the discrete, valid, one-dimensional convolution (k ?m) of kernel k and feature m by:",
        "where k = |k |and |k ?m |= |m |?",
        "k + 1.",
        "Each value in k ?m is a sum of k values of m weighted by values in k (Fig.",
        "3).",
        "To define the hierarchical architecture of the model, we need to define a sequence of kernel sizes and associated weights.",
        "To this we turn next.",
        "Let l be the number of words in the sentence s. The sequence of kernel sizes ?kli?i?t depends only on the length of s and itself has length t =",
        "That is, kernel sizes increase by one until the resulting convolved vector is smaller or equal to the last kernel size; see for example the kernel sizes in Fig. 1.",
        "Note that, for a sentence of length l, the number of layers in the HCNN including the input layer will be t + 1 as convolution with the corresponding kernel is applied at every layer of the model.",
        "Let us now proceed to define the hierarchy of layers in the HCNN.",
        "Given a sentence s, its length l and a sequence of kernel sizes ?kli?i?t, we may now give the recursive definition that yields the hierarchy of one-dimensional convolution operations applied to each feature f that is a row in Ms.",
        "Specifically, for each feature f , let Kfi be a sequence of t kernels, where the size of the kernel |Kfi |= kli.",
        "Then we have the hierarchy of matrices and corresponding features as follows:",
        "for some non-linear sigmoid function ?",
        "and bias bif , where i ranges over 1, ..., t. In sum, one-dimensional convolution is applied feature-wise to each feature of a matrix at a certain layer, where the kernel weights depend both on the layer and the feature at hand (Fig.",
        "1).",
        "A hierarchy of matrices is thus generated with the top matrix being a single vector for the sentence.",
        "Optionally one may consider multiple parallel HCNNs that are merged according to different strategies either at the top sentence vector layer or at intermediate layers.",
        "The weights in the word vectors may be tied across different HCNNs.",
        "Although potentially useful, multiple merged HCNNs are not used in the experiment below.",
        "This concludes the description of the sentence model.",
        "Let us now proceed to the discourse model."
      ]
    },
    {
      "heading": "3 Discourse Model",
      "text": [
        "The discourse model adapts a RNN architecture in order to capture central properties of discourse.",
        "We here first describe such properties and then define the model itself."
      ]
    },
    {
      "heading": "3.1 Discourse Compositionality",
      "text": [
        "The meaning of discourse - and of words and utterances within it - is often a result of a rich ensemble of context, of speakers?",
        "intentions and actions and of other relevant surrounding circumstances (Ko-rta and Perry, 2012; Potts, 2011).",
        "Far from capturing all aspects of discourse meaning, we aim at capturing in the model at least two of the most prominent ones: the sequentiality of the utterances and the interactions between the speakers.",
        "Concerning sequentiality, just the way the meaning of a sentence generally changes if words in it are permuted, so does the meaning of a paragraph or dialogue change if one permutes the sentences or utterances within.",
        "The change of meaning is more marked the larger the shift in the order of the sentences.",
        "Especially in tasks where one is concerned with a specific sentence within the context of the previous discourse, capturing the order of the sentences preceding the one at hand may be particularly crucial.",
        "Concerning the speakers?",
        "interactions, the meaning of a speaker's utterance within a discourse is differentially affected by the speaker's previous utterances as opposed to other speakers?",
        "previous utterances.",
        "Where applicable we aim at making the computed meaning vectors reflect the current speaker and the sequence of interactions with the previous speakers.",
        "With these two aims in mind, let us now proceed to define the model."
      ]
    },
    {
      "heading": "3.2 Recurrent Convolutional Neural Network",
      "text": [
        "The discourse model coupled to the sentence model is based on a RNN architecture with inputs from a HCNN and with the recurrent and output weights conditioned on the respective speakers.",
        "We take as given a sequence of sentences or utterances s1, ..., sT , each in turn being a sequence of words si = yi1...yil , a sequence of labels x1, ..., xT and a sequence of speakers or agents a1, ..., aT , in such way that the i-th utterance is performed by the i-th agent and has label xi.",
        "We denote by si the sentence vector computed by way of the sentence model for the sentence si.",
        "The RCNN computes probability distributions pi for the label at step i by iterating the following equations:",
        "where I,Hi,Oi are corresponding weight matrices for each agent ai and softmax(y)k = e",
        "returns a probability distribution.",
        "Thus pi is taken to model the following predictive distribution:",
        "An RCNN and the unravelling to depth d = 2 are depicted respectively in Fig. 2 and Fig. 4.",
        "With regards to vector representations of discourse, we take the hidden layer hi as the vector representing the discourse up to step i.",
        "This concludes the description of the discourse model.",
        "Let us now consider the experiment."
      ]
    },
    {
      "heading": "4 Predicting Dialogue Acts",
      "text": [
        "We experiment with the prediction of dialogue acts within a conversation.",
        "A dialogue act specifies the pragmatic role of an utterance and helps identifying the speaker's intentions (Austin, 1962; Korta and Perry, 2012).",
        "The automated recognition of dialogue acts is crucial for dialogue state tracking within spoken dialogue systems (Williams, 2012).",
        "We first describe the Switchboard Dialogue Act (SwDA) corpus (Calhoun et al., 2010) that serves as the dataset in the experiment.",
        "We report on the training procedure and the results and we make some qualitative observations regarding the discourse representations produced by the model."
      ]
    },
    {
      "heading": "4.1 SwDA Corpus",
      "text": [
        "The SwDA corpus contains audio recordings and transcripts of telephone conversations between multiple speakers that do not know each other and are given a topic for discussion.",
        "For a given utterance we use the transcript of the utterance, the dialogue act label and the speaker's label; no other annotations are used in the model.",
        "Overall there are 42 distinct dialogue act labels such as Statement and Opinion (Tab.1).",
        "We adopt the same data split of 1115 train dialogues and 19 test dialogues as used in (Stolcke et al., 2000)."
      ]
    },
    {
      "heading": "4.2 Objective Function and Training",
      "text": [
        "We minimise the cross-entropy error of the predicted and the true distributions and include an l2 regularisation parameter.",
        "The RCNN is truncated to a depth d = 2 so that the prediction of a dialogue act depends on the previous two utterances, speakers and dialogue acts; adopting depths > 2 has not yielded improvements in the experiment.",
        "The derivatives are efficiently computed by back-propagation (Rumelhart et al., 1986).",
        "The word vectors are initialised to random vectors of length 25 and no pretraining procedure is performed.",
        "We minimise the objective using L-BFGS in mini-batch mode; the minimisation converges smoothly."
      ]
    },
    {
      "heading": "4.3 Prediction Method and Results",
      "text": [
        "The prediction of a dialogue act is performed in a greedy fashion.",
        "Given the two previously predicted acts x?i?1, x?i?2, one chooses the act x?i that has the maximal probability in the predicted distribution P (xi).",
        "The LM-HMM model of (Stolcke et al., 2000) learns a language model for each dialogue act and a Hidden Markov Model for the sequence of dialogue acts and it requires all the utterances in a dialogue in order to predict the dialogue act of any one of the utterances.",
        "The RCNN makes the weaker assumption that only the utterances up to utterance i are available to predict the dialogue act x?i.",
        "The accuracy results of the models are compared in Tab.",
        "3."
      ]
    },
    {
      "heading": "4.4 Discourse Vector Representations",
      "text": [
        "We inspect the discourse vector representations that the model generates.",
        "After a dialogue is processed, the hidden layer h of the RCNN is taken",
        "to be some ground, you know, some rules ?",
        "B: Well, I, we talk about it.",
        "B: Uh-huh.",
        "B: Uh, pretty close to it.",
        "Fourth NN A: Um, do you watch it every A: It sounds to me like, uh, A: Do you usually go out, uh, Sunday?",
        "you are doing well.",
        "with the children or without them?",
        "B: [Breathing] Uh, when I can.",
        "B: My husband's retired.",
        "B: Well, a variety.",
        "The LM-HMM results are from (Stolcke et al., 2000).",
        "Inter-annotator agreement and theoretical maximum is 84%.",
        "to be the vector representation for the dialogue (Sect.",
        "3.2).",
        "Table 2 includes three randomly chosen dialogues composed of two utterances each; for each dialogue the table reports the four nearest neighbours.",
        "As the word vectors and weights are initialised randomly without pretraining, the word vectors and the weights are induced during training only through the dialogue act labels attached to the utterances.",
        "The distance between two word, sentence or discourse vectors reflects a notion of pragmatic similarity: two words, sentences or discourses are similar if they contribute in a similar way to the pragmatic role of the utterance signalled by the associated dialogue act.",
        "This is suggested by the examples in Tab.",
        "2, where a centre dialogue and a nearest neighbour may have some semantically different components (e.g. ?repair your own car?",
        "and ?manage the money?",
        "), but be pragmatically similar and the latter similarity is captured by the representations.",
        "In the examples, the meaning of the relevant words in the utterances, the speakers?",
        "interactions and the sequence of pragmatic roles are well preserved across the nearest neighbours."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "Motivated by the compositionality of meaning both in sentences and in general discourse, we have introduced a sentence model based on a novel convolutional architecture and a discourse model based on a novel use of recurrent networks.",
        "We have shown that the discourse model together with the sentence model achieves state of the art results in a dialogue act classification experiment without feature engineering or pretraining and with simple greedy decoding of the output sequence.",
        "We have also seen that the discourse model produces compelling discourse vector representations that are sensitive to the structure of the discourse and promise to capture subtle aspects of discourse comprehension, especially when coupled to further semantic data and unsupervised pretraining."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank Ed Grefenstette and Karl Moritz Hermann for great conversations on the matter.",
        "The authors gratefully acknowledge the support of the Clarendon Fund and the EPSRC."
      ]
    }
  ]
}
