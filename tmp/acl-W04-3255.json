{
  "info": {
    "authors": [
      "Hajime Tsukada",
      "Masaaki Nagata"
    ],
    "book": "SIGDAT Conference on Empirical Methods in Natural Language Processing",
    "id": "acl-W04-3255",
    "title": "Efficient Decoding for Statistical Machine Translation With a Fully Expanded WFST Model",
    "url": "https://aclweb.org/anthology/W04-3255",
    "year": 2004
  },
  "references": [
    "acl-H91-1026",
    "acl-J03-1002",
    "acl-J03-1005",
    "acl-J93-2003",
    "acl-J99-4005",
    "acl-N01-1018",
    "acl-N03-1019",
    "acl-P01-1030",
    "acl-P02-1040",
    "acl-P97-1047",
    "acl-W01-1408"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper proposes a novel method to compile statistical models for machine translation to achieve efficient decoding.",
        "In our method, each statistical submodel is represented by a weighted finite-state transducer (WFST), and all of the submodels are expanded into a composition model beforehand.",
        "Furthermore, the ambiguity of the composition model is reduced by the statistics of hypotheses while decoding.",
        "The experimental results show that the proposed model representation drastically improves the efficiency of decoding compared to the dynamic composition of the submodels, which corresponds to conventional approaches."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Recently, research on statistical machine translation has grown along with the increase in computational power as well as the amount of bilingual corpora.",
        "The basic idea of modeling machine translation was proposed by Brown et al.",
        "(1993), who assumed that machine translation can be modeled on noisy channels.",
        "The source language is encoded from a target language by a noisy channel, and translation is performed as a decoding process from source language to target language.",
        "Knight (1999) showed that the translation problem defined by Brown et al.",
        "(1993) is NP-complete.",
        "Therefore, with this model it is almost impossible to search for optimal solutions in the decoding process.",
        "Several studies have proposed methods for searching suboptimal solutions.",
        "Berger et al.",
        "(1996) and Och et al.",
        "(2001) proposed such depth-first search methods as stack decoders.",
        "Wand and Waibel (1997) and Tillmann and Ney (2003) proposed breadth-first search methods, i.e. beam search.",
        "Germann (2001) and Watanabe and Sumita (2003) proposed greedy type decoding methods.",
        "In all of these search algorithms, better representation of the statistical model in systems can improve the search efficiency.",
        "For model representation, a search method based on weightedfinite-state transducer (WFST) (Mohri et al., 2002) has achieved great success in the speech recognition field.",
        "The basic idea is that each statistical model is represented by a WFST and they are composed beforehand; the composed model is optimized by WFST operations such as determinization and minimization.",
        "This fully expanded model permits efficient searches.",
        "Our motivation is to apply this approach to machine translation.",
        "However, WFST optimization operations such as determinization are nearly impossible to apply to WFSTs in machine translation because they are much more ambiguous than speech recognition.",
        "To reduce the ambiguity, we propose a WFST optimization method that considers the statistics of hypotheses while decoding.",
        "Some approaches have applied WFST to statistical machine translation.",
        "Knight and Al-Onaizan (1998) proposed the representation of IBM model 3 with WFSTs; Bangalore and Ric-cardi (2001) studied WFST models in call-routing tasks, and Kumar and Byrne (2003) modeled phrase-based translation by WFSTs.",
        "All of these studies mainly focused on the representation of each submodel used in machine translation.",
        "However, few studies have focued on the integration of each WFST submodel to improve the decoding efficiency of machine translation.",
        "To this end, we propose a method that expands all of the submodels into a composition model, reducing the ambiguity of the expanded model by the statistics of hypotheses while decoding.",
        "First, we explain the translation model (Brown et al., 1993; Knight and Al-Onaizan, 1998) that we used as a base for our decoding research.",
        "Second, our proposed method is introduced.",
        "Finally, experimental results show that our proposed method drastically improves decoding efficiency."
      ]
    },
    {
      "heading": "2 IBM Model",
      "text": [
        "For our decoding research, we assume the IBM-style modeling for translation proposed in Brown et al.",
        "(1993).",
        "In this model, translation from Japanese to English attempts to find the that maximizes .",
        "Using Bayes’ rule, is rewritten as",
        "where is referred to as a language model and is referred to as a translation model.",
        "In this paper, we use word trigram for a language model and IBM model 3 for a translation model.",
        "The translation model is represented as follows considering all possible word alignments.",
        "The IBM model only assumes a one-to-many word alignment, where a Japanese word in the -th position connects to the English word in the -th position.",
        "The IBM model 3 uses the following .",
        "the a number of words connecting to , and it is called fertility.",
        "Note, however, that is the number of words connecting to null words.",
        "is conditional probability where English word connects to words in .",
        "is called fertility probability.",
        "is conditional probability where English word is translated to Japanese word and called translation probability.",
        "is conditional probability where the English word in the -th position connects to the the Japanese word in the -th position on condition that the length of the English sentence and Japanese sentence are and , respectively.",
        "is called distortion probability.",
        "In our experiment, we used the IBM model 3 while assuming constant distortion probability for simplicity."
      ]
    },
    {
      "heading": "3 WFST Cascade Model",
      "text": [
        "WFST is a finite-state device in which output symbols and output weights are defined as well as input symbols.",
        "Using composition (Pereira and Riley, 1997), we can obtain the combined WFST by connecting each output of to an input of .",
        "If we assume that each submodel of Equation (1) is represented by a WFST, a conventional decoder can be considered to compose submodels dynamically.",
        "The main idea of the proposed approach is to compute the composition beforehand.",
        "Figure 1 shows the translation process modeled by a WFST cascade.",
        "This WFST cascade model (Knight and Al-Onaizan, 1998) represents the IBM model 3 described in the previous section.",
        "Any possible permutations of the Japanese sentence are inputed to the cascade.",
        "First, T model( ) translates the Japanese word to an English word.",
        "NULL model( ) deletes special word NULL.",
        "Fertility model( ) merges the same continuous words into one word.",
        "At each stage, the probability represented by the weight of a WFST is accumulated.",
        "Finally, the weight of language model ( ) is accumulated.",
        "If WFST represents all permutations of the input sentence, decoding can be considered to search for the best path of .",
        "Therefore, computing in advance can improve the efficiency of the decoder.",
        "For , , and , we adopt the representation of Knight and Al-Onaizan (1998).",
        "For , we adopt the representation of Mohri et al.",
        "(2002).",
        "Figures 2– 5 show examples of submodel representation with WFSTs.",
        "in Figure 5 stands for a back-off parameter.",
        "Conditional branches are represented by nondeterministic paths in the WFST."
      ]
    },
    {
      "heading": "4 Ambiguity Reduction",
      "text": [
        "If we can determinize a fully-expanded WFST, we can achieve the best performance of the decoder.",
        "However, the composed WFST for machine translation is not obviously determinizable.",
        "The word-to-word translation model strongly contributes to WFST’s ambiguity while the transition of other submodels also contributes to ambiguity.",
        "Mohri et al.",
        "(2002) proposed a technique that added special symbols allowing the WFST to be determinizable.",
        "Determinization using this technique, however, is not expected to achieve efficient decoding in machine translation because the WFSTs of machine translation are inherently ambiguous.",
        "To overcome this problem, we propose a novel WFST optimization approach that uses decoding information.",
        "First, our method merges WFST states by considering the statistics of hypotheses while decoding.",
        "After merging the states, redundant edges whose beginning states, end states, input symbols, and output symbols are the same are also reduced.",
        "IBM models consider all possible alignments while a decoder searches for only the most appropriate alignment.",
        "Therefore, there are many redundant states in the full-expansion WFST from the viewpoint of decoding.",
        "We adopted a standard decoding algorithm in the speech recognition field, where the forward is beam-search and the backward is search.",
        "Since beam-search is adopted in the forward pass, the obtained results are not optimal but suboptimal.",
        "All input permutations are represented by a finite-state acceptor (Figure 6), where each state corresponds to input positions that are already read.",
        "In the forward search, hypotheses are maintained for each state of",
        "the finite-state acceptor.",
        "The WFST states that always appear together in the same hypothesis list of the forward beam-search should be equated if the states contribute to correct translation.",
        "Let be a full-expansion WFST model and be a WFST that represents the correct translation of an input sentence .",
        "For each , the states of that always appear together in the same hypothesis list in the course of decoding with are merged in our method.",
        "Simply merging states of may increase model errors, but corrects the errors caused by merging states.",
        "Unlike ordinary FSA minimization, states are merged without considering their successor states.",
        "If the weight represents probability, thesum of the weights of output transitions may not be 1.0 after merging states, and then thecondition of probability may be destroyed.",
        "Since the decoder does not sum up all possible paths but searches for the most appropriate paths, this kind of state merging does not pose a serious problem in practice.",
        "In the following experiment, we measured the association between states by in Gale and Church (1991).",
        "is a like statistic that is bounded between 0 and 1.",
        "If the of two states is higher than the specified threshold, these two states are merged.",
        "The definition of is as follows, where",
        "Merging the beginning and end states of a transition whose input is ( transition for short) may cause a problem when decoding.",
        "In our implementation, weight is basically minus probability, and its lower bound is 0 in theory.",
        "However, there exists negative transition that originated from the back-off value of n-gram.",
        "If we merge the beginning and end states of the negative transition, the search process will not stop due to the negative loop.",
        "To avoid this problem, we rounded the negative weight to 0 if the negative loop appears during merging.",
        "In the preliminary experiment, a weight-pushing operation (Mohri and Riley, 2001) was also effective for deleting negative transition of our full-expansion models.",
        "However, pushing causes an imbalance of weights among paths if the WFST is not deterministic.",
        "As a result of this imbalance, we cannot compare path costs when pruning.",
        "In fact, our preliminary experiment showed that pushed full-expansion WFST does not work well.",
        "Therefore, we adopted a simpler method to deal with a negative loop as described above."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": []
    },
    {
      "heading": "5.1 Effect of Full Expansion",
      "text": [
        "To clarify the effectiveness of a full-expansion approach, we compared the computational costs while using the same decoder with both dynamic composition and static composition, a full-expansion model in other words.",
        "In the forward beam-search, any hypothesis whose probability is lower than of the top of the hypothesis list is pruned.",
        "In this experiment, permutation is restricted, and words can be moved 6 positions at most.",
        "The translation model was trained by GIZA++ (Och and Ney, 2003), and the trigram was trained by the CMU-Cambridge Statistical Language Modeling Toolkit v2 (Clarkson and Rosenfeld, 1997).",
        "For the experiment, we used a Japanese-to-English bilingual corpus consisting of example sentences for a rule-based machine translation system.",
        "Each language sentence is aligned in the corpus.",
        "The total number of sentence pairs is 20,204.",
        "We used 17,678 pairs for training and 2,526 pairs",
        "which appears (both and appear).",
        ", , , and .",
        "is the total number of hypothesis lists.",
        "for the test.",
        "The average length of Japanese sentences was 8.4 words, and that of English sentences was 6.7 words.",
        "The Japanese vocabulary consisted of 15,510 words, and the English vocabulary was 11,806 words.",
        "Table 1 shows the size of the WFSTs used in the experiment.",
        "In these WFSTs, special symbols that express beginning and end of sentence are added to the WFSTs described in the previous section.",
        "The NIST score (Doddington, 2002) and BLEU Score (Papineni et al., 2002) were used to measure translation accuracy.",
        "Table 2 shows the experimental results.",
        "The full-expansion model provided translations more than 10 times faster than conventional dynamic composition submodels without degrading accuracy.",
        "However, the NIST scores are slightly different.",
        "In the course of composition, some paths that do not reach the final states are produced.",
        "In the full-expansion model these paths are trimmed.",
        "These trimmed paths may cause a slight difference in NIST scores."
      ]
    },
    {
      "heading": "5.2 Effect of Ambiguity Reduction",
      "text": [
        "To show the effect of ambiguity reduction, we compared the translation results of three different models.",
        "Model is the full-expansion model described above.",
        "Model is a reduced model by using our proposed method with a 0.9 threshold.",
        "Model is a reduced model with the statistics of the decoder without using the correct translation WFST.",
        "In other words, reduces the states of the full-expansion model more roughly than .",
        "The threshold for is set to 0.85 so that the size of the produced WFST is almost the same as .",
        "Table 3 shows the model size.",
        "To obtain decoder statistics for calculating , all of the sentence pairs in the training set were used.",
        "When obtaining the statistics, any hypothesis whose probability is lower than of the top of the hypothesis list is pruned in the forward beam-search.",
        "The translation experiment was conducted by successively changing the beam width of the forward search.",
        "Figures 7 and 8 show the results of the translation experiments, revealing that our proposed model can reduce the decoding time by approximately half.",
        "This model can reduce decoding time to a much greater extent than the rough reduction model, indicating that our state merging criteria are valid."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "We proposed a method to compile statistical models to achieve efficient decoding in a machine translation system.",
        "In our method, each statistical submodel is represented by a WFST, and all submodels are composed beforehand.",
        "To reduce the ambiguity of the composed WFST, the states are merged according to the statistics of hypotheses while decoding.",
        "As a result, we reduced decoding time to approximately of dynamic composition of submodels, which corresponds to the conventional approach.",
        "In this paper, we applied the state merging method to a fully-expanded WFST and showed the effectiveness of this approach.",
        "However, the state merging method itself is general and independent of the fully-expanded WFST.",
        "We can apply this method to each submodel of machine translation.",
        "More generally, we can apply it to all WFST-like models, including HMMs."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We would like to thank F. J. Och for providing GIZA++ and mkcls toolkits, and P. R. Clarkson for the CMU-Cambridge statistical language modeling toolkit v2.",
        "We also thank T. Hori for providing the n-gram conversion program for WFSTs and F. Bond and S. Fujita for providing the bilingual corpus."
      ]
    }
  ]
}
