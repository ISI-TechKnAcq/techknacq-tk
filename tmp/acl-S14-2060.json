{
  "info": {
    "authors": [
      "Alex Rudnick",
      "Levi King",
      "Can Liu",
      "Markus Dickinson",
      "Sandra KÃ¼bler"
    ],
    "book": "*SEM",
    "id": "acl-S14-2060",
    "title": "IUCL: Combining Information Sources for SemEval Task 5",
    "url": "https://aclweb.org/anthology/S14-2060",
    "year": 2014
  },
  "references": [
    "acl-C10-1011",
    "acl-E12-1009",
    "acl-P00-1056",
    "acl-P03-1021",
    "acl-P03-1054",
    "acl-P07-2045",
    "acl-W11-2123"
  ],
  "sections": [
    {
      "text": [
        "Abstract",
        "We describe the Indiana University system for SemEval Task 5, the L2 writing assistant task, as well as some extensions to the system that were completed after the main evaluation.",
        "Our team submitted translations for all four language pairs in the evaluation, yielding the top scores for English-German.",
        "The system is based on combining several information sources to arrive at a final L2 translation for a given L1 text fragment, incorporating phrase tables extracted from bitexts, an L2 language model, a multilingual dictionary, and dependency-based collocational models derived from large samples of target-language text."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In the L2 writing assistant task, we must translate an L1 fragment in the midst of an existing, nearly complete, L2 sentence.",
        "With the presence of this rich target-language context, the task is rather different from a standard machine translation setting, and our goal with our design was to make effective use of the L2 context, exploiting collocational relationships between tokens anywhere in the L2 context and the proposed fragment translations.",
        "Our system proceeds in several stages: (1) looking up or constructing candidate translations for the L1 fragment, (2) scoring candidate translations via a language model of the L2, (3) scoring candidate translations with a dependency-driven word similarity measure (Lin, 1998) (which we call SIM), and (4) combining the previous scores in a log-linear model to arrive at a final n-best list.",
        "Step 1 models transfer knowledge between This work is licenced under a Creative Commons Attribution 4.0 International License.",
        "License details: http: //creativecommons.org/licenses/by/4.0/ the L1 and L2; step 2 models facts about the L2 syntax, i.e., which translations fit well into the local context; step 3 models collocational and semantic tendencies of the L2; and step 4 gives different weights to each of the three sources of information.",
        "Although we did not finish step 3 in time for the official results, we discuss it here, as it represents the most novel aspect of the system ?",
        "namely, steps towards the exploitation of the rich L2 context.",
        "In general, our approach is language-independent, with accuracy varying due to the size of data sources and quality of input technology (e.g., syntactic parse accuracy).",
        "More features could easily be added to the log-linear model, and further explorations of ways to make use of target-language knowledge could be promising.",
        "2 Data Sources The data sources serve two major purposes for our system: For L2 candidate generation, we use Europarl and BabelNet; and for candidate ranking based on L2 context, we use Wikipedia and the Google Books Syntactic N-grams.",
        "Europarl The Europarl Parallel Corpus (Eu- roparl, v7) (Koehn, 2005) is a corpus of proceedings of the European Parliament, containing 21 European languages with sentence alignments.",
        "From this corpus, we build phrase tables for English-Spanish, English-German, French-English, Dutch-English.",
        "BabelNet In the cases where the constructed phrase tables do not contain a translation for a source phrase, we need to back off to smaller phrases and find candidate translations for these components.",
        "To better handle sparsity, we extend look-up using the multilingual dictionary Babel-Net, v2.0 (Navigli and Ponzetto, 2012) as a way to find translation candidates.",
        "356 Wikipedia For German and Spanish, we use recent Wikipedia dumps, which were converted to plain text with the Wikipedia Extractor tool.",
        "1 To save time during parsing, sentences longer than 25 words are removed.",
        "The remaining sentences are POS-tagged and dependency parsed using Mate Parser with its pre-trained models (Bohnet, 2010; Bohnet and Kuhn, 2012; Seeker and Kuhn, 2013).",
        "To keep our English Wikipedia dataset to a manageable size, we choose an older (2006), smaller dump.",
        "Long sentences are removed, and the remaining sentences are POS-tagged and dependency parsed using the pre-trained Stanford Parser (Klein and Manning, 2003; de Marneffe et al., 2006).",
        "The resulting sizes of the datasets are (roughly): German: 389M words, 28M sentences; Spanish: 147M words, 12M sentences; English: 253M words, 15M sentences.",
        "Dependencies extracted from these parsed datasets serve as training for the SIM system described in section 3.3.",
        "Google Books Syntactic N-grams For English, we also obtained dependency relationships for our word similarity statistics using the arcs dataset of the Google Books Syntactic N-Grams (Goldberg and Orwant, 2013), which has 919M items, each of which is a small ?syntactic n-gram?, a term Goldberg and Orwant use to describe short dependency chains, each of which may contain several tokens.",
        "This data set does not contain the actual parses of books from the Google Books cor-pus, but counts of these dependency chains.",
        "We converted the longer chains into their component (head, dependent, label) triples and then collated these triples into counts, also for use in the SIM system.",
        "3 System Design As previously mentioned, at run-time, our system decomposes the fragment translation task into two parts: generating many possible candidate transla-tions, then scoring and ranking them in the target-language context.",
        "3.1 Constructing Candidate Translations As a starting point, we use phrase tables constructed in typical SMT fashion, built with the training scripts packaged with Moses (Koehn et al., 2007).",
        "These scripts preprocess the bitext, estimate word alignments with GIZA++ (Och and 1 http://medialab.di.unipi.it/wiki/ Wikipedia_Extractor Ney, 2000) and then extract phrases with the grow-diag-final-and heuristic.",
        "At translation time, we look for the given source-language phrase in the phrase table, and if it is found, we take all translations of that phrase as our candidates.",
        "When translating a phrase that is not found in the phrase table, we try to construct a ?synthetic phrase?",
        "out of the available components.",
        "This is done by listing, combinatorially, all ways to decompose the L1 phrase into sub-phrases of at least one token long.",
        "Then for each decomposition of the input phrase, such that all of its components can be found in the phrase table, we generate a translation by concatenating their target-language sides.",
        "This approach naively assumes that generating valid L2 text requires no reordering of the components.",
        "Also, since there are 2 n?1 possible ways to split an n-token phrase into sub-sequences (i.e., each token is either the first token in a new sub-sequence, or it is not), we perform some heuristic pruning at this step, taking only the first 100 decompositions, preferring those built from longer phrase-table entries.",
        "Every phrase in the phrase table, including these synthetic phrases, has both a ?direct?",
        "and ?inverse?",
        "probability score; for synthetic phrases, we estimate these scores by taking the product of the corresponding probabilities for the individual components.",
        "In the case that an individual word cannot be found in the phrase table, the system attempts to look up the word in BabelNet, estimating the probabilities as uniformly distributed over the available BabelNet entries.",
        "Thus, synthetic phrase table entries can be constructed by combining phrases found in the training data and words available in BabelNet.",
        "For the evaluation, in cases where an L1 phrase contained words that were neither in our training data nor BabelNet (and thus were simply out-of-vocabulary for our system), we took the first translation for that phrase, without regard to con-text, from Google Translate, through the semi-automated Google Docs interface.",
        "This approach is not particularly scalable or reproducible, but simulates what a user might do in such a situation.",
        "3.2 Scoring Candidate Translations via a L2 Language Model To model how well a phrase fits into the L2 con-text, we score candidates with an n-gram lan-357 guage model (LM) trained on a large sample of target-language text.",
        "Constructing and querying a large language model is potentially computationally expensive, so here we use the KenLM Language Model Toolkit and its Python interface (Heafield, 2011).",
        "Here our models were trained on the Wikipedia text mentioned previously (with- out filtering long sentences), with KenLM set to 5-grams and the default settings.",
        "3.3 Scoring Candidate Translations via Dependency-Based Word Similarity The candidate ranking based on the n-gram language model ?",
        "while quite useful ?",
        "is based on very shallow information.",
        "We can also rank the candidate phrases based on how well each of the components fits into the L2 context using syntactic information.",
        "In this case, the fitness is measured in terms of dependency-based word similarity computed from dependency triples consisting of the the head, the dependent, and the dependency label.",
        "We slightly adapted the word similarity measure by Lin (1998): SIM(w 1 , w 2 ) = 2 ?",
        "c(h, d, l) c(h,?, l) + c(?, d, l) (1) where h = w 1 and d = w 2 and c(h, d, l) is the frequency with which a particular (head, dependent, label) dependency triple occurs in the L2 corpus.",
        "c(h,?, l) is the frequency with which a word occurs as a head in a dependency labeled l with any dependent.",
        "c(?, d, l) is the frequency with which a word occurs as a dependent in a dependency labeled l with any head.",
        "In the measure by Lin (1998), the numerator is defined as the information of all dependency features that w 1 and w 2 share, computed as the negative sum of the log probability of each dependency feature.",
        "Similarly, the denominator is computed as the sum of information of dependency features for w 1 and w 2 .",
        "To compute the fitness of a word w i for its context, we consider a set D of all words that are directly dependency-related to w i .",
        "The fitness of w i is thus computed as: FIT (w i ) = ?",
        "D w j SIM(w i , w j ) |D| (2) The fitness of a phrase is the average word similarity over all its components.",
        "For example, the fitness of the phrase ?eat with chopsticks?",
        "would be computed as: FIT (eat with chopsticks) = FIT (eat) + FIT (with) + FIT (chopsticks) 3 (3) Since we consider the heads and dependents of a target phrase component, these may be situated inside or outside the phrase.",
        "Both cases are included in our calculation, thus enabling us to consider a broader, syntactically determined local context of the phrase.",
        "By basing the calculation on a single word's head and dependents, we attempt to avoid data sparseness issues that we might get from rare n-gram contexts.",
        "Back-Off Lexical-based dependency triples suffer from data sparsity, so in addition to computing the lexical fitness of a phrase, we also calculate the POS fitness.",
        "For example, the POS fitness of ?eat with chopsticks?",
        "would be computed as follows: FIT (eat/VBG with/IN chopsticks/NNS) = FIT (VBG) + FIT (IN) + FIT (NNS) 3 (4) Storing and Caching The large vocabulary and huge number of combinations of our (head, dependent, label) triples poses an efficiency problem when querying the dependency-based word similarity values.",
        "Thus, we stored the dependency triples in a database with a Python programming interface (SQLite3) and built database indices on the frequent query types.",
        "However, for frequently searched dependency triples, re-querying the database is still inefficient.",
        "Thus, we built a query cache to store the recently-queried triples.",
        "Using the database and cache significantly speeds up our system.",
        "This database only stores dependency triples and their corresponding counts; the dependency-based similarity value is calculated as needed, for each particular context.",
        "Then, these FIT scores are combined with the scores from the phrase table and language model, using weights tuned by MERT.",
        "358 system acc wordacc oofacc oofwordacc run2 0.665 0.722 0.806 0.857 SIM 0.647 0.706 0.800 0.852 nb 0.657 0.717 0.834 0.868 Figure 1: Scores on the test set for English-German; here next-best is CNRC-run1.",
        "system acc wordacc oofacc oofwordacc run2 0.633 0.72 0.781 0.847 SIM 0.359 0.482 0.462 0.607 best 0.755 0.827 0.920 0.944 Figure 2: Scores on the test set for English-Spanish; here best is UEdin-run2.",
        "3.4 Tuning Weights with MERT In order to rank the various candidate translations, we must combine the different sources of information in some way.",
        "Here we use a familiar log-linear model, taking the log of each score ?",
        "the direct and inverse translation probabilities, the LM probability, and the surface and POS SIM scores ?",
        "and producing a weighted sum.",
        "Since the original scores are either probabilities or probability-like (in the range [0, 1]), their logs are negative num-bers, and at translation time we return the translation (or n-best) with the highest (least negative) score.",
        "This leaves us with the question of how to set the weights for the log-linear model; in this work, we use the ZMERT package (Zaidan, 2009), which implements the MERT optimization algorithm (Och, 2003), iteratively tuning the feature weights by repeatedly requesting n-best lists from the system.",
        "We used ZMERT with its default settings, optimizing our system's BLEU scores on the provided development set.",
        "We chose, for convenience, BLEU as a stand-in for the word-level accuracy score, as BLEU scores are maximized when the system output matches the reference translations.",
        "4 Experiments In figures 1-4, we show the scores on this year's test set for running the two variations of our sys-tem: run2, the version without the SIM exten-sions, which we submitted for the evaluation, and SIM, with the extensions enabled.",
        "For compar-ison, we also include the best (or for English-German, next-best) submitted system.",
        "We see here system acc wordacc oofacc oofwordacc run2 0.545 0.682 0.691 0.800 SIM 0.549 0.687 0.693 0.800 best 0.733 0.824 0.905 0.938 Figure 3: Scores on the test set for French-English; here best is UEdin-run1.",
        "system acc wordacc oofacc oofwordacc run2 0.544 0.679 0.634 0.753 SIM 0.540 0.676 0.635 0.753 best 0.575 0.692 0.733 0.811 Figure 4: Scores on the test set for Dutch-English; here best is UEdin-run1.",
        "that the use of the SIM features did not improve the performance of the base system, and in the case of English-Spanish caused significant degra-dation, which is as of yet unexplained, though we suspect difficulties parsing the Spanish test set, as for all of the other language pairs, the effects of adding SIM features were small.",
        "5 Conclusion We have described our entry for the initial running of the ?L2 Writing Assistant?",
        "task and explained some possible extensions to our base log-linear model system.",
        "In developing the SIM extensions, we faced some interesting software engineering challenges, and we can now produce large databases of dependency relationship counts for various languages.",
        "Unfortunately, these extensions have not yet led to improvements in performance on this particular task.",
        "The databases themselves seem at least intuitively promising, capturing interesting information about common usage patterns of the target language.",
        "Finding a good way to make use of this information may involve computing some measure that we have not yet considered, or perhaps the insights captured by SIM are covered effectively by the language model.",
        "We look forward to future developments around this task and associated applications in helping language learners communicate effectively.",
        "359 References"
      ]
    }
  ]
}
