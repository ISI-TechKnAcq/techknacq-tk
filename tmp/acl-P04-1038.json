{
  "info": {
    "authors": [
      "Jinying Chen",
      "Martha Palmer"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P04-1038",
    "title": "Chinese Verb Sense Discrimination Using an EM Clustering Model With Rich Linguistic Features",
    "url": "https://aclweb.org/anthology/P04-1038",
    "year": 2004
  },
  "references": [
    "acl-C00-2108",
    "acl-C02-1143",
    "acl-J02-3001",
    "acl-J98-1004"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper discusses the application of the Expectation-Maximization (EM) clustering algorithm to the task of Chinese verb sense discrimination.",
        "The model utilized rich linguistic features that capture predicate-argument structure information of the target verbs.",
        "A semantic taxonomy for Chinese nouns, which was built semi-automatically based on two electronic Chinese semantic dictionaries, was used to provide semantic features for the model.",
        "Purity and normalized mutual information were used to evaluate the clustering performance on 12 Chinese verbs.",
        "The experimental results show that the EM clustering model can learn sense or sense group distinctions for most of the verbs successfully.",
        "We further enhanced the model with certain fine-grained semantic categories called lexical sets.",
        "Our results indicate that these lexical sets improve the model's performance for the three most challenging verbs chosen from the first set of experiments."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Highly ambiguous words may lead to irrelevant document retrieval and inaccurate lexical choice in machine translation (Palmer et al., 2000), which suggests that word sense disambiguation (WSD) is beneficial and sometimes even necessary in such NLP tasks.",
        "This paper addresses WSD in Chinese through developing an Expectation-Maximization (EM) clustering model to learn Chinese verb sense distinctions.",
        "The major goal is to do sense discrimination rather than sense labeling, similar to (Schiitze, 1998).",
        "The basic idea is to divide instances of a word into several clusters that have no sense labels.",
        "The instances in the same cluster are regarded as having the same meaning.",
        "Word sense discrimination can be applied to document retrieval and similar tasks in information access, and to facilitating the building of large annotated corpora.",
        "In addition, since the clustering model can be trained on large unannotated corpora and evaluated on a relatively small sense-tagged corpus, it can be used to find indicative features for sense distinctions through exploring huge amount of available unannotated text data.",
        "The EM clustering algorithm (Hofinann and Puzicha, 1998) used here is an unsupervised machine learning algorithm that has been applied in many NLP tasks, such as inducing a semantically labeled lexicon and determining lexical choice in machine translation (Booth et al., 1998), automatic acquisition of verb semantic classes (Schulte im Walde, 2000) and automatic semantic labeling (Gildea and Jurafsky, 2002).",
        "In our task, we equipped the EM clustering model with rich linguistic features that capture the predicate-argument structure information of verbs and restricted the feature set for each verb using knowledge from dictionaries.",
        "We also semi-automatically built a semantic taxonomy for Chinese nouns based on two Chinese electronic semantic dictionaries, the Hownet dictionary' and the Rocling dictionary?",
        "The 7 top-level categories of this taxonomy were used as semantic features for the model.",
        "Since external knowledge is used to obtain the semantic features and guide feature selection, the model is not completely unsupervised from this perspective; however, it does not make use of any annotated training data.",
        "Two external quality measures, purity and normalized mutual information (NMI) (Strehl.",
        "2002), were used to evaluate the model's performance on 12 Chinese verbs.",
        "The experimental results show that rich linguistic features and the semantic taxonomy are both very useful in sense discrimination.",
        "The model generally performs well in learning sense group distinctions for difficult, highly polysemous verbs and sense distinctions for other verbs.",
        "Enhanced by certain fine-grained semantic categories called lexical sets (Hanks, 1996), the model's ' http://www.keenage.com/.",
        "z A Chinese electronic dictionary liscenced from The Association for Computational Linguistics and Chinese Language Processing (ACLCLP), Nankang, Taipei, Taiwan.",
        "performance improved in a preliminary experiment for the three most difficult verbs chosen from the first set of experiments.",
        "The paper is organized as follows: we briefly introduce the EM clustering model in Section 2 and describe the features used by the model in Section 3.",
        "In Section 4, we introduce a semantic taxonomy for Chinese nouns, which is built semi-automatically for our task but can also be used in other NLP tasks such as co-reference resolution and relation detection in information extraction.",
        "We report our experimental results in Section 5 and conclude our discussion in Section 6."
      ]
    },
    {
      "heading": "2 EM Clustering Model",
      "text": [
        "The basic idea of our EM clustering approach is similar to the probabilistic model of co-occurrence described in detail in (Hofmann and Puzicha 11j998).",
        "In our model, we treat a set of features 1f ,f2,...J.",
        "}, which are extracted from the parsed sentences that contain a target verb, as observed variables.",
        "These variables are assumed to be independent given a hidden variable c, the sense of the target verb.",
        "Therefore the joint probability of the observed variables (features) for each verb instance, i.e., each parsed sentence containing the target verb, is defined in equation (1),",
        "The f,.",
        "'s are discrete-valued features that can take multiple values.",
        "A typical feature used in our model is shown in (2),",
        "0 iff the target verb has no sentential complement 1 iff the target verb has a nonfinite (2) sentential complement 2 iff the target verb has a finite sentential complement",
        "At the beginning of training (i.e., clustering), the model's parameters p(c) and p(f I c) are randomly initialized.",
        "Then, the probability of c conditioned on the observed features is computed in the expectation step (E-step), using equation (3),",
        "3 In our experiments, for verbs with more than 3 senses, syntactic and semantic restrictions derived from dictionary entries are used to constrain the random initialization.",
        "In the maximization step (M-step), p(c) and p(f,.",
        "I c) are recomputed by maximizing the log-likelihood of all the observed data which is calculated by using p(c I fl, f2,..., fm) estimated in the E-step.",
        "The E-step and M-step are repeated for a fixed number of rounds, which is set to 20 in our experiments,4 or till the amount of change of p(c) and p(f,.",
        "I c) is under the threshold 0.001.",
        "When doing classification, for each verb instance, the model calculates the same conditional probability as in equation (3) and assigns the instance {to the cluster with the maximal AC I ✓ l I 2 l..., fm) ."
      ]
    },
    {
      "heading": "3 Features Used in the Model",
      "text": [
        "The EM clustering model uses a set of linguistic features to capture the predicate-argument structure information of the target verbs.",
        "These features are usually more indicative of verb sense distinctions than simple features such as words next to the target verb or their POS tags.",
        "For example, the Chinese verb \"出 l chul\" has a sense of produce, the distinction between this sense and the verb's other senses, such as happen and go out, largely depends on the semantic category of the verb's direct object.",
        "Typical examples are shown in (1),",
        "c. 他们/their 县/county 出/go out 门�/door 就 /right away 是/be 山 /mountain \"In their county, you can see mountains as soon as you step out of the doors.\" The verb has the sense produce in (la) and its object should be something producible, such as \"香蕉/banana\".",
        "While in (lb), with the sense happen, the verb typically takes an event or event-like object, such as \" 大事�/big event\", \"事 故 /accident\" or \"问题/problem\" etc.",
        "In (lc), the verb's object '1门/door\" is closely related to location, consistent with the sense go out.",
        "In contrast, simple lexical or POS tag features sometimes fail to capture such information, which can be seen clearly in (2), 4 In our experiments, we set 20 as the maximal number of rounds after trying different numbers of rounds (20, 40, 60, 80, 100) in a preliminary experiment.",
        "to the verb in (2a), is far away from the verb in (2b).",
        "For (2b), a classifier only looking at the adjacent positions of the target verb tends to be misled by the NP right after the verb, i.e., 海 /Hainan\", which is a Province in China and a typical object of the verb with the sense go out.",
        "Five types of features are used in our model:",
        "1.",
        "Semantic category of the subject of the target verb 2.",
        "Semantic category of the object of the target verb 3.",
        "Transitivity of the target verb 4.",
        "Whether the target verb takes a sentential complement and which type of sentential complement (finite or nonfmite) it takes 5.",
        "Whether the target verb occurs in a verb compound",
        "We obtain the values for the first two types of features (1) and (2) from a semantic taxonomy for Chinese nouns, which we will introduce in detail in the next section.",
        "In our implementation, the model uses different features for different verbs.",
        "The criteria for feature selection are from the electronic CETA dictionary file s and a hard copy English-Chinese dictionary, The Warmth Modern Chinese-English Dictionary.6 For example, the verb � 出 lchul\" never takes sentential complements, thus the fourth type of feature is not used for it.",
        "It could be supposed that we can still have a uniform model, i.e., a model using the same set of features for all the target verbs, and just let the EM clustering algorithm find useful features for different verbs automatically.",
        "The problem here is that unsupervised learning models (i.e., models trained on unlabeled data) are more likely to be affected by noisy data than supervised ones.",
        "Since all the features used in our model are extracted from automatically parsed sentences that inevitably have preprocessing errors such as segmentation, POS tagging and parsing errors, using verb-specific sets of features can alleviate the problem caused by noisy data to some extent.",
        "For example, if the model already knows Wa g-Wen Books Ltd, 1997. that a verb like chul\" can never take sentential complements (i.e., it does not use the fourth type of feature for that verb), it will not be misled by erroneous parsing information saying that the verb takes sentential complements in certain sentences.",
        "Since the corresponding feature is not included, the noisy data is filtered out.",
        "In our EM clustering model, all the features selected for a target verb are treated in the same way, as described in Section 2. sense in WordNet a d chooses the first WordNet top-level category it finds.",
        "Examples in (1) have shown that the semantic category of the object of a verb sometimes is crucial in distinguishing certain Chinese verb senses.",
        "And our previous work on information extraction in Chinese (Chen et al., 2004) has shown that semantic features, which are more general than lexical features but still contain rich information about words, can be used to improve a model's capability of handling unknown words, thus alleviating potential sparse data problems.",
        "We have two Chinese electronic semantic dictionaries: the Hownet dictionary, which assigns 26,106 nouns to 346 semantic categories, and the Rocling dictionary, which assigns 4,474 nouns to 110 semantic categories.",
        "A preliminary experimental result suggests that these semantic categories might be too fine-grained for the EM clustering model (see Section 5.2 for greater details).",
        "An analysis of the sense distinctions of several Chinese verbs also suggests that more general categories on top of the Hownet and Rocling categories could still be informative and most importantly, could enable the model to generate meaningful clusters more easily.",
        "We therefore built athree-level semantic taxonomy based on the two semantic dictionaries using both automatic methods and manual effort.",
        "The taxonomy was built in three steps.",
        "First, a simple mapping algorithm was used to map semantic categories defined in Hownet and Rocling into 27 top-level WordNet categories.$ The Hownet or Rocling semantic categories have English glosses.",
        "For each category gloss, the algorithm looks through the hypernyms of its first 7 Hownet assigns multiple entries (could be different semantic categories) to polysemous words.",
        "The Rocling dictionary we used only assigns one entry (i.e., one semantic category) to each noun.",
        "s The 27 categories contain 25 unique beginners for noun source files in WordNet, as defined in (Fellbaum, 1998) and two higher level categories Entity a d",
        "The mapping obtained from step 1 needs further modification for two reasons.",
        "First, the glosses of Hownet or Rocling semantic categories usually have multiple senses in WordNet.",
        "Sometimes, the first sense in WordNet for a category gloss is not its intended meaning in Hownet or Rocling.",
        "In this case, the simple algorithm cannot get the correct mapping.",
        "Second, Hownet and Rocling sometimes use adjectives or non-words as category glosses, such as animate and LandYehicle etc., which have no WordNet nominal hypernyms at all.",
        "However, those adjectives or non-words usually have straightforward meanings and can be easily reassigned to an appropriate WordNet category.",
        "Although not accurate, the automatic mapping in step 1 provides a basic framework or skeleton for the semantic taxonomy we want to build and makes subsequent work easier.",
        "In step 2, hand correction, we found that we could make judgments and necessary adjustments on about 80% of the mappings by only looking at the category glosses used by Hownet or Rocling, such as livestock, money, building and so on.",
        "For the other 20%, we could make quick decisions by looking them up in an electronic table we created.",
        "For each Hownet or Rocling category, our table lists all the nouns assigned to it by the two dictionaries.",
        "We merged two WordNet categories into others and subdivided three categories that seemed more coarse-grained than others into 2-5 subcategories.",
        "Step 2 took three days and 35 intermediate-level categories were generated.",
        "In step 3, we manually clustered the 35 intermediate-level categories into 7 top-level semantic categories.",
        "Figure 1 shows part of the taxonomy.",
        "The EM clustering model uses the 7 top-level categories to define the first two types of features that were introduced in Section 3.",
        "For example, the",
        "value of a feature fk is 1 if and only if the object NP of the target verb belongs to the semantic category Event and is otherwise 0."
      ]
    },
    {
      "heading": "5 Clustering Experiments",
      "text": [
        "Since we need labeled data to evaluate the clustering performance but have limited sense-tagged corpora, we applied the clustering model to 12 Chinese verbs in our experiments.",
        "The verbs are chosen from 28 annotated verbs in Penn Chinese Treebank so that they have at least two verb meanings in the corpus and for each of them, the number of instances for a single verb sense does not exceed 90% of the total number of instances.",
        "In our task, we generally do not include senses for other parts of speech of the selected words, such as noun, preposition, conjunction and particle etc., since the parser we used has a very high accuracy in distinguishing different parts of speech of these words (>98% for most of them).",
        "However, we do include senses for conjunctional and/or prepositional usage of two words, \"到1dao4\" and \"为Awei4\", since our parser cannot distinguish the verb usage from the conjunctional or prepositional usage for the two words very well.",
        "Five verbs, the first five listed in Table 1, are both highly polysemous and difficult for a supervised word sense classifier (Dang et al., 2002).",
        "9 In our experiments, we manually grouped the verb senses for the five verbs.",
        "The criteria for the grouping are similar to Palmer et al.",
        "'s (to appear) work on English verbs, which considers both sense coherence and predicate-argument structure distinctions.",
        "Figure 2 gives an example of 9 In the supervised task, their accuracies are lower than 85%, and four of them are even lower than the baselines.",
        "Senses for�\" 到1dao4\" Sense groups for�'到1dao4\"",
        "1. to go to, leave for 2. to come 3. to arrive 4. to reach a particular stage, condition, or level 5. marker for completion of activities (after a verb) 6. marker for direction of activities (after a verb) 7. to reach a time point 8. up to, until (prepositional usage) 9. up to, until, (from ...) to ... (conjunctional usage)",
        "the definition of sense groups.",
        "The manually defined sense groups are used to evaluate the model's performance on the five verbs.",
        "The model was trained on an unannotated corpus, People's Daily News (PDN), and tested on the manually sense-tagged Chinese Treebank (with some additional sense-tagged PDN data).10 We parsed the training and test data using a Maximum Entropy parser and extracted the features from the parsed data automatically.",
        "The number of clusters used by the model is set to the number of the defined senses or sense groups of each target verb.",
        "For each verb, we ran the EM clustering algorithm ten times.",
        "Table 2 shows the average performance and the standard deviation for each verb.",
        "Table 1 summarizes the data used in the experiments, where we also give the normalized sense perplexity'1 of each verb in the test data."
      ]
    },
    {
      "heading": "5.1 Evaluation Methods",
      "text": [
        "We use two external quality measures, purity and normalized mutual information (NMI) (Strehl.",
        "2002) to evaluate the clustering performance.",
        "Assuming a verb has I senses, the clustering model assigns n instances of the verb into k clusters, n; is the size of the ith cluster, ni is the number of instances hand-tagged with the jth sense, and n; is the number of instances with the jth sense in the ith cluster, purity is defined in equation (4):",
        "10 The sense-tagged PDN data we used here are the same as in (Dang et al., 2002).",
        "11 It is calculated as the entropy of the sense distribution of a verb in the test data divided by the largest possible entropy, i.e., loge (the number of senses of the verb in the test data).",
        "It can be interpreted as classification accuracy when for each cluster we treat the majority of instances that have the same sense as correctly classified.",
        "The baseline purity is calculated by treating all instances for a target verb in a single cluster.",
        "The purity measure is very intuitive.",
        "In our case, since the number of clusters is preset to the number of senses, purity for verbs with two senses is equal to classification accuracy defined in supervised WSD.",
        "However, for verbs with more than 2 senses, purity is less informative in that a clustering model could achieve high purity by making the instances of 2 or 3 dominant senses the majority instances of all the clusters.",
        "Mutual information (MI) is more theoretically well-founded than purity.",
        "Treating the verb sense and the cluster as random variables S and C, the MI between them is defined in equation (5):",
        "MI(S,C) characterizes the reduction in uncertainty of one random variable S (or C) due to knowing the other variable C (or S).",
        "A single cluster with all instances for a target verb has a zero MI.",
        "Random clustering also has a zero MI in the limit.",
        "In our experiments, we used [0,1]- normalized mutual information (NMI) (Strehl.",
        "2002).",
        "A shortcoming of this measure, however, is that the best possible clustering (upper bound) evaluates to less than 1, unless classes are balanced.",
        "Unfortunately, unbalanced sense distribution is the usual case in WSD tasks, which makes NMI itself hard to interpret.",
        "Therefore, in addition to NMI, we also give its upper bound (upper-NMI) and the ratio of NMI and its upper bound (NMI-ratio) for each verb, as shown in columns 6 to 8 in Table 2."
      ]
    },
    {
      "heading": "5.2 Experimental Results",
      "text": [
        "Table 2 summarizes the experimental results for the 12 Chinese verbs.",
        "As we see, the EM clustering model performs well on most of them, except the verb \"要lyao4\".12 The NMI measure NMI-ratio turns out to be more stringent than purity.",
        "A high purity does not necessarily mean a high NMI-ratio.",
        "Although intuitively, NMI-ratio should be related to sense perplexity and purity, it is hard to formalize the relationships between them from the results.",
        "In fact, the NMI-ratio for a particular verb is eventually determined by its concrete sense distribution in the test data and the model's clustering behavior for that verb.",
        "For example, the verbs \"出 lchul\" and \"见Jian4\" have the same sense perplexity and \" 见 jian4\" has a higher purity than \"出IchuI\" (72.20% vs. 63.31%), but the NMI-ratio for \" 见Jian4\" is much lower than \"出 lchul\" (22.41% vs. 43.24%).",
        "An analysis of the 12 For all the verbs except \"要lyao4\", the model's purities outperformed the baseline purities significantly (p<0.05, and p<0.001 for 8 of them).",
        "classification results for \" 见Jian4\" shows that the clustering model made the instances of the verb's most dominant sense the majority instances of three clusters (of total 5 clusters), which is penalized heavily by the NMI measure.",
        "Rich linguistic features turn out to be very effective in learning Chinese verb sense distinctions.",
        "Except for the two verbs, \" 发现lfalxian4\" and \" 表示Jbiao3shi4\", the sense distinctions of which can usually be made only by syntactic alternations,13 features such as semantic features or combinations of semantic features and syntactic alternations are very beneficial and sometimes even necessary for learning sense distinctions of other verbs.",
        "For example, the verb \" 见jian4\" has one sense see, in which the verb typically takes a Human subject and a sentential complement, while in another sense show, the verb typically takes an Entity subject and a State object.",
        "An inspection of the classification results shows 13 For example, the verb \"发现1falxian4\" takes an object in one sense discover and a sentential complement in the other sense realize.",
        "that the EM clustering model has indeed learned such combinatory patterns from the training data.",
        "The experimental results also indicate that the semantic taxonomy we built is beneficial for the task.",
        "For example, the verb \" 投入ltoulru4\" has two senses, input and plunge into.",
        "It typically takes an Event object for the second sense but not for the first one.",
        "A single feature obtained from our semantic taxonomy, which tests whether the verb takes an Event object, captures this property neatly (achieves purity 95.65% and NMI-ratio 78.38% when using 2 clusters).",
        "Without the taxonomy, the top-level category Event is split into many fine-grained Hownet or Rocling categories, which makes it very difficult for the EM clustering model to learn sense distinctions for this verb.",
        "In fact, in a preliminary experiment only using the Hownet and Rocling categories, the model had the same purity as the baseline (52.17%) and a low NMI-ratio (4.22%) when using 2 clusters.",
        "The purity improved when using more clusters (70.43% with 4 clusters and 76.09% with 6), but it was still much lower than the purity achieved by using the semantic taxonomy and the NMI-ratio dropped further (1.19% and 1.20% for the two cases).",
        "By looking at the classification results, we identified three major types of errors.",
        "First, preprocessing errors create noisy data for the model.",
        "Second, certain sense distinctions depend heavily on global contextual information (cross-sentence information) that is not captured by our model.",
        "This problem is especially serious for the verb \" 要 lyao4\".",
        "For example, without global contextual information, the verb can have at least three meanings want, need or should in the same clause, as shown in (3).",
        "(3) 他/he 要/want/need/should 马上/at once 读完/finish reading 这本/this 书�/book.",
        "\"He wants to/needs to/should finish reading this book at once.\" Third, a target verb sometimes has specific types of NP arguments or co-occurs with specific types of verbs in verb compounds in certain senses.",
        "Such information is crucial for distinguishing these senses from others, but is not captured by the general semantic taxonomy used here.",
        "We did further experiments to investigate how much improvement the model could gain by capturing such information, as discussed in Section 5.3."
      ]
    },
    {
      "heading": "5.3 Experiments with Lexical Sets",
      "text": [
        "As discussed by Patrick Hanks (1996), certain senses of a verb are often distinguished by very narrowly defined semantic classes (called lexical sets) that are specific to the meaning of that verb sense.",
        "For example, in our case, the verb \"恢复Jhuilfu4\" has a sense recover in which its direct object should be something that can be recovered naturally.",
        "A typical set of object NPs of the verb for this particular sense is partially listed in (4), (4) Lexical set for naturally recoverable things J体力/physical strength, 身体/body, 健 康 /health, 精力/mental energy, 听 力/hearing, 知 觉/feeling, 记忆力/memory, } Most words in this lexical set belong to the Hownet category attribute and the top-level category State in our taxonomy.",
        "However, even the lower-level category attribute still contains many other words irrelevant to the lexical set, some of which are even typical objects of the verb for two other senses, resume and regain, such as \"邦交Zdiplomatic relations\" in \"恢复/resume 邦交/diplomatic relations\" and \"名 誉/reputation\" in \"恢复/regain名誉/reputation\".",
        "Therefore, a lexical set like (4) is necessary for distinguishing the recover sense from other senses of the verb.",
        "It has been argued that the extensional definition of lexical sets can only be done using corpus evidence and it cannot be done fully automatically (Hanks, 1997).",
        "In our experiments, we use a bootstrapping approach to obtain five lexical sets semi-automatically for three verbs \"出 lchul\", \" 见 jian4\" and \"恢复Jhuilfu4\" that have both low purity and low NMI-ratio in the first set of experiments.",
        "14 We first extracted candidates for the lexical sets from the training data.",
        "For example, we extracted all the direct objects of the verb \"恢复Jhuilfu4\" and all the verbs that combined with the verb \"出 lchul\" to form verb compounds from the automatically parsed training data.",
        "From the candidates, we manually selected words to form five initial seed sets, each of which contains no more than ten words.",
        "A simple algorithm was used to search for all the words that have the same detailed Hownet semantic definitions (semantic category plus certain supplementary information) as the seed words.",
        "We did not use Rocling because its semantic definitions are so general that a seed word tends to extend to a huge set of irrelevant words.",
        "Highly relevant words were manually selected from all the words found by the searching algorithm and added to the initial seed sets.",
        "The enlarged sets were used as lexical sets.",
        "The enhanced model first uses the lexical sets to obtain the semantic category of the NP arguments 14 We did not include \"要lyao4\", since its meaning rarely depends on local predicate-argument structure information.",
        "of the three verbs.",
        "Only when the search fails does the model resort to the general semantic taxonomy.",
        "The model also uses the lexical sets to determine the types of the compound verbs that contain the target verb \"出 lchul\" and uses them as new features.",
        "Table 3 shows the model's performance on the three verbs with or without using lexical sets.",
        "As we see, lexical sets improves the model's performance on all of them, especially on the verb \"出 lchul\".",
        "Although the results are still preliminary, they nevertheless provide us hints of how much a WSD model for Chinese verbs could gain from lexical sets."
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "We have shown that an EM clustering model that uses rich linguistic features and a general semantic taxonomy for Chinese nouns generally performs well in learning sense distinctions for 12 Chinese verbs.",
        "In addition, using lexical sets improves the model's performance on three of the most challenging verbs.",
        "Future work is to extend our coverage and to apply the semantic taxonomy and the same types of features to supervised WSD in Chinese.",
        "Since the experimental results suggest that a general semantic taxonomy and more constrained lexical sets are both beneficial for WSD tasks, we will develop automatic methods to build large-scale semantic taxonomies and lexical sets for Chinese, which reduce human effort as much as possible but still ensure high quality of the obtained taxonomies or lexical sets."
      ]
    },
    {
      "heading": "7 Acknowledgements",
      "text": [
        "This work has been supported by an ITIC supplement to a National Science Foundation Grant, NSF-ITR-EIA-0205448.",
        "Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation."
      ]
    }
  ]
}
