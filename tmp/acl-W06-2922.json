{
  "info": {
    "authors": [
      "Giuseppe Attardi"
    ],
    "book": "Conference on Computational Natural Language Learning CoNLL",
    "id": "acl-W06-2922",
    "title": "Experiments With a Multilanguage Non-Projective Dependency Parser",
    "url": "https://aclweb.org/anthology/W06-2922",
    "year": 2006
  },
  "references": [
    "acl-C04-1010",
    "acl-H05-1066",
    "acl-J96-1002",
    "acl-P05-1013",
    "acl-W02-2016",
    "acl-W04-2407",
    "acl-W06-2920"
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": [
        "Parsing natural language is an essential step in several applications that involve document analysis, e.g. knowledge extraction, question answering, summarization, filtering.",
        "The best performing systems at the TREC Question Answering track employ parsing for analyzing sentences in order to identify the query focus, to extract relations and to disambiguate meanings of words.",
        "These are often demanding applications, which need to handle large collections and to provide results in a fraction of a second.",
        "Dependency parsers are promising for these applications since a dependency tree provides predicate-argument relations which are convenient for use in the later stages.",
        "Recently statistical dependency parsing techniques have been proposed which are deterministic and/or linear (Yamada and Matsumoto, 2003; Nivre and Scholz, 2004).",
        "These parsers are based on learning the correct sequence of Shift/Reduce actions used to construct the dependency tree.",
        "Learning is based on techniques like SVM (Vapnik 1998) or Memory Based Learning (Daelemans 2003), which provide high accuracy but are often computationally expensive.",
        "Kudo and Matsumoto (2002) report a two week learning time on a Japanese corpus of about 8000 sentences with SVM.",
        "Using Maximum Entropy (Berger, et al.",
        "1996) classifiers I built a parser that achieves a throughput of over 200 sentences per second, with a small loss in accuracy of about 2 3 %.",
        "The efficiency of Maximum Entropy classifiers seems to leave a large margin that can be exploited to regain accuracy by other means.",
        "I performed a series of experiments to determine whether increasing the number of features or combining several classifiers could allow regaining the best accuracy.",
        "An experiment cycle in our setting requires less than 15 minutes for a treebank of moderate size like the Portuguese treebank (Afonso et al., 2002) and this allows evaluating the effectiveness of adding/removing features that hopefully might apply also when using other learning techniques.",
        "I extended the Yamada-Matsumoto parser to handle labeled dependencies: I tried two approaches: using a single classifier to predict pairs of actions and labels and using two separate classifiers, one for actions and one for labels.",
        "Finally, I extended the repertoire of actions used by the parser, in order to handle non-projective relations.",
        "Tests on the PDT (Böhmovà et al., 2003) show that the added actions are sufficient to handle all cases of non-projectivity.",
        "However, since the cases of non-projectivity are quite rare in the corpus, the general learner is not supplied enough of them to learn how to classify them accurately, hence it may be worthwhile to exploit a second classifier trained specifically in handling non-projective situations."
      ]
    },
    {
      "heading": "1. Summary of the approach",
      "text": [
        "The overall parsing algorithm is an inductive statistical parser, which extends the approach by Yamada and Matsumoto (2003), by adding six new reduce actions for handling non-projective relations and also performs dependency labeling.",
        "Parsing is deterministic and proceeds bottom-up.",
        "Labeling is integrated within a single processing step.",
        "Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL -X), pages 166–170, New York City, June 2006. c�2006 Association for Computational Linguistics The parser is modular: it can use several learning algorithms: Maximum Entropy, SVM, Winnow, Voted Perceptron, Memory Based Learning, as well as combinations thereof.",
        "The submitted runs used Maximum Entropy and I present accuracy and performance comparisons with other learning algorithms.",
        "No additional resources are used.",
        "No preprocessing or post-processing is used, except stemming for Danish, German and Swedish."
      ]
    },
    {
      "heading": "2 Features",
      "text": []
    },
    {
      "heading": "Columns from input data were used as follows.",
      "text": [
        "LEMMA was used in features whenever available, otherwise the FORM was used.",
        "For Danish, German and Swedish the Snowball stemmer (Porter 2001) was used to generate a value for LEMMA.",
        "This use of stemming slightly improved both accuracy and performance.",
        "Only CPOSTAG were used.",
        "PHEAD/PDEPREL were not used.",
        "FEATS were used to extract a single token combining gender, number, person and case, through a language specific algorithm.",
        "The selection of features to be used in the parser is controlled by a number of parameters.",
        "For example, the parameter PosFeatures determines for which tokens the POS tag will be included in the context, PosLeftChi 7dren determines how many left outermost children of a token to consider, PastActions tells how many previous actions to include as features.",
        "The settings used in the submitted runs are listed below and configure the parser for not using any word forms.",
        "Positive numbers refer to input tokens, negative ones to token on the stack.",
        "The context for POS tags consisted of 1 token left and 3 tokens to the right of the focus words, except for Czech and Chinese were 2 tokens to the left and 4 tokens to the right were used.",
        "These values were chosen by performing experiments on the training data, using 10% of the sentences as held-out data for development."
      ]
    },
    {
      "heading": "3 Inductive Deterministic Parsing",
      "text": [
        "The parser constructs dependency trees employing a deterministic bottom-up algorithm which performs Shift/Reduce actions while analyzing input sentences in left-to-right order.",
        "Using a notation similar to (Nivre and Scholz, 2003), the state of the parser is represented by a quadruple (S, I, T, A), where S is the stack, I is the list of (remaining) input tokens, T is a stack of temporary tokens and A is the arc relation for the dependency graph.",
        "Given an input string W, the parser is initialized to ((), W, (), ()), and terminates when it reaches a configuration (S, (), (), A).",
        "The parser by Yamada and Matsumoto (2003) used the following actions: Shift in a configuration (S, n|I, T, A), pushes n to the stack, producing the configuration (n|S, I, T, A).",
        "Right1 in a configuration (s1|S, n|I, T, A), adds an arc from s1 to n and pops s1 from the stack, producing the configuration (S, n|I, T, A∪{(s1, r, n)}).",
        "Left in a configuration (s1|S, n|I, T, A), adds an arc from n to s1, pops n from input, pops s1 from the stack and moves it back to I, producing the configuration (S, s1|I, T, Au{(n, r, s1)}).",
        "At each step the parser uses classifiers trained on treebank data in order to predict which action to perform and which dependency label to assign given the current configuration."
      ]
    },
    {
      "heading": "4 Non-Projective Relations",
      "text": [
        "For handling non-projective relations, Nivre and Nilsson (2005) suggested applying a preprocessing step to a dependency parser, which consists in lifting non-projective arcs to their head repeatedly, until the tree becomes pseudo-projective.",
        "A post-processing step is then required to restore the arcs to the proper heads.",
        "I adopted a novel approach, which consists in adding six new parsing actions: Right2 in a configuration (s1|s2|S, n|I, T, A), adds an arc from s2 to n and removes s2 from the stack, producing the configuration (s1|S, n|I, T, Au{(s2, r, n)}).",
        "Left2 in a configuration (s1|s2|S, n|I, T, A), adds an arc from n to s2, pops n from input, pops s1 from the stack and moves it back to I, producing the configuration (s2|S, s1|I, T, Au{(n, r, s2)}).",
        "Right3 in a configuration (s1|s2|s3|S, n|I, T, A), adds an arc from s3 to n and removes s3 from the stack, producing the configuration (s1|s2|S, n|I, T, Au{(s3, r, n)}).",
        "Left3 in a configuration (s1|s2|s3|S, n|I, T, A), adds an arc from n to s3, pops n from input, pops s1 from the stack and moves it back to I, producing the configuration (s2|s3|S, s1|I, T, Au{(n, r, s3)}).",
        "Extract in a configuration (s1|s2|S, n|I, T, A), move s2 from the stack to the temporary stack, then Shift, producing the configuration (n|s1|S, I, s2|T, A).",
        "Insert in a configuration (S, I, s1|T, A), pops s1 from T and pushes it to the stack, producing the configuration (s1|S, I, T, A).",
        "The actions R i gh t2 and Le ft2 are sufficient to handle almost all cases of non-projectivity: for instance the training data for Czech contain 28081 non-projective relations, of which 26346 can be handled by Le ft2/ R i gh t2, 1683 by Left3/Right3 and just 52 require Extract/Insert.",
        "Here is an example of non-projectivity that can be handled with R i gh t2 (nejen --+ ale) and L e ft3 (fax --+ Vetšinu): Vetšinu techto prístroju lze take používat nejen jako fax, ale soucasne ... V6tšinu t6chto pfístroju lze take používat nejen jako fax , ale The remaining cases are handled with the last two actions: Extract is used to postpone the creation of a link, by saving the token in a temporary stack; Insert restores the token from the temporary stack and resumes normal processing.",
        "zou gemaakt moeten worden in This fragment in Dutch is dealt by performing an Extract in configuration (moeten|gemaakt|zou, worden|in, A) followed immediately by an Insert, leading to the following configuration, which can be handled by normal Shift/Reduce actions: zou moeten worden gemaakt in Another linguistic phenomenon is the anticipation of pronouns, like in this Portuguese fragment: Tudo a possive7 encontrar em o IX Sa7�o de Antiguidades, desde objectos de ouro e prata, moedas, ...",
        "The problem here is due to the pronoun Tudo (Anything), which is the object of encontrar (find), but which is also the head of desde (from) and its preceding comma.",
        "In order to be able to properly link desde to Tudo, it is necessary to postpone its processing; hence it is saved with Extract to the temporary stack and put back later in front of the comma with Insert.",
        "In fact the pair Extract/Insert behaves like a generalized R i gh tn/ Le ftn, when n is not known.",
        "As in the example, except for the case where n=2, it is difficult to predict the value of n, since there can be an arbitrary long sequence of tokens before reaching the position where the link can be inserted."
      ]
    },
    {
      "heading": "5 Performance",
      "text": [
        "I used my own C++ implementation of Maximum Entropy, which is very fast both in learning and classification.",
        "On a 2.8 MHz Pentium Xeon PC, the learning time is about 15 minutes for Portuguese and 4 hours for Czech.",
        "Parsing is also very fast, with an average throughput of 200 sentences per second: Table 1 reports parse time for parsing each whole test set.",
        "Using Memory Based Learning increases considerably the parsing time, while as expected learning time is quite shorter.",
        "On the other hand MBL achieves an improvement up to 5% in accuracy, as shown in detail in Table 1.",
        "1. three classifiers: one to decide between Shift/Reduce, one to decide which Reduce action and a third one to choose the dependency in case of Left/Right action",
        "For details on the CoNLL-X shared task and the measurements see (Buchholz, et al.",
        "2006)."
      ]
    },
    {
      "heading": "6 Experiments",
      "text": [
        "I performed several experiments to tune the parser.",
        "I also tried alternative machine learning algorithms, including SVM, Winnow, Voted Perceptron.",
        "The use of SVM turned out quite impractical since the technique does not scale to the size of training data involved: training an SVM with such a large number of features was impossible for any of the larger corpora.",
        "For smaller ones, e.g. Portuguese, training required over 4 days but produced a bad model which could not be used (I tried both the TinySVM (Kudo 2002) and the LIBSVM (Chang and Lin 2001) implementations).",
        "Given the speed of the Maximum Entropy classifier, I explored whether increasing the number of features could improve accuracy.",
        "I experimented adding various features controlled by the parameters above: none appeared to be effective, except the addition of the previous action.",
        "The classifier returns both the action and the label to be assigned.",
        "Some experiments were carried out splitting the task among several specialized classifiers.",
        "I experimented with: 2. two classifiers: one to decide which action to perform and a second one to choose the dependency in case of Left/Right action None of these variants produced improvements in precision.",
        "Only a small improvement in labeled attachment score was noticed using the full, non-specialized classifier to decide the action but discarding its suggestion for label and using a specialized classifier for labeling.",
        "However this was combined with a slight decrease in unlabeled attachment score, hence it was not considered worth the effort."
      ]
    },
    {
      "heading": "7 Error Analysis",
      "text": [
        "The parser does not attempt to assign a dependency relation to the root.",
        "A simple correction of assigning a default value for each language gave an improvement in the LAS as shown in Table 1."
      ]
    },
    {
      "heading": "7.1 Portuguese",
      "text": [
        "Out of the 45 dependency relations that the parser had to assign to a sentence, the largest number of",
        "errors occurred assigning N<PRED (62), ACC (46), PIV (43), CJT (40), N< (34), P< (30).",
        "The highest number of head error occurred at the CPOS tags PRP with 193 and V with 176.",
        "In particular just four prepositions (em, de, a, para) accounted for 120 head errors.",
        "Most of the errors occur near punctuations.",
        "Often this is due to the fact that commas introduce relative phrases or parenthetical phrases (e.g. “o suspe i to, de 38 anos, que traba 7ha”), that produce diversions in the flow.",
        "Since the parser makes decisions analyzing only a window of tokens of a limited size, it gets confused in creating attachments.",
        "I tried to add some global context features, to be able to distinguish these cases, in particular, a count of the number of punctuation marks seen so far, whether punctuation is present between the focus words.",
        "None of them helped improving precision and were not used in the submitted runs."
      ]
    },
    {
      "heading": "7.2 Czech",
      "text": [
        "Most current parsers for Czech do not perform well on Apos (apposition), Coord (coordination) and ExD (ellipses), but they are not very frequent.",
        "The largest number of errors occur on Obj (166), Adv (155), Sb (113), Atr (98).",
        "There is also often confusion among these: 33 times Obj instead of Adv, 32 Sb instead of Obj, 28 Atr instead of Adv.",
        "The high error rate of J (adjective) is expected, mainly due to coordination problems.",
        "The error of R (preposition) is also relatively high.",
        "Prepositions are problematic, but their error rate is higher than expected since they are, in terms of surface order, rather regular and close to the noun.",
        "It could be that the decision by the PDT to hang them as heads instead of children, causes a problem in attaching them.",
        "It seems that a post-processing may correct a significant portion of these errors.",
        "The labels ending with _Co, _Ap or _Pa are nodes who are members of the Coordination, Apposition or the Parenthetical relation, so it may be worth while omitting these suffixes in learning and restore them by post-processing.",
        "An experiment using as training corpus a subset consisting of just sentences which include non-projective relations achieved a LAS of 65.28 % and UAS of 76.20 %, using MBL.",
        "Acknowledgments.",
        "Kiril Ribarov provided insightful comments on the results for Czech.",
        "The following treebanks were used for training the parser: (Afonso et al., 2002; Atalay et al., 2003; Böhmovà et al., 2003; Brants et al., 2002; Chen et al., 2003; Civit Torruella and Martì Antonìn, 2002; Džeroski et al., 2006; Hajiç et al., 2004; Kawata and Bartels, 2000; Kromann, 2003; Nilsson et al., 2005; Oflazer et al., 2003; Simov et al., 2005; van der Beek et al., 2002)."
      ]
    }
  ]
}
