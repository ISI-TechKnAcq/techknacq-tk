{
  "info": {
    "authors": [
      "Enrique Alfonseca",
      "Maria Ruiz-Casado",
      "Manabu Okumura",
      "Pablo Castells"
    ],
    "book": "Workshop on Ontology Learning and Population: Bridging the Gap Between Text and Knowledge",
    "id": "acl-W06-0507",
    "title": "Towards Large-Scale Non-Taxonomic Relation Extraction: Estimating the Precision of Rote Extractors",
    "url": "https://aclweb.org/anthology/W06-0507",
    "year": 2006
  },
  "references": [
    "acl-C92-2082",
    "acl-H05-1091",
    "acl-N03-1011",
    "acl-P02-1006",
    "acl-P05-1052",
    "acl-P05-1060",
    "acl-P06-2002",
    "acl-P99-1008",
    "acl-P99-1050",
    "acl-W03-0405",
    "acl-W04-3206",
    "acl-W98-1106"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we describe a rote extractor that learns patterns for finding semantic relations in unrestricted text, with new procedures for pattern generalisation and scoring.",
        "An improved method for estimating the precision of the extracted patterns is presented.",
        "We show that our method approximates the precision values as evaluated by hand much better than the procedure traditionally used in rote extractors."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "With the large growth of the information stored in the web, it is necessary to have available automatic or semi-automatic tools so as to be able to process all this web content.",
        "Therefore, a large effort has been invested in developing automatic or semiautomatic techniques for locating and annotating patterns and implicit information from the web, a task known as Web Mining.",
        "In the particular case of web content mining, the aim is automatically mining data from textual web documents that can be represented with machine-readable semantic formalisms such as ontologies and semantic-web languages.",
        "Recently, there is an increasing interest in automatically extracting structured information from large corpora and, in particular, from the Web (Craven et al., 1999).",
        "Because of the characteristics of the web, it is necessary to develop efficient algorithms able to learn from unannotated data (Riloff and Schmelzenbach, 1998; Soderland, 1999; Mann and Yarowsky, 2005).",
        "New types of web content such as blogs and wikis, are also a *This work has been sponsored by MEC, project number TIN-2005-06885.",
        "source of textual information that contain an underlying structure from which specialist systems can benefit.",
        "Consequently, rote extractors (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002) have been identified as an appropriate method to look for textual contexts that happen to convey a certain relation between two concepts.",
        "In this paper, we describe a new procedure for estimating the precision of the patterns learnt by a rote extractor, and how it compares to previous approaches.",
        "The solution proposed opens new possibilities for improving the precision of the generated patterns, as described below.",
        "This paper is structured as follows: Section 2 describe related work; Section 3 and 4 describe the proposed procedure and its evaluation, and Section 5 presents the conclusions and future work."
      ]
    },
    {
      "heading": "2 Related work",
      "text": [
        "Extracting information using Machine Learning algorithms has received much attention since the nineties, mainly motivated by the Message Understanding Conferences.",
        "From the mid-nineties, there are systems that learn extraction patterns from partially annotated and unannotated data (Huffman, 1995; Riloff, 1996; Riloff and Schmelzenbach, 1998; Soderland, 1999).",
        "Generalising textual patterns (both manually and automatically) for the identification of relations has been proposed since the early nineties (Hearst, 1992), and it has been applied to extending ontologies with hyperonymy and holonymy relations (Morin and Jacquemin, 1999; Kietz et al., 2000; Cimiano et al., 2004; Berland and Char-niak, 1999).",
        "Finkelstein-Landau and Morin (1999) learn patterns for company merging relations with exceedingly good accuracies.",
        "Recently, kernel",
        "Sydney, July 2006. c�2006 Association for Computational Linguistics methods are also becoming widely used for relation extraction (Bunescu and Mooney, 2005; Zhao and Grishman, 2005).",
        "Concerning rote extractors from the web, they have the advantage that the training corpora can be collected easily and automatically, so they are useful in discovering many different relations from text.",
        "Several similar approaches have been proposed (Brin, 1998; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002), with various applications: Question-Answering (Ravichandran and Hovy, 2002), multi-document Named Entity Coreference (Mann and Yarowsky, 2003), and generating biographical information (Mann and Yarowsky, 2005).",
        "Szpektor et al.",
        "(2004) applies a similar, with no seed lists, to extract automatically entailment relationships between verbs, and Etzioni et al.",
        "(2005) report very good results extracting Named Entities and relationships from the web."
      ]
    },
    {
      "heading": "2.1 Rote extractors",
      "text": [
        "Rote extractors (Mann and Yarowsky, 2005) estimate the probability of a relation r(p, q) given the surrounding context A1pA2qA3.",
        "This is calculated, with a training corpus T, as the number of times that two related elements r(x, y) from T appear with that same context A1 xA2 yA3, divided by the total number of times that x appears in that context together with any other word:",
        "x is called the hook, and y the target.",
        "In order to train a Rote extractor from the web, this procedure is mostly used (Ravichandran and Hovy, 2002):",
        "1.",
        "Select a pair of related elements to be used as seed.",
        "For instance, (Dickens, 1812) for the relation birth year.",
        "2.",
        "Submit the query Dickens AND 1812 to a search engine, and download a number of documents to build the training corpus.",
        "3.",
        "Keep all the sentences containing both elements.",
        "4.",
        "Extract the set of contexts between them and identify repeated patterns.",
        "This may just be the m characters to the left or to the right (Brin, 1998), the longest common substring of several contexts (Agichtein and Gravano, 2000), or all substrings obtained with a suffix tree constructor (Ravichandran and Hovy, 2002).",
        "5.",
        "Download a separate corpus, called hook corpus, containing just the hook (in the example, Dickens).",
        "6.",
        "Apply the previous patterns to the hook corpus, calculate the precision of each pattern in the following way: the number of times it identifies a target related to the hook divided by the total number of times the pattern appears.",
        "7.",
        "Repeat the procedure for other examples of the same relation.",
        "To illustrate this process, let us suppose that we want to learn patterns to identify birth years.",
        "We may start with the pair (Dickens, 1812).",
        "From the downloaded corpus, we extract sentences such as",
        "The system identifies that the contexts of the last two sentences are very similar and chooses their longest common substring to produce the following patterns:",
        "The rote extractor needs to estimate automatically the precision of the extracted patterns, in order to keep the best ones.",
        "So as to measure these precision values, a hook corpus is now downloaded using the hook Dickens as the only query word, and the system looks for appearances of the patterns in this corpus.",
        "For every occurrence in which the hook of the relation is Dickens, if the target is 1812 it will be deemed correct, and otherwise it will be deemed incorrect (e.g. in Dickens was born in Portsmouth)."
      ]
    },
    {
      "heading": "3 Our proposal",
      "text": []
    },
    {
      "heading": "3.1 Motivation",
      "text": [
        "In a rote extractor as described above, we believe that the procedure for calculating the precision of the patterns may be unreliable in some cases.",
        "For example, the following patterns are reported by Ravichandran and Hovy (2002) for identifying the relations Inventor, Discoverer and Location:",
        "In the particular application in which they are used (relation extraction for Question Answering), they are useful because there is initially a question to be answered that indicates whether we are",
        "looking for an invention, a discovery or a location.",
        "However, if we want to apply them to unrestricted relation extraction, we have the problem that the same pattern, the genitive construction, represents all these relations, apart from the most common use indicating possession.",
        "If patterns like these are so ambiguous, then why do they receive so high a precision estimate?",
        "One reason is that the patterns are only evaluated for the same hook for which they were extracted.",
        "To illustrate this with an example, let us suppose that we obtain a pattern for the relation located-at using the pairs (New York, Chrysler Building).",
        "The genitive construction can be extracted from the context New York’s Chrysler Building.",
        "Afterwards, when estimating the precision of this pattern, only sentences containing <target>’s Chrysler Building are taken into account.",
        "Because of this, most of the pairs extracted by this pattern may extract the target New York, apart from a few that extract the name of the architect that built it, van Allen.",
        "Thus we can expect that the genitive pattern will receive a high precision estimate as a located-at pattern.",
        "For our purposes, however, we want to collect patterns for several relations such as writer-book, painter-picture, director - lm, actor - lm, and we want to make sure that the obtained patterns are only applicable to the desired relation.",
        "Patterns like <target> ’s <hook> are very likely to be applicable to all of these relations at the same time, so we would like to be able to discard them automatically by assigning them a low precision."
      ]
    },
    {
      "heading": "3.2 Suggested improvements",
      "text": [
        "Therefore, we propose the following three improvements to this procedure:",
        "1.",
        "Collecting not only a hook corpus but also a target corpus should help in calculating the precision.",
        "In the example of the Chrysler building, we have seen that in most cases that we look for the pattern ‘s Chrysler building the previous words are New York, and so the pattern is considered accurate.",
        "However, if we look for the pattern New York’s, we shall surely find it followed by many different terms representing different relations, and the precision estimate will decrease.",
        "2.",
        "Testing the patterns obtained for one relation using the hook and target corpora collected for other relations.",
        "For instance, if the geni",
        "tive construction has been extracted as a possible pattern for the writer-book relation, and we apply it to a corpus about painters, the rote extractor can detect that it also extracts pairs with painters and paintings, so that particular pattern will not be very precise for that relation.",
        "3.",
        "Many of the pairs extracted by the patterns in the hook corpora were not evaluated at all when the hook in the extracted pair was not present in the seed lists.",
        "To overcome this, we propose to use the web to check whether the extracted pair might be correct, as shown below."
      ]
    },
    {
      "heading": "3.3 Algorithm",
      "text": [
        "In our implementation, the rote extractor starts with a table containing some information about the relations for which we want to learn patterns.",
        "This procedure needs a little more information than just the seed list, which is provided as a table in the format displayed in Table 1.",
        "The data provided for each relation is the following: (a) The name of the relation, used for naming the output files containing the patterns; (b) the name of the file containing the seed list; (c) the cardinality of the relation.",
        "For instance, given that many people can be born on the same year, but for every person there is just one birth year, the cardinality of the relation birth year is n:1; (d) the restrictions on the hook and the target.",
        "These can be of the following three categories: unrestricted, if the pattern can extract any sequence of words as hook or target of the relation, Entity, if the pattern can extract as hook or target only things of the same entity type as the words in the seed list (as annotated by the NERC module), or PoS, if the pattern can extract as hook or target any sequence of words whose sequence of PoS labels was seen in the training corpus; and (e) a sequence of queries that could be used to check, using the web, whether an extracted pair is correct or not.",
        "We assume that the system has used the seed list to extract and generalise a set of patterns for each of the relations using training corpora (Ravichandran and Hovy, 2002; Alfonseca et al., 2006a).",
        "Our procedure for calculating the patterns’ precisions is as follows:",
        "1.",
        "For every relation, (a) For every hook, collect a hook corpus from the web.",
        "(b) For every target, collect a target corpus from the web.",
        "2.",
        "For every relation r, (a) For every pattern P, collected during training, apply it to every hook and target corpora to extract a set of pairs.",
        "For every pair p = (ph, pt), • If it appears in the seed list of r, consider it correct.",
        "• If it appears in the seed list of other relation, consider it incorrect.",
        "• If the hook ph appears in the seed list of r with a different target, and the cardinality is 1:1 or n:1, consider it incorrect.",
        "• If the target pt appears in r’s seed list with a different hook, and the cardinality is 1:1 or 1:n, incorrect.",
        "• Otherwise, the seed list does not provide enough information to evaluate p, so we perform a test on the web.",
        "For every query provided for r, the system replaces $1 with ph and $2 with pt, and sends the query to Google.",
        "The pair is deemed correct if and only if there is at least one answer.",
        "The precision of P is estimated as the number of extracted pairs that are supposedly correct divided by the total number of pairs extracted.",
        "In this step, every pattern that did not apply at least twice in the hook and target corpora is also discarded."
      ]
    },
    {
      "heading": "3.4 Example",
      "text": [
        "After collecting and generalising patterns for the relation director -film, we apply each pattern to the hook and target corpora collected for every relation.",
        "Let us suppose that we want to estimate the precision of the pattern <target> ’s <hook> and we apply it to the hook and the target corpora for this relation and for author-book.",
        "Possible pairs extracted are (Woody Allen, Bananas), (Woody Allen, Without Fears), (Charles Dickens, A Christmas Carol).",
        "Only the first one is correct.",
        "The rote extractor proceeds as follows:",
        "• The first pair appears in the seed list, so it is considered correct.",
        "• Although Woody Allen appears as hook in the seed list and Without Fears does not appear as target, the second pair is still not considered incorrect because the directed-by relation has n:n cardinality.",
        "• The third pair appears in the seed list for writer-book, so it is directly marked as incorrect.",
        "• Finally, because still the system has not made a decision about the second pair, it queries Google with the sequences",
        "Woody Allen directed Without Fears Without Fears directed by Woody Allen Because neither of those queries provide any answer, it is considered incorrect.",
        "In this way, it can be expected that the patterns that are equally applicable to several relations, such as writer-book, director film or painter-picture will attain a low precision because they will extract many incorrect relations from the corpora corresponding to the other relations."
      ]
    },
    {
      "heading": "4 Experiment and results",
      "text": []
    },
    {
      "heading": "4.1 Rote extractor settings",
      "text": [
        "The initial steps of the rote extractor follows the general approach: downloading a training corpus using the seed list and extracting patterns.",
        "The training corpora are processed with a part-of-speech tagger and a module for Named Entity Recognition and Classification (NERC) that annotates people, organisations, locations, dates, relative temporal expressions and numbers (Alfonseca et al., 2006b), so this information can be included in the patterns.",
        "Furthermore, for each of the terms in a pair in the training corpora, the system also",
        "stores in a separate file the way in which they are annotated in the training corpus: the sequences of part-of-speech tags of every appearance, and the entity type (if marked as such).",
        "So, for instance, typical PoS sequences for names of authors are “NNP”1 (surname) and “NNP NNP” (first name and surname).",
        "A typical entity kind for an author is person.",
        "In the case that a pair from the seed list is found in a sentence, a context around the two words in the pair is extracted, including (a) at most five words to the left of the first word; (b) all the words in between the pair words; (c) at most five words to the right of the second word.",
        "The context never jumps over sentence boundaries, which are marked with the symbols BOS (Beginning of sentence) and EOS (End of sentence).",
        "The two related concepts are marked as <hook> and <target>.",
        "Figure 1 shows several example contexts extracted for the relations birth year, birth place, writer-book and country-capital city.",
        "The approach followed for the generalisation is the one described by (Alfonseca et al., 2006a; Ruiz-Casado et al., in press), which has a few modifications with respect to Ravichandran and Hovy (2002)’s, such as the use of the wildcard * to represent any sequence of words, and the addition of part-of-speech and Named Entity labels to the patterns.",
        "The input table has been built with the following nineteen relations: birth year, death year, birth place, death place, author–book, actor– film, director–film, painter–painting, Employee– organisation, chief of state, soccer player–team,",
        "soccer team-city, soccer team-manager, country or region–capital city, country or region–area, country or region–population, country–bordering country, country-name of inhabitant (e.g. Spain-Spaniard), and country-continent.",
        "The time required to build the table and the seed lists was less than one person-day, as some of the seed lists were directly collected from web pages.",
        "For each step, the following settings have been set:",
        "• The size of the training corpus has been set to 50 documents for each pair in the original seed lists.",
        "Given that the typical sizes of the lists collected are between 50 and 300 pairs, this means that several thousand documents are downloaded for each relation.",
        "• Before the generalisation step, the rote extractor discards those patterns in which the hook and the target are too far away to each other, because they are usually difficult to generalise.",
        "The maximum allowed distance",
        "procedure and with the traditional hook corpus approach, and precision evaluated by hand).",
        "between them has been set to 8 words.",
        "• At each step, the two most similar patterns are generalised, and their generalisation is added to the set of patterns.",
        "No pattern is discarded at this step.",
        "This process stops when all the patterns resulting from the generalisation of existing ones contain wildcards adjacent to either the hook or the target.",
        "• For the precision estimation, for each pair in the seed lists, 50 documents are collected for the hook and other 50 for the target.",
        "Because of time constraints, and given that the total size of the hook and the target corpora exceeds 100,000 documents, for each pattern a sample of 250 documents is randomly chosen and the patterns are applied to it.",
        "This sample is built randomly but with the following constraints: there should be an equal amount of documents selected from the corpora from each relationship; and there should be an equal amount of documents from hook corpora and from target corpora.",
        "4.2 Output obtained",
        "Table 2 shows the number of patterns obtained for each relation.",
        "Note that the generalisation procedure applied produces new (generalised) patterns to the set of original patterns, but no original pattern is removed, so they all are evaluated; this is why the set of patterns increases after the generalisation.",
        "The filtering criterion was to keep the patterns that applied at least twice on the test corpus.",
        "It is interesting to see that for most relations the reduction of the pruning is very drastic.",
        "This is because of two reasons: Firstly, most patterns are far too specific, as they include up to 5 words at each side of the hook and the target, and all the words in between.",
        "Only those patterns that have generalised very much, substituting large portions with wildcards or disjunctions are likely to apply to the sentences in the hook and target corpora.",
        "Secondly, the samples of the hook and target corpora used are too small for some of the relations to apply, so few patterns apply more than twice.",
        "Note that, for some relations, the output of the generalisation step contains less patterns that the output of the initial extraction step: that is due to the fact that the patterns in which the hook and the target are not nearby were removed in between these two steps.",
        "Concerning the precision estimates, a full evaluation is provided for the birth-year relation.",
        "Table 3 shows in detail the thirty patterns obtained.",
        "It can also be seen that some of the patterns with good precision contain the wildcard *.",
        "For instance, the first pattern indicates that the presence of any of the words biography, poetry, etc.",
        "anywhere in a sentence before a person name and a date or number between parenthesis is a strong indication that the target is a birth year.",
        "The last columns in the table indicate the number of times that each rule applied in the hook and target corpora, and the precision of the rule in each of the following cases: • As estimated by the complete program (Prec1).",
        "• As estimated by the traditional hook corpus approach (Prec2).",
        "Here, cardinality is not taken into account, patterns are evaluated only on the hook corpora from the same relation, and those pairs whose hook is not in the seed list are ignored.",
        "• The real precision of the rule (real).",
        "In order to obtain this metric, two different annotators evaluated the pairs applied independently, and the precision was estimated from the pairs in which they agreed (there was a 96.29% agreement, Kappa=0.926).",
        "As can be seen, in most of the cases our procedure produces lower precision estimates.",
        "If we calculate the total precision of all the rules altogether, shown in the last row of the table, we can see that, without the modifications, the whole set of rules would be considered to have a total precision of 0.84, while that estimate decreases sharply to 0.46 when they are used.",
        "This value is nearer the precision of 0.54 evaluated by hand.",
        "Although it may seem surprising that the precision estimated by the new procedure is even lower than the real precision of the patterns, as measured by hand, that is due to the fact that the web queries consider unknown pairs as incorrect unless they",
        "appear in the web exactly in the format of the query in the input table.",
        "Specially for not very well-known people, we cannot expect that all of them will appear in the web following the pattern “X was born in date”, so the web estimates tend to be over-conservative.",
        "Table 4 shows the precision estimates for every pair extracted with all the rules using both procedures, with 0.95 confidence intervals.",
        "The real precision has been estimating by sampling randomly 200 pairs and evaluating them by hand, as explained above for the birth year relation.",
        "As can be observed, out of the 19 relations, the precision estimate of the whole set of rules for 11 of them is not statistically dissimilar to the real precision, while that only holds for two relationships using the previous approach.",
        "Please note as well that the precisions indicated in the table refer to all the pairs extracted by all the rules, some of which are very precise, but some of which are very imprecise.",
        "If the rules are to be applied in an annotation system, only those with a high precision estimate would be used, and expectedly much better overall results would be obtained."
      ]
    },
    {
      "heading": "5 Conclusions and future work",
      "text": [
        "We have described here a new procedure for estimating the precision of the patterns learnt by a rote extractor that learns from the web.",
        "Compared to other similar approaches, it has the following improvements: • For each pair (hook,target) in the seed list, a target corpora is also collected (apart from the hook corpora), and the evaluation is performed using corpora from several relations.",
        "This has been observed to improve the estimate of the rule’s precision, given that the evaluation pairs not only refer to the elements in the seed list.",
        "• The cardinality of the relations is taken into consideration in the estimation process using the seed list.",
        "This is important, for instance, to be able to estimate the precision in n:n relations like author-work, given that we cannot assume that the only books written by someone are those in the seed list.",
        "• For those pairs that cannot be evaluated using the seed list, a simple query to the Google search engine is employed.",
        "The precisions estimated with this procedure are significantly lower than the precisions obtained with the usual hook corpus approach, specially for ambiguous patterns, and much near the precision estimate when evaluated by hand.",
        "Concerning future work, we plan to estimate the precision of the patterns using the whole hook and target corpora, rather than using a random sample.",
        "A second objective we have in mind is not to throw away the ambiguous patterns with low precision (e.g. the possessive construction), but to train a model so that we can disambiguate which is the relation they are conveying in each context (Girju et al., 2003)."
      ]
    }
  ]
}
