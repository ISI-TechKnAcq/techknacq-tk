{
  "info": {
    "authors": [
      "Anne Li-E Liu",
      "David Wible",
      "Nai-Lung Tsao"
    ],
    "book": "Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications",
    "id": "acl-W09-2107",
    "title": "Automated Suggestions for Miscollocations",
    "url": "https://aclweb.org/anthology/W09-2107",
    "year": 2009
  },
  "references": [
    "acl-I08-1059",
    "acl-W07-1604"
  ],
  "sections": [
    {
      "text": [
        "Anne Li-E Liu Research Centre for English and Applied Linguistics University of Cambridge Cambridge, CB3 9DP,",
        "One of the most common and persistent error types in second language writing is collocation errors, such as learn knowledge instead of gain or acquire knowledge, or make damage rather than cause damage.",
        "In this work-in-progress report, we propose a probabilistic model for suggesting corrections to lexical collocation errors.",
        "The probabilistic model incorporates three features: word association strength (MI), semantic similarity (via Word-Net) and the notion of shared collocations (or intercollocability).",
        "The results suggest that the combination of all three features outperforms any single feature or any combination of two features."
      ]
    },
    {
      "heading": "1. Collocation in Language Learning",
      "text": [
        "The importance and difficulty of collocations for second language users has been widely acknowledged and various sources of the difficulty put forth (Granger 1998, Nesselhauf 2004, Howarth 1998, Liu 2002, inter alia).",
        "Liu's study of a 4-million-word learner corpus reveals that verb-noun (VN) miscollocations make up the bulk of the lexical collocation errors in learners' essays.",
        "Our study focuses, therefore, on VN miscollocation correction."
      ]
    },
    {
      "heading": "2. Error Detection and Correction in NLP",
      "text": [
        "Error detection and correction have been two major issues in NLP research in the past decade.",
        "Projects involving learner corpora in analyzing and categorizing learner errors include NICT Japanese Learners of English (JLE), the Chinese Learners of",
        "English Corpus (Gamon et al., 2008) and English Taiwan Learner Corpus (or TLC) (Wible et al., 2003).",
        "Studies that focus on providing automatic correction, however, mainly deal with errors that derive from closed-class words, such as articles (Han et al., 2004) and prepositions (Chodorow et al., 2007).",
        "One goal of this work-in-progress is to address the less studied issue of open class lexical errors, specifically lexical collocation errors."
      ]
    },
    {
      "heading": "3. The Present Study",
      "text": [
        "We focus on providing correct collocation suggestions for lexical miscollocations.",
        "Three features are employed to identify the correct collocation substitute for a miscollocation: word association measurement, semantic similarity between the correction candidate and the misused word to be replaced, and intercollocability (i.e., the concept of shared collocates in collocation clusters proposed by Cowie and Howarth, 1995).",
        "NLP research on learner errors includes work on error detection and error correction.",
        "While we are working on both, here we report specifically on our work on lexical miscollocation correction."
      ]
    },
    {
      "heading": "4. Method",
      "text": [
        "We incorporate both linguistic and computational perspectives in our approach.",
        "84 VN miscol-locations from Liu's (2002) study were employed as the training and the testing data in that each comprised 42 randomly chosen miscollocations.",
        "Two experienced English teachers manually went through the 84 miscollocations and provided a list of correction suggestions.",
        "Only when the system output matches to any of the suggestions offered by the two annotators would the data be included in the result.",
        "The two main knowledge resources that we incorporated are British National Corpus and WordNet (Miller, 1990).",
        "BNC was utilized to measure word association strength and to extract shared collocates while WordNet was used in determining semantic similarity.",
        "Our probabilistic model that combines the features is described in subsection 4.4.",
        "Note that all the 84 VN miscollo-cations are combination of incorrect verbs and focal nouns, our approach is therefore aimed to find the correct verb replacements.",
        "The role of word association in miscollocation suggestions are twofold: 1. all suggested correct collocations in any case have to be identified as collocations; thus, we assume candidate replacements for the miscollocate verbs must exceed a threshold word association strength with the focal noun; 2. we examine the possibility that the higher the word association score the more likely it is to be a correct substitute for the wrong collocate.",
        "We adopt Mutual Information (Church et al.",
        "1991) as our association measurement.",
        "Both Gitsaki et al.",
        "(2000) and Liu (2002) suggest a semantic relation holds between a miscollo-cate and its correct counterpart.",
        "Following this, we assume that in the 84 miscollocations, the miscol-locates should stand in more or less a semantic relation with the corrections.",
        "For example, say in an attested learner miscollocation say story is found to be a synonym of the correct verb tell in WordNet.",
        "Based on this assumption, words that show some degree of semantic similarity with the miscollocate are considered possible candidates for replacing it.",
        "To measure similarity we take the synsets of WordNet to be nodes in a graph.",
        "We quantify the semantic similarity of the incorrect verb in a mis-collocation with other possible substitute verbs by measuring graph-theoretic distance between the synset containing the miscollocate verb and the synset containing candidate substitutes.",
        "In cases of polysemy, we take the closest synsets for the distance measure.",
        "If the miscollocate and the candidate substitute occur in the same synset, then the distance between them is zero.",
        "The similarity measurement function is as follows (Tsao et al., 2003):",
        "dis(si, s j)",
        ",where dis(st, sj) means the node path length between the synset si and s j in WordNet hyper/hypo tree.",
        "Ls means the level number of s in hyper/hypo tree and the level of top node is 1.",
        "Multiplying max(Ls , Ls ) by 2 ensures the similarity is less than 1.",
        "If si and sj are synonymous, the similarity will be 1.",
        "Futagi et al. (2008) review several studies which adopt computational approaches in tackling collocation errors; yet none of them, including Futagi et al., include the notion of collocation cluster.",
        "We borrow the cluster idea from Cowie & Howarth (1995) who propose 'overlapping cluster' to denote sets of collocations that carry similar meaning and shared collocates.",
        "Figure 1 represents a collocation cluster that expresses the concept of 'bringing something into actuality.'",
        "The key here is that not all VN combinations in Figure 1 are acceptable.",
        "While fulfill and achieve collocate with the four nouns on the right, realize does not collocate with purpose, as indicated by the dotted line.",
        "Cowie and Howarth's point is that collocations that can be clustered via overlapping collocates can be the source of collocation errors for language learners.",
        "That both fulfill and reach collocate with goal and the further collocability of fulfill with ambition and purpose plausibly lead learners to assume that reach shares this collocability as well, leading by overgeneralization to the miscollocations reach an ambition or reach a purpose.",
        "fulfill",
        "dream",
        "achieve T^-_r_ - -:",
        "• ,«*\" ^^F?",
        "ambition",
        "realize r~-* »-",
        "We employ the ideas of 'collocation cluster' and 'shared collocates' in identifying correct counterparts to the miscollocations.",
        "Specifically, taking the miscollocation reach their purpose as a starting point, our system generates a collocation cluster by finding the verbs that collocate with purpose and nouns that reach collocates with.",
        "We consider this formed cluster the source that contains the possible correct replacement for reach in reach their purpose.",
        "By finding verbs that not only collocate with purpose but also share the most other collocating nouns with the wrong verb reach, successfully, we identified candidate substitutes fulfill and achieve for the incorrect verb reach.",
        "The three features we described above are integrated into a probabilistic model.",
        "Each feature is used to look up the correct collocation suggestion for a miscollocation.",
        "For instance, cause damage, one of the possible suggestions for the miscollocation make damage, is found to be ranked the 5th correction candidate by using word association measurement merely, the 2nd by semantic similarity and the 14th by using shared collocates.",
        "If we combine the three features, however, cause damage is ranked first.",
        "The conditional probability of the case where the candidate is a correct one can be presented as:",
        "P (c is a correct verb F ) where c means a candidate for a specific miscollo-cation and FCi m means the features values between m (misused words) and c (candidates).",
        "According to Bayes theorem and Bayes assumption, which assume that these features are independent, the probability can be computed by:",
        "where Sc means the situation 'c is a correct verb', as described above and f is one of the three particular features.",
        "We use probability values to choose and rank the K-best suggestions.",
        "Experimental Results",
        "Any found VN combination via our probabilistic approach was compared to the suggestions made by the two human experts.",
        "A match would be counted as a true positive.",
        "A discrete probability distribution is produced for each feature.",
        "We divided feature value into five levels and obtained prior predicting value for each level of the three features.",
        "For example, we divided MI value to five levels (<1.5, 1.5~3.0, 3.0~4.5, 4.5~6, >6).",
        "The five ranks for semantic similarity and normalized shared collocates number are 0.0~0.2, 0.2~0.4, 0.4~0.6, 0.6~0.8 and 0.8 ~1.0.",
        "For every feature, we obtain a predicting value for each level after the training process.",
        "The predicting value is shown",
        "In line with that, P(MI>6) means the probability of all VN collocations retrieved from BNC in which the MI value is higher than 6 whereas P(MI>6\\SC) shows the probability of all correct VN collocations with the MI value higher than 6.",
        "Different combinations of the three features are made on the basis of the probabilistic model described in Section 4.4.",
        "Seven models derive from such combinations (See Table 1).",
        "Table 2 shows the precision of k-best suggestions for each model.",
        "Models_Feature(s) considered_",
        "M 1 MI (Mutual Information)",
        "M 2 SS (Semantic Similarity)",
        "M 3 SC (Shared Collocates)",
        "rate of Model 1 7.",
        "knowledge.",
        "K-Best suggestions for get",
        "K-Best",
        "Ml",
        "M2",
        "M3",
        "M4",
        "M5",
        "M6",
        "M7",
        "1",
        "16.67",
        "4G.48",
        "22.62",
        "48.81",
        "29.76",
        "55.95",
        "53.57",
        "2",
        "36.9G",
        "53.57",
        "38.1G",
        "6G.71",
        "44.G5",
        "63.1",
        "67.86",
        "3",
        "47.62",
        "64.29",
        "5G.GG",
        "71.43",
        "59.52",
        "77.38",
        "78.57",
        "4",
        "52.38",
        "67.86",
        "63.1G",
        "77.38",
        "72.62",
        "8G.95",
        "82.14",
        "5",
        "64.29",
        "75.GG",
        "72.62",
        "83.33",
        "78.57",
        "83.33",
        "85.71",
        "6",
        "65.48",
        "77.38",
        "75.GG",
        "85.71",
        "83.33",
        "84.52",
        "88.1G",
        "7",
        "67.86",
        "8G.95",
        "77.38",
        "86.9G",
        "86.9G",
        "86.9",
        "89.29",
        "8",
        "7G.24",
        "83.33",
        "82.14",
        "86.9G",
        "89.29",
        "88.1",
        "91.67",
        "9",
        "72.62",
        "86.9G",
        "85.71",
        "88.1G",
        "92.86",
        "9G.48",
        "92.86",
        "1G",
        "76.19",
        "86.9G",
        "88.1G",
        "88.1G",
        "94.G5",
        "9G.48",
        "94.G5",
        "K-Best",
        "M2",
        "M6",
        "M7",
        "1",
        "aim",
        "*obtain",
        "*acquire",
        "2",
        "generate",
        "share",
        "share",
        "3",
        "draw",
        "*develop",
        "*obtain",
        "4",
        "*obtain",
        "generate",
        "*develop",
        "5",
        "*develop",
        "*acquire",
        "*gain",
        "Table 2 shows that, considering the results for each feature run separately (M1-M3), the feature 'semantic similarity' (M2) outperforms the other two.",
        "Among combined feature models (M4-M7), M7 (MI + SS+ SC), provides the highest proportion of true positives at every value of k except k = 1.",
        "The full hybrid of all three features (M7) outperforms any single feature.",
        "The best results are achieved when taking into account both statistical and semantic features.",
        "This is illustrated with results for the example get knowledge in Table 3 (the asterisks (*) indicate the true positives.)"
      ]
    },
    {
      "heading": "6. Conclusion",
      "text": [
        "In this report of work in progress, we present a probabilistic model that adopts word association measurement, semantic similarity and shared collocates in looking for corrections for learners' mis-collocations.",
        "Although only VN miscollocations are examined, the model is designed to be applicable to other types of miscollocations.",
        "Applying such mechanisms to other types of miscollocations as well as detecting miscollocations will be the next steps of this research.",
        "Further, a larger amount of miscollocations should be included in order to verify our approach and to address the issue of the small drop of the full-hybrid M7 at k=1."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": []
    }
  ]
}
