{
  "info": {
    "authors": [
      "Hiroki Imai",
      "Hozumi Tanaka"
    ],
    "book": "Workshop on New Methods in Language Processing and Computational Natural Language Learning",
    "id": "acl-W98-1227",
    "title": "A Method of Incorporating Bigram Constraints into an LR Table and Its Effectiveness in Natural Language Processing",
    "url": "https://aclweb.org/anthology/W98-1227",
    "year": 1998
  },
  "references": [
    "acl-J93-1002",
    "acl-J95-3002"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we propose a method for constructing bigram LR tables by way of incorporating bigram constraints into an LR table.",
        "Using a bigram LR table, it is possible for a GLR parser to make use of both bigram and CFG constraints in natural language processing.",
        "Applying bigram LR tables to our GLR method has the following advantages:",
        "(1) Language models utilizing bigram LR tables have lower perplexity than simple bigram language models, since local constraints (bigram) and global constraints (CFG) are combined in a single bigram LR table.",
        "(2) Bigram constraints are easily acquired from a given corpus.",
        "Therefore data sparseness is not likely to arise.",
        "(3) Separation of local and global constraints keeps down the number of CFG rules.",
        "The first advantage leads to a reduction in complexity, and as the result, better performance in GLR parsing.",
        "Our experiments demonstrate the effectiveness of our method."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In natural language processing, stochastic language models are commonly used for lexical and syntactic disambiguation (Fujisaki et al., 1991; Franz, 1996).",
        "Stochastic language models are also helpful in reducing the complexity of speech and language processing by way of providing probabilistic linguistic constraints (Lee, 1989).",
        "N-gram language models (Jelinek, 1990), including bigram and trigram models, are the most commonly used method of applying local probabilistic constraints.",
        "However, context-free grammars (CFGs) produce more global linguistic constraints than N-gram models.",
        "It seems better to combine both local and global constraints and use them both concurrently in natural language processing.",
        "The reason why N-gram models are preferred over CFGs is that N-grain constraints are easily acquired from a given corpus.",
        "However, the larger N is, the more serious the problem of data sparseness becomes.",
        "CFGs are commonly employed in syntactic parsing as global linguistic constraints, since many efficient parsing algorithms are available.",
        "GLR (Generalized LR) is one such parsing algorithm that uses an LR table, into which CFG constraints are pre-compiled in advance (Knuth, 1965; Tomita, 1986).",
        "Therefore if we can incorporate N-gram constraints into an LR table, we can make concurrent use of both local and global linguistic constraints in GLR parsing.",
        "In the following section, we will propose a method that incorporates bigram constraints into an LR table.",
        "The advantages of the method are summarized as follows: First, it is expected that this method produces a lower perplexity than that for a simple bigram language model, since it is possible to utilize both local (bigram) and global (CFG) constraints in the LR table.",
        "We will evidence this reduction in perplexity by considering states in an LR table for the case of GLR parsing.",
        "Second, bigram constraints are easily acquired from smaller-sized corpora.",
        "Accordingly, data sparseness is not likely to arise.",
        "Third, the separation of local and global constraints makes it easy to describe CFG rules, since CFG writers need not take into account tedious descriptions of local connection constraints within th CFG1."
      ]
    },
    {
      "heading": "2 CFG, Connection Matrix and LR table",
      "text": []
    },
    {
      "heading": "2.1 Relation between CFG and Connection Constraints",
      "text": [
        "Figure 1 represents a situation in which a, and bj are adjacent to each other, where a, belongs to Set/ (i",
        "'(Tanaka et al., 1997) reported that the separate description of local and global constraints reduced the CFG rules to one sixth of their original number.",
        "Imai and Tanaka 225 A Method ofincorporating Bigram Constraints Hiroki Imai and Hozumi Tanaka (1998) A Method of Incorporating Bigram Constraints into an LR Table and Its Effectiveness in Natural Language Processing.",
        "In D.M.W.",
        "Powers (ed.)",
        "NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 225-233.",
        "and S et j are defined by lastl (Y) and first!",
        "(Z) (Aho et al., 1986), respectively.",
        "If a E Set' and b E Set j happen not to be able to occur in this order, it becomes a non-trivial task to express this adjacency restriction within the framework of a CFG.",
        "One solution to this problem is to introduce a new nonterminal symbol A, for each a, and a nonterminal symbol Bi for each by We then add rules of the form A â€“ + A, and A, a,, and B Bi and Bi 4 by As a result of this rule expansion, the order of the number of rules will become I x J in the worst case.",
        "The introduction of such new nonterminal symbols leads to an increase in grammar rules, which not only makes the LR table very large in size, but also diminishes efficiency of the GLR parsing method.",
        "The second solution is to augment X Y Z with a procedure that checks the connection between a, and by This solution can avoid the problem of the expansion of CFG rules, but we have to take care of the information flow from the bottom leaves to the upper nodes in the tree, Y, Z, and X.",
        "Neither the first nor the second solution are preferable, in terms of both efficiency of GLR parsing and description of CFG rules.",
        "Additionally, it is a much easier task to describe local connection constraints between two adjacent terminal symbols by way of a connection matrix such as in Figure 2, than to express these constraints within the CFG.",
        "The connection matrix in Figure 2 is defined as:",
        "The best solution seems to be to develop a method that can combine both a CFG and a connection matrix, avoiding the expansion of CFG rules.",
        "Consequently, the size of the LR table will become smaller and we will get better GLR parsing performance.",
        "In the following section, we will propose one such method.",
        "Note that we are considering connections between preterminals rather than words.",
        "Thus, we will have Connect(ai,bi) = 0 in the preterminal connection matrix similarly to the case of words."
      ]
    },
    {
      "heading": "2.2 Relation between the LR Table and Connection Matrix",
      "text": [
        "First we discuss the relation between the LR table and a connection matrix.",
        "The action part of an LR table consists of lookahead symbols and states.",
        "Let a shift action sh m be in state 1 with the lookahead symbol a.",
        "After the GLR parser executes action sh m, the symbol a is pushed onto the top of the stack and the GLR parser shifts to state m. Suppose there is an action A in state m with lookahead b (see Figure 3).",
        "The action A is executable if Connect(a,b) 0 (b can follow a), whereas if Connect(a,b) = 0 (b cannot follow a), the action A in state m with lookahead b is not executable and we can remove it from the LR table as an invalid action.",
        "Removing such invalid actions enables us to incorporate connection constraints into the LR table in addition to the implicit CFG constraints.",
        "In section 3.2, we will propose a method that integrates both bigram and CFG constraints into an LR table.",
        "After this integration process, we obtain a table called a bigram LR table."
      ]
    },
    {
      "heading": "3 Integration of Bigram and CFG Constraints into an LR Table",
      "text": []
    },
    {
      "heading": "3.1 The Definition of a Probabilistic Connection Matrix",
      "text": [
        "A close relation exists between bigrams and connection matrices, in that the bigram probability P(bla) corresponds to the matrix element of Connect(a,b).",
        "A connection matrix incorporating bigram probabilities is called a probabilistic connection matrix, in which Connect(a,b) = 0 still means b cannot follow a, but instead of connection matrix entries having a binary value of 0 or 1, a probability is associated with each element.",
        "This is then used to construct a probabilistic LR table.",
        "The N-gram model is the most commonly used probabilistic language model, and it assumes that a symbol sequence can be described by a higher order Markov process.",
        "The simplest N-gram model with N = 2 is called a bigram model, and approximates the probability of a string X = x1x2x3---xn as the product of conditional probabilities:",
        "In the above expression, \"#\" indicates the sentence beginning marker and \"S\" indicates the sentence ending marker.",
        "The above bigrain model can be represented in a probabilistic connection matrix defined as follows.",
        "DEFINITION 1 (probabilistic connection matrix) Let G = (VN, VT P, S) be a context-free grammar.",
        "For Va, b E VT (the set of terminal symbols), the probabilistic connection matrix named PConnect is defined as follows.",
        "PConnect(a,b) = 0 means that a and b cannot occur consecutively in the given order.",
        "PConnect(a,b) 0 means b can follow a with probability P(bia)."
      ]
    },
    {
      "heading": "3.2 An algorithm to construct a bigram LR table",
      "text": [
        "An algorithm to construct a probabilistic LR table, combining both bigram and CFG constraints, is given in Algorithm 1:"
      ]
    },
    {
      "heading": "Algorithm 1",
      "text": [
        "Input: A CFG G = (VN VT P, S) and a probabilistic connection matrix PConnect.",
        "Output: An LR table T with CFG and bigram constraints.",
        "Method: Step 1 Generate an LR table To from the given CFG G. Step 2 Removal of actions: For each shift action sh m with lookahead a in the LR table To, delete actions in the state in with lookahead b if PC onnect(a, b) = 0.",
        "Step 3 Constraint Propagation (Tanaka et al., 1994): Repeat the following two procedures until no further actions can be removed:",
        "1.",
        "Remove actions which have no succeeding action, 2.",
        "Remove actions which have no preceding action.",
        "Step 4 Compact the LR table if possible.",
        "Step 5 Incorporation of bigram constraints into the LR table: For each shift action sh m with lookahead a in the LR table To, let",
        "where {b1 : i = 1, , N} is the set of looka-heads for state m. For each action A3 in state m with lookahead b, assign a probability p to action A,:",
        "where n is the number of conflict actions in state m with lookahead k. The denominator is clearly a normalization factor.",
        "Step 6 For each shift action A with lookahead a in state 0, assign A a probability p = P(al#), where \"#\" is the sentence beginning marker.",
        "Step 7 Assign a probability p = 1/n to each action A in state m with lookahead symbol a that has not been assigned a probability, where n is the number of conflict actions in state m with lookahead symbol a.",
        "Step 8 Return the LR table T produced at the completion of Step 7 as the Bigrarn LR table.",
        "As explained above, the removal of actions at Step 2 corresponds to the operation of incorporating connection constraints into an LR table.",
        "We call Step",
        "actions are removed from the LR table during Steps 2 and 3, it becomes possible to compress the LR table in Step 4.",
        "We will demonstrate one example of this process in the following section.",
        "It should be noted that the above algorithm can be applied to any type of LR table, that is a canonical LR table, an LALR table, or an SLR table."
      ]
    },
    {
      "heading": "4 An Example",
      "text": []
    },
    {
      "heading": "4.1 Generating a Bigram LR Table",
      "text": [
        "In this section, we will provide a simple example of the generation of a bigram LR table by way of applying Algorithm 1 to both a CFG and a probabilistic connection matrix, to create a bigram LR table.",
        "From the CFG given in Figure 4, we can generate an LR table, Table 1, in Step 1 using the conventional LR table generation algorithm.",
        "Table 2 is the resultant LR table at the completion of Step 2 and Step 3, produced based on Table 1.",
        "Actions numbered (2) and (3) in Table 2 are those which are removed by Step 2 and Step 3, respectively.",
        "In state 1 with a lookahead symbol bl, re6 is carried out after executing action shl in state 0, pushing al onto the stack.",
        "Note that al and blare now consecutive, in this order.",
        "However, the probabilistic connection matrix (see Figure 5) does not allow such a sequence of terminal symbols, since PConnect(al , hi) = 0.",
        "Therefore, the action re6 in state 1 with lookahead bl is removed from Table 1 in Step 2, and thus marked as (2) in Table 2.",
        "Next, we would like to consider the reason why action sh9 in state 4 with lookahead al is removed from Table 1.",
        "In state 9, re6 with lookahead symbol $ has already been removed in Step 2, and there is no succeeding action for sh9.",
        "Therefore, action sh9 in state 3 is removed in Step 3, and hence marked as (3).",
        "Let us consider action re3 in state 8 with lookahead al.",
        "After this action is carried out, the GLR parser goes to state 4 after pushing X onto the stack.",
        "However, sh9 in state 4 with lookahead al has already been removed, and there is no succeeding action for re3.",
        "As a result, re3 in state 8 with lookahead symbol al is removed in Step 3.",
        "Similarly, re9 in state 7 with lookahead symbol al is also removed in Step 3.",
        "In this way, the removal of actions prop",
        "As a final step, we would like to assign bigram constraints to each action in Table 3.",
        "Let us consider the two re8s in state 6, reached after executing sh6 in state 4 by pushing a lookalead of bl onto the stack.",
        "In state 6, P is calculated at Step 5 as shown below:",
        "After assigning a probability to each action in the LR table at Step 5, there remain actions without probabilities.",
        "For example, the two conflict actions (re2 I sh6) in state 3 with lookahead blare not assigned a probability.",
        "Therefore, each of these actions is assigned the same probability, 0.5, in Step 7.",
        "A probability of 1 is assigned to remaining actions, since there is no conflict among them.",
        "Table 4 shows the final results of applying Algorithm 1 to G1 and Mi.",
        "We can assign the following probabilities p to each re8 in state 6 by way of Step 5:",
        "Imai and Tanaka 230 A Method of Incorporating Bigram Constraints"
      ]
    },
    {
      "heading": "4.2 Comparison of Language Models",
      "text": [
        "Using the bigram LR table as shown in Table 4, the probability PI of the string \"a2 bl aff' is calculated as:",
        "where P(Tree,) means the probability of the i-th parsing tree generated by the GLR parser and P(S, L, A) means the probability of an action A in state S with lookahead L. On the other hand, using only bigrarn constraints, the probability P2 of the string \"a2 bl a2\" is calculated as:",
        "The reason why PI > P2 can be explained as follows.",
        "Consider the beginning symbol a2 of a sentence.",
        "In the case of the bigram model, a can only be followed by either of the two symbols bland $ (see Figure 5).",
        "However, consulting the bigram LR table reveals that in state 0 with lookahead a2, sh2 is carried out, entering state 2.",
        "State 2 has only one action re7 with lookahead symbol bl.",
        "In other words, in state 2, $ is not predicted as a succeeding symbol of al.",
        "The exclusion of an ungrammatical prediction in $ makes P1 larger than P2.",
        "Perplexity is a measure of the complexity of a language model.",
        "The larger the probability of the language model, the smaller the perplexity of the language model.",
        "The above result (P1 > P2) indicates",
        "that the bigram LR table model gives smaller perplexity than the bigram model.",
        "In the next section, we will demonstrate this fact."
      ]
    },
    {
      "heading": "5 Evaluation of Perplexity",
      "text": [
        "Perplexity is a measure of the constraint imposed by the language model.",
        "Test-set perplexity (Jelinek, 1990) is commonly used to measure the perplexity of a language model from a test-set.",
        "Test-set perplexity for a language model L is simply the geometric mean of probabilities defined by:",
        "where",
        "Here N is the number of terminal symbols in the test set, M is the number of test sentences and P(Se) is the probability of generating i-th test sentence In the case of the bigram model, Pb,(S,) is:",
        "And in the case of the trigram model, Pir,(S,) is:",
        "Table 5 shows the test-set perplexity of preterminals for each language model.",
        "Here the preterminal bigram models were trained on a corpus with 20663 sentences, containing 230927 preterminals.",
        "The test-set consists of 1320 sentences, which contain 13311 preterminals.",
        "The CFG used is a phrase context-free grammar used in speech recognition tasks, and the number of rules and preterminals is 777 and 407, respectively.",
        "As is evident from Table 5, the use of a bigram LR table decreases the test-set perplexity from 6.50 to 5.99.",
        "Note that in this experiment, we used the LALR table generation algorithm2 to construct the bigram LR table.",
        "Despite the disadvantages of 21n the case of LALR tables, the sum of the probabilities of all the possible parsing trees generated by a given CFG may be less than 1 (Inui et al., 1997).",
        "mai and Tanaka 231 A Method ofincorporating Bigram Constraints LALR tables, the bigram.",
        "LR table has better performance than the simple bigram language model, showing the effectiveness of a bigram LR table.",
        "On the other hand, the perplexity of the trigram language model is smaller than that of the bigrain LR table.",
        "However, with regard to data sparseness, the bigram LR table is better than the trigram language model because bigram constraints are more easily acquired from a given corpus than trigram constraints.",
        "Although the experiment described above is concerned with natural language processing, our method is also applicable to speech recognition."
      ]
    },
    {
      "heading": "6 Conclusions",
      "text": [
        "In this paper, we described a method to construct a bigram LR table, and then discussed the advantage of our method, comparing our method to the bigram and trigram language models.",
        "The principle advantage over the bigram language model is that, in using a bigram LR table, we can combine both local probabilistic connection constraints (bigram constraints) and global constraints (CFG).",
        "Our method is applicable not only to natural language processing but also speech recognition.",
        "We are currently testing our method using a large-sized grammar containing dictionary rules for speech recognition.",
        "Su et al.",
        "(Su et al., 1991) and Chiang et al.",
        "(Chiang et al., 1995) have proposed a very interesting corpus-based natural language processing method that takes account not only of lexical, syntactic, and semantic scores concurrently, but also context-sensitivity in the language model.",
        "However, their method seems to suffer from difficulty in acquiring probabilities from a given corpus.",
        "Wright (Wright, 1990) developed a method of distributing the probability of each PCFG rule to each action in an LR table.",
        "However, this method only calculates syntactic scores of parsing trees based on a context-free framework.",
        "Briscoe and Carroll (Briscoe and Carroll., 1993) attempt to incorporate probabilities into an LR table.",
        "They insist that the resultant probabilistic LR table can include probabilities with context-sensitivity.",
        "Inui et.",
        "al.",
        "(Inui et al., 1997) reported that the resultant probabilistic LR table has a defect in terms of the process used to normalize probabilities associated with each action in the LR table.",
        "Finally, we would like to mention that Klavans and Resnik (Klavans and Resnik, 1996) have advocated a similar approach to ours which combines symbolic and statistical constraints, CFG and bigram constraints."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We would like to thank Mr. Toshiyuki Takezawa and Mr. Junji Etoh for providing us the dialog corpus and the grammar for our experiments.",
        "We would also like to thank Mr. Timothy Baldwin for his help in writing this paper."
      ]
    }
  ]
}
