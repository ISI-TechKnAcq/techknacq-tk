{
  "info": {
    "authors": [
      "Xipeng Qiu",
      "Wenjun Gao",
      "Xuanjing Huang"
    ],
    "book": "ACL-IJCNLP: Short Papers",
    "id": "acl-P09-2042",
    "title": "Hierarchical Multi-Label Text Categorization with Global Margin Maximization",
    "url": "https://aclweb.org/anthology/P09-2042",
    "year": 2009
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Text categorization is a crucial and well-proven method for organizing the collection of large scale documents.",
        "In this paper, we propose a hierarchical multi-class text categorization method with global margin maximization.",
        "We not only maximize the margins among leaf categories, but also maximize the margins among their ancestors.",
        "Experiments show that the performance of our algorithm is competitive with the recently proposed hierarchical multi-class classification algorithms."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In the past serval years, hierarchical text categorization has become an active research topic in database area (Koller and Sahami, 1997; Weigend et al., 1999) and machine learning area (Rousu et al., 2006; Cai and Hofmann, 2007).",
        "Hierarchical categorization methods can be divided in two types: local and global approaches (Wang et al., 1999; Sun and Lim, 2001).",
        "A local approach usually proceeds in a top-down fashion, which firstly picks the most relevant categories of the top level and then recursively making the choice among the low-level categories.",
        "The global approach builds only one classifier to discriminate all categories in a hierarchy.",
        "Due that the global hierarchical categorization can avoid the drawbacks about those high-level irrecoverable error, it is more popular in the machine learning domain.",
        "The essential idea behind global approach is that the close classes(nodes) have some common underlying factors.",
        "Especially, the descendant classes can share the characteristics of the ancestor classes, which is similar with multi-task learn-ing(Caruana, 1997).",
        "A key problem for global hierarchical categorization is how to combine these underlying factors.",
        "In this paper, we propose an method for hierarchical multi-class text categorization with global margin maximization.",
        "We emphasize that it is important to separate all the nodes of the correct path in the class hierarchy from their sibling node, then we incorporate such information into the formulation of hierarchical support vector machine.",
        "The rest of the paper is organized as follows.",
        "Section 2 describes the basic model of multi-class hierarchical categorization with maximizing margin.",
        "Then we propose our improved versions in section 3.",
        "Section 4 gives the experimental analysis.",
        "Section 5 concludes the paper."
      ]
    },
    {
      "heading": "2. Hierarchical Multi-Class Text Categorization",
      "text": [
        "Multiclass SVM can be generalized to the problem of hierarchical categorization (Cai and Hofmann, 2007), which has more than two categories in most of the case.",
        "Denote Yi as the multilabels of Xj and Y the multilabels set not in Yi.",
        "The separation margin of w, with respect to Xj, can be approximated as:",
        "The loss function can be accommodated to multi-class SVM to scale the penalties for margin violations proportional to the loss.",
        "This is motivated by the fact that margin violations involving an incorrect class with high loss should be penalized more severely.",
        "So the cost-sensitive hierarchical multiclass formulation takes takes the following form:",
        "where <g> is the tensor product.",
        "A(y) is the feature representation of y.",
        "Thus, we can classify a document x to label y*:",
        "where F(-) is a map function.",
        "There are different kinds of loss functions",
        "Ky,y).",
        "One is thezero-one loss, io/i(y>u) = [y / u]-Another is specially designed for the hierarchy is tree loss(Dekel et al., 2004).",
        "Tree loss is defined as the length of the path between two multilabels with positive microlabels,",
        "(Rousu et al., 2006) proposed a simplified version of Ih, namely 1^:",
        "that penalizes a mistake in a child only if the label of the parent was correct.",
        "There are some different choices for setting Cj.",
        "One naive idea is to use a uniform weighting (cj = 1).",
        "Another possible choice is to divide the loss among the sibling:",
        "Another possible choice is to scale the loss by the proportion of the hierarchy that is in the subtree T(j) rooted by j:",
        "Using these scaling weights, the derived losses are referred as lunJsib and lsub respectively.",
        "3 Hierarchical Multi-Class Text Categorization with Global Margin Maximization",
        "In previous literature (Cai and Hofmann, 2004; Tsochantaridis et al., 2005), they focused on separating the correct path from those incorrect path.",
        "Inspired by the example in Figure 1, we emphasize it is also important to separate the ancestor node in the correct path from their sibling node.",
        "The vector w can be decomposed in to the set of Wj for each node (category) in the hierarchy.",
        "In Figure 1, the example hierarchy has 7 nodes and 4 of them are leaf nodes.",
        "The category is encode as an integer, 1,..., 7.",
        "Suppose that the training pattern x belongs to category 4.",
        "Both w in the Figure la and Figure lb can successfully classify x into category 4, since F(w,$(x, y^)) = Si 2 4 (w*>x) is me maximal among all the possible discriminate functions.",
        "So both learned parameter w is acceptable in current hierarchical support vector machine.",
        "Here we claim the w in Figure lb is better than the w in Figure la.",
        "Since we notice in Figure la, the discriminate function (w2,x) is smaller than the discriminate function (w3,x).",
        "The discriminate function (wj,x) measures the similarity of x to category i.",
        "The larger the discriminate function is, the more similar x is to category i.",
        "Since category 2 is in the path from the root to the correct category and category 3 is not, intuitively, x should be closer to category 2 than category 3.",
        "But the discriminate function in Figure la is contradictive to this assumption.",
        "But such information is reflected correctly in Figure lb.",
        "So we conclude w in Fig.",
        "lb is superior to w in la.",
        "Here we propose a novel formulation to incorporate such information.",
        "Denote Ai as the mul-tilabel in Yi that corresponds to the nonleaf categories and Sib(z) denotes the sibling nodes of z, that is the set of nodes that have the same parent with z, except z itself.",
        "Implementing the above idea, we can get the following formulation:",
        "It arrives at the following Lagrangian:",
        "a) b) Figure 1: Two different discriminant function in a hierarchy",
        "The dual QP becomes",
        "aiyyajrr(5$i(y,y),<J$j(r,r)) and ^z _ kl{ = Azz/3ikk(^i(z, z), SQjQs., k))."
      ]
    },
    {
      "heading": "3. .1 Optimization Algorithm",
      "text": [
        "The derived QP can be very large, since the number of a and [3 variables is up to 0(n * 2N), where n is number of training pattern and N is the number of nodes in the hierarchy.",
        "But two properties of the dual problem can be exploited to design a much more efficient optimization.",
        "First, the constraints in the dual problem Eq.",
        "11 - Eq.",
        "15 factorize over the instance index for both a-variables and /3-variables.",
        "The constraints in",
        "Eq.",
        "14 do not couple a-variables and /3-variables together.",
        "Further, dual variables ctiyy and ctj ,-, belonging to different training instances i and j do not join in a same constraints.",
        "This inspired an optimization procedure which iteratively performs subspace optimization over all dual variables ctiyybelonging to the same training instance.",
        "This will in general reduced to a much smaller QP, since it freezes all <x,yy with j / i and /3-variables at their current values.",
        "This strategy can be applied in solving [3-variables.",
        "Secondly, the number of active constraints at the solution is expected to be relatively small, since only a small fraction of categories y G Y% ( or y G Sib(y) when y e A{) will typically fail to achieve the required margin.",
        "The expected sparse -ness of the variable for the dual problem can be exploited by employing a variable selection strategy.",
        "Equivalently, this corresponds to a cutting plane algorithm for the primal QP.",
        "Intuitively, we will identify the most violated margin constraint with index (i, y, y) and then add the corresponding variable to the optimization problem.",
        "This means that we start with extremely sparse problems and only successively increase the number of variables in the active set.",
        "This general approach to deal with large linear or quadratic optimization problems is also known as column selection.",
        "In practice, it is often not necessary to optimize until final convergence, which adds to the attractiveness of this approach.",
        "We have used the LOQO optimization package (Vanderbei, 1999) in our experiments."
      ]
    },
    {
      "heading": "4. Experiment",
      "text": [
        "We evaluate our proposed model on the section D in the WIPO-alpha collection, which consists of the 1372 training and 358 testing document.",
        "The",
        "S.t.Q^yy",
        ">",
        "o,",
        "(12)",
        "ÃŸjzz",
        ">",
        "o,",
        "(13)",
        "0>iyy",
        "Ky,y)",
        "<",
        "Ci,",
        "(14)",
        "Table 1: Prediction losses (%) obtained on WIPO.",
        "The values per column is calculated with the different loss function.",
        "number of nodes in the hierarchy is 188, with maximum depth 3.",
        "We compared the performance of our proposed method HSVM-S with two algorithms: HSVM(Cai and Hofmann, 2007) and HM3(Rousu et al., 2006).",
        "We compare the methods based on different loss functions, l0/i,l&,ltr, hni, I sib and hub-The performances for three algorithms can be seen in Table 1.",
        "Those empty cells, denoted by are not available in (Rousu et al., 2006).",
        "As expected, Z0/i is inferior to other hierarchical losses by getting poorest performance in all the testing losses, since it can not take into account the hierarchical information between categories.",
        "The results suggests that training with a hierarchical losses function, like lÂ§ib or l^ni, would lead to a better reduced Z0/i on the test set as well as in terms of the hierarchical loss.",
        "In Table 1, we can also point out that when training with the same hierarchical loss, the performance of HSVM-S is better than HSVM under the measure of most hierarchical losses, since HSVM-S includes more hierarchical information,the relationship between the sibling categories, than HSVM which only separate the leave categories."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "In this paper we present a hierarchical multi-class document categorization, which focus on maximize the margin of the classes at the different levels in the class hierarchy.",
        "In future work, we plan to extend the proposed hierarchical learning method to the case where the hierarchy is a DAG instead of tree and scale up the method further."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work was (partially) funded by Chinese NSF 60673038, Doctoral Fund of Ministry of Education of China 200802460066, and Shanghai Science and Technology Development Funds 08511500302.",
        "Test Train ---",
        "h/i",
        "Ia",
        "hr",
        "hib",
        "hub",
        "l-o/i",
        "HSVM",
        "48.6",
        "188.8",
        "94.4",
        "97.2",
        "5.4",
        "7.5",
        "HSVM-S",
        "48.3",
        "186.6",
        "93.3",
        "96.6",
        "5.2",
        "7.4",
        "Ia",
        "HSVM",
        "49.7",
        "187.7",
        "93.9",
        "99.4",
        "5.0",
        "7.1",
        "HSVM-S",
        "47.8",
        "165.3",
        "89.7",
        "90.5",
        "4.8",
        "6.9",
        "HM3",
        "70.9",
        "167.0",
        "-",
        "89.1",
        "5.0",
        "7.0",
        "hr",
        "HSVM",
        "49.4",
        "186.0",
        "93.0",
        "98.9",
        "5.0",
        "7.5",
        "HSVM-S",
        "48.9",
        "181.4",
        "90.2",
        "97.8",
        "4.9",
        "7.1",
        "HSVM",
        "47.2",
        "181.0",
        "90.5",
        "94.4",
        "5.0",
        "7.0",
        "HSVM-S",
        "46.9",
        "179.3",
        "88.7",
        "91.9",
        "4.9",
        "6.9",
        "HM3",
        "70.1",
        "172.1",
        "-",
        "88.8",
        "5.2",
        "7.4",
        "hib",
        "HSVM",
        "49.4",
        "184.9",
        "92.5",
        "98.9",
        "4.8",
        "7.4",
        "HSVM-S",
        "48.9",
        "170.2",
        "91.6",
        "90.8",
        "4.7",
        "7.4",
        "HM3",
        "64.8",
        "172.9",
        "-",
        "92.7",
        "4.8",
        "7.1",
        "hÃ¼b",
        "HSVM",
        "50.6",
        "189.9",
        "95.0",
        "101.1",
        "5.2",
        "7.5",
        "HSVM-S",
        "47.2",
        "169.4",
        "85.2",
        "89.4",
        "4.3",
        "6.6",
        "HM3",
        "65.0",
        "170.9",
        "-",
        "91.9",
        "4.8",
        "7.2"
      ]
    }
  ]
}
