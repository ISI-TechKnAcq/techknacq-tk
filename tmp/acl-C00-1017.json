{
  "info": {
    "authors": [
      "Thorsten Brants",
      "Matthew W. Crocker"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C00-1017",
    "title": "Probabilistic Parsing and Psychological Plausibility",
    "url": "https://aclweb.org/anthology/C00-1017",
    "year": 2000
  },
  "references": [
    "acl-E99-1016",
    "acl-J93-2004",
    "acl-J98-4004",
    "acl-P96-1025",
    "acl-P99-1054",
    "acl-W97-0301",
    "acl-W98-1115"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Given the recent evidence for probabilistic mechanisms in models of human ambiguity resolution, this paper investigates the plausibility of exploiting current; wide-coverage, probabilistic parsing techniques to model human linguistic performance.",
        "In particular, we investigate the performance of standard stochastic parsers when they are revised to operate incrementally, and with reduced memory resources.",
        "We present; techniques for ranking and filtering analyses, together with experimental results.",
        "Our results confirm that stochastic parsers which adhere to these psychologically motivated constraints achieve good performance.",
        "Memory can be reduced down to 1% (compared to exhausitve search) without reducing recall and precision.",
        "Additionally, these models exhibit substantially faster performance.",
        "Finally, we argue that this general result is likely to hold for more sophisticated, and psycholin-guistically plausible, probabilistic parsing models."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Language engineering and computational psycholinguistics are often viewed as distinct; research programmes: engineering solutions aim at practical methods which can achieve good performance, typically paying little attention to linguistic or cognitive modelling.",
        "Computational psycholinguistics, on the other hand, is often focussed on detailed modelling of human behaviour for a relatively small number of well-studied constructions.",
        "In this paper we suggest that, broadly, the human sentence processing mechanism (HSPM) and current statistical parsing technology can be viewed as having similar objectives: to optimally (i.e. rapidly and accurately) understand the text; and utterances they encounter.",
        "Our aim is to show that large scale probabilistic parsers, when subjected to basic cognitive constraints, can still achieve high levels of parsing accuracy.",
        "If successful, this will contribute to a plausible explanation of the fact that; people, in general, are also extremely accurate and robust.",
        "Such a result would also strengthen existing results showing that related probabilistic mechanisms can explain specific psycholinguistic phenomena.",
        "To investigate this issue, we construct a standard 'baseline' stochastic parser, which mirrors the performance of a similar systems (e.g. (Johnson, 1998)).",
        "We then consider an incremental version of the parser, and evaluate the effects of several probabilistic filtering strategies which are used to prune the parser's search space, and thereby reduce memory load.",
        "To assess the generality of our results for more sophisticated probabilistic models, we also conduct experiments using a model in winch parent-node information is encoded on the daughters.",
        "This increase in contextual information has been shown to improve performance (Johnson, 1998), and the model is also shown to be robust to the incrementality and memory constraints investigated here.",
        "We present the results of parsing performance experiments, showing the accuracy of these systems with respect to both a parsed corpus and the baseline parser.",
        "Our experiments suggest that a strictly incremental model, in which memory resources are substantially reduced through filtering, can achieve precision and recall which equals that of 'unrestricted' systems.",
        "Furthermore, implementation of these restrictions leads to substantially faster performance.",
        "In conclusion, we argue that; such broad-coverage probabilistic parsing",
        "models provide a valuable framework for explaining the human capacity to rapidly, accurately, and robustly understand \"garden variety\" language.",
        "This lends further support to psycholinguistic accounts which posit probabilistic ambiguity resolution mechanisms to explain \"garden path\" phenomena.",
        "It is important to reiterate that our intention here is only to investigate the performance of probabilistic parsers under psycholinguistically motivated constraints.",
        "We do not argue for the psychological plausibility of SCFG parsers (or the parent-encoded variant) per se.",
        "Our investigation of these models was motivated rather by our desire to obtain a generalizable result for these simple and well-understood models, since obtaining similar results for more sophisticated models (e.g. (Collins, 1996; Ratnaparkhi, 1997)) might have been attributed to special properties of these models.",
        "Rather, the current result should be taken as support for the potential scaleability and performance of probabilistic psychological models such as those proposed by (Jurafsky, 1996) and (Crocker and Brants, to appear)."
      ]
    },
    {
      "heading": "2 Psycholinguistic Motivation",
      "text": [
        "Theories of human sentence processing have largely been shaped by the study of pathologies in human language processing behaviour.",
        "Most psycholinguistic models seek to explain the difficulty people have in comprehending structures that are ambiguous or memory-intensive (see (Crocker, 1999) for a recent overview).",
        "While often insightful, this approach diverts attention from the fact that people are in fact extremely accurate and effective in understanding the vast majority of their \"linguistic experience\".",
        "This observation, combined with the mounting psycholinguistic evidence for statistically-based mechanisms, leads us to investigate the merit of exploiting robust, broad coverage, probabilistic parsing systems as models of human linguistic performance.",
        "The view that human language processing can be viewed as an optimally adapted system, within a probabilistic framework, is advanced by (Chater et al., 1998), while (Jurafsky, 1996) has proposed a specific probabilistic parsing model of human sentence processing.",
        "In work on human lexical category disambiguation,- (Crocker and Corley, to appear), have demonstrated that a standard (incremental) HMM-based part-of-speech tagger models the finding from a range of psycholinguistic experiments.",
        "In related research, (Crocker and Brants, 1999) present evidence that an incremental stochastic parser based on Cascaded Markov Models (Brants, 1999) can account for a range of experimentally observed local ambiguity preferences.",
        "These include NP/S complement ambiguities, reduced relative clauses, noun-verb category ambiguities, and 'that'-ambiguities (where 'that' can be either a complementizer or a determiner) (Crocker and Brants, to appear).",
        "Crucially, however, there are differences between the classes of mechanisms which are psychologically plausible, and those which prevail in current language technology.",
        "We suggest that two of the most important differences concern incrernentality, and memory resources.",
        "There is overwhelming experimental evidence that people construct connected (i.e. semantically interpretable) analyses for each initial substring of an utterance, as it is encountered.",
        "That is, processing takes place incrementally, from left to right, on a word by word basis.",
        "Secondly, it is universally accecpted that people can at most consider a relatively small number of competing analyses (indeed, some would argue that number is one, i.e. processing is strictly serial).",
        "In contrast, many existing stochastic parsers are \"unrestricted\", in that they are optimised for accuracy, and ignore such psychologically motivated constraints.",
        "Tlms the appropriateness of using broad-coverage probabilistic parsers to model the high level of human performance is contingent upon being able to maintain these levels of accuracy when the constraints of incrementality and resource limitations are imposed."
      ]
    },
    {
      "heading": "3 Incremental Stochastic Context-Free Parsing",
      "text": [
        "The following assumes that the reader is familiar with stochastic context-free grammars (SCFG) and stochastic chart-parsing techniques.",
        "A good introduction can be found, e.g., in (Manning and Schiitze, 1999).",
        "We use standard abbreviations for terminial nodes, non-terminal nodes, rules and probabilities.",
        "This paper investigates stochastic context-free parsing based on a grammar that is derived from a treebank, starting with part-of-speech tags as terminals.",
        "The grammar is derived by collecting all rules X (1 that occur in the treebank and their frequencies The probability of a rule is set to only the one with the highest inside probability is kept; in the chart.",
        "The others cannot contribute to the most; probable parse.",
        "For an inactive edge spanning i to j and representing the rule X Y1 ... Irk, the inside probability /7j is set to",
        "For a description of treebank grammars see (Charniak, 1996).",
        "The grammar does not contain c-rules, otherwise there is no restriction on the rules.",
        "In particular, we do not require C lionisky-Normal-lAwm.",
        "In addition to the rules that correspond to structures in the corpus, we add a new start symbol ROOT to the grammar and rules ROOT â€“ 4 X for all non-terminals X together with probabilities derived from the root nodes in the corpus'.",
        "For parsing these grammars, we rely upon a standard bottom-up chart-parsing technique with a modification for incremental parsing, i.e., for each word, all edges are processed and possibly pruned before proceeding to the next word.",
        "The outline of the algorithm is as follows.",
        "A chart entry E consists of a start and end position 7 and j, a dotted rule X â€“ > cc.7.",
        "the inside probability 0(Xi,j) that X generates the terminal string from position i to j, and information about; the most probable inside structure.",
        "If the dot of the (lotted rule is at the rightmost position, the corresponding edge is an inactive edge.",
        "lf the dot is at any other position, it; is an active edge.",
        "Inactive edges represent; recognized hypothetical constituents, while active edges represent; prefixes of hypothetical constituents.",
        "The ith terminal node Li that enters the chart generates an inactive edge for the span (7-1, i).",
        "Based on this, new active and inactive edges are generated according to the standard algorithm.",
        "Since we are interested in the most probable parse, the chart can be minimized in the following way while still performing an exhaustive search.",
        "If there is more than one edge that covers a span (i, j) having the same non-terminal symbol on the left-hand side of the dotted rule, where it and ji mark the start and end postilion of Y1, having i = i7 and j The inside probability for an active edge OA with the dot after the kth symbol of the right-hand side is set to",
        "We do not; use the probability of the rule at this point.",
        "Tins allows us to combine all edges with the same span and the dot at the same position but; with different symbols on the left-hand side.",
        "Introducing a distinguished left-hand side only for inactive edges significantly reduces the number of active edges in the chart.",
        "This goes one step further than implicitly right-binarizing the grammar; not only suffixes of right-hand sides are joined, but also the corresponding left-hand sides."
      ]
    },
    {
      "heading": "4 Memory Restrictions",
      "text": [
        "We investigate the elimination (pruning) of edges from the chart in our incremental parsing scheme.",
        "After processing a word and before proceeding to the next word during incremental parsing, low ranked edges are removed.",
        "This is equivalent to imposing memory restrictions on the processing system.",
        "The original algorithm keeps one edge in the chart for each combination of span (start and end position) and non-terminal symbol (for inactive edges) or right-hand side prefixes of dotted rules (for active edges).",
        "With pruning, we restrict the number of edges allowed per span.",
        "The limitation can be expressed in two ways: I.",
        "Variable beam.",
        "Select a threshold 0 > I.",
        "Edge c is removed, if its probability is Pc, the best; probability for the span is , and",
        "2.",
        "Fixed beam.",
        "Select a maximum number of edges per span m. An edge a is removed, if its probability is not in the first in highest; probabilities for edges with the same span.",
        "We performed experiments using both types of beams.",
        "Fixed beams yielded consistently better results than variable beams when plotting chart size vs. F-score.",
        "Therefore, the following results are reported for fixed beams.",
        "We compare and rank edges covering the same span only, and we rank active and inactive edges separately.",
        "This is in contrast to (Charniak et al., 1998) who rank all edges.",
        "They use normalization in order to account for different spans since in general, edges for longer spans involve more multiplications of probabilities, yielding lower probabilities.",
        "Charniak at al.",
        "'s normalization value is calculated by a different probability model than the inside probabilities of the edges.",
        "So, in addition to the normalization for different span lengths, they need a normalization constant that accounts for the different probability models.",
        "Tins investigation is based on a much simpler ranking formula.",
        "We use what can be described as the unigram probability of a non-terminal node, i.e., the a priori probability of the corresponding non-terminal symbol(s) times the inside probability.",
        "Thus, for an inactive edge (i, j, X /3/(Xi,j)), we use the probability",
        "for ranking.",
        "Tins is the probability of the node and its yield being present; in a parse.",
        "The higher tins value, the better is this node.",
        "01 is the inside probability for inactive edges as given in equation 2, P(X) is the a priori probability for non-terminal X, (as estimated from the frequency in the training corpus) and PHI is the probability of the edge for the non-terminal X spanning positions i to j that is used for ranking.",
        "For an active edge j, X Y1- â€¢ â€¢ Yk.",
        "P(Y' â€¢ â€¢ â€¢ Yk) can be read off the corpus.",
        "It is the a priori probability that the right-hand side of a production has the prefix Y1- Yk, which is estimated by",
        "where N is the total number of productions in the corpus, i = jk and OA is the inside probability of the prefix."
      ]
    },
    {
      "heading": "5 Experiments",
      "text": []
    },
    {
      "heading": "5.1 Data",
      "text": [
        "We use sections 2 â€“ 21 of the Wall Street Journal part; of the Penn Treebank (Marcus et al., 1993) 1;o generate a treebank grammar.",
        "Traces, functional tags and other tag extensions that do not mark syntactic category are removed before training3.",
        "No other modifications are made.",
        "For testing, we use the 1578 sentences of length 40 or less of section 22.",
        "The input to the parser is the sequence of part-of-speech tags."
      ]
    },
    {
      "heading": "5.2 Evaluation",
      "text": [
        "For evaluation, we use the parseval measures and report labeld F-score (the harmonic mean of labeled recall and labeled precision).",
        "Reporting the F-score makes our results comparable to those of other previous experiments using the same data sets.",
        "As a measure of the amount; of work done by the parser, we report the size of the chart.",
        "The number of active and inactive edges that enter the chart is given for the exhaustive search, not counting those hypothetical edges that are replaced or rejected because there is an alternative edge with higher probability.",
        "For pruned search, we give the percentage of edges required."
      ]
    },
    {
      "heading": "5.3 Fixed Beam",
      "text": [
        "For our experiments, we define the beam by a maximum number of edges per span.",
        "Beams for active and inactive edges are set separately.",
        "The beams run from 2 to 12, and we test all.",
        "2Here, we use proper prefixes, i.e., all prefixes not including the last element."
      ]
    },
    {
      "heading": "Results with Original and Parent Encoding",
      "text": []
    },
    {
      "heading": "5.4 Experimental Results",
      "text": [
        "The results of our 121 test runs with different settings for active and inactive beams are given in figure 1..",
        "The diagram shows chart sizes vs. labeled F-scores.",
        "It sorts chart sizes across different settings of the beams.",
        "if several beam settings result in equivalent chart sizes, the diagram contains the one yielding the highest F-score.",
        "The main finding is that we can reduce the size of the chart to between 1% and 3% of the size required for exhaustive search without affecting the results.",
        "Only very small beams degrade performance.",
        "The effect occurs for both models despite the simple ranking formula.",
        "This significantly reduces memory requirements 'Given the amount of test data (26,322 non-terminal nodes), results within a range of around 0.7% are equivalent with a confidence degree of o' = 99%.",
        "(given as size of the chart) and increases parsing speed.",
        "Exhaustive search yields an 1.0-Score of 71.21% when using the original Penn neebank encoding.",
        "Only around 1% the edges are required to yield equivalent results with incremental processing and pruning after each word is added to the chart.",
        "This result is, among other settings, obtained by a fixed beam of 2 for inactive edges and 3 for active edges(' For the parent encoding, exhaustive search.",
        "yields an F-Score of 79.28%.",
        "Only between 2 and 3% of the edges are required to yield an equivalent result with incremental processing and pruning.",
        "As an example, the point at size = 3.0% F-score = 79.1% is generated by the beam setting of 1.2 for inactive and 9 for active edges.",
        "The parent encoding yields around 8% higher F-scores but it also imposes a higher absolute and relative memory load on the process.",
        "The higher degree of parallelism in the inactive `Using variable beams, we would need 1.95% of the chart, entries to achieve an equivalent F-score.",
        "chart stems from the parent hypothesis in each node.",
        "In terms of pure node categories, the average number of parallel nodes at this point is 3.57.",
        "Exhaustive search for the base encoding needs in average 140,000 edges per sentence, for the parent encoding 200,000 edges; equivalent results for the base encoding can be achieved with around 1% of these edges, equivalent results for the parent encoding need between 2 and 3%.",
        "The lower number of edges significantly increases parsing speed.",
        "Using exhaustive search for the base model, the parser processes 3.0 tokens per second (measured on a Pentium III 500; no serious efforts of optimization have gone into the parser).",
        "With a chart size of 1%, speed is 630 tokens/second.",
        "This is a factor of 210 without decreasing accuracy.",
        "Speed for the parent model is 0.5 tokens/second (exhaustive) and 111 tokens/seconds (3.0% chart size), yielding an improvement by factor 220."
      ]
    },
    {
      "heading": "6 Related Work",
      "text": [
        "Probably mostly related to the work reported here are (Charniak et al., 1998) and (Roark and Johnson, 1999).",
        "Both report on significantly improved parsing efficiency by selecting only a subset of edges for processing.",
        "There are three main differences to our approach.",
        "One is that they use a ranking for best-first search while we immediately prune hypotheses.",
        "They need to store a large number edges because it is not known in advance how many of the edges will be used until a parse is found.",
        "The second difference is that we proceed strictly incrementally without lookahead.",
        "(Charniak et al., 1998) use a non-incremental procedure, (Roark and Johnson, 1999) use a lookahead of one word.",
        "Thirdly, we use a much simpler ranking formula.",
        "Additionally, (Charniak et al., 1998) and (Roark and Johnson, 1999) do not use the original Penntree encoding for the context-free structures.",
        "Before training and parsing, they change/remove some of the productions and introduce new part-of-speech tags for auxiliaries.",
        "The exact effect of these modifications is unknown, and it is unclear if these affect compa7For the active chart, paralellism cannot be given for different nodes types since active edges are introduced for right-hand side prefixes, collapsing all possible left-hand sides.",
        "rability to our results.",
        "The heavy restrictions in our method (immediate pruning, no look-ahead, very simple ranking formula) have consequences on the accuracy.",
        "Using right context and sorting instead of pruning yields roughly 2% higher results (compared to our base encoding8).",
        "But our work shows that even with these massive restrictions, the chart size can be reduced to 1% without a decrease ill accuracy when compared to exhaustive search."
      ]
    },
    {
      "heading": "7 Conclusions",
      "text": [
        "A central challenge in computational psycholinguistics is to explain how it is that people are so accurate and robust in processing language.",
        "Given the substantial psycholinguistic evidence for statistical cognitive mechanisms, our objective in this paper was to assess the plausibility of using wide-coverage probabilistic parsers to model human linguistic performance.",
        "In particular, we set out to investigate the effects of imposing incremental processing and significant memory limitations on such parsers.",
        "The central finding of our experiments is that incremental parsing with massive (97% â€“ 99%) pruning of the search space does not impair the accuracy of stochastic context-free parsers.",
        "This basic finding was robust across different settings of the beams and for the original Penn Treebank encoding as well as the parent encoding.",
        "We did however, observe significantly reduced memory and time requirements when using combined active/inactive edge filtering.",
        "To our knowledge, this is the first investigation on treebank grammars that systematically varies the beam for pruning.",
        "Our aim in this paper is not to challenge state-of-the-art parsing accuracy results.",
        "For our experiments we used a purely context-free stochastic parser combined with a very simple pruning scheme based on simple \"unigram\" probabilities, and no use of right context.",
        "We do, however suggest that our result should apply to richer, more sophistacted probabilistic 8Comparison of results is not straightforward since (Roark and Johnson, 1999) report accuracies only for those sentences for which a parse tree was generated (between 93 and 98% of the sentences), while our parser (except for very small beams) generates parses for virtually all sentences, hence we report accuracies for all sentences.",
        "models, e.g. when adding word statistics to the model (Charniak, 1997).",
        "We therefore conclude that wide-coverage, probabilistic parsers do not suffer impaired accuracy when subject to strict; cognitive memory limitations and incremental processing.",
        "Furthermore, parse times are substantially reduced.",
        "This suggests that it may be fruitful to pursue the use of these models within computational psycholinguistics, where it is necessary to explain not only the relatively rare 'pathologies' of the human parser, but also its more frequently observed accuracy and robustness."
      ]
    }
  ]
}
