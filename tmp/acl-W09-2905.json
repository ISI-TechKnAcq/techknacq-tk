{
  "info": {
    "authors": [
      "Hung Huu Hoang",
      "Su Nam Kim",
      "Min-Yen Kan"
    ],
    "book": "Proceedings of the Workshop on Multiword Expressions: Identification, Interpretation, Disambiguation and Applications (MWE 2009)",
    "id": "acl-W09-2905",
    "title": "A re-examination of lexical association measures",
    "url": "https://aclweb.org/anthology/W09-2905",
    "year": 2009
  },
  "references": [],
  "sections": [
    {
      "text": [
        "A Re-examination of Lexical Association Measures",
        "of Computer Science National University of Singapore",
        "hoanghuu@comp.nus.edu.sg",
        "Su Nam Kim Min-Yen Kan",
        "of Computer Science Dept.",
        "of Computer Science",
        "and Software Engineering National University",
        "snkim@csse.unimelb.edu.au kanmy@comp.nus.edu.sg",
        "We review lexical Association Measures (AMs) that have been employed by past work in extracting multiword expressions.",
        "Our work contributes to the understanding of these AMs by categorizing them into two groups and suggesting the use of rank equivalence to group AMs with the same ranking performance.",
        "We also examine how existing AMs can be adapted to better rank English verb particle constructions and light verb constructions.",
        "Specifically, we suggest normalizing (Pointwise) Mutual Information and using marginal frequencies to construct penalization terms.",
        "We empirically validate the effectiveness of these modified AMs in detection tasks in English, performed on the Penn Treebank, which shows significant improvement over the original AMs."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Recently, the NLP community has witnessed a renewed interest in the use of lexical association measures in extracting Multiword Expressions (MWEs).",
        "Lexical Association Measures (hereafter, AMs) are mathematical formulas which can be used to capture the degree of connection or association between constituents of a given phrase.",
        "Well-known AMs include Pointwise Mutual Information (PMI), Pearson's / and the Odds Ratio.",
        "These AMs have been applied in many different fields of study, from information retrieval to hypothesis testing.",
        "In the context of MWE extraction, many published works have been devoted to comparing their effectiveness.",
        "Krenn and Evert (2001) evaluate Mutual Information (MI), Dice, Pearson's x, log-likelihood ratio and the T score.",
        "In Pearce (2002), AMs such as Z score, Pointwise MI, cost reduction, left and right context entropy, odds ratio are evaluated.",
        "Evert (2004) discussed a wide range of AMs, including exact hypothesis tests such as the binomial test and Fisher's exact tests, various coefficients such as Dice and Jaccard.",
        "Later, Ramisch et al.",
        "(2008) evaluated MI, Pearson'sx and Permutation Entropy.",
        "Probably the most comprehensive evaluation of AMs was presented in Pecina and Schlesinger (2006), where 82 AMs were assembled and evaluated over Czech collocations.",
        "These collocations contained a mix of idiomatic expressions, technical terms, light verb constructions and stock phrases.",
        "In their work, the best combination of AMs was selected using machine learning.",
        "While the previous works have evaluated AMs, there have been few details on why the AMs perform as they do.",
        "A detailed analysis of why these AMs perform as they do is needed in order to explain their identification performance, and to help us recommend AMs for future tasks.",
        "This weakness of previous works motivated us to address this issue.",
        "In this work, we contribute to further understanding of association measures, using two different MWE extraction tasks to motivate and concretize our discussion.",
        "Our goal is to be able to predict, a priori, what types of AMs are likely to perform well for a particular",
        "MWE class.",
        "We focus on the extraction of two common types of English MWEs that can be captured by bigram model: Verb Particle Constructions (VPCs) and Light Verb Constructions (LVCs).",
        "VPCs consist of a verb and one or more particles, which can be prepositions (e.g. put on, bolster up), adjectives (cut short) or verbs (make do).",
        "For simplicity, we focus only on bigram VPCs that take prepositional particles, the most common class of VPCs.",
        "A special characteristic of VPCs that affects their extraction is the mobility of noun phrase complements in transitive VPCs.",
        "They can appear after the particle (Take off your hat) or between the verb and the particle (Take your hat off).",
        "However, a pronominal complement can only appear in the latter configuration (Take it off).",
        "In comparison, LVCs comprise of a verb and a complement, which is usually a noun phrase (make a presentation, give a demonstration).",
        "Their meanings come mostly from their complements and, as such, verbs in LVCs are termed semantically light, hence the name light verb.",
        "This explains why modifiers of LVCs modify the complement instead of the verb (make a serious mistake vs. *make a mistake seriously).",
        "This phenomenon also shows that an LVC's constituents may not occur contiguously."
      ]
    },
    {
      "heading": "2. Classification of Association Measures",
      "text": [
        "Although different AMs have different approaches to measuring association, we observed that they can effectively be classified into two broad classes.",
        "Class I AMs look at the degree of institutionalization; i.e., the extent to which the phrase is a semantic unit rather than a free combination of words.",
        "Some of the AMs in this class directly measure this association between constituents using various combinations of co-occurrence and marginal frequencies.",
        "Examples include MI, PMI and their variants as well as most of the association coefficients such as Jaccard, Hamann, Brawn-Blanquet, and others.",
        "Other Class I AMs estimate a phrase's MWE-hood by judging the significance of the difference between observed and expected frequencies.",
        "These AMs include, among others, statistical hypothesis tests such as T score, Z score and Pearson's x test.",
        "Class II AMs feature the use of context to measure non-compositionality, a peculiar characteristic of many types of MWEs, including VPCs and idioms.",
        "This is commonly done in one of the following two ways.",
        "First, non-compositionality can be modeled through the diversity of contexts, measured using entropy.",
        "The underlying assumption of this approach is that non-compositional phrases appear in a more restricted set of contexts than compositional ones.",
        "Second, non-compositionality can also be measured through context similarity between the phrase and its constituents.",
        "The observation here is that non-compositional phrases have different semantics from those of their constituents.",
        "It then follows that contexts in which the phrase and its constituents appear would be different (Zhai, 1997).",
        "Some VPC examples include carry out, give up.",
        "A close approximation stipulates that contexts of a non-compositional phrase's constituents are also different.",
        "For instance, phrases such as hot dog and Dutch courage are comprised of constituents that have unrelated meanings.",
        "Metrics that are commonly used to compute context similarity include cosine and dice similarity; distance metrics such as Euclidean and Manhattan norm; and probability distribution measures such as Kullback-Leibler divergence and Jensen-Shannon divergence.",
        "Table 1 lists all AMs used in our discussion.",
        "The lower left legend defines the variables a, b, c, and d with respect to the raw co-occurrence statistics observed in the corpus data.",
        "When an AM is introduced, it is prefixed with its index given in Table 1(e.g., [M2] Mutual Information) for the reader's convenience."
      ]
    },
    {
      "heading": "3. Evaluation",
      "text": [
        "We will first present how VPC and LVC candidates are extracted and used to form our evaluation data set.",
        "Second, we will discuss how performances of AMs are measured in our experiments.",
        "In this study, we employ the Wall Street Journal (WSJ) section of one million words in the Penn Tree Bank.",
        "To create the evaluation data set, we first extract the VPC and LVC candidates from our corpus as described below.",
        "We note here that the mobility property of both VPC and LVC constituents have been used in the extraction process.",
        "For VPCs, we first identify particles using a pre-compiled set of 38 particles based on Baldwin (2005) and Quirk et al.",
        "(1985) (Appendix A).",
        "Here we do not use the WSJ particle tag to avoid possible inconsistencies pointed out in Baldwin (2005).",
        "Next, we search to the left of the located particle for the nearest verb.",
        "As verbs and particles in transitive VPCs may not occur contiguously, we allow an intervening NP of up to 5 words, similar to Baldwin and Villavicencio (2002) and Smadja (1993), since longer NPs tend to be located after particles.",
        "Contingency table of a bigram (x y), recording cooccurrence and marginal frequencies; w stands for all words except w; * stands for all words; N is total number of bigrams.",
        "The expected frequency under the",
        "independence assumption is f (xy) = f (x*) f (* y) / N. Table 1.",
        "Association measures discussed in this paper.",
        "Starred AMs (*) are developed in this work.",
        "Extraction of LVCs is carried out in a similar frequently used English light verbs: do, get, give, fashion.",
        "First, occurrences of light verbs are have, make, put and take.",
        "Next, we search to the located based on the following set of seven right of the light verbs for the nearest noun, permitting a maximum of 4 intervening words to allow for quantifiers (a/an, the, many, etc.",
        "), adjectival and adverbial modifiers, etc.",
        "If this search fails to find a noun, as when LVCs are used in the passive (e.g. the presentation was made), we search to the right of the light verb, also allowing a maximum of 4 intervening words.",
        "The above extraction process produced a total of 8,652 VPC and 11,465 LVC candidates when run on the corpus.",
        "We then filter out candidates with observed frequencies less than 6, as suggested in Pecina and Schlesinger (2006), to obtain a set of 1,629 VPCs and 1,254 LVCs.",
        "M1.",
        "Joint Probability",
        "f(xy)/N",
        "M2.",
        "Mutual Information",
        "Z flog i",
        "NT?",
        "f",
        "M3.",
        "Log likelihood ratio",
        "Z flog y",
        "M4.",
        "Pointwise MI (PMI)",
        "log P(xy)",
        "log-",
        "P( x*) P(* y )",
        "M5.",
        "Local-PMI",
        "f ( xy ) x PMI",
        "M6.",
        "PMIk",
        "l Nf(xy)k",
        "log-",
        "f ( x*) f (* y )",
        "M7.",
        "PMI",
        "l Nf(xy)",
        "log-",
        "f ( x*) f (* y )",
        "M8.",
        "Mutual Dependency",
        "log P(xy)",
        "log- – ",
        "P( x*) P(*y)",
        "M9.",
        "Driver-Kroeber",
        "a",
        "4( a + b)( a + c)",
        "M10.",
        "Normalized expectation",
        "2a 2a+b+c",
        "M11.",
        "Jaccard",
        "a",
        "a+b+c",
        "M12.",
        "First Kulczynski",
        "a b+c",
        "M13.",
        "Second",
        "Sokal-Sneath",
        "a",
        "a+2(b+c)",
        "M14.",
        "Third",
        "Sokal-Sneath",
        "a+d b+c",
        "M15.",
        "Sokal-Michiner",
        "a+d a+b+c+d",
        "M16.",
        "Rogers-Tanimoto",
        "a+d a+2b+2c+d",
        "M17.",
        "Hamann",
        "(a+d) (b+c) a+b+c+d",
        "M18.",
        "Odds ratio",
        "ad Tbc",
        "M19.",
        "Yule's CO",
        "4 ad +4 bc",
        "M20.",
        "Yule's Q",
        "ad bc ad + bc",
        "M21.",
        "Brawn-",
        "Blanquet",
        "a",
        "max(a+b, a+c)",
        "M22.",
        "Simpson",
        "a",
        "min(a+b, a+c)",
        "M23.",
        "S cost",
        "min(b, c) – ",
        "log(1 + – ) a+1",
        "M24*.",
        "Adjusted S Cost",
        "max(b, c) – ",
        "log(1 +-)",
        "a+1",
        "M25.",
        "Laplace",
        "a+1 a +min(b, c) +2",
        "M26*.",
        "Adjusted Laplace",
        "a+1",
        "a +max(b, c) +2",
        "M27.",
        "Fager",
        "[M9] – -max(b, c )",
        "2",
        "M28*.",
        "Adjusted Fager",
        "[M9] J_max(b, c)",
        "M29*.",
        "Normalized PMIs",
        "PMI / NF(a)",
        "PMI / NFMax",
        "M30*.",
        "Simplified normalized PMI for",
        "VPCs",
        "log(ad) ax b + (1 – a) x c",
        "M31*.",
        "Normalized",
        "MIs",
        "MI / NF(a) MI / NFMax",
        "NF(a) = aP(x*) + (1 – a)P(*y) ae [0, 1] NFMax = max(P( x*), P(*y))",
        "a = fn",
        "C = f21",
        "=f(xy)",
        "= f ( xy )",
        "d = f22 = f ( xy)",
        "f ( x*)",
        "f ( x *)",
        "f(*y)",
        "f (* y )",
        "N",
        "Separately, we use the following two available sources of annotations: 3,078 VPC candidates extracted and annotated in (Baldwin, 2005) and 464 annotated LVC candidates used in (Tan et al., 2006).",
        "Both sets of annotations give both positive and negative examples.",
        "Our final VPC and LVC evaluation datasets were then constructed by intersecting the goldstandard datasets with our corresponding sets of extracted candidates.",
        "We also concatenated both sets of evaluation data for composite evaluation.",
        "This set is referred to as \"Mixed\".",
        "Statistics of our three evaluation datasets are summarized in",
        "Table 2.",
        "While these datasets are small, our primary goal in this work is to establish initial comparable baselines and describe interesting phenomena that we plan to investigate over larger datasets in future work.",
        "To evaluate the performance of AMs, we can use the standard precision and recall measures, as in much past work.",
        "We note that the ranked list of candidates generated by an AM is often used as a classifier by setting a threshold.",
        "However, setting a threshold is problematic and optimal threshold values vary for different AMs.",
        "Additionally, using the list of ranked candidates directly as a classifier does not consider the confidence indicated by actual scores.",
        "Another way to avoid setting threshold values is to measure precision and recall of only the n most likely candidates (the n-best method).",
        "However, as discussed in Evert and Krenn (2001), this method depends heavily on the choice of n. In this paper, we opt for average precision (AP), which is the average of precisions at all possible recall values.",
        "This choice also makes our results comparable to those of Pecina and Schlesinger (2006).",
        "Figure 1(a, b) gives the two average precision profiles of the 82 AMs presented in Pecina and Schlesinger (2006) when we replicated their experiments over our English VPC and LVC datasets.",
        "We observe that the average precision profile for VPCs is slightly concave while the one for LVCs is more convex.",
        "This can be interpreted as VPCs being more sensitive to the choice of AM than LVCs.",
        "Another point we observed is that a vast majority of Class I AMs, including PMI, its variants and association coefficients (excluding hypothesis tests), perform reasonably well in our application.",
        "In contrast, the performances of most of context-based and hypothesis test AMs are very modest.",
        "Their mediocre performance indicates their inapplicability to our VPC and LVC tasks.",
        "In particular, the high frequencies of particles in VPCs and light verbs in LVCs both undermine their contexts' discriminative power and skew the difference between observed and expected frequencies that are relied on in hypothesis tests."
      ]
    },
    {
      "heading": "4. Rank Equivalence",
      "text": [
        "We note that some AMs, although not mathematically equivalent (i.e., assigning identical scores to input candidates) produce the same lists of ranked candidates on our datasets.",
        "Hence, they achieve the same average precision.",
        "The ability to identify such groups of AMs is helpful in simplifying their formulas, which in turn assisting in analyzing their meanings.",
        "Definition: Association measures Mj andM2 are rank equivalent over a set C, denoted by Mj",
        "M , if and only if Mj(cj) > Mj(ck) <Z> M2(cj) > M2(ck) andMj(c) = Mj(ct) <Z>M2(c) = Mrfc,) for all cj, ck belongs to C where Mk(ci) denotes the score assigned to ci by the measure Mk.",
        "As a corollary, the following also holds for rank equivalent AMs:",
        "VPC data",
        "LVC data",
        "Mixed",
        "Total",
        "freq > 6)",
        "413",
        "100",
        "513",
        "Positive instances",
        "117",
        "(28.33%)",
        "28 (28%)",
        "145 (23.26%)",
        "Figure 1a.",
        "AP profile of AMs examined over our VPC data set.",
        "Figure l. Average precision (AP) performance of the 82 AMs from Pecina and Schlesinger (2006), on our English VPC and LVC datasets.",
        "Bold points indicate AMs discussed in this paper.",
        "□ Hypothesis test AMs ◊ Class I AMs, excluding hypothesis test AMs + Context-based AMs.",
        "Corollary: If Mj = M2 then APC(MJ) = APC(M2) where APC(Mi) stands for the average precision of the AM Mi over the data set C.",
        "Essentially, M1 and M2 are rank equivalent over a set C if their ranked lists of all candidates taken from C are the same, ignoring the actual calculated scores.",
        "As an example, the following 3 AMs: Odds ratio, Yule's co and Yule's Q (Table 3, row 5), though not mathematically equivalent, can be shown to be rank equivalent.",
        "Five groups of rank equivalent AMs that we have found are listed in Table 3.",
        "This allows us to replace the below 15 AMs with their (most simple) representatives from each rank equivalent group.",
        "[M3] Log likelihood ratio_"
      ]
    },
    {
      "heading": "5. Examination of Association Measures",
      "text": [
        "We highlight two important findings in our analysis of the AMs over our English datasets.",
        "Section 5.1 focuses on MI and PMI and Section 5.2 discusses penalization terms.",
        "In Figure 1, over 82 AMs, PMI ranks 11th in identifying VPCs while MI ranks 35th in identifying LVCs.",
        "In this section, we show how their performances can be improved significantly.",
        "Mutual Information (MI) measures the common information between two variables or the reduction in uncertainty of one variable given knowledge of the context of bigrams, the above formula can be holds between random variables, [M4] Pointwise MI (PMI) holds between specific values: PMI(x, been pointed out that PMI favors bigrams with low-frequency constituents, as evidenced by the product of two marginal frequencies in its denominator.",
        "To reduce this bias, a common solution is to assign more weight to the cooccurrence frequency f(xy)in the numerator by either raising it to some power k (Daille, 1994) or multiplying PMI with f(xy).",
        "Table 4 lists these adjusted versions of PMI and their performance over our datasets.",
        "We can see from Table 4 that the best performance of PMIk is obtained at k values less than one, indicating that it is better to rely less on f(xy).",
        "Similarly, multiplying f(xy)directly to PMI reduces the performance of PMI.",
        "As such, assigning more weight to f(xy) does not improve the AP performance of PMI.",
        "Another shortcoming of (P)MI is that both grow not only with the degree of dependence but also with frequency (Manning and Schiitze, 1999, p. 66).",
        "In particular, we can show that MI(X; Y) < min(H(X), H(Y)), where H(.)",
        "denotes entropy, and PMI(x,y) < min( - log P(x*), - log P(* y)).",
        "These two inequalities suggest that the allowed score ranges of different candidates vary and consequently, MI and PMI scores are not directly comparable.",
        "Furthermore, in the case of VPCs and LVCs, the differences among score ranges of different candidates are large, due to high frequencies of particles and light verbs.",
        "This has motivated us to normalize these scores before using them for comparison.",
        "We suggest MI and PMI be divided by one of the following two normalization factors: NF(a) = aP(x*) + (1 -a)P(*y) with ae [0, 1] and NFmax = max(P(x*), P(*y)) .",
        "NF(a), being dependent on alpha, can be optimized by setting an appropriate alpha value, which is inevitably affected by the MWE type and the corpus statistics.",
        "On the other hand, NFmax is independent of alpha and is recommended when one needs to apply normalized (P)MI to a mixed set of different MWE types or when sufficient data for parameter tuning is unavailable.",
        "As shown in Table 5, normalized MI and PMI show considerable improvements of up to 80%.",
        "Also, PMI and MI, after being normalized with NFmax, rank number one in VPC and LVC task, respectively.",
        "If one rewrites MI as = (1/ N) V fj x PMIij , it is easy to see the heavy dependence of MI on direct frequencies compared with PMI and this explains why normalization is a pressing need for MI.",
        "Table 5.",
        "AP performance of normalized (P)MI versus standard (P)MI.",
        "Best alpha settings shown in parentheses.",
        "It can be seen that given equal co-occurrence frequencies, higher marginal frequencies reduce the likelihood of being MWEs.",
        "This motivates us to use marginal frequencies to synthesize penalization terms which are formulae whose values are inversely proportional to the likelihood of being MWEs.",
        "We hypothesize that incorporating such penalization terms can improve the respective AMs detection AP.",
        "Take as an example, the AMs [M21] Brawn-Blanquet (a.k.a.",
        "Minimum Sensitivity) and [M22] Simpson.",
        "These two AMs are identical, except for one difference in the denominator: Brawn-Blanquet uses max(b, c); Simpson uses min(b, c).",
        "It is intuitive and confirmed by our experiments that penalizing against the more frequent constituent by choosing max(b, c) is more effective.",
        "This is further attested in AMs [M23] 5 Cost and [M25] Laplace, where we tried to replace the min(b, c) term with max(b, c).",
        "Table 6 shows the average precision on our datasets for all these AMs.",
        "AM",
        "VPCs",
        "LVCs",
        "Mixed",
        "MI / NF(a)",
        ".508",
        "( a = .48)",
        ".583",
        "(a = .47)",
        ".516",
        "(a = .5)",
        "MI / NFmax",
        ".508",
        ".584",
        ".518",
        "[M2] MI",
        ".273",
        ".435",
        ".289",
        "PMI / NF(a)",
        ".592",
        "(a = .8)",
        ".554",
        "( a = .48)",
        ".588",
        "( a = .77)",
        "PMI / NFmax",
        ".565",
        ".517",
        ".556",
        "[M4] PMI",
        ".510",
        ".566",
        ".515",
        "AM",
        "VPCs",
        "LVCs",
        "Mixed",
        "Best [M6] PMIk",
        ".547",
        "(k =.13)",
        ".573",
        "(k = .85)",
        ".544",
        "(k = .32)",
        "[M4] PMI",
        ".510",
        ".566",
        ".515",
        "[M5] Local-PMI",
        ".259",
        ".393",
        ".272",
        "[M1] Joint Prob.",
        ".170",
        ".28",
        ".175",
        "In the [M27] Fager AM, the penalization term max(b, c) is subtracted from the first term, which is no stranger but rank equivalent to [M7] PMI.",
        "In our application, this AM is not good since the second term is far larger than the first term, which is less than 1.",
        "As such, Fager is largely equivalent to just -'/2 max(b, c).",
        "In order to make use of the first term, we need to replace the constant V2 by a scaled down version of max(b, c).",
        "We have approximately derived 1/40N as a lower bound estimate of max(b, c) using the independence assumption, producing [M28] Adjusted Fager.",
        "We can see from Table 7 that this adjustment improves Fager on both datasets.",
        "The next experiment involves [M14] Third Sokal Sneath, which can be shown to be rank equivalent to b -c. We further notice that frequencies c of particles are normally much larger than frequencies b of verbs.",
        "Thus, this AM runs the risk of ranking VPC candidates based on only frequencies of particles.",
        "So, it is necessary that we scale b and c properly as in [M14'] -axb -(1 -a)xc .",
        "Having scaled the constituents properly, we still see that [M14'] by itself is not a good measure as it uses only constituent frequencies and does not take into consideration the co-occurrence frequency of the two constituents.",
        "This has led us to experiment denominator of [MR14''] is obtained by removing the minus sign from [MR14'] so that it can be used as a penalization term.",
        "The choice of PMI in the numerator is due to the fact that the denominator of [MR14''] is in essence similar to NF(a) = aP(x*) +(1 -a)P(*y) , which has been successfully used to divide PMI in the normalized PMI experiment.",
        "We heuristically tried to simplify [MR14''] to the following AM",
        "The setting of alpha in",
        "Table 8 below is taken from the best alpha setting obtained the experiment on the normalized PMI (Table 5).",
        "It can be observed from Table 8 that [MR14'''], being computationally simpler than normalized PMI, performs as well as normalized PMI and better than Third Sokal-Sneath over the VPC data set.",
        "With the same intention and method, we have found that while addition of marginal frequencies is a good penalization term for VPCs, the product of marginal frequencies is more suitable for LVCs (rows l and 2, Table 9).",
        "As with the linear combination, the product bc should also be weighted accordingly as bac(l~a>.",
        "The best alpha value is also taken from the normalized PMI experiments (Table 5), which is nearly .5.",
        "Under this setting, this penalization term is exactly the denominator of the [M18] Odds Ratio.",
        "Table 9 below show our experiment results in deriving the penalization term for LVCs.",
        "AM",
        "VPCs",
        "LVCs",
        "Mixed",
        "[M21]Brawn-Blanquet",
        ".478",
        ".578",
        ".486",
        "[M22] Simpson",
        ".249",
        ".382",
        ".260",
        "[M24] Adjusted S Cost",
        ".485",
        ".577",
        ".492",
        "[M23] S cost",
        ".249",
        ".388",
        ".260",
        "[M26] Adjusted Laplace",
        ".486",
        ".577",
        ".493",
        "[M25] Laplace",
        ".241",
        ".388",
        ".254",
        "AM",
        "VPCs",
        "LVCs",
        "Mixed",
        "PMI / NF(a)",
        ".592 (a =.8)",
        ".554 (a =.48)",
        ".588 (a =.77)",
        "log(ad)",
        "[M30]",
        "ax b + (1 -a) x c",
        ".600 (a =.8)",
        ".484 (a =.48)",
        ".588 (a =.77)",
        "[M14] Third",
        "Sokal Sneath",
        ".565",
        ".453",
        ".546",
        "AM",
        "VPCs",
        "LVCs",
        "Mixed",
        "[M28] Adjusted Fager",
        ".564",
        ".543",
        ".554",
        "[M27] Fager",
        ".552",
        ".439",
        ".525"
      ]
    },
    {
      "heading": "6. Conclusions",
      "text": [
        "We have conducted an analysis of the 82 AMs assembled in Pecina and Schlesinger (2006) for the tasks of English VPC and LVC extraction over the Wall Street Journal Penn Treebank data.",
        "In our work, we have observed that AMs can be divided into two classes: ones that do not use context (Class I) and ones that do (Class II), and find that the latter is not suitable for our VPC and LVC detection tasks as the size of our corpus is too small to rely on the frequency of candidates' contexts.",
        "This phenomenon also revealed the inappropriateness of hypothesis tests for our detection task.",
        "We have also introduced the novel notion of rank equivalence to MWE detection, in which we show that complex AMs may be replaced by simpler AMs that yield the same average precision performance.",
        "We further observed that certain modifications to some AMs are necessary.",
        "First, in the context of ranking, we have proposed normalizing scores produced by MI and PMI in cases where the distributions of the two events are markedly different, as is the case for light verbs and particles.",
        "While our claims are limited to the datasets analyzed, they show clear improvements: normalized PMI produces better performance over our mixed MWE dataset, yielding an average precision of 58.8% compared to 51.5% when using standard PMI, a significant improvement as judged by paired T test.",
        "Normalized MI also yields the best performance over our LVC dataset with a significantly improved AP of 58.3%.",
        "We also show that marginal frequencies can be used to form effective penalization terms.",
        "In particular, we find that axb + (1 - a) x c is a good penalization term for VPCs, while bac(1~a) is suitable for LVCs.",
        "Our introduced alpha tuning parameter should be set to properly scale the values b and c, and should be optimized per MWE type.",
        "In cases where a common factor is applied to different MWE types, max(b, c) is a better choice than min(b, c).",
        "In future work, we plan to expand our investigations over larger, web-based datasets of English, to verify the performance gains of our modified AMs.",
        "Acknowledgement",
        "This work was partially supported by a National Research Foundation grant \"Interactive Media Search\" (grant # R 252 000 325 279).",
        "AM",
        "VPCs",
        "LVCs",
        "Mixed",
        "-b -c",
        ".565",
        ".453",
        ".546",
        "1/bc",
        ".502",
        ".532",
        ".502",
        "[M18] Odds ratio",
        ".443",
        ".567",
        ".456",
        "about, aback, aboard, above, abroad, across, adrift, ahead, along, apart, around, aside, astray, away, back, backward, backwards, behind, by, down, forth, forward, forwards, in, into, off, on, out, over, past, round, through, to, together, under, up, upon, without."
      ]
    }
  ]
}
