{
  "info": {
    "authors": [
      "Alessandra Zarcone",
      "Jason Utt",
      "Sebastian Padó"
    ],
    "book": "Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012)",
    "id": "acl-W12-1707",
    "title": "Modeling covert event retrieval in logical metonymy: probabilistic and distributional accounts",
    "url": "https://aclweb.org/anthology/W12-1707",
    "year": 2012
  },
  "references": [
    "acl-C00-2094",
    "acl-C10-1011",
    "acl-D10-1029",
    "acl-J03-2004",
    "acl-J07-2002",
    "acl-J10-4006",
    "acl-L08-1019",
    "acl-P07-1028",
    "acl-P98-2127",
    "acl-P99-1004",
    "acl-W10-2803",
    "acl-W11-0115"
  ],
  "sections": [
    {
      "text": [
        "In: R. Levy & D. Reitter (Eds.",
        "), Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics (CMCL 2012), pages 70?79, Montre?al, Canada, June 7, 2012. c?2012 Association for Computational Linguistics Modeling covert event retrieval in logical metonymy: probabilistic and distributional accounts"
      ]
    },
    {
      "heading": "Abstract",
      "text": [
        "Logical metonymies (The student finished the beer) represent a challenge to composi-tionality since they involve semantic content not overtly realized in the sentence (covert events ?",
        "drinking the beer).",
        "We present a contrastive study of two classes of computational models for logical metonymy in German, namely a probabilistic and a distributional, similarity-based model.",
        "These are built using the SDEWAC corpus and evaluated against a dataset from a self-paced reading and a probe recognition study for their sensitivity to thematic fit effects via their accuracy in predicting the correct covert event in a metonymical context.",
        "The similarity-based models allow for better coverage while maintaining the accuracy of the probabilistic models."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Logical metonymies (The student finished the beer) require the interpretation of a covert event which is not overtly realized in the sentence (?",
        "drinking the beer).",
        "Logical metonymy has received much attention as it raises issues that are relevant to both theoretical as well as cognitive accounts of language.",
        "On the theoretical side, logical metonymies constitute a challenge for theories of compositionality (Par-tee et al., 1993; Baggio et al., in press) since their interpretation requires additional, inferred information.",
        "There are two main accounts of logical metonymy: According to the lexical account, a type clash between an event-subcategorizing verb (finish) and an entity-denoting object (beer) triggers the recovery of a covert event from complex lexical entries, such as qualia structures (Pustejovsky, 1995).",
        "The pragmatic account of logical metonymy suggests that covert events are retrieved through post-lexical inferences triggered by our world knowledge and communication principles (Fodor and Lepore, 1998; Cartson, 2002; De Almeida and Dwivedi, 2008).",
        "On the experimental side, logical metonymy leads to higher processing costs (Pylkk?nen and McEl-ree, 2006; Baggio et al., 2010).",
        "As to covert event retrieval, it has been found that verbs cue fillers with a high thematic fit for their argument positions (e.g. arrest agent ????",
        "cop, (Ferretti et al., 2001)) and that verbs and arguments combined cue fillers with a high thematic fit for the remaining argument slots (e.g. ?journalist , check?",
        "patient ?????",
        "spelling but ?mechanic, check?",
        "patient ?????",
        "car (Bick-nell et al., 2010).",
        "The interpretation of logical metonymy is also highly sensitive to context (e.g. ?confectioner , begin, icing?",
        "covertevent ????????",
        "spread but ?child , begin, icing?",
        "covertevent ????????",
        "eat (Zarcone and Pad?, 2011; Zarcone et al., 2012).",
        "It thus provides an excellent test bed for cognitively plausible computational models of language processing.",
        "We evaluate two classes of computational models for logical metonymy.",
        "The classes represent the two main current approaches in lexical semantics: probabilistic and distributional models.",
        "Probabilistic models view the interpretation as the assignment of values to random variables.",
        "Their advantage is that they provide a straightforward way to include context, by simply including additional random variables.",
        "However, practical estimation of complex models typically involves independence assumptions, which",
        "may or may not be appropriate, and such models only take first-order co-occurrence into account1.",
        "In contrast, distributional models represent linguistic entities as co-occurrence vectors and phrase interpretation as a vector similarity maximization problem.",
        "Distributional models typically do not require any independence assumptions, and include second-order co-occurrences.",
        "At the same time, how to integrate context into the vector computation is essentially an open research question (Mitchell and Lapata, 2010).",
        "In this paper, we provide the first (to our knowledge) distributional model of logical metonymy by extending the context update of Lenci's ECU model (Lenci, 2011).",
        "We compare this model to a previous probabilistic approach (Lapata and Las-carides, 2003a; Lapata et al., 2003b).",
        "In contrast to most experimental studies on logical metonymy, which deal with English data (with the exception of Lapata et al. (2003b)), we focus on German.",
        "We estimate our models on a large web corpus and evaluate them on a psycholinguistic dataset (Zarcone and Pad?, 2011; Zarcone et al., 2012).",
        "The task we use to evaluate our models is to distinguish covert events with a high typicality / thematic fit (e.g.",
        "The student finished the beer ??",
        "drinking) from low typicality / thematic fit covert events (??",
        "brewing)."
      ]
    },
    {
      "heading": "2 Probabilistic models of logical metonymy",
      "text": [
        "Lapata et al. (2003b; 2003a) model the interpretation of a logical metonymy (e.g.",
        "The student finished the beer) as the joint distribution P (s, v, o, e) of the variables s (the subject, e.g. student), v (the metonymic verb, e.g. finish), o (the object, e.g. beer), e (the covert event, drinking).",
        "This model requires independence assumptions for estimation.",
        "We present two models with different independence assumptions.",
        "1This statement refers to the simple probabilistic models we consider, which are estimated directly from corpus co-occurrence frequencies.",
        "The situation is different for more complex probabilistic models, for example generative models that introduce latent variables, which can amount to clustering based on higher-order co-occurrences, as in, e.g., Prescher et al. (2000)."
      ]
    },
    {
      "heading": "2.1 The SOVp model",
      "text": [
        "Lapata et al. develop a model which we will refer to as the SOVp model.2 It assumes a generative process which first generates the covert event e and then generates all other variables based on the choice of e: P (s, v, o, e) ?",
        "P (e) P (o|e) P (v|e) P (s|e) They predict that the selected covert event e?",
        "for a given context is the event which maximizes",
        "These distributions are estimated as follows:",
        "where N is the number of occurrences of full verbs in the corpus; f(e) is the frequency of the verb e;",
        "another full verb."
      ]
    },
    {
      "heading": "2.2 The SOp model",
      "text": [
        "In Lapata et al's covert event model, v, the metonymic verb, was used to prime different choices of e for the same object (begin book??",
        "writing; enjoy book??",
        "reading).",
        "In our dataset (Sec.",
        "4), we keep v constant and consider e only as a function of s and o.",
        "Thus, the second model we consider is the SOp model which does not consider v: P (s, v, o, e) ?",
        "P (s, o, e) ?",
        "P (e) P (o|e) P (s|e) Again, the preferred interpretation e?",
        "is the one that maximizes P (s, v, o, e):",
        "simplified model to distinguish it from a full model.",
        "Since the full model performs worse, we do not include it into consideration and use a more neutral name for the simplified model."
      ]
    },
    {
      "heading": "3 Similarity-based models",
      "text": []
    },
    {
      "heading": "3.1 Distributional semantics",
      "text": [
        "Distributional or vector space semantics (Turney and Pantel, 2010) is a framework for representing word meaning.",
        "It builds on the Distributional Hypothesis (Harris, 1954; Miller and Charles, 1991) which states that words occurring in similar contexts are semantically similar.",
        "In distributional models, the meaning of a word is represented as a vector whose dimensions represent features of its linguistic context.",
        "These features can be chosen in different ways; popular choices are simple words (Schütze, 1992) or lexicalized dependency relations (Lin, 1998; Pad?",
        "and Lapata, 2007).",
        "Semantic similarity can then be approximated by vector similarity using a wide range of similarity metrics (Lee, 1999).",
        "A recent multi-purpose framework in distributional semantics is Distributional Memory (DM, Baroni and Lenci (2010)).",
        "DM does not immediately construct vectors for words.",
        "Instead, it extracts a three-dimensional tensor of weighted word-link-word tuples each of which is mapped onto a score by a function ?",
        ": ?w1 l w2?",
        "?",
        "R+.",
        "For example, ?pencil obj use?",
        "has a higher weight than ?elephant obj use?.",
        "The set of links can be defined in different ways, yielding various DM instances.",
        "Ba-roni and Lenci present DepDM (mainly syntactic links such as subj_tr ), LexDM (strongly lexicalized links, e.g., such_as), or TypeDM (syntactic and lexicalized links).3 The benefit of the tensor-based representation is that it is general, being applicable to many tasks.",
        "Once a task is selected, a dedicated semantic space for this task can be generated efficiently from the tensor.",
        "For example, the word by link-word space (W1 ?",
        "LW2) contains vectors for the words w1 whose dimensions are labeled with ?l, w2?",
        "pairs.",
        "The word-word by link space (W1W2 ?",
        "L) contains co-occurrence vectors for word pairs ?w1, w2?",
        "whose dimensions are labeled with l."
      ]
    },
    {
      "heading": "3.2 Compositional Distributional Semantics",
      "text": [
        "Probabilistic models can account for composition-ality by estimating conditional probabilities.",
        "Com3l?1 is used to denote the inverse link of l (i.e., exchanging the positions of w1 and w2).",
        "positionality is less straightforward in a similarity-based distributional model, because similarity-based distributional models traditionally model meaning at word level.",
        "Nevertheless, the last years have seen a wave of distributional models which make progress at building compositional representations of higher-level structures such as noun-adjective or verb-argument combinations (Mitchell and Lapata, 2010; Guevara, 2011; Reddy et al., 2011).",
        "Lenci (2011) presents a model to predict the degree of thematic fit for verb-argument combinations: the Expectation Composition and Update (ECU) model.",
        "More specifically, the goal of ECU is explain how the choice of a specific subject for a given verb impacts the semantic expectation for possible objects.",
        "For example, the verb draw alone might have fair, but not very high, expectations for the two possible objects landscape and card.",
        "When it is combined with the subject painter, the resulting phrase painter draw the expectation for the object landscape should increase, while it should drop for card.",
        "The idea behind ECU is to first compute the verb's own expectations for the object from a TypeDM W1 ?",
        "LW2 matrix and then update it with the subject's expectations for the object, as mediated by the TypeDM verb link type.4 More formally, the verb's expectations for the object are defined as",
        "The subject's expectations for the object are EXS(s) = ?o.",
        "?",
        "('s verb o?)",
        "And the updated expectation is",
        "where ?",
        "is a composition operation which Lenci instantiates as sum and product, following common practice in compositional distributional semantics (Mitchell and Lapata, 2010).",
        "The product composition approximates a conjunction, promoting objects that are strongly preferred by both verb and subject.",
        "It is, however, also prone to sparsity problems as well",
        "shortcomings of the scoring function ?.",
        "The sum composition is more akin to a disjunction where it suffices that an object is strongly preferred by either the verb or the subject.",
        "It would be possible to use these scores as direct estimates of expectations, however, sinceEXSV contains three lexical variables, sparsity is a major issue.",
        "ECU thus introduces a distributional generalization step.",
        "It only uses the updated expectations to identify the 20 most expected nouns for the object position.",
        "It then determines the prototype of the updated expectations as the centroid of their W1?LW2 vectors.",
        "Now, the thematic fit for any noun can be computed as the similarity of its vector to the prototype.",
        "Lenci evaluates ECU against a dataset from Bick-nell et al. (2010), where objects (e.g. spelling) are matched with a high-typicality subject-verb combinations (e.g. ?journalist, check?",
        "- high thematic fit) and with a low-typicality subject-verb combination (e.g. ?mechanic, check?",
        "- low thematic fit).",
        "ECU is in fact able to correctly distinguish between the two contexts differing in thematic fit with the object."
      ]
    },
    {
      "heading": "3.3 Cognitive relevance",
      "text": [
        "Similarity-based models build upon the Distributional Hypothesis, which, in its strong version, is a cognitive hypothesis about the form of semantic representations (Lenci, 2008): the distributional be-havior of a word reflects its semantic behavior but is also a direct correlate of its semantic content at the cognitive level.",
        "Also, similarity-based models are highly compatible with known features of human cognition, such as graded category membership (Rosch, 1975) or multiple sense activation (Erk, 2010).",
        "Their cognitive relevance for language has been supported by studies of child lexical development (Li et al., 2004), category-related deficits (Vigliocco et al., 2004), selectional preferences (Erk, 2007), event types (Zarcone and Lenci, 2008) and more (see Landauer et al. (2007) and Baroni and Lenci (2010) for a review)."
      ]
    },
    {
      "heading": "3.4 Modeling Logical Metonymy with ECU",
      "text": [
        "The hypothesis that we follow in this paper is that the ECU model can also be used, with modifications, to predict the interpretation of logical metonymy.",
        "The underlying assumption is that the interpretation of logical metonymy is essentially the recovery of a covert event with a maximal thematic fit (high-typicality) and can thus make use of ECU's mechanisms to treat verb-argument composition.",
        "Strong evidence for this assumption has been found in psycholinguistic studies, which have established that thematic fit dynamically affects processing, with on-line updates of expectations for typical fillers during the incremental processing of linguistic input (see McRae and Matsuki (2009) for a review).",
        "Thus, we can hope to transfer the benefits of similarity-based models (notably, high coverage) to the interpretation of logical metonymy.",
        "The ECU model nevertheless requires some modifications to be applicable to logical metonymy.",
        "Both the entity of interest and the knowledge sources change.",
        "The entity of interest used to be the object of the sentence; now it is the covert event, which we will denote with e. As for knowledge sources, there are three sources in logical metonymy.",
        "These are (a), the subject (compare the author began the beer and the reader began the book)); (b), the object the reader began the book vs. the reader began the sandwich); and (c), the metonymic verb (compare Peter began the report vs. Peter enjoyed the report).",
        "The basic equations of ECU can be applied to this new scenario as follows.",
        "We first formulate three basic equations that express the expectations of the covert event given the subject, object, and metonymic verb individually.",
        "They are all derived from direct dependency relations in the DM tensor (e.g., the novel metonymic verb?covert event relation from the verbal complement relation):",
        "To combine (or update) these basic expectations into a final expectation, we propose two variants: ECU SOV In this model, we compose all three expectations:",
        "We will refer to this model as SOV?",
        "when the composition function is sum, and as the SOV?",
        "model when the composition function is product.",
        "ECU SO Analogous to the SO probabilistic model, this model abstracts away from the metonymic verb.",
        "We assume most information about an event to be determined by the subject and object:",
        "After the update, the prototype computation proceeds as defined in the original ECU.",
        "We will refer to this model as SO?",
        "when the composition function is sum, and as the SO?",
        "model when the composition function is product."
      ]
    },
    {
      "heading": "4 Experimental Setup",
      "text": [
        "We evaluate the probabilistic models (Sec.",
        "2) and the similarity-based models (Sec.",
        "3) on a dataset constructed from two German psycholinguistic studies on logical metonymy.",
        "One study used self-paced reading and the second one probe recognition.",
        "Dataset The dataset we use is composed of 96 sentences.",
        "There are 24 sets of four ?s, v, o, e?",
        "tuples, where s is the object, v the metonymic verb, o the object and e the covert event.",
        "The materials are illustrated in Table 1.",
        "As can be seen, all tuples within a set share the same metonymic verb and the same object.",
        "Each of the two subject e is matched once with a high-typicality covert event and once with a low-typicality covert event.",
        "This results in 2 high-typicality tuples and 2 low-typicality tuples in each set.",
        "Typical events (e) were elicited by 20 participants given the corresponding object o, subjects were elicited by 10 participants as the prototypical agents subjects for each e, o combination.",
        "The experiments yielded a main effect of typicality on self-paced reading times (Zarcone and Pad?, 2011) and on probe recognition latencies (Zarcone et al., 2012): typical events involved in logical metonymy interpretation are read faster and take longer to be rejected as probe words after sentences which evoke them.",
        "The effect is seen early on (after the patient position in the self-paced reading and at short ISI for the probe recognition), suggesting that knowledge of typical events is quickly integrated in processing and that participants access a broader pool of knowledge than what has traditionally been argued to be in the lexical entries of nouns (Pustejovsky, 1995).",
        "The finding is in agreement with results of psycholinguistic studies which challenge the very distinction between world knowledge and linguistic knowledge (Hagoort et al., 2004; McRae and Matsuki, 2009).",
        "DM for German Since DM exists only for English, we constructed a German analog using the 884M word SDEWAC web corpus (Faa?",
        "et al, 2010) parsed with the MATE German dependency parser (Bohnet, 2010).",
        "From this corpus, we extract 55M instances of simple syntactic relations (subj_tr, subj_intr, obj, iobj, comp, nmod) and 104M instances of lexicalized patterns such as noun?prep?noun e.g. ?Recht auf Auskunft?",
        "(?right to information?",
        "), or adj?noun-(of)noun such as ?strittig Entscheidung Schiedsrichter?",
        "(?contested decision referee?).",
        "These lexicalized patterns make our model roughly similar to the English TypeDM model (Sec.",
        "3.1.1).",
        "As for ?, we used local mutual information (LMI) as proposed by Baroni and Lenci (2010).",
        "The LMI of a triple is defined as Ow1lw2 log(Ow1lw2/Ew1lw2), where Ow1lw2 is the observed co-occurrence frequency of the triple and Ew1lw2 its expected co-occurrence frequency (under the assumption of independence).",
        "Like standard MI, LMI measures the informativity or surprisal of a co-occurrence, but",
        "weighs it by the observed frequency to avoid the overestimation for low-probability events."
      ]
    },
    {
      "heading": "4.1 Task",
      "text": [
        "We evaluate the models using a binary selection task, similar to Lenci (2011).",
        "Given a triple ?s, v, o?",
        "and a pair of covert events e, e?",
        "(cf. rows in Tab.",
        "1), the task is to pick the high-typicality covert event for the given triple: ?Chauffeur, vermeiden, Auto?",
        "?",
        "fahren/reparieren (?driver, avoid, car?",
        "?",
        "drive/repair).",
        "Since our dataset consists of 96 sentences, we have 48 such contexts.",
        "With the probabilistic models, we compare the probabilities P (s, v, o, e) and P (s, v, o, e?)",
        "(ignoring v in the SO model).",
        "Analogously, for the similarity-based models, we compute the similarities of the vectors for e and e?",
        "to the prototype vectors for the expectations EXSOV (s, v, o) and predict the one with higher similarity.",
        "For the simplified ECU SO model, we use EXSO(s, o) as the point of comparison."
      ]
    },
    {
      "heading": "4.2 Baseline",
      "text": [
        "Following the baseline choice in Lapata et al. (2003b), we evaluated the probabilistic models against a baseline (Bp) which, given a ?s, v, o?",
        "triplet (e.g. ?Chauffeur, vermeiden, Auto?",
        "), scores a ?hit?",
        "if the P?",
        "(e|o) for the high-typicality e is higher than the P?",
        "(e|o) for the low-typicality e. The similarity-based models were evaluated against a baseline (Bs) which, given an ?s, v, o?",
        "triplet (e.g. ?Chauffeur, vermeiden, Auto?",
        "), makes a correct prediction if the prototypical event vector for o has a higher thematic fit (i.e. similarity) with the high-typicality e than with the low-typicality e. Since our dataset is counterbalanced ?",
        "that is, each covert event appears once as the high-typicality event for a given object (with a congruent subject) and once as the low-typicality event ?",
        "the baseline predicts the correct covert event in exactly 50% of the cases.",
        "Note, however, that this is not a random baseline: the choice of the covert event is made deterministically on the basis of the input parameters."
      ]
    },
    {
      "heading": "4.3 Evaluation measures",
      "text": [
        "We evaluate the output of the model with the standard measures coverage and accuracy.",
        "Coverage is defined as the percentage of datapoints for which a model can make a prediction.",
        "Lack of coverage arises primarily from sparsity, that is, zero counts for co-occurrences that are necessary in the estimation of a model.",
        "Accuracy is computed on the covered contexts only, as the ratio of correct predictions to the number of predictions of the model.",
        "This allows us to judge the quality of the model's predictions independent of its coverage.",
        "We also consider a measure that combines coverage and accuracy, Backoff Accuracy, defined as: coverage?accuracy+((1?coverage)?0.5).",
        "Back-off Accuracy emulates a backoff procedure: the model's predictions are adopted where they are available; for the remaining datapoints, it assumes baseline performance (in the current setup, 50%).",
        "The Backoff Accuracy of low-coverage models tends to degrade towards baseline performance.",
        "We determine the significance of differences between models with a ?2 test, applied to a 2?2 contingency matrix containing the number of correct and incorrect answers.",
        "Datapoints outside a model's coverage count half for each category, which corresponds exactly to the definition of Backoff Accuracy."
      ]
    },
    {
      "heading": "5 Results",
      "text": [
        "The results are shown in Table 2.",
        "Looking at the probabilistic models, we find SOp yields better coverage and better accuracy than SOVp (Lapata's simplified model).",
        "It is worth noting the large difference in coverage, namely .75 as opposed to .44: The SOVp model is unable to make a prediction for more than half of all contexts.",
        "This is due to the fact that many ?o, v?",
        "combinations are unattested in the corpus.",
        "Even on those contexts for which the probabilistic SOVp model can make a prediction, it is less reliable than the more general SOp model (0.62 versus 0.75 accuracy).",
        "This indicates that, at least on our dataset, the metonymic verb does not systematically help to predict the covert event; it rather harms performance by introducing noisy estimates.",
        "As the lower half of the Table shows, the SOVp model does not significantly outperform any other model (including both baselines Bp and Bs).",
        "The distributional models do not have such coverage issues.",
        "The main problematic combination for the similarity model is ?Pizzabote hassen Pizza?",
        "(i.e. ?Pizza delivery man hate pizza?)",
        "which is paired with the covert events liefern (deliver) and backen (bake).",
        "The computation of ECU predictions for",
        "for all probabilistic and similarity-based models (**: p<0.01, *: p?0.05, -: p>0.05).",
        "For ??",
        "(SO?",
        "?",
        "SOVp and SO?",
        "?",
        "SOV?)",
        "p was just above 0.05 (p=0.053).",
        "this combination requires corpus transitive corpus constructions for Pizzabote, in the corpus it is only attested once as the subject of the intransitive verb kommen (come).",
        "Among distributional models, the difference between SO and SOV is not as clear-cut as on the probabilistic side.",
        "We observe an interaction with the composition operation.",
        "Sum is less sensitive to complexity of updating: for sum models, the inclusion of the metonymic verb (SOV?",
        "vs.",
        "SOV?)",
        "does not make any difference.",
        "On the side of the product models, there is a major difference similar to the one for the probabilistic models: SOV?",
        "is the worst model at near-baseline performance, and SO?",
        "is the best one.",
        "This supports our interpretation from above that the metonymic model introduces noisy expectations which, in the product model, have the potential of disrupting the update process.",
        "Comparing the best models from the probabilistic and similarity-based classes (SOp and SO?",
        "), we find that both significantly outperform the baselines.",
        "This shows that the subject contributes to the models with a significant improvement over the baseline models, which are only informed by the object.",
        "Their backoff accuracies do not significantly differ from one another, which is not surprising given the small size of our dataset, however, the similarity-based model outperforms the probabilistic model by 1% Backoff Accuracy.",
        "The two models have substantially different profiles: the accuracy of the probabilistic model is 5% higher (0.70 vs. 0.75); at the same time, its coverage is much lower.",
        "It covers only 75% of the contexts, while the distributional model SO?",
        "covers all but one (98%)."
      ]
    },
    {
      "heading": "6 Discussion",
      "text": [
        "As mentioned above, the main issue with the probabilistic models is coverage.",
        "This is due to the reliance of these models on first-order co-occurrence.",
        "For example, probabilistic models cannot assign a probability to any of the triples ?Dieb/Juwelier schmuggeln/schleifen Diamant?",
        "(?thief/jeweler smuggle/cut diamond?",
        "), since the subjects do not occur with either of the verbs in corpus, even though Diamant does occur as the object of both.",
        "In contrast, the similarity-based models are able to compute expectations for these triples from second-order co-occurrences by taking into account other verbs that co-occur with Diamant.",
        "The ECU model is not punished by the extra context, as both Dieb and Diamant are associated with the verbs: stehlen (steal),",
        "(chauffeur), Mechaniker (mechanic) and Auto (car).",
        "rauben (thieve), holen (get), entwenden (purloin), erbeuten (snatch), verkaufen (sell), nehmen (take), klauen (swipe).",
        "We also note that these are typical events for a thief, which fits the intuition that Dieb is more predictive of the event than Diamant.",
        "For both ?Chauffeur Auto?",
        "and ?Mechaniker Auto?",
        "the probabilistic model predicts fahren due to the high overall frequency of fahren.",
        "The distributional model, however, takes the mutual information into account and is thus able to determine events that are more strongly associated with Mechaniker (e.g. bauen, reparieren, etc.)",
        "while at the same time discounting the uninformative verb fahren.",
        "There are, however, items that all models have difficulty with.",
        "Three such cases are due to a frequency disparity between the high and low-typicality event.",
        "E.g. for ?Lehrerin Klausur benoten/schreiben?",
        "(?teacher exam grade/take?",
        "), schreiben occurs much more frequently than benoten.",
        "In the case of ?Sch?ler Geschichte lernen/schreiben?",
        "(?student story learn/write?",
        "), none of the models or baselines correctly assigned lernen.",
        "The probabilistic models are influenced by the very frequent Geschichte schreiben which is part of an idiomatic expression (to write history).",
        "On the other hand, the distributional models judge the story and history sense of the word to have the most informative events, e.g. erz?hlen (tell), lesen (read), h?ren (hear), erfinden (invent), and studieren (study), lehren (teach).",
        "The baselines were able to correctly choose auspacken (unwrap) over einpacken (wrap) for ?Geburtstagskind Geschenk?",
        "(?birthday-boy/girl present?)",
        "while the models were not.",
        "The prob-5The combination Mechaniker fahren was seen once more often than Mechaniker reparieren.",
        "abilistic models lacked coverage and were not able to make a prediction.",
        "For the distributional models, while both auspacken and verpacken (wrap) are highly associated with Geschenk, the most strongly associated actions of Geburtstagskind are extraordinarily diverse, e.g.: bekommen (receive), sagen (say), auffuttern (eat up), herumkommandieren (boss around), ausblasen (blow out).",
        "Neither of the events of interest though were highly associated."
      ]
    },
    {
      "heading": "7 Future Work",
      "text": [
        "We see a possible improvement in the choice of the number of fillers, with which we construct the prototype vectors.",
        "A smaller number might lead to less noisy prototypes.",
        "It has been shown (Bergsma et al., 2010) that the meaning of the prefix verb can be accurately predicted using the stem's vector, when compositional-ity applies.",
        "We suspect covert events that are prefix verbs to suffer from sparser representations than the vectors of their stem.",
        "E.g., absaugen (vacuum off ) is much less frequent than the semantically nearly identical saugen (vacuum).",
        "Thus, by leveraging the richer representation of the stem, our distributional models could more likely assign the correct event."
      ]
    },
    {
      "heading": "8 Conclusions",
      "text": [
        "We have presented a contrastive study of two classes of computational models, probabilistic and distributional similarity-based ones, for the prediction of covert events for German logical metonymies.",
        "We found that while both model classes models outperform baselines which only take into account information coming from the object, similarity-based models rival and even outperform probabilistic models.",
        "The reason is that probabilistic models have to rely on first-order co-occurrence information which suffers from sparsity issues even in large web corpora.",
        "This is particularly true for languages like German that have a complex morphology, which tends to aggravate sparsity (e.g., through compound nouns).",
        "In contrast, similarity-based models can take advantage of higher-order co-occurrences.",
        "Provided that some care is taken to identify reasonable vector composition strategies, they can maintain the accuracy of probabilistic models while guaranteeing higher coverage."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank Alessandro Lenci, Siva Reddy and Sabine Schulte im Walde for useful feedback and discussion.",
        "The research for this paper has been funded by the German Research Foundation (Deutsche Forschungsgemeinschaft) as part of the SFB 732 ?Incremental specification in context?",
        "/ project D6 ?Lexical-semantic factors in event interpretation?",
        "at the University of Stuttgart."
      ]
    }
  ]
}
