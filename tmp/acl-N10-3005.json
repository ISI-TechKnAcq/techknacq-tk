{
  "info": {
    "authors": [
      "Eugenie Giesbrecht"
    ],
    "book": "Proceedings of the NAACL HLT 2010 Student Research Workshop",
    "id": "acl-N10-3005",
    "title": "Towards a Matrix-based Distributional Model of Meaning",
    "url": "https://aclweb.org/anthology/N10-3005",
    "year": 2010
  },
  "references": [
    "acl-D08-1094",
    "acl-J06-3003",
    "acl-J07-2002",
    "acl-L08-1204",
    "acl-N03-1036",
    "acl-P98-2127",
    "acl-W09-0211"
  ],
  "sections": [
    {
      "text": [
        "FZI Forschungszentrum Informatik at the University of Karlsruhe Haid-und-Neu-Str.",
        "10-14, Karlsruhe, Germany",
        "giesbrecht@fzi.de",
        "Vector-based distributional models of semantics have proven useful and adequate in a variety of natural language processing tasks.",
        "However, most of them lack at least one key requirement in order to serve as an adequate representation of natural language, namely sensitivity to structural information such as word order.",
        "We propose a novel approach that offers a potential of integrating order-dependent word contexts in a completely unsupervised manner by assigning to words characteristic distributional matrices.",
        "The proposed model is applied to the task of free associations.",
        "In the end, the first results as well as directions for future work are discussed."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "In natural language processing as well as in information retrieval, Vector Space Model (VSM) (Salton et al., 1975) and Word Space Model (WSM) (Schütze, 1993; Lund and Burgess, 1996) have become the mainstream for text representation.",
        "VSMs embody the distributional hypothesis of meaning, the main assumption of which is that a word is known \"by the company it keeps\" (Firth, 1957).",
        "VSMs proved to perform well in a number of cognitive tasks such as synonymy identification (Landauer and Dumais, 1997), automatic thesaurus construction (Grefen-stette, 1994) and many others.",
        "However, it has been long recognized that these models are too weak to represent natural language to a satisfactory extent.",
        "With VSMs, the assumption is made that word cooccurrence is essentially independent of word order.",
        "All the co-occurrence information is thus fed into one vector per word.",
        "Suppose our \"background knowledge\" corpus consists of one sentence: Peter kicked the ball.",
        "It follows that the distributional meanings of both PETER and BALL would be in a similar way defined by the co-occurring KICK which is insufficient, as BALL can be only kicked by somebody but not kick itself; in case of PETER, both ways of interpretation should be possible.",
        "To overcome the aforementioned problems with vector-based models, we suggest a novel distributional paradigm for representing text in that we introduce a further dimension into a \"standard\" two-dimensional word space model.",
        "That allows us to count correlations for three words at a time.",
        "In short, given a vocabulary V, context width w = m and tokens ti, t2, t3,ti G V, for token ti a matrix of size V x V is generated that has nonzero values in cells where ti appears between ti-m and ti+m.",
        "Note that this 3-dimensional representation allows us to integrate word order information into the model in a completely unsupervised manner as well as to achieve a richer word representation as a matrix instead of a vector.",
        "The remainder of the paper is organized as follows.",
        "After a recap of basic mathematical notions and operations used in the model in Section 2, we introduce the proposed three-dimensional tensor-based model of text representation in Section 3.",
        "First evaluation experiments are reported in Section 4.",
        "After a brief overview of related work in Section 5, we provide some concluding remarks and suggestions for future work in Section 6."
      ]
    },
    {
      "heading": "2. Preliminaries",
      "text": [
        "In this section, we provide a brief introduction to tensors and the basics of mathematical operations that are employed in the suggested model.",
        "First, given d natural numbers ni,..., nd, a (real) nix ... x nd tensor can be defined as a function T : {1,... ,ni} x ... x {1,... ,nd} – R, mapping d-tuples of natural numbers to real numbers.",
        "Intuitively, a tensor can best be thought of as a d-dimensional table (or array) carrying real numbers as entries.",
        "Thereby ni,..., nd determine the extension of the array in the different directions.",
        "Obviously, matrices can be conceived as nix n2-tensors and vectors as ni -tensors.",
        "In our setting, we will work with tensors where d = 3 and for the sake of better understandability we will introduce the necessary notions for this case only.",
        "Our work employs higher-order singular value decomposition (HOSVD), which generalizes the method of singular value decomposition (SVD) from matrices to arbitrary tensors.",
        "Given an nix n2 xn3 tensor T, its Tucker decomposition (Tucker, 1966) for given natural numbers mi, m2, m3 consists of an mi x m2 x m3 tensor G and three matrices A, B, and C of formats nix mi, n2 x m2, and n3 x m3, respectively, such that",
        "E E E G(r, s, t)-A(i, r)-B(j, s)-C(k, t).",
        "The idea here is to represent the large-size tensor T by the smaller \"core\" tensor G. The matrices A, B, and C can be seen as linear transformations \"compressing\" input vectors from dimension ni into dimension mi.",
        "Note that a precise representation of T is not always possible.",
        "Rather one may attempt to approximate T as well as possible, i.e. find the tensor T' for which a Tucker decomposition exists and which has the least distance to T. Thereby, the notion of distance is captured by ||T – T'||, where T – T' is the tensor obtained by entry-wise subtraction and || • || is the Frobenius norm defined by",
        "EEE(M (r,s,t)).",
        "In fact, the described way ofapproximating atensor is called dimensionality reduction and is often used for reducing noise in multidimensional data."
      ]
    },
    {
      "heading": "3. Proposed Model",
      "text": [
        "Our motivation is to integrate structure into the geometrical representation of text meaning while adhering to the ideas of distributional semantics.",
        "For this, we introduce a third dimension that allows us to separate the left and right contexts of the words.",
        "As we process text, we accumulate the left and right word co-occurrences to represent the meaning ofthe current word.",
        "Formally, given a corpus K, a list L of tokens, and a context width w, we define its tensor representation TK by letting TK(i, j, k) be the number of occurrences of L(j) s L(i) s' L(k) in sentences in K where s, s' are (possibly empty) sequences of at most w – 1 tokens.",
        "For example, suppose our corpus consists of three sentences: \"Paul kicked the ball slowly.",
        "Peter kicked the ball slowly.",
        "Paul kicked Peter.\"",
        "We let w = 1, presuming prior stop words removal.",
        "We obtain a 5 x 5 x 5 tensor.",
        "Table 1 displays two i-slices of the resulting tensor T showing left vs. right context dependencies.",
        "Similarly to traditional vector-based distributional models, dimensionality reduction needs to be performed in three dimensions either, as the resulting tensor is very sparse (see the examples of kick and ball).",
        "To this end, we employ Tucker decomposition for 3 dimensions as introduced in Section 2.",
        "For this, Matlab Tensor Toolbox (Bader and Kolda, 2006) is used.",
        "kick",
        "Peter",
        "Paul",
        "kick",
        "ball",
        "slowly",
        "Peter",
        "0",
        "0",
        "0",
        "1",
        "0",
        "Paul",
        "1",
        "0",
        "0",
        "1",
        "0",
        "kick",
        "0",
        "0",
        "0",
        "0",
        "0",
        "ball",
        "0",
        "0",
        "0",
        "0",
        "0",
        "slowly",
        "0",
        "0",
        "0",
        "0",
        "0",
        "ball",
        "Peter",
        "Paul",
        "kick",
        "ball",
        "slowly",
        "Peter",
        "0",
        "0",
        "0",
        "0",
        "0",
        "Paul",
        "0",
        "0",
        "0",
        "0",
        "0",
        "kick",
        "0",
        "0",
        "0",
        "0",
        "2",
        "ball",
        "0",
        "0",
        "0",
        "0",
        "0",
        "slowly",
        "0",
        "0",
        "0",
        "0",
        "0",
        "A detailed overview of computational complexity of Tucker decomposition algorithms in Tensor Toolbox is provided in Turney (2007).",
        "The drawback of those is that their complexity is cubic in the number of factorization dimensions and unfeasible for large datasets.",
        "However, new memory efficient tensor decomposition algorithms have been proposed in the meantime.",
        "Thus, Memory Efficient Tucker (MET) is available in Matlab Tensor Toolbox since Version 2.3.",
        "Rendle and Schmidt-Thieme (2010) present a new factorization method with linear complexity."
      ]
    },
    {
      "heading": "4. Evaluation Issues 4.1 Task",
      "text": [
        "Vector-based distributional similarity methods have proven to be a valuable tool for a number of tasks on automatic discovery of semantic relatedness between words, like synonymy tests (Rapp, 2003) or detection of analogical similarity (Turney, 2006).",
        "A somewhat related task is the task of finding out to what extent (statistical) similarity measures correlate with free word associations.",
        "Furthermore, this task was suggested as a shared task for the evaluation of word space models at Lexical Semantics Workshop at ESSLLI 2008.",
        "Free associations are the words that come to the mind of a native speaker when he or she is presented with a so-called stimulus word.",
        "The percent of test subjects that produce certain response to a given stimulus determines the degree of a free association between a stimulus and a response.",
        "Despite the widespread usage of vector-based models to retrieve semantically similar words, it is still rather unclear what type of linguistic phenomena they model (cf. Heylen et al.",
        "(2008), Wandmacher et al.",
        "(2008)).",
        "The same is true for free associations.",
        "There are a number of relations according to which a word may be associated with another word.",
        "For example, Aitchison (2003) distinguishes four types of associations: co-ordination, collocation, superordination and synonymy.",
        "This affords an opportunity to use the task of free associations as a \"baseline\" for distributional similarity.",
        "For this task, workshop organizers have proposed three subtasks, one of which - discrimination - we adapt in this paper.",
        "Test sets have been provided by the workshop organizers.",
        "The former are based on the Edinburgh Associative Thesaurus (EAT), a freely available database of English association norms.",
        "Discrimination task includes a test set of overall 300 word pairs that were classified according to three classes of association strengths:",
        "• first strongly associated word pairs as indicated by more than 50% of test subjects as first responses;",
        "• hapax word associations that were produced by a single test subject;",
        "• random random combinations of words from EAT that were never produced as a stimulus response pair.",
        "To collect the three-way co-occurrence information, we experiment with the ukWaC corpus (A. Fer-raresi and Bernardini, 2008), as suggested by the workshop organizers, in order to get comparable results.",
        "As ukWaC is a huge Web-derived corpus consisting of about 2 billion tokens, it was impossible at the current stage to process the whole corpus.",
        "As the subsections of ukWaC contain randomly chosen documents, one can train the model on any of the subsections.",
        "We limited out test set to the word pairs for which the constituent words occur more than 50 times in the test corpus.",
        "Thereby, we ended up with a test set consisting of 222 word pairs.",
        "We proceed in the following way.",
        "For each pair ofwords:",
        "1.",
        "Gather N sentences, i.e. contexts, for each of the two words, here N = 50;",
        "2.",
        "Build a 3-dimensional tensor from the subcorpus obtained in (1), given a context width w=5, i.e. 5 words to the left and 5 words to the right of the target word), taking sentence boundaries into consideration;",
        "3.",
        "Reduce 5 times the dimensionality of the tensor obtained in (2) by means of Tucker decomposition;",
        "4.",
        "Extract two matrices of both constituents of the word pair and compare those by means of cosine similarity.",
        "Here, we follow the tradition of vector-based models where cosine is usually used to measure semantic relatedness.",
        "One of the future direction in matrix-based meaning representation is to investigate further matrix comparison metrics.",
        "Tables 2 and 3 show the resulting accuracies for training and test sets.",
        "th denotes cosine threshold values that were used for grouping the results.",
        "Here, this taken to be the function of the size s of the data set.",
        "Thus, given a training set of size s = 60 and 3 classes, we define an \"equally distributed\" threshold thi = 60/3 = 20 (s. Table 2) and a \"linearly growing\" threshold th2 = 4,, rest (s. Table 3).",
        "It is not quite apparent, how the threshold for differentiating between the groups should be determined under given conditions.",
        "Usually, such measures are defined on the basis of training data (e.g. Wandmacher et al.",
        "(2008)).",
        "It was not applicable in our case as, due to the current implementation of the model as well as insufficient computational resources for the time being, we could not build one big model for all experiment iterations.",
        "Also, the intuition we have gained with this kind of thresholds is that as soon as you change the underlying corpus or the model parameters, you may need to define new thresholds (cf.",
        "Tables 2 and 3).",
        "ited processing power we had at our disposal at the moment the experiments were conducted.",
        "With this step, we considerably reduced the size of the corpus and guaranteed a certain number of contexts per relevant word.",
        "Thresholds in geometric models of meaning can not be just fixed, just as the measure of similarity cannot be easily quantified by humans.",
        "It would be straightforward to compare the performance of the proposed model with its 2-dimensional analogue.",
        "Wandmacher et al.",
        "(2008) obtain in average better results with their LSA-based model for this task.",
        "Specifically, they observe very good results for RANDOM associations (78.2% accuracy) but the lowest results for the FIRST, i.e. strongest, associations (50%).",
        "In constrast, the outcome for RANDOM in our model is the worst.",
        "However, the bigger the threshold, the more accurate is getting the model for the first associations.",
        "For example, with a threshold of th = 0.2 for the test set - 4 out of 5 highest ranked pairs were highly associated (FIRST) and the fifth pair was from the HAPAX group.",
        "For HAPAX word associations, no similar regularities could be observed.",
        "The resulting accuracies may seem to be poor at this stage.",
        "However, it is worth mentioning that this is a highly difficult and corpus-dependent task for automatic processing.",
        "The reported results have been obtained based on very small corpora, containing ca.",
        "100 sentences per iteration (cf. Wandmacher et al.",
        "(2008) use a corpus of 108 million words to train their LSA-Model).",
        "Consequently, it is not possible to compare both results directly, as they have been produced under very different conditions."
      ]
    },
    {
      "heading": "5. Related Work",
      "text": [
        "There have been a number of efforts to integrate syntax into vector-based models with alternating success.",
        "Some used (dependency) parsing to feed the models (Grefenstette, 1994; Lin, 1998; Padó andLa-pata, 2007); the others utilized only part of speech information, e.g., Widdows (2003).",
        "In many cases, these syntactically enhanced models improved the performance (Grefenstette, 1994; Lin, 1998; Padó and Lapata, 2007).",
        "Sometimes, however, rather controversial results were observed.",
        "Thus, Widdows (2003) reported both positive and negative effects for the task of developing taxonomies.",
        "On the one side, POS information increased the performance for common nouns; on the other side, it degraded the outcome for proper nouns",
        "first hapax random Total (F/H/R) first/HorR",
        "random Total (F/H/R) first/HorR",
        "TRAIN TEST and verbs.",
        "Sahlgren et al.",
        "(2008) incorporate word order information into context vectors in an unsupervised manner by means of permutation.",
        "Recently, Erk and Padó (2008) proposed a structured vector space model where a word is represented by several vectors reflecting the words lexical meaning as well as its selectional preferences.",
        "The motivation behind their work is very close to ours, namely, that single vectors are too weak to represent word meaning.",
        "However, we argue that a matrix-based representation allows us to integrate contextual information in a more general manner.",
        "Among the early attempts to apply higher-order tensors instead of vectors to text data is the work of Liu et al.",
        "(2005) who show that Tensor Space Model is consistently better than VSM for text classification.",
        "Cai et al.",
        "(2006) suggest a 3-dimensional representation for documents and evaluate the model on the task of document clustering.",
        "The above as well as a couple of other projects in this area in information retrieval community leave open the question of how to convey text into a three-dimensional tensor.",
        "They still use vector-based representation as the basis and then just mathematically convert vectors into tensors, without linguistic justification of such transformations.",
        "Further, there are few works that extend the term-document matrix with metadata as a third dimension (Chew et al., 2007; Sun et al., 2006).",
        "Turney (2007) is one of the few to study the application of tensors to word space models.",
        "However, the emphasis in that paper is more on the evaluation of different tensor decomposition models for such spaces than on the formal model of text representation in three dimensions.",
        "Van de Cruys (2009) suggests a three-way model of co-occurrence similar to ours.",
        "In contrast to Van de Cruys (2009), we are not using any explicit syntactic preprocessing.",
        "Furthermore, our focus is more on the model itself as a general model of meaning."
      ]
    },
    {
      "heading": "6. Summary and Future Work",
      "text": [
        "In this paper, we propose a novel approach to text representation inspired by the ideas of distributional semantics.",
        "In particular, our model suggests a solution to the problem of integrating word order information in vector spaces in an unsupervised manner.",
        "First experiments on the task of free associations are reported.",
        "However, we are not in the position yet to commit ourselves to any representative statements.",
        "A thorough evaluation of the model still needs to be done.",
        "Next steps include, amongst others, evaluating the suggested model with a bigger data corpus as well as using stemming and more sophisticated filling of word matrices, e.g., by introducing advanced weighting schemes into the matrices instead of simple counts.",
        "Furthermore, we started with evaluation on the task which has been proposed for the evaluation of word space models at the level of word meaning.",
        "We need, however, to evaluate the model for the tasks where word order information matters more, e.g. on selectional preferences or paraphrasing.",
        "Last but not least, we plan to address the issue of modeling compositional meaning with matrix-based distributional model of meaning."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This work is supported by German \"Federal Ministry of Economics\" (BMWi) under the project Theseus (number 01MQ07019).",
        "Many thanks to the anonymous reviewers for their insightful comments."
      ]
    }
  ]
}
