{
  "info": {
    "authors": [
      "Charles Greenbacker",
      "Nicole Sparks",
      "Kathleen F. McCoy",
      "Che-Yu Kuo"
    ],
    "book": "Proceedings of the International Natural Language Generation Conference",
    "id": "acl-W10-4231",
    "title": "UDel: Refining a Method of Named Entity Generation",
    "url": "https://aclweb.org/anthology/W10-4231",
    "year": 2010
  },
  "references": [
    "acl-W09-2817",
    "acl-W09-2818",
    "acl-W09-2819",
    "acl-W09-2821"
  ],
  "sections": [
    {
      "text": [
        "Charles F. Greenbacker, Nicole L. Sparks, Kathleen F. McCoy,and Che-Yu Kuo",
        "This report describes the methods and results of a system developed for the GREC Named Entity Challenge 2010.",
        "We detail the refinements made to our 2009 submission and present the output of the self-evaluation on the development data set."
      ]
    },
    {
      "heading": "1Introduction",
      "text": [
        "The GREC Named Entity Challenge 2010 (NEG) is an NLG shared task whereby submitted systems must select a referring expression from a list of options for each mention of each person in a text.",
        "The corpus is a collection of 2,000 introductory sections from Wikipedia articles about individual people in which all mentions of person entities have been annotated.",
        "An in-depth description of the task, along with the evaluation results from the previous year, is provided by Belz et al.",
        "(2009).",
        "Our 2009 submission (Greenbacker and McCoy, 2009a) was an extension of the system we developed for the GREC Main Subject Reference Generation Challenge (MSR) (Greenbacker and McCoy, 2009b).",
        "Although our system performed reasonably-well in predicting REG08-Type in the NEG task, our string accuracy scores were disappointingly-low, especially when compared to the other competing systems and our own performance in the MSR task.",
        "As suggested by the evaluators (Belz et al., 2009), this was due in large part to our reliance on the list of REs being in a particular order, which had changed for the NEG task."
      ]
    },
    {
      "heading": "2. Method",
      "text": [
        "The first improvement we made to our existing methods related to the manner by which we selected the specific RE to employ.",
        "In 2009, we trained a series of decision trees to predict REG08-Type based on our psycholinguistically-inspired feature set (described in (Greenbacker and McCoy, 2009c)), and then simply chose the first option in the list of REs matching the predicted type.",
        "For 2010, we incorporated the case of each RE into our target attribute so that the decision tree classifier would predict both the type and case for the given reference.",
        "Then, we applied a series of rules governing the length of initial and subsequent REs involving a person's name (following Nenkova and McKeown (2003)), as well as 'backoffs' if the predicted type or case were not available.",
        "Another improvement we made involved our method of determining whether the use of a pronoun would introduce ambiguity in a given context.",
        "Previously, we searched for references to other people entities since the most recent mention of the entity at hand, and if any were found, we assumed these would cause the use of a pronoun to be ambiguous.",
        "However, this failed to account for the fact that personal pronouns in English are gender-specific (ie.",
        "the mention of a male individual would not make the use of \"she\" ambiguous).",
        "So, we refined this by determining the gender of each named entity (by seeing which personal pronouns were associated with it in the list of REs), and only noting ambiguity when the current entity and candidate interfering antecedent were of the same gender.",
        "Other small changes from 2009 include an expanded abbreviation set in the sentence segmenter, separate decision trees for the main subject and other entities, and fixing how we handled embedded REF elements with unspecified mention IDs.",
        "3Results",
        "Scores for REG08-Type precision & recall, string accuracy, and string-edit distance are presented in Figure 1.",
        "These were computed on the entire development set, as well as the three subsets, using the geval.pl self-evaluation tool provided in the",
        "NEG participants' pack.",
        "While we were able to achieve an improvement of nearly 50% over our 2009 scores in string accuracy, we saw less than a 1% gain in overall REG08-Type performance.",
        "(d) Scores on the 'Inventors' subset.",
        "Figure 1: Scores on the development set obtained via the geval.pl self-evaluation tool.",
        "REG08-Type precision and recall were equal in all four sets.",
        "4Conclusions",
        "The fact that our string accuracy scores improved over our 2009 submission far more than REG08-Type prediction is hardly surprising.",
        "Our efforts during this iteration of the NEG task were primarily focused on enhancing our methods of choosing the best RE once the reference type was selected.",
        "We remain several points below the best-performing team from 2009 (ICSI-Berkeley), possibly due to the inclusion of additional items in their feature set, or the use of Conditional Random Fields as their learning technique (Favre and",
        "Bohnet, 2009).",
        "5Future Work",
        "Moving forward, we hope to expand our feature set by including the morphology of words immediately surrounding the reference, as well as a more extensive reference history, as suggested by (Favre and Bohnet, 2009).",
        "We suspect that these features may play a significant role in determining the type of referenced used, the prediction of which acts as a'bottleneck' in generating exact REs.",
        "We would also like to compare the efficacy of several different machine learning techiques as applied to our feature set and the NEG task.",
        "Metric",
        "Score",
        "Type Precision/Recall String Accuracy Mean Edit Distance Normalized Distance",
        "0.757995735607676 0.650496141124587 0.875413450937156 0.319266300067796",
        "(a) Scores on the entire development set.",
        "Metric",
        "Score",
        "Type Precision/Recall String Accuracy Mean Edit Distance Normalized Distance",
        "0.735294117647059 0.623287671232877 0.839041095890411 0.345490867579909",
        "(b) Scores on the 'Chefs' subset.",
        "Metric",
        "Score",
        "Type Precision/Recall String Accuracy Mean Edit Distance Normalized Distance",
        "0.790769230769231 0.683544303797468 0.882911392405063 0.279837251356239",
        "(c) Scores on the 'Composers' subset.",
        "Metric",
        "Score",
        "Type Precision/Recall String Accuracy Mean Edit Distance Normalized Distance",
        "0.745928338762215 0.642140468227425 0.903010033444816 0.335326519731057"
      ]
    }
  ]
}
