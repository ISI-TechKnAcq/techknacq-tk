{
  "info": {
    "authors": [
      "Liang Huang",
      "Wenbin Jiang",
      "Qun Liu"
    ],
    "book": "EMNLP",
    "id": "acl-D09-1127",
    "title": "Bilingually-Constrained (Monolingual) Shift-Reduce Parsing",
    "url": "https://aclweb.org/anthology/D09-1127",
    "year": 2009
  },
  "references": [
    "acl-C02-1145",
    "acl-C04-1010",
    "acl-D08-1022",
    "acl-D08-1059",
    "acl-D08-1092",
    "acl-D09-1086",
    "acl-J08-4003",
    "acl-J97-3002",
    "acl-N04-1035",
    "acl-N07-1051",
    "acl-P03-2041",
    "acl-P04-1015",
    "acl-P05-1012",
    "acl-P05-1066",
    "acl-P06-1096",
    "acl-P06-2089",
    "acl-P08-1067",
    "acl-W02-1001",
    "acl-W04-0308",
    "acl-W04-3207"
  ],
  "sections": [
    {
      "text": [
        "Jointly parsing two languages has been shown to improve accuracies on either or both sides.",
        "However, its search space is much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations.",
        "Here we propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser learns to exploit reorderings as additional observation, but not bothering to build the target-side tree as well.",
        "We show specifically how to enhance a shift-reduce dependency parser with alignment features to resolve shift-reduce conflicts.",
        "Experiments on the bilingual portion of Chinese Treebank show that, with just 3 bilingual features, we can improve parsing accuracies by 0.6% (absolute) for both English and Chinese over a state-of-the-art baseline, with negligible (~6%) efficiency overhead, thus much faster than biparsing."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Ambiguity resolution is a central task in Natural Language Processing.",
        "Interestingly, not all languages are ambiguous in the same way.",
        "For example, prepositional phrase (PP) attachment is (notoriously) ambiguous in English (and related European languages), but is strictly unambiguous in Chinese and largely unambiguous Japanese; see (la) I [saw Bill ] [with a telescope].",
        "wo [yong wangyuanjin] [kandao le Bi 'er].",
        "\"I used a telescope to see Bill.\"",
        "(lb) j saw [Bill [with a telescope]].",
        "wo kandao le [[na wangyuanjin ] de Bi 'er].",
        "\"I saw Bill who had a telescope at hand.\"",
        "Figure 1 for an example.",
        "It is thus intuitive to use two languages for better disambiguation, which has been applied not only to this PP-attachment problem (Fossum and Knight, 2008; Schwartz et al., 2003), but also to the more fundamental problem of syntactic parsing which subsumes the former as a subproblem.",
        "For example, Smith and Smith (2004) and Burkett and Klein (2008) show that joint parsing (or reranking) on a bitext improves accuracies on either or both sides by leveraging bilingual constraints, which is very promising for syntax-based machine translation which requires (good-quality) parse trees for rule extraction (Galley et al., 2004; Mi and Huang, 2008).",
        "However, the search space of joint parsing is inevitably much bigger than the monolingual case, forcing existing approaches to employ complicated modeling and crude approximations.",
        "1 Chinese uses word-order to disambiguate the attachment (see below).",
        "By contrast, Japanese resorts to case-markers and the unambiguity is limited: it works for the \"V or N\" attachment ambiguities like in Figure 1 (see (Schwartz et al., 2003)) but not for the \"Ni or N2\" case (Mitch Marcus, p.c.).",
        "Joint parsing with a simplest synchronous context-free grammar (Wu, 1997) is (9(n) as opposed to the monolingual 0(n) time.",
        "To make things worse, languages are non-isomorphic, i.e., there is no 1-to-1 mapping between tree nodes, thus in practice one has to use more expressive formalisms such as synchronous tree-substitution grammars (Eisner, 2003; Galley et al., 2004).",
        "In fact, rather than joint parsing per se, Burkett and Klein (2008) resort to separate monolingual parsing and bilingual reranking over k tree pairs, which covers a tiny fraction of the whole space (Huang, 2008).",
        "We instead propose a much simpler alternative, bilingually-constrained monolingual parsing, where a source-language parser is extended to exploit the reorderings between languages as additional observation, but not bothering to build a tree for the target side simultaneously.",
        "To illustrate the idea, suppose we are parsing the sentence (1) I saw Bill [pP with a telescope ].",
        "which has 2 parses based on the attachment of PP:",
        "(la) I [ saw Bill ] [PP with a telescope ].",
        "(lb) I saw [ Bill [PP with a telescope ]].",
        "Both are possible, but with a Chinese translation the choice becomes clear (see Figure 1), because a Chinese PP always immediately precedes the phrase it is modifying, thus making PP-attachment strictly unambiguous.",
        "We can thus use Chinese to help parse English, i.e., whenever we have a PP-attachment ambiguity, we will consult the Chinese translation (from a bitext), and based on the alignment information, decide where to attach the English PP.",
        "On the other hand, English can help Chinese parsing as well, for example in deciding the scope of relative clauses which is unambiguous in English but ambiguous in Chinese.",
        "This method is much simpler than joint parsing because it remains monolingual in the backbone, with alignment information merely as soft evidence, rather than hard constraints since automatic word alignment is far from perfect.",
        "It is thus straightforward to implement within a monolingual parsing algorithm.",
        "In this work we choose shift-reduce dependency parsing for its simplicity and efficiency.",
        "Specifically, we make the following contributions:",
        "• we develop a baseline shift-reduce dependency parser using the less popular, but classical, \"arc-standard\" style (Section 2), and achieve similar state-of-the-art performance with the the dominant but complicated \"arc-eager\" style of Nivre and Scholz (2004);",
        "• we propose bilingual features based on word-alignment information to prefer \"target-side contiguity\" in resolving shift-reduce conflicts (Section 3);",
        "• we verify empirically that shift-reduce conflicts are the major source of errors, and correct shift-reduce decisions strongly correlate with the above bilingual contiguity conditions even with automatic alignments (Section 5.3);",
        "• finally, with just three bilingual features, we improve dependency parsing accuracy by 0.6% for both English and Chinese over the state-of-the-art baseline with negligible (~6%) efficiency overhead (Section 5.4)."
      ]
    },
    {
      "heading": "2. Simpler Shift-Reduce Dependency Parsing with Three Actions",
      "text": [
        "The basic idea of classical shift-reduce parsing from compiler theory (Aho and Ullman, 1972) is to perform a left-to-right scan of the input sentence, and at each step, choose one of the two actions: either shift the current word onto the stack, or reduce the top two (or more) items on the stack, replacing them with their combination.",
        "This idea has been applied to constituency parsing, for example in Sagae and Lavie (2006), and we describe below a simple variant for dependency parsing similar to Yamada and Matsumoto (2003) and the \"arc-standard\" version of Nivre (2004).",
        "Basically, we just need to split the reduce action into two symmetric (sub-)actions, reduceLand reduceR, depending on which one of the two",
        "Table 1: Formal description of the three actions.",
        "Note that shift requires non-empty queue while reduce requires at least two elements on the stack.",
        "items becomes the head after reduction.",
        "More formally, we describe a parser configuration by a tuple (S, Q, A) where S is the stack, Q is the queue of remaining words of the input, and A is the set of dependency arcs accumulated so far.",
        "At each step, we can choose one of the three actions:",
        "1. shift: move the head of (a non-empty) queue Q onto stack S;",
        "2. reduceL: combine the top two items on the stack, St and St-i (t > 2), and replace them with st (as the head), and add a left arc",
        "3. reduceR: combine the top two items on the stack, stand St-i (t > 2), and replace them with st-i (as the head), and add a right arc (st-i, st) to A.",
        "These actions are summarized in Table 1.",
        "The initial configuration is always (0, W\\... wn, 0) with empty stack and no arcs, and the final configuration is (wj,$, A) where Wj is recognized as the root of the whole sentence, and A encodes a spanning tree rooted at Wj.",
        "For a sentence of n words, there are exactly 2n – 1 actions: n shifts and n – 1 reductions, since every word must be pushed onto stack once, and every word except the root will eventually be popped in a reduction.",
        "The time complexity, as other shift-reduce instances, is clearly 0(n).",
        "a \"configuration\" is sometimes called a \"state\" (Zhang and Clark, 2008), but that term is confusing with the states in shift-reduce LR/LL parsing, which are quite different.",
        "Figure 2: A trace of 3-action shift-reduce on the example sentence.",
        "Shaded words are on stack, while gray words have been popped from stack.",
        "After step (4), the process can take either (5 a) or (5b), which correspond to the two attachments (la) and (lb) in Figure 1, respectively.",
        "(0) and (1), only shift is possible since there are not enough items on the stack for reduction.",
        "At step (3), we perform a reduceL, making word \"i\" a modifier of \"saw\"; after that the stack contains a single word and we have to shift the next word \"Bill\" (step 4).",
        "Now we face a shift-reduce conflict: we can either combine \"saw\" and \"Bill\" in a reduceR action (5a), or shift \"Bill\" (5b).",
        "We will use features extracted from the configuration to resolve the conflict.",
        "For example, one such feature could be a bigram st o St-i, capturing how likely these two words are combined; see Table 2 for the complete list of feature templates we use in this baseline parser.",
        "We argue that this kind of shift-reduce conflicts are the major source of parsing errors, since the other type of conflict, reduce-reduce conflict (i.e., whether left or right) is relatively easier to resolve given the part-of-speech information.",
        "For example, between a noun and an adjective, the former is much more likely to be the head (and so is a verb vs. a preposition or an adverb).",
        "Shift-reduce resolution, however, is more non-local, and often involves a triple, for example, (saw, Bill, with) for a typical PP-attachment.",
        "On the other hand, if we indeed make a wrong decision, a reduce-reduce mistake just flips the head and the modifier, and often has a more local effect on the shape of the tree, whereas a shift-reduce mistake always leads",
        "stack",
        "queue",
        "arcs",
        "previous",
        "S",
        "Wi\\Q",
        "A",
        "shift",
        "S\\wi",
        "Q",
        "A",
        "previous",
        "S\\st-i\\st",
        "Q",
        "A",
        "reduceL",
        "S\\st",
        "Q",
        "Au{(st,st",
        "-i)}",
        "reduceR",
        "S\\st-i",
        "Q",
        "Au{(st-i,",
        "«*)}",
        "0",
        "-",
        "i",
        "saw",
        "Bill",
        "with",
        "a ...",
        "1",
        "shift",
        "i",
        "saw",
        "Bill",
        "with",
        "a ...",
        "2",
        "shift",
        "saw",
        "Bill",
        "with",
        "a...",
        "3",
        "reduceL",
        "saw",
        "Bill",
        "with",
        "a...",
        "i",
        "\"büT",
        "4",
        "shift",
        "saw",
        "with",
        "a...",
        "5a",
        "reduceR",
        "saw",
        "with",
        "a...",
        "\"ßill",
        "5b",
        "shift",
        "saw",
        "Bill",
        "with",
        "a...",
        "i",
        "Table 2: Feature templates of the baseline parser, st, st-i denote the top and next to top words on the stack; Wi and Wi+\\ denote the current and next words on the queue.",
        "T( ) denotes the POS tag of a given word, and lc(-) and rc(-) represent the leftmost and rightmost child.",
        "Symbol o denotes feature conjunction.",
        "Each of these templates is further conjoined with the 3 actions shift, reduceL, and reduceR.",
        "to vastly incompatible tree shapes with crossing brackets (for example, [saw Bill] vs. [Bill with a telescope]).",
        "We will see in Section 5.3 that this is indeed the case in practice, thus suggesting us to focus on shift-reduce resolution, which we will return to with the help of bilingual constraints in Section 3.",
        "The three action system was originally described by Yamada and Matsumoto (2003) (although their methods require multiple passes over the input), and then appeared as \"arc-standard\" in Nivre (2004), but was argued against in comparison to the four-action \"arc-eager\" variant.",
        "Most subsequent works on shift-reduce or \"transition-based\" dependency parsing followed \"arc-eager\" (Nivre and Scholz, 2004; Zhang and Clark, 2008), which now becomes the dominant style.",
        "But we argue that \"arc-standard\" is preferable because:",
        "1. in the three action \"arc-standard\" system, the stack always contains a list of unrelated subtrees recognized so far, with no arcs between any of them, e.g. saw) and (Bill) in step 4 of Figure 2), whereas the four action \"arc-eager\" style can have left or right arrows between items on the stack;",
        "2. the semantics of the three actions are atomic and disjoint, whereas the semantics of 4 actions are not completely disjoint.",
        "For example, their Left action assumes an implicit Reduce of the left item, and their Right action assumes an implicit Shift.",
        "Furthermore, these two actions have non-trivial preconditions which also causes the next problem (see below).",
        "We argue that this is rather complicated to implement.",
        "3. the \"arc-standard\" scan always succeeds, since at the end we can always reduce with empty queue, whereas the \"arc-eager\" style sometimes goes into deadends where no action can perform (prevented by preconditions, otherwise the result will not be a well-formed tree).",
        "This becomes parsing failures in practice (Nivre and Scholz, 2004), leaving more than one fragments on stack.",
        "As we will see in Section 5.1, this simpler arc-standard system performs equally well with a state-of-the-art arc-eager system (Zhang and Clark, 2008) on standard English Treebank parsing (which is never shown before).",
        "We argue that all things being equal, this simpler paradigm should be preferred in practice.",
        "We also enhance deterministic shift-reduce parsing with beam search, similar to Zhang and Clark (2008), where k configurations develop in parallel.",
        "Pseudocode 1 illustrates the algorithm, where we keep an agenda V of the current active configurations, and at each step try to extend them by applying one of the three actions.",
        "We then dump the best k new configurations from the buffer back Pseudocode 1 beam-search shift-reduce parsing.",
        "Type",
        "Features",
        "Unigram",
        "St",
        "St-1 Wi",
        "T(st)",
        "T(st-i) T(wi)",
        "stoT(st) st-i o T(st-i) Wi o T{wi)",
        "Bigram",
        "st o st-i",
        "T(st)ost-ioT(st-i) st o T(st) o st-i",
        "T(st)oT(st-i)",
        "st o st-i o T(st-i)",
        "st o T(st) o st-i o T(st-i)",
        "T(st) o T(wi) stoT{st)oT{st-",
        "i)",
        "Tri gram",
        "T(st) o Tiwi) o T(wi+i) st o T{wi) o T(wi+1)",
        "T(st-i)oT(st)oT(wi) T(st-i) o st o T{wi)",
        "T(st-2) oT(st-i)",
        "oT(st)",
        "Modifier",
        "T(st-i)oT(lc(st-i))oT(st)",
        "T(st-i)oT(st)oT(rc(st))",
        "T(st-i)ostoT(lc(st))",
        "T(st-i) oT(rc(st-i)) oT(st) T(st-i) oT(lc(st-i)) ost",
        "T(st-i) oT(st) o T(st-i) oT(rc(st",
        "T(lc(st)) -l)) ° st",
        "6: for each config in agenda V do 8: if act is applicable to config then 9: next < – apply act to config 10: insert next into buffer BUF 12: Output: the tree of the best config in V",
        "into the agenda for the next step.",
        "The complexity of this algorithm is 0{nk), which subsumes the determinstic mode as a special case (k = 1).",
        "(a) j saw^Bill with a telescope.",
        "wo yorig wangyuanjin kandao le Bi 'er.",
        "c(st-i, St) =+; reduce is correct (b) I *^saw Bill with a telescope.",
        "wo kandao le na wangyuanjin de Bi 'er.",
        "c(st-i, St) = – ; reduce is wrong (c) ^saw Bill with a telescope .",
        "wo kandao le na wangyuanjin de Bi 'er.",
        "cR(st, Wi) =+; shift is correct",
        "To train the parser we need an \"oracle\" or gold-standard action sequence for gold-standard dependency trees.",
        "This oracle turns out to be non-unique for the three-action system (also non-unique for the four-action system), because left dependents of a head can be reduced either before or after all right dependents are reduced.",
        "For example, in Figure 2, \"I\" is a left dependent of \"saw\", and can in principle wait until \"Bill\" and \"with\" are reduced, and then finally combine with \"saw\".",
        "We choose to use the heuristic of \"shortest stack\" that always prefers reduceL over shift, which has the effect that all left dependents are first recognized inside-out, followed by all right dependents, also inside-out, which coincides with the head-driven constituency parsing model of Collins (1999).",
        "We use the popular online learning algorithm of structured perceptron with parameter averaging (Collins, 2002).",
        "Following Collins and Roark (2004) we also use the \"early-update\" strategy, where an update happens whenever the gold-standard action-sequence falls off the beam, with the rest of the sequence neglected.",
        "As a special case, for the deterministic mode, updates always co-occur with the first mistake made.",
        "The intuition behind this strategy is that future mistakes are often caused by previous ones, so with the parser on the wrong track, future actions become irrelevant for learning.",
        "See Section 5.3 for more discussions.",
        "(d) I*~^aw Bill with a telescope .",
        "wo yong wangyuanjin kandao le Bi 'er.",
        "cR(st,Wi) = – ; shift is wrong",
        "Figure 3: Bilingual contiguity features c(st-i, St) and cR(st, Wi) at step (4) in Fig. 2 (facing a shift-reduce decision).",
        "Bold words are currently on stack while gray ones have been popped.",
        "Here the stack tops are St = Bill, st-i = saw, and the queue head is Wi = with; underlined texts mark the source and target spans being considered, and wavy underlines mark the allowed spans (Tab.",
        "3).",
        "Red bold alignment links violate contiguity constraints."
      ]
    },
    {
      "heading": "3. Soft Bilingual Constraints as Features",
      "text": [
        "As suggested in Section 2.2, shift-reduce conflicts are the central problem we need to address here.",
        "Our intuition is, whenever we face a decision whether to combine the stack tops St-i and St or to shift the current word Wi, we will consult the other language, where the word-alignment information would hopefully provide a preference, as in the running example of PP-attachment (see Figure 1).",
        "We now develop this idea into bilingual contiguity features.",
        "Informally, if the correct decision is a reduction, then it is likely that the corresponding words of st-i and St on the target-side should also form a contiguous span.",
        "For example, in Figure 3(a), the source span of a reduction is [saw .. Bill], which maps onto [kandao .. .Bi 'er] on the Chinese side.",
        "This target span is contiguous, because no word within this span is aligned to a source word outside of the source span.",
        "In this case we say feature c(st-i,st) =+, which encourages \"reduce\".",
        "However, in Figure 3(b), the source span is still [saw .. Bill], but this time maps onto a much longer span on the Chinese side.",
        "This target span is discontiguous, since the Chinese words na and wangyuanjin are alinged to English \"with\" and \"telescope\", both of which fall outside of the source span.",
        "In this case we say feature c(st-i, St) = – , which discourages \"reduce\" .",
        "Similarly, we can develop another feature cR(st,Wi) for the shift action.",
        "In Figure 3(c), when considering shifting \"with\", the source span becomes [Bill .. with] which maps to [na .. Bi'er] on the Chinese side.",
        "This target span looks like discontiguous in the above definition with wangyuanjin aligned to \"telescope\", but we tolerate this case for the following reasons.",
        "There is a crucial difference between shift and reduce: in a shift, we do not know yet the subtree spans (unlike in a reduce we are always combining two well-formed subtrees).",
        "The only thing we are sure of in a shift action is that stand Wi will be combined before st-i and St are combined (Aho and Ullman, 1972), so we can tolerate any target word aligned to source word still in the queue, but do not allow any target word aligned to an already recognized source word.",
        "This explains the notational difference between cR(st,Wi) and c(st-i,St), where subscript \"R\" means \"right contiguity\".",
        "As a final example, in Figure 3(d), Chinese word kandao aligns to \"saw\", which is already recognized, and this violates the right contiguity.",
        "So cR(st, Wi) = – , suggesting that shift is probably wrong.",
        "To be more precise, Table 3 shows the formal definitions of the two features.",
        "We basically",
        "source target allowed feature / span sp span tp span ap",
        "Table 3: Formal definition of bilingual features.",
        "M(-) is maps a source span to the target language, and M_1( ) is the reverse operation mapping back to the source language.",
        "map a source span sp to its target span M(sp), and check whether its reverse image back onto the source language M~l(M(sp)) falls inside the allowed span ap.",
        "For cR(st,Wi), the allowed span extends to the right end of the sentence.",
        "To conclude so far, we have got two alignment-based features, c(st-i,St) correlating with reduce, and cR(st,Wi) correlating with shift.",
        "In fact, the conjunction of these two features, is another feature with even stronger discrimination power.",
        "If it is strongly recommending reduce, while is a very strong signal for shift.",
        "So in total we got three bilingual feature (templates), which in practice amounts to 24 instances (after crossproduct with { – , +} and the three actions).",
        "We show in Section 5.3 that these features do correlate with the correct shift/reduce actions in practice.",
        "The naive implemention of bilingual feature computation would be of 0{kn) complexity in the worse case because when combining the largest spans one has to scan over the whole sentence.",
        "We envision the use of a clever datastructure would reduce the complexity, but leave this to future work, as the experiments (Table 8) show that the parser is only marginally (~6%) slower with the new bilingual features.",
        "This is because the extra work, with just 3 bilingual features, is not the bottleneck in practice, since the extraction of the vast amount of other features in Table 2 dominates the computation."
      ]
    },
    {
      "heading": "4. Related Work in Grammar Induction",
      "text": [
        "Besides those cited in Section 1, there are some other related work on using bilingual constraints for grammar induction (rather than parsing).",
        "For example, Hwa et al.",
        "(2005) use simple heuristics to project English trees to Spanish and Chinese, but get discouraging accuracy results learned from those projected trees.",
        "Following this idea, Ganchev et al.",
        "(2009) and Smith and Eisner (2009) use constrained EM and parser adaptation techniques, respectively, to perform more principled projection, and both achieve encouraging results.",
        "Our work, by constrast, never uses bilingual tree pairs not tree projections, and only uses word alignment alone to enhance a monolingual grammar, which learns to prefer target-side contiguity."
      ]
    },
    {
      "heading": "5. Experiments 5.1 Baseline Parser",
      "text": [
        "We implement our baseline monolingual parser (in C++) based on the shift-reduce algorithm in Section 2, with feature templates from Table 2.",
        "We evaluate its performance on the standard Penn English Treebank (PTB) dependency parsing task, i.e., train on sections 02-21 and test on section 23 with automatically assigned POS tags (at 97.2% accuracy) using a tagger similar to Collins (2002), and using the headrules of Yamada and Matsumoto (2003) for conversion into dependency trees.",
        "We use section 22 as dev set to determine the optimal number of iterations in per-ceptron training.",
        "Table 4 compares our baseline against the state-of-the-art graph-based (McDonald et al., 2005) and transition-based (Zhang and Clark, 2008) approaches, and confirms that our system performs at the same level with those state-of-the-art, and runs extremely fast in the deterministic mode (fc=l), and still quite fast in the beam-search mode (fc=16).",
        "Table 4: Baseline parser performance on standard Penn English Treebank dependency parsing task.",
        "The speed numbers are not exactly comparable since they are reported on different machines.",
        "Table 5: Training, dev, and test sets from bilingual Chinese Treebank à la Burkett and Klein (2008).",
        "The bilingual data we use is the translated portion of the Penn Chinese Treebank (CTB) (Xue et al., 2002), corresponding to articles 1-325 of PTB, which have English translations with gold-standard parse trees (Bies et al., 2007).",
        "Table 5 shows the split of this data into training, development, and test subsets according to Burkett and Klein (2008).",
        "Note that not all sentence pairs could be included, since many of them are not one-to-one aligned at the sentence level.",
        "Our word-alignments are generated from the HMM aligner of Liang et al.",
        "(2006) trained on approximately 1.7M sentence pairs (provided to us by David Burkett, p.c.).",
        "This aligner outputs \"soft alignments\", i.e., posterior probabilities for each source-target word pair.",
        "We use a pruning threshold of 0.535 to remove low-confidence alignment links, and use the remaining links as hard alignments; we leave the use of alignment probabilities to future work.",
        "For simplicity reasons, in the following experiments we always supply gold-standard POS tags as part of the input to the parser.",
        "Before evaluating our bilingual approach, we need to verify empirically the two assumptions we made about the parser in Sections 2 and 3:",
        "parser",
        "accuracy",
        "sees/sent",
        "McDonald et al.",
        "(2005)",
        "90.7",
        "0.150",
        "Zhang and Clark (2008)",
        "91.4",
        "0.195",
        "our baseline at k= 1",
        "90.2",
        "0.009",
        "our baseline at k=\\6",
        "91.3",
        "0.125",
        "Training",
        "Dev",
        "Test",
        "CTB Articles Bilingual Paris",
        "1-270 2745",
        "301-325 273",
        "271-300 290",
        "Table 6: [Hypothesis 1] Error distribution in the baseline model (k = 1) on English dev set.",
        "\"sh > re\" means \"should shift, but reduced\".",
        "Shift-reduce conflicts overwhelmingly dominate.",
        "1.",
        "(monolingual) shift-reduce conflict is the major source of errors while reduce-reduce conflict is a minor issue;",
        "2.",
        "(bilingual) the gold-standard decisions of shift or reduce should correlate with contiguities of c(st_i, st), and of cR(st, w{).",
        "Hypothesis 1 is verified in Table 6, where we count all the first mistakes the baseline parser makes (in the deterministic mode) on the English dev set (273 sentences).",
        "In shift-reduce parsing, further mistakes are often caused by previous ones, so only the first mistake in each sentence (if there is one) is easily identifiable; this is also the argument for \"early update\" in applying perceptron learning to these incremental parsing algorithms (Collins and Roark, 2004) (see also Section 2).",
        "Among the 197 first mistakes (other 76 sentences have perfect output), the vast majority, 190 of them (96.4%), are shift-reduce errors (equally distributed between shift-becomes-reduce and reduce-becomes-shift), and only 7 (3.6%) are due to reduce-reduce conflicts.",
        "These statistics confirm our intuition that shift-reduce decisions are much harder to make during parsing, and contribute to the overwhelming majority of errors, which is studied in the next hypothesis.",
        "Hypothesis 2 is verified in Table 7.",
        "We take the gold-standard shift-reduce sequence on the English dev set, and classify them into the four categories based on bilingual contiguity features: (a) c(st-i,st), i.e. whether the top 2 spans on stack is contiguous, and (b) cR(st,Wi), i.e. whether the",
        "Table 7: [Hyp.",
        "2] Correlation of gold-standard shift/reduce decisions with bilingual contiguity conditions (on English dev set).",
        "Note there is always one more shift than reduce in each sentence.",
        "stack top is contiguous with the current word Wi.",
        "According to discussions in Section 3, when (a) is contiguous and (b) is not, it is a clear signal for reduce (to combine the top two elements on the stack) rather than shift, and is strongly supported by the data (first line: 1209 reduces vs. 172 shifts); and while when (b) is contiguous and (a) is not, it should suggest shift (combining stand Wi before st-i and St are combined) rather than reduce, and is mildly supported by the data (second line: 1432 shifts vs. 805 reduces).",
        "When (a) and (b) are both contiguous or both discontiguous, it should be considered a neutral signal, and is also consistent with the data (next two lines).",
        "So to conclude, this bilingual hypothesis is empirically justified.",
        "On the other hand, we would like to note that these correlations are done with automatic word alignments (in our case, from the Berkeley aligner) which can be quite noisy.",
        "We suspect (and will finish in the future work) that using manual alignments would result in a better correlation, though for the main parsing results (see below) we can only afford automatic alignments in order for our approach to be widely applicable to any bitext.",
        "We incorporate the three bilingual features (again, with automatic alignments) into the baseline parser, retrain it, and test its performance on the English dev set, with varying beam size.",
        "Table 8 shows that bilingual constraints help more with larger beams, from almost no improvement with the deterministic mode (k=\\) to +0.5% better with the largest beam (k=\\6).",
        "This could be explained by the fact that beam-search is more robust than the deterministic mode, where in the latter, if our",
        "sh > re re > sh",
        "sh-re",
        "re-re",
        "# %",
        "92 98 46.7% 49.7%",
        "190 96.4%",
        "7",
        "3.6%",
        "c(st-i,st) CR(st,Wi)",
        "shift",
        "reduce",
        "+",
        "172",
        "<",
        "1,209",
        "+",
        "1,432",
        ">",
        "805",
        "+ +",
        "4,430",
        "3,696",
        "- -",
        "525",
        "576",
        "total",
        "6,559",
        "=",
        "6,286",
        "Table 8: Effects of beam size k on efficiency and accuracy (on English dev set).",
        "Time is average per sentence (in sees).",
        "Bilingual constraints show more improvement with larger beams, with a fractional efficiency overhead over the baseline.",
        "bilingual features misled the parser into a mistake, there is no chance of getting back, while in the former multiple configurations are being pursued in parallel.",
        "In terms of speed, both parsers run proportionally slower with larger beams, as the time complexity is linear to the beam-size.",
        "Computing the bilingual features further slows it down, but only fractionally so (just 1.06 times as slow as the baseline at k=\\6), which is appealing in practice.",
        "By contrast, Burkett and Klein (2008) reported their approach of \"monolingual fc-best parsing followed by bilingual fc-best reranking\" to be \"3.8 times slower\" than monolingual parsing.",
        "Our final results on the test set (290 sentences) are summarized in Table 9.",
        "On both English and Chinese, the addition of bilingual features improves dependency arc accuracies by +0.6%, which is mildly significant using the Z-test of Collins et al.",
        "(2005).",
        "We also compare our results against the Berkeley parser (Petrov and Klein, 2007) as a reference system, with the exact same setting (i.e., trained on the bilingual data, and testing using gold-standard POS tags), and the resulting trees are converted into dependency via the same headrules.",
        "We use 5 iterations of split-merge grammar induction as the 6th iteration overfits the small training set.",
        "The result is worse than our baseline on English, but better than our bilingual parser on Chinese.",
        "The discrepancy between English and Chinese is probably due to the fact that our baseline feature templates (Table 2) are engineered on English not Chinese."
      ]
    },
    {
      "heading": "6. Conclusion and Future Work",
      "text": [
        "We have presented a novel parsing paradigm, bilingually-constrained monolingual parsing, which is much simpler than joint (bi-)parsing, yet still yields mild improvements in parsing accuracy in our preliminary experiments.",
        "Specifically, we showed a simple method of incorporating alignment features as soft evidence on top of a state-of-the-art shift-reduce dependency parser, which helped better resolve shift-reduce conflicts with fractional efficiency overhead.",
        "The fact that we managed to do this with only three alignment features is on one hand encouraging, but on the other hand leaving the bilingual feature space largely unexplored.",
        "So we will engineer more such features, especially with lexical-ization and soft alignments (Liang et al., 2006), and study the impact of alignment quality on parsing improvement.",
        "From a linguistics point of view, we would like to see how linguistics distance affects this approach, e.g., we suspect English-French would not help each other as much as English-Chinese do; and it would be very interesting to see what types of syntactic ambiguities can be resolved across different language pairs.",
        "Furthermore, we believe this bilingual-monolingual approach can easily transfer to shift-reduce constituency parsing (Sagae and Lavie, 2006)."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank the anonymous reviewers for pointing to us references about \"arc-standard\".",
        "We also thank Aravind Joshi and Mitch Marcus for insights on PP attachment, Joakim Nivre for discussions on arc-eager, Yang Liu for suggestion to look at manual alignments, and David A. Smith for sending us his paper.",
        "The second and third authors were supported by National Natural Science Foundation of China, Contracts 60603095 and 60736014, and 863 State Key Project No.",
        "2006AA010108.",
        "baseline",
        "+bilingual",
        "k",
        "accuracy",
        "time (s)",
        "accuracy",
        "time (s)",
        "1",
        "84.58",
        "0.011",
        "84.67",
        "0.012",
        "2",
        "85.30",
        "0.025",
        "85.62",
        "0.028",
        "4",
        "85.42",
        "0.040",
        "85.81",
        "0.044",
        "8",
        "85.50",
        "0.081",
        "85.95",
        "0.085",
        "16",
        "85.57",
        "0.158",
        "86.07",
        "0.168",
        "English",
        "Chinese",
        "monolingual baseline",
        "86.9",
        "85.7",
        "+bilingual features",
        "87.5",
        "86.3",
        "improvement",
        "+0.6",
        "+0.6",
        "signficance level",
        "p < 0.05",
        "p < 0.08",
        "Berkeley parser",
        "86.1",
        "87.9"
      ]
    }
  ]
}
