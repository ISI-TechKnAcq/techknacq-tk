{
  "info": {
    "authors": [
      "Anja Belz"
    ],
    "book": "Human Language Technologies 2007: the Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference",
    "id": "acl-N07-1021",
    "title": "Probabilistic Generation of Weather Forecast Texts",
    "url": "https://aclweb.org/anthology/N07-1021",
    "year": 2007
  },
  "references": [
    "acl-C00-2093",
    "acl-E06-1040",
    "acl-N01-1001",
    "acl-P02-1040",
    "acl-P05-1008",
    "acl-P06-1130",
    "acl-P98-1116",
    "acl-W05-1601",
    "acl-W94-0319"
  ],
  "sections": [
    {
      "text": [
        "This paper reports experiments in which pCRU – a generation framework that combines probabilistic generation methodology with a comprehensive model of the generation space – is used to semi-automatically create several versions of a weather forecast text generator.",
        "The generators are evaluated in terms of output quality, development time and computational efficiency against (i) human forecasters, (ii) a traditional handcrafted pipelined NLG system, and (iii) a HALOGEN-style statistical generator.",
        "The most striking result is that despite acquiring all decision-making abilities automatically, the best pCRU generators receive higher scores from human judges than forecasts written by experts."
      ]
    },
    {
      "heading": "1. Introduction and background",
      "text": [
        "Over the last decade, there has been a lot of interest in statistical techniques among researchers in natural language generation (nlg), a field that was largely unaffected by the statistical revolution in nlp that started in the 1980s.",
        "Since Langkilde and Knight's influential work on statistical surface realisation (Knight and Langkilde, 1998), a number of statistical and corpus-based methods have been reported.",
        "However, this interest does not appear to have translated into practice: of the 30 implemented systems and modules with development starting in or after 2000 that are listed on a key nlg website, only five have any statistical component at all (another six involve techniques that are in some way corpus-based).",
        "The likely reasons for this lack of take-up are that (i) many existing statistical NLG techniques are inherently expensive, requiring the set of alternatives to be generated in full before the statistical model is applied to select the most likely; and (ii) statistical NLG techniques have not been shown to produce outputs of high enough quality.",
        "There has also been a rethinking of the traditional modular nlg architecture (Reiter, 1994).",
        "Some research has moved towards a more comprehensive view, e.g. construing the generation task as a single constraint satisfaction problem.",
        "precursors to current approaches were Hovy's pauline which kept track of the satisfaction status of global 'rhetorical goals' (Hovy, 1988), and Power et al.",
        "'s iconoclast which allowed users to fine-tune different combinations of global constraints (power, 2000).",
        "in recent comprehensive approaches, the focus is on automatic adaptability, e.g. automatically determining degrees of constraint violability on the basis of corpus frequencies.",
        "Examples include Langkilde's (2005) general approach to generation and parsing based on constraint optimisation, and Marciniak and Strube's (2005) integrated, globally optimisable network of classifiers and constraints.",
        "Both probabilistic and recent comprehensive trends have developed at least in part to address two interrelated issues in nlg: the considerable amount of time and expense involved in building new systems, and the almost complete lack in the field of reusable systems and modules.",
        "Both trends have the potential to improve on development time and reusability, but have drawbacks.",
        "Existing statistical nlg (i) uses corpus statistics to inform heuristic decisions in what is otherwise symbolic generation (Varges and Mellish, 2001; White, 2004; Paiva and Evans, 2005); (ii) applies n-gram models to select the overall most likely realisation after generation (halogen family); or (iii) reuses an existing parsing grammar or treebank for surface realisation (Velldal et al., 2004; Cahill and van Genabith, 2006).",
        "N-gram models are not linguistically informed, (i) and (iii) come with a substantial manual overhead, and (ii) overgenerates vastly and has a high computational cost (see also Section 3).",
        "Existing comprehensive approaches tend to incur a manual overhead (finetuning in iconoclast, corpus annotation in Langkilde and Marciniak & Strube).",
        "Handling violability of soft constraints is problematic, and converting corpus-derived probabilities into costs associated with constraints (Langkilde, Marciniak & Strube) turns straightforward statistics into an ad hoc search heuristic.",
        "Older approaches are not globally optimisable (pauline) or involve exhaustive search (iconoclast).",
        "The pcru language generation framework combines a probabilistic generation methodology with a comprehensive model of the generation space, where probabilistic choice informs generation as it goes along, instead of after all alternatives have been generated.",
        "pcru uses existing techniques (Belz, 2005), but extends these substantially.",
        "This paper describes the pcru framework and reports experiments designed to rigorously test pcru in practice and to determine whether improvements in development time and reusability can be achieved without sacrificing quality of outputs."
      ]
    },
    {
      "heading": "2. />CRU language generation",
      "text": [
        "pcru (Belz, 2006) is a probabilistic language generation framework that was developed with the aim of providing the formal underpinnings for creating nlg systems that are driven by comprehensive probabilistic models of the entire generation space (including deep generation).",
        "nlg systems tend to be",
        "composed of generation rules that apply transformations to representations (performing different tasks in different modules).",
        "The basic idea in pcru is that as long as the generation rules are all of the set of all generation rules can be seen as defining a context-free language and a single probabilistic model can be estimated from raw or annotated text to guide generation processes.",
        "pcru uses straightforward context-free technology in combination with underspecification techniques, to encode a base generator as a set of expansion rules G composed of nary relations with variable and constant arguments (Section 2.1).",
        "In non-probabilistic mode, the output is the set offully expanded (fully specified) forms that can be derived from the input.",
        "The pcru (probabilistic cru) decision-maker is created by estimating a probability distribution over the base generator from an unannotated corpus of example texts.",
        "This distribution is used in one of several ways to drive generation processes, maximising the likelihood either of individual expansions or of entire generation processes (Section 2.2).",
        "Using context-free representational underspecification, or cru, (Belz, 2004), the generation space is encoded as (i) a set G of expansion rules composed of nary relations relation(arg\\, ...argn) where the argi are constants or variables over constants; and (ii) argument and relation type hierarchies.",
        "Any sentential form licensed by G can be the input to the generation process which expands it under unifying variable substitution until no further expansion is possible.",
        "The output (in non-probabilistic mode) is the set of fully expanded forms (i.e. consisting only of terminals) that can be derived from the input.",
        "The rules in G define the steps in which inputs can be incrementally specified from, say, content to semantic, syntactic and finally surface representations.",
        "G therefore defines specificity relations between all sentential forms, i.e. defines which representation is underspecified with respect to which other representations.",
        "The generation process is construed explicitly as the task of incrementally specifying one or more word strings.",
        "Within the limits of context-freeness and atomicity of feature values, CRU is neutral with respect to actual linguistic knowledge representation formalisms used to encode generation spaces.",
        "The main motivation for a context-free formalism is the advantage of low computational cost, while the inclusion of arguments on (non)terminals permits keeping track of contextual features.",
        "The pCRU decision-making component is created by estimating a probability distribution over the set of expansion rules that encodes the generation space (the base generator), as follows:",
        "1 Convert corpus into multi-treebank: determine for each sentence all (left-most) derivation trees licensed by the base generator's CRU rules, using maximal partial derivations ifthere is no complete derivation tree; annotate the (sub)strings in the sentence with the derivation trees, resulting in a set of generation trees for the sentence.",
        "2 Train base generator: obtain frequency counts for each individual generation rule from the multi-treebank, adding 1/n to the count for every rule, where n is the number of alternative derivation trees; convert counts into probability distributions over alternative rules, using add-1 smoothing and standard maximum likelihood estimation.",
        "The resulting probability distribution is used in one of the following three ways to control generation.",
        "Of these, only the first requires the generation forest to be created in full, whereas both greedy modes prune the generation space to a single path:",
        "1 Viterbi generation: do a Viterbi search of the generation forest for a given input, which maximises the joint likelihood of all decisions taken in the generation process.",
        "This selects the most likely generation process, but is considerably more expensive than the greedy modes.",
        "2 Greedy generation: make the single most likely decision at each choice point (rule expansion) in a generation process.",
        "This is not guaranteed to result in the most likely generation process, but the computational cost is very low.",
        "3 Greedy roulette-wheel generation: use a nonuniform random distribution proportional to the likelihoods of alternatives.",
        "E.g. if there are two alternative decisions D1 and D2, with the model giving p(D1) = 0.8 and p(D2) = 0.2, then the proportion of times the generator decides D1 approaches 80% and D2 20% in the limit.",
        "The technology described in the two preceding sections has been implemented in the pCRU-1.0 software package.",
        "The user defines a generation space by creating a base generator composed of:"
      ]
    },
    {
      "heading": "1.. the set N of underspecified nary relations",
      "text": []
    },
    {
      "heading": "2.. the set W of fully specified nary relations",
      "text": [
        "3. a set R of context-free generation rules n – a, n € N, a € (W U N)*",
        "4. a typed feature hierarchy defining argument types and values",
        "This base generator is then trained (as described above) on raw text corpora to provide a probability distribution over generation rules.",
        "Optionally, an n-gram language model can also be created from the same corpus.",
        "The generator is then run in one ofthe three modes above or one of the following:",
        "1.",
        "Random: ignoring pCRU probabilities, randomly select generation rules.",
        "2.",
        "N-gram: ignoring pCRU probabilities, generate set of alternatives and select the most likely according to the n-gram language model.",
        "The random mode serves as a baseline for generation quality: a trained generator must be able to do better, otherwise all the work is done by the base generator (and none by the probabilities).",
        "The n-gram mode works exactly like HALOGEN-style generation: the generator generates all realisations that the rules allow and then picks one based on the n-gram model.",
        "This is a point of comparison with existing statistical nlg techniques and also serves as a baseline in terms of computational expense: a generator using pCRU probabilities should be able to produce realisations faster."
      ]
    },
    {
      "heading": "3. Building and evaluating p CRU wind forecast text generators",
      "text": [
        "The automatic generation of weather forecasts is one of the success stories of nlp.",
        "The restrictiveness of the sublanguage has made the domain of",
        "FORECAST FOR:-Oil1/Oil2/Oil3 FIELDS",
        "THEN FALLING VARIABLE 04-08 BY",
        "LATE EVENING",
        "weather forecasting particularly attractive to nlg researchers, and a number of weather forecast generation systems have been created.",
        "A recent example of weather forecast text generation is the SumTime project (Reiter et al., 2005) which developed a commercially used nlg system that generates marine weather forecasts for offshore oil rigs from numerical forecast data produced by weather simulation programs.",
        "The SumTime corpus is used in the experiments below.",
        "Each instance in the SumTime corpus consists of three numerical data files (the outputs of weather simulators) and the forecast file written by the forecaster on the basis of the data (Figure 1 shows an example).",
        "The experiments below focused on a.m. forecasts of wind characteristics.",
        "Content determination (deciding which meteorological data to include in a forecast) was carried out off-line.",
        "The corpus consists of 2,123 instances (22,985 words) of which half are a.m. forecasts.",
        "This may not seem much, but considering the small number of vocabulary items and syntactic structures, the corpus provides extremely good coverage (an initial impression confirmed by the small differences between training and testing data results below).",
        "The base generator was written semi-auto-matically in two steps.",
        "First, a simple chunker was run over the corpus to split wind statements into wind direction, wind speed, gust speed, gust statements, time expressions, verb phrases, pre-modifiers, and post-modifiers.",
        "Preterminal generation rules were automatically created from the resulting chunks.",
        "Then, higher-level rules which combine chunks into larger components, taking care of text structuring, aggregation and elision, were manually authored.",
        "The top-level generation rules interpret wind statements as sequences of independent units of information, ensuring a linear increase in complexity with increasing input length.",
        "inputs encode meteorological data (as shown in Table 1), and were preprocessed to determine certain types of information, including whether a change in wind direction was clockwise or anti-clockwise, and whether change in wind speed was an increase or a decrease.",
        "The final generator takes as inputs number vectors of length 7 to 60, and generates up to 1.6 x 10 alternative realisations for an input.",
        "The job of the base generator is to describe the textual variety found in the corpus.",
        "it makes no decisions about when to prefer one variant over another.",
        "The corpus was divided at random into 90% training data and 10% testing data.",
        "The training set was multi-treebanked with the base generator and the multi-treebank then used to create the probability distribution for the base generator (as described in Section 2.2).",
        "A back-off 2-gram model with Good-Turing discounting and no lexical classes was also created from the training set, using the srilm toolkit, (Stolcke, 2002).",
        "pcru-1.0 was then run in all five modes to generate forecasts for the inputs in both training and test sets.",
        "This procedure was repeated five times for holdout cross-validation.",
        "The small amount of variation across the five repeats, and the small differences between results for training and test sets (Table 2) indicated that five repeats were sufficient.",
        "The two automatic metrics used in the evaluations, nist and bleu have been shown to correlate highly with expert judgments (pearson correlation coefficients 0.82 and 0.79 respectively) in this domain (Belz and Reiter, 2006).",
        "Corpus ssw 16-20 gradually backing sse then falling variable 4-8 by late evening",
        "Reference 1 sswly 16-20 gradually backing sse'ly then decreasing variable 4-8 by late evening",
        "Reference 2 ssw 16-20 gradually backing sse by 1800 then falling variable 4-8 by late evening",
        "sUMtIME-Hyb.",
        "ssw 16-20 gradually backing sse then becoming variable 10 or less by midnight",
        "pCRU-greedy ssw 16-20 backing sse for a time then falling variable 4-8 by late evening pCRU-roulette ssw 16-20 gradually backing sse and variable 4-8 pCRU-viterbi ssw 16-20 backing sse variable 4-8 later pCRU-2gram ssw 16-20 backing sse variable 4-8 later pCRU-random ssw 16-20 at first from midday becoming sse during the afternoon then variable 4-8",
        "Table 1: Forecast texts (for 05-10-2000) generated by each of the pCRU generators, the SuMTiME-Hybrid system and three experts.",
        "The corresponding input to the generators is shown in the first row.",
        "bleu (Papineni et al., 2002) is a precision metric that assesses the quality of a translation in terms of the proportion of its word n-grams (n < 4 has become standard) that it shares with several reference translations.",
        "bleu also incorporates a 'brevity penalty' to counteract scores increasing as length decreases.",
        "bleu scores range from 0 to 1.",
        "The nist metric (Doddington, 2002) is an adaptation of bleu, but where bleu gives equal weight to all n-grams, nist gives more weight to less frequent (hence more informative) n-grams.",
        "There is evidence that nist correlates better with human judgments than BLEU (Doddington, 2002; Belz and Reiter, 2006).",
        "The results below include human scores from two separate experiments.",
        "The first was an experiment with 9 subjects experienced in reading marine forecasts (Belz and Reiter, 2006), the second is a new experiment with 14 similarly experienced subjects .",
        "The main differences were that in Experiment 1, subjects rated on a scale from 0 to 5 and were asked for overall quality scores, whereas in Experiment 2, subjects rated on a 1-7 scale and were asked for language quality scores.",
        "In comparing different pCRU modes, nist and bleu scores were computed against the test set part of the corpus which contains texts by five different authors.",
        "In the two human experiments, NIST and bleu scores were computed against sets of multiple reference texts (2 for each date in Experiment 1, and 3 in Experiment 2) written by forecasters who had not contributed to the corpus.",
        "One-way anovas with post-hoc Tukey hsd tests were used to analyse variance and statistical significance of all results.",
        "Table 2: nist-5 and bleu-4 scores for training and test sets (average variation from the mean).",
        "the systems included in the evaluations reported below, together with the corresponding input and three texts created by humans for the same data.",
        "Table 2 shows results for the five different pCRU generation modes, for training sets (top) and test sets (bottom), in terms of nist-5 and bleu-4 scores averaged over the five runs of the holdout validation, with average mean deviation figures across the runs shown in brackets.",
        "The Tukey Test produced the following results for the differences between means in Table 2.",
        "For the training set, results are the same for nist and bleu scores: all differences are significant at P < 0.01, except for the differences in scores for pCRU-2gram andpCRU-viterbi.",
        "For the test set and nist, again all differences are significant at P < 0.01, except for pCRU-2gram vs. pCRU-viterbi.",
        "For the test set and bleu, three differences are non-significant: pCRU-2gram vs. pCRU-viterbi, pCRU-2gram vs. pCRUroulette, and pcru-viterbi vs. pcru-roulette.",
        "nist-5",
        "bleu-4",
        "T",
        "pCRU-greedy",
        "8.208 (0.033)",
        "0.647 (0.002)",
        "R",
        "pCRU-roulette",
        "7.035 (0.138)",
        "0.496 (0.010)",
        "A",
        "pCRU-2gram",
        "6.734 (0.086)",
        "0.523 (0.008)",
        "I",
        "pCRU-viterbi",
        "6.643 (0.023)",
        "0.524 (0.002)",
        "N",
        "pCRU-random",
        "4.799 (0.036)",
        "0.296 (0.002)",
        "pCRU-greedy",
        "6.927 (0.131)",
        "0.636 (0.016)",
        "T",
        "pCRU-roulette",
        "6.193 (0.121)",
        "0.496 (0.022)",
        "E",
        "pCRU-2gram",
        "5.663 (0.185)",
        "0.514 (0.019)",
        "S",
        "pCRU-viterbi",
        "5.650 (0.161)",
        "0.519 (0.021)",
        "T",
        "pCRU-random",
        "4.535 (0.078)",
        "0.313 (0.005)",
        "nist-5 depends on test set size, and is necessarily lower for the (smaller) test set, but the bleu-4 scores indicate that performance was slightly worse on test sets.",
        "The deviation figures show that variation was also higher on the test sets.",
        "The clearest result is that pcru-greedy is ranked highest, and pcru-random lowest, by considerable margins.",
        "pcru-roulette is ranked second by nist-5 and fourth by bleu-4.",
        "pcru-2gram and pcru-viterbi are virtually indistinguishable.",
        "Experts in both human experiments agreed with the nist-5 rankings of the modes exactly.",
        "The pcru modes were also evaluated against the SumTime-Hybrid system (running in 'hybrid' mode, taking inputs as in Table 1).",
        "Table 3 shows averaged evaluation scores by subjects in the two independent experiments described above.",
        "There were altogether 6 and 7 systems evaluated in these experiments, respectively, and the differences between the scores shown here were not significant when subjected to the Tukey Test, meaning that both experiments failed to show that experts can tell the difference in the language quality ofthe texts generated by the handcrafted SumTime-Hybrid system and the two best pcru-greedy systems.",
        "In the first experiment, the human evaluators gave an average score of 3.59 to pcru-greedy, 3.22 to the corpus texts, and 3.03 to another (human) forecaster.",
        "In Experiment 2, the average human scores were 4.79 for pcru-greedy, and 4.50 for the corpus texts.",
        "Although in each experiment separately, statistical significance could not be shown for the differences between these means, in combination the scores provide evidence that the evaluators thought pcru-greedy better than the human-written texts.",
        "The following table shows average number ofsec-onds taken to generate one forecast, averaged over the five cross-validation runs (mean variation figures across the runs in brackets):",
        "Training sets Test sets",
        "Forecasts for the test sets were generated somewhat faster than for the training sets in all modes.",
        "Variation was greater for test sets.",
        "Differences between pcru-greedy and pcru-roulette are very small, but pcru-viterbi took 1/10 of a second longer, and pcru-2gram took more than 1 second longer to generate the average forecast .",
        "N-gram models have a built-in bias in favour of shorter strings, because they calculate the likelihood of a string of words as the joint probability of the words, or, more precisely, as the product ofthe probabilities of each word given the n – 1 preceding words.",
        "The likelihood of any string will therefore generally be lower than that of any of its substrings.",
        "Using a smaller data set for which all systems had outputs, the average number of words in the forecasts generated by the different systems was:",
        "pcru-random has no preference for shorter strings, its average string length is almost twice that of the other pcru-generators.",
        "The 2-gram generator prefers shorter strings, while the Viterbi generator prefers shorter generation processes, and these preferences result in the shortest texts.",
        "The poor evaluation results above for the n-gram and Viterbi generators indicate that this brevity bias can be harmful in nlg.",
        "The remaining generators achieve good matches to the average forecast length in the corpus.",
        "Experiment 1",
        "Experiment 2",
        "SuMTlME-Hyb.",
        "pCRU-greedy",
        "pCRU-roulette",
        "3.82 (1) 3.59 (2) 3.22 (3)",
        "4.61 (2) 4.79 (1) 4.54 (3)",
        "The most time-consuming part of nlg system development is not encoding the range of alternatives, but the decision-making capabilities that enable selection among them.",
        "In SumTime (Section 3), these were the result of corpus analysis and consultation with writers and readers of marine forecasts.",
        "in the pcru wind forecast generators, the decision-making capabilities are acquired automatically, no expert knowledge or corpus annotation is used.",
        "The SumTime team estimate that very approximately 12 person months went directly into developing the SumTime microplanner and realiser (the components functionally analogous to the pcru-generators), and 24 on generic activities such as expert consultation, which also benefited the mi-croplanner/realiser.",
        "The pcru wind forecasters were built in less than a month, including familiarisation with the corpus, building the chunker and creating the generation rules themselves.",
        "However, the SumTime system also generates wave forecasts and appropriate layout and canned text.",
        "A generous estimate is that it would take another two person months to equip the pcru forecaster with these capabilities.",
        "This is not to say that the two research efforts resulted in exactly the same thing.",
        "it is clear that forecast readers prefer the SumTime system, but the point is that it did come with a substantial price tag attached.",
        "The pcru approach allows control over the trade-off between cost and quality."
      ]
    },
    {
      "heading": "4. Discussion",
      "text": [
        "The main contributions of the research described in this paper are: (i) a generation methodology that improves substantially on development time and reusability compared to traditional hand-crafted systems; (ii) techniques for training linguistically informed decision-making components for probabilistic nlg from raw corpora; and (iii) results that show that probabilistic nlg can produce high-quality text.",
        "Results also show that (i) a preference for shorter realisations can be harmful in nlg; and that (ii) linguistically literate, probabilistic nlg can outperform halogen-style shallow statistical methods, in terms of quality and efficiency.",
        "An interesting question concerns the contribution of the manually built component (the base generator) to the quality of the outputs.",
        "The random mode serves as an absolute baseline in this respect: it indicates how well a particular base generator performs on its own.",
        "However, different base generators have different effects on the generation modes.",
        "The base generator that was used in previous experiments (Belz, 2005) encoded a less structured generation space and the set of concepts it used were less fine-grained (e.g. it did not distinguish between an increase and a decrease in wind speed, considering both simply a change), and therefore it lacked some information necessary for deriving conditional probabilities for lexical choice (e.g. freshening vs. easing).",
        "As predicted (Belz, 2005, p. 21), improvements to the base generator made little difference to the results for pcru-2gram (up from bleu 0.45 to 0.5), but greatly improved the performance of the greedy mode (up from 0.43 to 0.64).",
        "A basic question for statistical nlg is whether surface string likelihoods are enough to resolve remaining non-determinism in generators, or whether likelihoods at the more abstract level of generation rules are needed.",
        "The former always prefers the most frequent variant regardless ofcontext, whereas in the latter probabilities can attach to linguistic objects and be conditioned on contextual features (e.g. one useful feature in the forecast text generators encoded whether a rule was being applied at the beginning of a text).",
        "The results reported in this paper provide evidence that probabilistic generation can be more powerful than n-gram based post-selection."
      ]
    },
    {
      "heading": "5. Conclusions",
      "text": [
        "The pcru approach to generation makes it possible to combine the potential accuracy and subtlety of symbolic generation rules with detailed linguistic features on the one hand, and the robustness and handle on nondeterminism provided by probabilities associated with these rules, on the other.",
        "The evaluation results for the pcru generators show that outputs of high quality can be produced with this approach, that it can speed up development and improve reusability of systems, and that in some modes it is more efficient and less brevity-biased than existing halogen-style n-gram techniques.",
        "The current situation in nlg recalls nlu in the late 1980s, when symbolic and statistical nlp were separate research paradigms, a situation memorably caricatured by Gazdar (1996), before rapidly moving towards a paradigm merger in the early 1990s.",
        "A similar development is currently underway in mt where – after several years of statistical mt dominating the field – researchers are now beginning to bring linguistic knowledge into statistical techniques (Charniak et al., 2003; Huang et al., 2006), and this trend looks set to continue.",
        "The lesson from nlu and mt appears to be that higher quality results when the symbolic and statistical paradigms join forces.",
        "The research reported in this paper is intended to be a first step in this direction for nlg."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "This research was in part supported under uk epsrc Grant GR/S24480/01.",
        "Many thanks to the anonymous reviewers for very helpful comments."
      ]
    }
  ]
}
