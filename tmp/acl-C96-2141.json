{
  "info": {
    "authors": [
      "Stephan Vogel",
      "Hermann Ney",
      "Christoph Tillmann"
    ],
    "book": "International Conference on Computational Linguistics",
    "id": "acl-C96-2141",
    "title": "HMM-Based Word Alignment in Statistical Translation",
    "url": "https://aclweb.org/anthology/C96-2141",
    "year": 1996
  },
  "references": [
    "acl-C94-2178",
    "acl-J93-1006",
    "acl-J93-2003",
    "acl-W93-0301"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we describe a new model for word alignment in statistical translation and present experimental results.",
        "The idea of the model is to make the alignment probabilities dependent on the differences in the alignment positions rather than on the absolute positions.",
        "To achieve this goal, the approach uses a first-order Hidden Markov model (HMM) for the word alignment problem as they are used successfully in speech recognition for the time alignment problem.",
        "The difference to the time alignment HMM is that there is no monotony constraint for the possible word orderings.",
        "We describe the details of the model and test the model on several bilingual corpora."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In this paper, we address the problem of word alignments for a bilingual corpus.",
        "In the recent years, there have been a number of papers considering this or similar problems: (Brown et al., 1990), (Dagan et al., 1993), (Kay et al., 1993), (rung et al., 1993).",
        "In our approach, we use a first-order Hidden Markov model (HMM) (Jelinek, 1976), which is similar, but not identical to those used in speech recognition.",
        "The key component of this approach is to make the alignment probabilities dependent not on the absolute position of the word alignment, but on its relative position; i.e. we consider the differences in the index of the word positions rather than the index itself.",
        "The organization of the paper is as follows.",
        "After reviewing the statistical approach to machine translation, we first describe the conventional model (mixture model).",
        "We then present our first-order HMM approach in full detail.",
        "Finally we present some experimental results and compare our model with the conventional model."
      ]
    },
    {
      "heading": "2 Review: Translation Model",
      "text": [
        "The goal is the translation of a text given in some language F into a target language E. For convenience, we choose for the following exposition as language pair French and English, i.e. we are given a French string fl ...fi...fj, which is to be translated into an English string el Among all possible English strings, we will choose the one with the highest probability which is given by Hayes' decision rule:",
        "Pr(ef) is the language model of the target language, whereas PrUil is the string translation model.",
        "The argmax operation denotes the search problem.",
        "In this paper, we address the problem of introducing structures into the probabilistic dependencies in order to model the string translation probability Pr(fif lef)•"
      ]
    },
    {
      "heading": "3 Alignment Models",
      "text": [
        "A key issue in modeling the string translation probability Pr(glef) is the question of how we define the correspondence between the words of the English sentence and the words of the French sentence.",
        "In typical cases, we can assume a sort of pairwise dependence by considering all word pairs (fi,ci) for a given sentence pair [fif ; en.",
        "We further constrain this model by assigning each French word to exactly one English word.",
        "Models describing these types of dependencies are referred to as alignment models.",
        "In this section, we describe two models for word alignment in detail:",
        "• a mixture-based alignment model, which was introduced in (Brown et al., 1990); • an IIMM-based alignment model.",
        "In this paper, we address the question of how to define specific models for the alignment probabilities.",
        "The notational convention will be as follows.",
        "We use the symbol Pr(.)",
        "to denote general",
        "probability distributions with (nearly) no specific assumptions.",
        "In contrast, for model-based probability distributions, we use the generic symbol P(•)•"
      ]
    },
    {
      "heading": "3.1 Alignment with Mixture Distribution",
      "text": [
        "Here, we describe the mixture-based alignment model in a formulation which is different from the original formulation in (-Brown et 1990).",
        "We will use this model as reference for the IIM M-based alignments to be presented later.",
        "The model is based on a decomposition of the joint probability for fif into a product over the probabilities for each word",
        "where, for normalization reasons, the sentence length probability p(J1.1) has been included.",
        "The next step now is to assume a sort of pairwise interaction between the French word fi and each English word ell,i 1, These dependencies are captured in the form of a mixture distribution: For uniform alignment probabilities, it can be shown (Brown et al., 1990), that there is only one 01)611,11in and therefore the EM algorithm (Baum, 1972) always finds the global optimum.",
        "For mixture alignment model with nonuniform alignment probabilities (subsequently referred to as .113M2 model), there are too many alignment parameters P(ilji to be estimated for small corpora.",
        "Therefore, a specific model for the alignment probabilities is used:",
        "'Phis model assumes that the position distance relative to the diagonal line of the (j, i) plane is the dominating factor (see Fig. 1).",
        "To train tins model, we use the maximum likelihood criterion in the so-called maximum approximation, i.e. the likelihood criterion covers only the most likely alignment rather than the set of all alignments:",
        "with the following ingredients:",
        "• sentence length probability: 7010; • mixture alignment probability: p(ilj, 1); • translation probability: p( Pc).",
        "Assuming a uniform alignment probability = we arrive at the first model proposed by (Brown et al., 1990).",
        "This model will be referred to as IBM1 'model.",
        "To train the translation probabilities p(f(e), we use a bilingual corpus consisting of sentence pairs [fit s; ells] , s = 1, ..., S. Using the maximum likelihood criterion, we obtain the following iterative equation (Brown et al., 1990): ACT , e 100 6(f, i•j,) h(e, cis)",
        "In training, this criterion amounts to a sequence of iterations, each of winch consists of two steps:",
        "• position alignment: Given the model parameters, determine the most likely position align] nent • parameter estimation: Given the position alignment, i.e. going along the alignment paths for all sentence pairs, perform maximum likelihood estimation of the model parameters; for model-free distributions, these estimates result in relative frequencies.",
        "Due to the nature of the mixture model, there is no interaction between adjacent word positions.",
        "Therefore, the optimal position i for each position j can be determined independently of the neighbouring positions.",
        "Thus the resulting training procedure is straightforward."
      ]
    },
    {
      "heading": "3.2 Alignment with HMM",
      "text": [
        "We now propose an 11MM-based alignment model.",
        "The motivation is that typically we have a strong localization effect in aligning the words in parallel texts (for language pairs from hidoeuropean languages): the words are not distributed arbitrarily over the sentence positions, but tend to form clusters.",
        "Fig.",
        "1 illustrates tins effect for the language pair German - English.",
        "Each word of the German sentence is assigned to a word of the English sentence.",
        "The alignments have a strong tendency to preserve the local neighborhood when going from the one language to the other language.",
        "In many cases, although not always, there is an even stronger restriction: the difference in the position index is smaller than 3.",
        "To describe these word-by-word alignments, we introduce the mapping j aj, which assigns a word fi in position j to a word ei in position i = aj.",
        "The concept of these alignments is similar to the ones introduced by (Brown et al., 1990), but we will use another type of dependence in the probability distributions.",
        "Looking at such alignments produced by a human expert, it is evident that the mathematical model should try to capture the strong dependence of aj on the previous alignment.",
        "Therefore the probability of alignment aj for position j should have a dependence on the previous alignment aj _ p(ajlai , where we have included the conditioning on the total length I of the English sentence for normalization reasons.",
        "A similar approach has been chosen by (Dagan et al., 1993).",
        "Thus the problem formulation is similar to that of the time alignment problem in speech recognition, where the so-called Hidden Markov models have been successfully used for a long time (Jelinek, 1976).",
        "Using the same basic principles, we can rewrite the probability by introducing the 'hidden' alignments ai •.= aj...aj...aj for a sentence pair [fit ; en:",
        "So far there has been no basic restriction of the approach.",
        "We now assume a first-order dependence on the alignments aj only:",
        "where, in addition, we have assumed that the translation probability depends only on aj and not on aj_j.",
        "Putting everything together, we have the following IIMM-based model:",
        "with the following ingredients:",
        "• TIMM alignment probability: kik', I) or t, I); • translation probability: v(elle).",
        "In addition, we assume that the HMM alignment probabilities depend only on the jump width – Using a set of non-negative parameters {s(i – i')}, we can write the IIMM alignment probabilities in the form:",
        "This form ensures that for each word position i', = 1, ..., I, the 11MM alignment probabilities satisfy the normalization constraint.",
        "Note the similarity between Equations (2) and (5).",
        "The mixture model can be interpreted as a zeroth-order model in contrast to the first-order HMM model.",
        "As with the IBM2 model, we use again the maximum approximation: I a – , i) • P(.6 le,,;)] (6) In this case, the task of finding the optimal alignment is more involved than in the case of the mixture model (IBM2).",
        "Therefore, we have to resort to dynamic programming for which we have the following typical recursion formula: Q(i, j) = PUi flax f [P(i1 I) • Q(i', – 1)] Here, Q(i, j) is a sort of partial probability as in time alignment for speech recognition (Jelinek, 1976)."
      ]
    },
    {
      "heading": "4 Experimental Results",
      "text": []
    },
    {
      "heading": "4.1 The Task and the Corpus",
      "text": [
        "The models were tested on several tasks:",
        "• the Avalanche Bulletins published by the Swiss Federal Institute for Snow and Avalanche Research (SFISAR,) in Davos, Switzerland and made available by the European Corpus Initiative (ECl/MCI, 1994); • the Verbmobil Corpus consisting of spontaneously spoken dialogs in the domain of appointment scheduling (Wahlster, 1993);",
        "• the EuTrans Corpus which contains typical phrases from the tourists and travel domain.",
        "(EuTrans, 1996).",
        "'table 1 gives the details on the size of the corpora and their vocabulary.",
        "should be noted that in all these three cases the ratio of vocabulary size and number of running words is not very favorable.",
        "For several years between 83 and 92, the.",
        "Avalanche Bulletins are available for botll German and French.",
        "The following is a typical sentence pair ['Mill the corpus: Bei zuerst reclrt hohen, spitter tieleren Tem-peraturen sind von Sanistag his Dienstag morgon auf der Alpennordseite and AlpenhauptkaiI1M oherhalb 2000 ]n 60 his 80 cm Nenschnee gefailen.",
        "1-)ar des teinf)eratures abord elevees, puffs plus basses, 60 h 80 cm de neige soot tombs de samedi Mardi matin sur le versant Hord et la Crete des Mites au-dessus de 2000 in.",
        "An plc from Hie Yerhmohii corpus is given in Figure 1."
      ]
    },
    {
      "heading": "4.2 Training and Results",
      "text": [
        "Each of the three corpora were used to train both aligns cart models, the mixture-based alignment model in Eq.",
        "(1) and the HMM-based alignment model in Eq.(4).",
        "Here, we will consider the experimental tests ort the Avalanche corpus in more detail.",
        "The training procedure consisted of the following steps:",
        "• Initialization training: 11041 model trained for 10 iterations of the EMI algorithm.",
        "• Refinement training: The translation probabilities from the initialization training were used to initialize both the 1BM2 model and the IIMM-based alignment model",
        "-- 1111VI2 Model: 5 iterations using the maximum approximation (Eq.",
        "(3)) - .11M.A4 Model: 5 iterations using the II 1;1K-haunt approximation (E1.",
        "(6)) Tin resulting perplexity (inverse geometric iti-erage of the likelihoods) for the different models are given in the 'fables 2 and 3 for the Avalanche corpus.",
        "Iii addition to the total perplexity, which is the global optimization criterion, the tables also show the perplexities of the translation probabilities and of the alignment probabilities.",
        "The last line in Table 2 gives the perplexity measures when applying the maximum approximation and computing the perplexity in this approximation.",
        "These values are equal to the ones after initializing the 113M2 and fl:MM models, as they should be.",
        "From Table :3, we can see that the mixture alignment gives slightly better perplexity values for the translation.",
        "probabilities, whereas the IIMM model produces a smaller perplexity for the alignment probabilities.",
        "In the calculation of the perplexities, the sentence length probability was not in-chided.",
        "Another interesting question is whether the 111\\41\\4 alignment model_ helps in finding good and sharply focussed word-to-word correspondences.",
        "As an example, 4 gives a comparison of the translation probabilities v(JIe) between the mixture and the II M M alignment model for the German.",
        "word Alpe P9 ildhang.",
        "'the counts of the words are given in brackets.",
        "'there is virtually no difference between the translation tables for the two models (111M2 and I1MM).",
        "lint in general, the HMM model seems to give slightly better results in the cases of German compound words like",
        "This is a result of the smoother position alignments produced by the HMM model.",
        "A pronounced example is given in Figure 2.",
        "'the problem of the absolute position alignment can be demonstrated at the positions (a) and (c): both Schneebreitgefahr and Schneeverfrachtungen have a high probability on neige.",
        "The IBM2 models chooses the position near the diagonal, as this is the one with the higher probability.",
        "Again, Schneebrettgefahr generates de which explains the wrong alignment near the diagonal in (c).",
        "However, this strength of the HMM model can also be a weakness as in the case of est developpe - ist entstanden (see (b) in Figure 2.",
        "The required two large jumps are correctly found by the mixture model, but not by the HMM model.",
        "These cases suggest an extention to the HMM model.",
        "In general, there are only a small number of big jumps in the position alignments in a given sentence pair.",
        "Therefore a model could be useful that distinguishes between local and big jumps.",
        "The models have also been tested on the Verbmobil Translation Corpus as well as on a small Corpus used in the EuTrans project.",
        "The sentences in the EuTrans corpus are in general short phrases with simple grammatical structures.",
        "However, the training corpus is very small and the produced alignments are generally of poor quality.",
        "There is no marked difference for the two alignment models.",
        "The Verbmobil Corpus consists of spontaneously spoken dialogs in the domain of appointment scheduling.",
        "The assumption that every word in the source language is aligned to a word in the target language breaks clown for many sentence pairs, resulting in poor alignment.",
        "This in turn affects the quality of the translation probabilities.",
        "Several extensions to the current TIMM based model could be used to tackle these problems: • The results presented here did not use the concept of the empty word.",
        ":For the HMM-based model this, however, requires a second-order rather than a first-order model.",
        "• We could allow for multi-word phrases in both languages.",
        "• In addition to the absolute or relative align",
        "ment positions, the alignment probabilities can be assumed to depend on part of speech tags or on the words themselves.",
        "(confer model 4 in (Brown et al., 1990))."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "In this paper, we have presented an IIMM-based approach for modelling word alignments in parallel texts.",
        "The characteristic feature of this approach is to make the alignment probabilities explicitly dependent on the alignment position of the previous word.",
        "We have tested the model successfully on real data.",
        "The HMM-based approach produces translation probabilities comparable to the mixture alignment model.",
        "When looking at the position alignments those generated by the HMM model are in general much smoother.",
        "This could be especially helpful for languages such as German, where compound words are matched to several words in the source language.",
        "On the other hand, large jumps due to different word orderings in the two languages are successfully modeled.",
        "We are presently studying and testing a multilevel HMM model that allows only a small number of large jumps.",
        "The ultimate test of the different alignment and translation models can only be Carried out in the framework of a fully operational translation system."
      ]
    },
    {
      "heading": "6 Acknowledgement",
      "text": [
        "This research was partly supported by the German Federal Ministery of Education, Science, Research and Technology under the Contract Number 01 IV 601 A (Verbmobil) and under the Esprit Research Project 20268 (EuTrans)."
      ]
    }
  ]
}
