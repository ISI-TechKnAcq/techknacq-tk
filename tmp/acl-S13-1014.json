{
  "info": {
    "authors": [
      "Nikolaos Malandrakis",
      "Elias Iosif",
      "Vassiliki Prokopi",
      "Alexandros Potamianos",
      "Shrikanth Narayanan"
    ],
    "book": "*SEM",
    "id": "acl-S13-1014",
    "title": "DeepPurple: Lexical, String and Affective Feature Fusion for Sentence-Level Semantic Similarity Estimation",
    "url": "https://aclweb.org/anthology/S13-1014",
    "year": 2013
  },
  "references": [
    "acl-C08-1107",
    "acl-H05-1079",
    "acl-I05-5003",
    "acl-J06-1003",
    "acl-J06-3003",
    "acl-J10-4006",
    "acl-J90-1003",
    "acl-N03-1033",
    "acl-P02-1040",
    "acl-P04-1077",
    "acl-P05-1045",
    "acl-P06-1114",
    "acl-P09-1089",
    "acl-P09-3004",
    "acl-S12-1051",
    "acl-S12-1059",
    "acl-S12-1060",
    "acl-S12-1082",
    "acl-S13-1004",
    "acl-W03-1604",
    "acl-W07-1407"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper describes our submission for the *SEM shared task of Semantic Textual Similarity.",
        "We estimate the semantic similarity between two sentences using regression models with features: 1) n-gram hit rates (lexical matches) between sentences, 2) lexical semantic similarity between non-matching words, 3) string similarity metrics, 4) affective content similarity and 5) sentence length.",
        "Domain adaptation is applied in the form of independent models and a model selection strategy achieving a mean correlation of 0.47."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Text semantic similarity estimation has been an active research area, thanks to a variety of potential applications and the wide availability of data afforded by the world wide web.",
        "Semantic textual similarity (STS) estimates can be used for information extraction (Szpektor and Dagan, 2008), question answering (Harabagiu and Hickl, 2006) and machine translation (Mirkin et al., 2009).",
        "Term-level similarity has been successfully applied to problems like grammar induction (Meng and Siu, 2002) and affective text categorization (Malandrakis et al., 2011).",
        "In this work, we built on previous research and our submission to SemEval?2012 (Malandrakis et al., 2012) to create a sentence-level STS model for the shared task of *SEM 2013 (Agirre et al., 2013).",
        "Semantic similarity between words has been well researched, with a variety of knowledge-based (Miller, 1990; Budanitsky and Hirst, 2006) and corpus-based (Baroni and Lenci, 2010; Iosif and Potamianos, 2010) metrics proposed.",
        "Moving to sentences increases the complexity exponentially and as a result has led to measurements of similarity at various levels: lexical (Malakasiotis and Androutsopoulos, 2007), syntactic (Malakasiotis, 2009; Zanzotto et al., 2009), and semantic (Rinaldi et al., 2003; Bos and Markert, 2005).",
        "Machine translation evaluation metrics can be used to estimate lexical level similarity (Finch et al., 2005; Perez and Alfonseca, 2005), including BLEU (Papineni et al., 2002), a metric using word n-gram hit rates.",
        "The pilot task of sentence STS in SemEval 2012 (Agirre et al., 2012) showed a similar trend towards multilevel similarity, with the top performing systems utilizing large amounts of partial similarity metrics and domain adaptation (the use of separate models for each input domain) (Ba?r et al., 2012; S?aric?",
        "et al, 2012).",
        "Our approach is originally motivated by BLEU and primarily utilizes ?hard?",
        "and ?soft?",
        "n-gram hit rates to estimate similarity.",
        "Compared to last year, we utilize different alignment strategies (to decide which n-grams should be compared with which).",
        "We also include string similarities (at the token and character level) and similarity of affective content, expressed through the difference in sentence arousal and valence ratings.",
        "Finally we added domain adaptation: the creation of separate models per domain and a strategy to select the most appropriate model."
      ]
    },
    {
      "heading": "2 Model",
      "text": [
        "Our model is based upon that submitted for the same task in 2012 (Malandrakis et al., 2012).",
        "To estimate semantic similarity metrics we use a supervised model with features extracted using corpus",
        "based word-level similarity metrics.",
        "To combine these metrics into a sentence-level similarity score we use a modification of BLEU (Papineni et al., 2002) that utilizes word-level semantic similarities, string level comparisons and comparisons of affective content, detailed below."
      ]
    },
    {
      "heading": "2.1 Word level semantic similarity",
      "text": [
        "Co-occurrence-based.",
        "The semantic similarity between two words, wi and wj , is estimated as their pointwise mutual information (Church and Hanks, 1990): I(i, j) = log p?(i,j)p?(i)p?",
        "(j) , where p?",
        "(i) and p?",
        "(j) are the occurrence probabilities of wi and wj , respectively, while the probability of their co-occurrence is denoted by p?",
        "(i, j).",
        "In our previous participation in SemEval12-STS task (Malandrakis et al., 2012) we employed a modification of the pointwise mutual information based on the maximum sense similarity assumption (Resnik, 1995) and the minimization of the respective error in similarity estimation.",
        "In particular, exponential weights ?",
        "were introduced in order to reduce the overestimation of denominator probabilities.",
        "The modified metric Ia(i, j), is defined as:",
        "The weight ?",
        "was estimated on the corpus of (Iosif and Potamianos, 2012) in order to maximize word sense coverage in the semantic neighborhood of each word.",
        "The Ia(i, j) metric using the estimated value of ?",
        "= 0.8 was shown to significantly outperform I(i, j) and to achieve state-of-the-art results on standard semantic similarity datasets (Rubenstein and Goodenough, 1965; Miller and Charles, 1998; Finkelstein et al., 2002).",
        "Context-based: The fundamental assumption behind context-based metrics is that similarity of context implies similarity of meaning (Harris, 1954).",
        "A contextual window of size 2H + 1 words is centered on the word of interest wi and lexical features are extracted.",
        "For every instance of wi in the corpus the H words left and right of wi formulate a feature vector vi.",
        "For a given value of H the context-based semantic similarity between two words, wi and wj , is computed as the cosine of their feature vectors: QH(i, j) = vi.vj||vi |vj ||.",
        "The elements of feature vectors can be weighted according various schemes [(Iosif and Potamianos, 2010)], while, here we use a binary scheme.",
        "Network-based: The aforementioned similarity metrics were used for the definition of a semantic network (Iosif and Potamianos, 2013; Iosif et al., 2013).",
        "A number of similarity metrics were proposed under either the attributional similarity (Turney, 2006) or the maximum sense similarity (Resnik, 1995) assumptions of lexical semantics1."
      ]
    },
    {
      "heading": "2.2 Sentence level similarities",
      "text": [
        "To utilize word-level semantic similarities in the sentence-level task we use a modified version of BLEU (Papineni et al., 2002).",
        "The model works in two passes: the first pass identifies exact matches (similar to baseline BLEU), the second pass compares non-matched terms using semantic similarity.",
        "Non-matched terms from the hypothesis sentence are compared with all terms of the reference sentence (regardless of whether they were matched during the first pass).",
        "In the case of bigram and higher order terms, the process is applied recursively: the bigrams are decomposed into two words and the similarity between them is estimated by applying the same method to the words.",
        "All word similarity metrics used are peak-to-peak normalized in the [0,1] range, so they serve as a ?degree-of-match?.",
        "The semantic similarity scores from term pairs are summed (just like n-gram hits) to obtain a BLEU-like hit-rate.",
        "Alignment is performed via maximum similarity: we iterate on the hypothesis n-grams, left-to-right, and compare each with the most similar n-gram in the reference.",
        "The features produced by this process are ?soft?",
        "hit-rates (for 1-, 2-, 3-, 4-grams)2.",
        "We also use the ?hard?",
        "hit rates produced by baseline BLEU as features of the final model."
      ]
    },
    {
      "heading": "2.3 String similarities",
      "text": [
        "We use the following string-based similarity features: 1) Longest Common Subsequence Similarity",
        "namic programming algorithm.",
        "LCSS represents the length of the longest string (or strings) that is a sub-string (or are substrings) of two or more strings.",
        "2) Skip bigram co-occurrence measures the overlap of skip-bigrams between two sentences or phrases.",
        "A skip-bigram is defined as any pair of words in the sentence order, allowing for arbitrary gaps between words (Lin and Och, 2004).",
        "3) Containment is defined as the percentage of a sentence that is contained in another sentence.",
        "It is a number between 0 and 1, where 1 means the hypothesis sentence is fully contained in the reference sentence (Broder, 1997).",
        "We express containment as the amount of n-grams of a sentence contained in another.",
        "The containment metric is not symmetric and is calculated as: c(X,Y ) = |S(X) ?",
        "S(Y )|/S(X), where S(X) and S(Y ) are all the n-grams of sentences X and Y respectively."
      ]
    },
    {
      "heading": "2.4 Affective similarity",
      "text": [
        "We used the method proposed in (Malandrakis et al., 2011) to estimate affective features.",
        "Continuous (valence and arousal) ratings in [?1, 1] of any term are represented as a linear combination of a function of its semantic similarities to a set of seed words and the affective ratings of these words, as follows:",
        "where wj is the term we mean to characterize, w1...wN are the seed words, v(wi) is the valence rating for seed word wi, ai is the weight corresponding to seed word wi (that is estimated as described next), dij is a measure of semantic similarity between wi andwj (for the purposes of this work, cosine similarity between context vectors is used).",
        "The weights ai are estimated over the Affective norms for English Words (ANEW) (Bradley and Lang, 1999) corpus.",
        "Using this model we generate affective ratings for every content word (noun, verb, adjective or adverb) of every sentence.",
        "We assume that these can adequately describe the affective content of the sentences.",
        "To create an ?affective similarity metric?",
        "we use the difference of means of the word affective ratings between two sentences.",
        "d?affect = 2?",
        "|?(v?(s1))?",
        "?(v?",
        "(s2)) |(3) where ?(v?",
        "(si)) the mean of content word ratings included in sentence i."
      ]
    },
    {
      "heading": "2.5 Fusion",
      "text": [
        "The aforementioned features are combined using one of two possible models.",
        "The first model is a",
        "where D?L is the estimated similarity, fk are the unsupervised semantic similarity metrics and an are the trainable parameters of the model.",
        "The second model is motivated by an assumption of cognitive scaling of similarity scores: we expect that the perception of hit rates is non-linearly affected by the length of the sentences.",
        "We call this the hierarchical fusion scheme.",
        "It is a combination of (overlapping) MLR models, each matching a range of sentence lengths.",
        "The first model DL1 is trained with sentences with length up to l1, i.e., l ?",
        "l1, the second model DL2 up to length l2 etc.",
        "During testing, sentences with length l ?",
        "[1, l1] are decoded with DL1, sentences with length l ?",
        "(l1, l2] with model DL2 etc.",
        "Each of these partial models is a linear fusion model as shown in (4).",
        "In this work, we use four models with l1 = 10, l2 = 20, l3 = 30,",
        "Domain adaptation is employed, by creating separate models per domain (training data source).",
        "Beyond that, we also create a unified model, trained on all data to be used as a fallback if an appropriate model can not be decided upon during evaluation."
      ]
    },
    {
      "heading": "3 Experimental Procedure and Results",
      "text": [
        "Initially all sentences are preprocessed by the CoreNLP (Finkel et al., 2005; Toutanova et al., 2003) suite of tools, a process that includes named entity recognition, normalization, part of speech tagging, lemmatization and stemming.",
        "We evaluated multiple types of preprocessing per unsupervised metric and chose different ones depending on the metric.",
        "Word-level semantic similarities, used for soft comparisons and affective feature extraction, were computed over a corpus of 116 million web snippets collected by posing one query for every word in the Aspell spellchecker (asp, ) vocabulary to the Yahoo!",
        "search engine.",
        "Word-level emotional ratings in continuous valence and arousal scales were produced by a model trained on the ANEW dataset",
        "and using contextual similarities.",
        "Finally, string similarities were calculated over the original unmodified sentences.",
        "Next, results are reported in terms of correlation between the generated scores and the ground truth, for each corpus in the shared task, as well as their weighted mean.",
        "Feature selection is applied to the large candidate feature set using a wrapper-based backward selection approach on the training data.The final feature set contains 15 features: soft hit rates calculated over content word 1 to 4 grams (4 features), soft hit rates calculated over un-igrams per part-of-speech, for adjectives, nouns, adverbs, verbs (4 features), BLEU unigram hit rates for all words and content words (2 features), skip and containment similarities, containment normalized by sum of sentence lengths or product of sentence lengths (3 features) and affective similarities for arousal and valence (2 features).",
        "Domain adaptation methods are the only difference between the three submitted runs.",
        "For all three runs we train one linear model per training set and a fallback model.",
        "For the first run, dubbed linear, the fallback model is linear and model selection during evaluation is performed by file name, therefore results for the OnWN set are produced by a model trained with OnWN data, while the rest are produced by the fallback model.",
        "The second run, dubbed length, uses a hierarchical fallback model and model selection is performed by file name.",
        "The third run, dubbed adapt, uses the same models as the first run and each test set is assigned to a model (i.e., the fallback model is never used).",
        "The test set - model (training) mapping for this run is: OnWN ?",
        "OnWN, headlines ?",
        "SMTnews, SMT ?",
        "Europarl and FNWN?",
        "OnWN.",
        "Results are shown in Tables 1 and 2.",
        "Results for the linear run using subsets of the final feature set are shown in Table 1.",
        "Lexical features (hit rates) are obviously the most valuable features.",
        "String similarities provided us with an improvement in the train",
        "ing set which is not reflected in the test set.",
        "Affect proved valuable, particularly in the most difficult sets of FNWN and SMT.",
        "Results for the three submission runs are shown in Table 2.",
        "Our best run was the simplest one, using a purely linear model and effectively no adaptation.",
        "Adding a more aggressive adaptation strategy improved results in the FNWN and SMT sets, so there is definitely some potential, however the improvement observed is nowhere near that observed in the training data or the same task of SemEval 2012.",
        "We have to question whether this improvement is an ar-tifact of the rating distributions of these two sets (SMT contains virtually only high ratings, FNWN contains virtually only low ratings): such wild mismatches in priors among training and test sets can be mitigated using more elaborate machine learning algorithms (rather than employing better semantic similarity features or algorithms).",
        "Overall the system performs well in the two sets containing large similarity rating ranges."
      ]
    },
    {
      "heading": "4 Conclusions",
      "text": [
        "We have improved over our previous model of sentence semantic similarity.",
        "The inclusion of string-based similarities and more so of affective content measures proved significant, but domain adaptation provided mixed results.",
        "While expanding the model to include more layers of similarity estimates is clearly a step in the right direction, further work is required to include even more layers.",
        "Using syntactic information and more levels of abstraction (e.g. concepts) are obvious next steps."
      ]
    },
    {
      "heading": "5 Acknowledgements",
      "text": [
        "The first four authors have been partially funded by the PortDial project (Language Resources for Portable Multilingual Spoken Dialog Systems) supported by the EU Seventh Framework Programme (FP7), grant number 296170."
      ]
    },
    {
      "heading": "References",
      "text": [
        "E. Agirre, D. Cer, M. Diab, and A. Gonzalez-Agirre.",
        "2012.",
        "Semeval-2012 task 6: A pilot on semantic textual similarity.",
        "In Proc.",
        "SemEval, pages 385?393.",
        "Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.",
        "2013.",
        "*sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity.",
        "In Proc.",
        "*SEM.",
        "Gnu aspell.",
        "http://www.aspell.net.",
        "D. Ba?r, C. Biemann, I. Gurevych, and T. Zesch.",
        "2012.",
        "Ukp: Computing semantic textual similarity by combining multiple content similarity measures.",
        "In Proc.",
        "SemEval, pages 435?440.",
        "M. Baroni and A. Lenci.",
        "2010.",
        "Distributional memory: A general framework for corpus-based semantics.",
        "Computational Linguistics, 36(4):673?721.",
        "J. Bos and K. Markert.",
        "2005.",
        "Recognising textual entailment with logical inference.",
        "In Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, page 628635.",
        "M. Bradley and P. Lang.",
        "1999.",
        "Affective norms for English words (ANEW): Stimuli, instruction manual and affective ratings.",
        "Technical report C-1.",
        "The Center for Research in Psychophysiology, University of Florida.",
        "Andrei Z. Broder.",
        "1997.",
        "On the resemblance and containment of documents.",
        "In In Compression and Complexity of Sequences (SEQUENCES97, pages 21?29."
      ]
    },
    {
      "heading": "IEEE Computer Society.",
      "text": [
        "A. Budanitsky and G. Hirst.",
        "2006.",
        "Evaluating WordNet-based measures of semantic distance.",
        "Computational Linguistics, 32:13?47.",
        "K. W. Church and P. Hanks.",
        "1990.",
        "Word association norms, mutual information, and lexicography.",
        "Computational Linguistics, 16(1):22?29.",
        "A. Finch, S. Y. Hwang, and E. Sumita.",
        "2005.",
        "Using machine translation evaluation techniques to determine sentence-level semantic equivalence.",
        "In Proceedings of the 3rd International Workshop on Paraphrasing, page 1724.",
        "J. R. Finkel, T. Grenager, and C. D. Manning.",
        "2005.",
        "Incorporating non-local information into information extraction systems by gibbs sampling.",
        "In Proceedings of the 43rd Annual Meeting on Association for Computa"
      ]
    }
  ]
}
