{
  "info": {
    "authors": [
      "Erik F. Tjong Kim Sang"
    ],
    "book": "Applied Natural Language Processing Conference and Meeting of the North American Association for Computational Linguistics",
    "id": "acl-A00-2007",
    "title": "Noun Phrase Recognition by System Combination",
    "url": "https://aclweb.org/anthology/A00-2007",
    "year": 2000
  },
  "references": [
    "acl-E99-1023",
    "acl-J93-2004",
    "acl-P98-1010",
    "acl-P98-1029",
    "acl-P98-1034",
    "acl-P98-1081",
    "acl-W95-0107",
    "acl-W99-0621",
    "acl-W99-0708"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The performance of machine learning algorithms can be improved by combining the output of different systems.",
        "In this paper we apply this idea to the recognition of noun phrases.",
        "We generate different classifiers by using different representations of the data.",
        "By combining the results with voting techniques described in (Van Halteren et al., 1998) we manage to improve the best reported performances on standard data sets for base noun phrases and arbitrary noun phrases."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "(Van Halteren et al., 1998) and (Brill and Wu, 1998) describe a series of successful experiments for improving the performance of part-of-speech taggers.",
        "Their results have been obtained by combining the output of different taggers with system combination techniques such as majority voting.",
        "This approach cancels errors that are made by the minority of the taggers.",
        "With the best voting technique, the combined results decrease the lowest error rate of the component taggers by as much as 19% (Van Halteren et al., 1998).",
        "The fact that combination of classifiers leads to improved performance has been reported in a large body of machine learning work.",
        "We would like to know what improvement combination techniques would cause in noun phrase recognition.",
        "For this purpose, we apply a single memory-based learning technique to data that has been represented in different ways.",
        "We compare various combination techniques on a part of the Penn Treebank and use the best method on standard data sets for base noun phrase recognition and arbitrary noun phrase recognition."
      ]
    },
    {
      "heading": "2 Methods and experiments",
      "text": [
        "In this section we start with a description of our task: recognizing noun phrases.",
        "After this we introduce the different data representations we use and our machine learning algorithms.",
        "We conclude with an outline of techniques for combining classifier results."
      ]
    },
    {
      "heading": "2.1 Task description",
      "text": [
        "Noun phrase recognition can be divided in two tasks: recognizing base noun phrases and recognizing arbitrary noun phrases.",
        "Base noun phrases (baseNPs) are noun phrases which do not contain another noun phrase.",
        "For example, the sentence In [ early trading ] in [ Hong Kong ] [ Monday ] , [ gold ] was quoted at [ $ 366.50 ] [ an ounce ] .",
        "contains six baseNPs (marked as phrases between square brackets).",
        "The phrase $ 366.50 an ounce is a noun phrase as well.",
        "However, it is not a baseNP since it contains two other noun phrases.",
        "Two baseNP data sets have been put forward by (Ramshaw and Marcus, 1995).",
        "The main data set consist of four sections (15-18) of the Wall Street Journal (WSJ) part of the Penn Treebank (Marcus et al., 1993) as training material and one section (20) as test material'.",
        "The baseNPs in this data are slightly different from the ones that can be derived from the Treebank, most notably in the attachment of genitive markers.",
        "The recognition task involving arbitrary noun phrases attempts to find both baseNPs and noun phrases that contain other noun phrases.",
        "A standard data set for this task was put forward at the CoNLL-99 workshop.",
        "It consist on the same parts of the Penn Treebank as the main baseNP data set: WSJ sections 15-18 as training data and section 20 as test data2.",
        "The noun phrases in this data set are the same as in the Treebank and therefore the baseNPs in this data set are slightly different from the ones in the (Ramshaw and Marcus, 1995) data sets.",
        "In both tasks, performance is measured with three scores.",
        "First, with the percentage of detected noun phrases that are correct (precision).",
        "Second, with the percentage of noun phrases in the data that were found by the classifier (recall).",
        "And third,",
        "with the Fo-,i rate which is equal to (2*preci-sion*recall)/(precision+recall).",
        "The latter rate has been used as the target for optimization."
      ]
    },
    {
      "heading": "2.2 Data representation",
      "text": [
        "In our example sentence in section 2.1, noun phrases are represented by bracket structures.",
        "Both (Munoz et al., 1999) and (Tjong Kim Sang and Veenstra, 1999) have shown how classifiers can process bracket structures.",
        "One classifier can be trained to recognize open brackets (0) while another will process close brackets (C).",
        "Their results can be converted to baseNPs by making pairs of open and close brackets with large probability scores (Munoz et al., 1999) or by regarding only the shortest phrases between open and close brackets as baseNPs (Tjong Kim Sang and Veenstra, 1999).",
        "We have used the bracket representation (0+C) in combination with the second baseNP construction method.",
        "An alternative representation for baseNPs has been put forward by (Ramshaw and Marcus, 1995).",
        "They have defined baseNP recognition as a tagging task: words can be inside a baseNP (I) or outside of baseNPs (0).",
        "In the case that one baseNP immediately follows another baseNP, the first word in the second baseNP receives tag B.",
        "Example: In0 early/ trading/ ino Hong/ Kong/ MondayB ,0 gold' wasp quotedÂ° at0 $r 366.50/ anB ounce/ .o This set of three tags is sufficient for encoding baseNP structures since these structures are non-recursive and nonoverlapping.",
        "(Tjong Kim Sang and Veenstra, 1999) have presented three variants of this tagging representation.",
        "First, the B tag can be used for the first word of every noun phrase (I0B2 representation).",
        "Second, instead of the B tag an E tag can be used to mark the last word of a baseNP immediately before another baseNP (IOE1).",
        "And third, the E tag can be used for every noun phrase final word (10E2).",
        "They have used the (Ramshaw and Marcus, 1995) representation as well (IOB1).",
        "We will use these four tagging representations as well as the 0+C representation."
      ]
    },
    {
      "heading": "2.3 Machine learning algorithms",
      "text": [
        "We have used the memory-based learning algorithm IB1-IG which is part of TiMBL package (Daelemans et al., 1999b).",
        "In memory-based learning the training data is stored and a new item is classified by the most frequent classification among training items which are closest to this new item.",
        "Data items are represented as sets of feature-value pairs.",
        "In IB1-IG each feature receives a weight which is based on the amount of information which it provides for computing the classification of the items in the training data.",
        "These feature weights are used for computing the distance between a pair of data items (Daelemans et al., 1999b).",
        "IB1-IG has been used successfully on a large variety of natural language processing tasks.",
        "Beside 1131-1G, we have used IGTREE in the combination experiments.",
        "IGTREE is a decision tree variant of IB1-IG (Daelemans et al., 1999b).",
        "It uses the same feature weight method as IB1-IG.",
        "Data items are stored in a tree with the most important features close to the root node.",
        "A new item is classified by traveling down from the root node until a leaf node is reached or no branch is available for the current feature value.",
        "The most frequent classification of the current node will be chosen."
      ]
    },
    {
      "heading": "2.4 Combination techniques",
      "text": [
        "Our experiments will result in different classifications of the data and we need to find out how to combine these.",
        "For this purpose we have evaluated different voting mechanisms, effectively the voting methods as described in (Van Halteren et al., 1998).",
        "All combination methods assign some weight to the results of the individual classifier.",
        "For each input token, they pick the classification score with the highest total score.",
        "For example, if five classifiers have weights 0.9, 0.4, 0.8, 0.6 and 0.6 respectively and they classify some token as npstart, null, npstart, null and null, then the combination method will pick npstart since it has a higher total score (1.7) than null (1.6).",
        "The values of the weights are usually estimated by processing a part of the training data, the tuning data, which has been kept separate as training data for the combination process.",
        "In the first voting method, each of the five classifiers receives the same weight (majority).",
        "The second method regards as the weight of each individual classification algorithm its accuracy on the tuning data (TotPrecision).",
        "The third voting method computes the precision of each assigned tag per classifier and uses this value as a weight for the classifier in those cases that it chooses the tag (TagPrecision).",
        "The fourth method uses the tag precision weights as well but it subtracts from them the recall values of the competing classifier results.",
        "Finally, the fifth method uses not only a weight for the current classification but it also computes weights for other possible classifications.",
        "The other classifications are determined by examining the tuning data and registering the correct values for every pair of classifier results (pair-wise voting).",
        "Apart from these five voting methods we have also processed the output streams with two classifiers: 1B 1-1G (memory-based) and IGTREE (decision tree).",
        "This approach is called classifier stacking.",
        "Like (Van Halteren et al., 1998), we have used different input versions: one containing only the classifier output and another containing both classifier output and a compressed representation of the classifier input.",
        "five classifiers applied to the baseNP training data after conversion to the open bracket (0) and the close bracket representation (C).",
        "For the latter purpose we have used the part-of-speech tag of the current word."
      ]
    },
    {
      "heading": "3 Results",
      "text": [
        "Our first goal was to find out whether system combination could improve performance of baseNP recognition and, if this was the fact, to select the best combination technique.",
        "For this purpose we performed a 10-fold cross validation experiment on the baseNP training data, sections 15-18 of the WSJ part of the Penn Treebank (211727 tokens).",
        "Like the data used by (Ramshaw and Marcus, 1995), this data was retagged by the Brill tagger in order to obtain realistic part-of-speech (POS) tags3.",
        "The data was segmented into baseNP parts and non-baseNP parts in a similar fashion as the data used by (Ramshaw and Marcus, 1995).",
        "The data was converted to the five data representations (IOB1, I0B2, IOE1, 10E2 and 0+C) and IB 1 ia was used to classify it by using 10-fold cross validation.",
        "This means that the data was divided in ten consecutive parts of about the same size after which each part was used as test data with the other nine parts as training data.",
        "The standard parameters of is 1-ic have been used except for k, the number of examined nearest neighbors, which was set to three.",
        "Each word in the data was represented by itself and its POS tag and additionally a left and right context of four word-POS tag pairs.",
        "For the first four representations, we have used a second processing stage as well.",
        "In this stage, a word was represented by itself, its POS tag, a left and right context of three word-P OS tag pairs and a left and right context of two classification results of the first processing stage (see figure 1).",
        "The second processing stage improved the Fp-1 scores with almost 0.7 on average.",
        "The classifications of the IOB1, I0B2, IOE1 and 10E2 representations were converted to the open bracket (0) and close bracket (C) representations.",
        "baseNP training data (211727 tokens).",
        "Each combination performs significantly better than any of the five individual classifiers listed under Representation.",
        "The performance differences between the combination methods are not significant.",
        "After this conversion step we had five 0 results and five C results.",
        "In the bracket representations, tokens can be classified as either being the first token of an NP (or the last in the C representation) or not.",
        "The results obtained with these representations have been measured with accuracy rates: the percentage of tokens that were classified correctly.",
        "Only about one in four tokens are at a baseNP boundary so guessing that a text does not contains baseNPs will already give us an accuracy of 75%.",
        "Therefore the accuracy rates obtained with these representations are high and the room for improvement is small (see table 1).",
        "However, because of the different treatment of neighboring chunks, the five classifiers disagree in about 2.5% of the classifications.",
        "It seems useful to use combination methods for finding the best classification for those ambiguous cases.",
        "The five 0 results and the five C results were processed by the combination techniques described in section 2.4.",
        "The accuracies per input token for the combinations can be found in table 2.",
        "For both data representations, all combinations perform significantly better than the best individual classifier (p<0.001 according to a x2 test)4.",
        "Unlike in (Van",
        "CoNLL-99 workshop (82.98 (CoNLL-99, 1999), an error reduction of 5%)."
      ]
    },
    {
      "heading": "4 Related work",
      "text": [
        "(Abney, 1991) has proposed to approach parsing by starting with finding correlated chunks of words.",
        "The chunks can be combined to trees by a second processing stage, the attacher.",
        "(Ramshaw and Marcus, 1995) have build a chunker by applying transformation-based learning to sections of the Penn Treebank.",
        "Rather than working with bracket structures, they have represented the chunking task as a tagging problem.",
        "POS-like tags were used to account for the fact that words were inside or outside chunks.",
        "They have applied their method to two segments of the Penn Treebank and these are still being used as benchmark data sets.",
        "Several groups have continued working with the Ramshaw and Marcus data sets for base noun phrases.",
        "(Argamon et al., 1998) use Memory-Based Sequence Learning for recognizing both NP chunks and VP chunks.",
        "This method records POS tag sequences which contain chunk boundaries and uses these sequences to classify the test data.",
        "Its performance is somewhat worse than that of Ramshaw and Marcus (F0=1=91.6 vs. 92.0) but it is the best result obtained without using lexical informations.",
        "(Cardie and Pierce, 1998) store POS tag sequences that make up complete chunks and use these sequences as rules for classifying unseen data.",
        "This approach performs worse than the method of Argamon et al.",
        "(F0=1=90.9).",
        "Three papers mention having used the memory-based learning method u31-1G.",
        "(Veenstra, 1998) introduced cascaded chunking, a two-stage process in which the first stage classifications are used to improve the performance in a second processing stage.",
        "This approach reaches the same performance level as Argamon et al.",
        "but it requires lexical information.",
        "(Daelemans et al., 1999a) report a good performance for baseNP recognition but they use a different data set and do not mention precision and recall rates.",
        "(Tjong Kim Sang and Veenstra, 1999) compare different data representations for this task.",
        "Their baseNP results are slightly better than those of Ramshaw and Marcus (F0=1=92.37).",
        "(XTAG, 1998) describes a baseNP chunker built from training data by a technique called supertag-ging.",
        "The performance of the chunker was an improvement of the Ramshaw and Marcus results (F0,_1=92.4).",
        "(Munoz et al., 1999) use SNoW, a network of linear units, for recognizing baseNP phrases 6We have applied majority voting of five data representations to the Ramshaw and Marcus data set without using lexical information and the results were: accuracy 0: 97.60%, accuracy C: 98.10%, precision: 92.19%, recall: 91.53% and F0=1: 91.86. and SV phrases.",
        "They compare two data representations and report that a representation with bracket structures outperforms the IOB tagging representation introduced by (Ramshaw and Marcus, 1995).",
        "SNoW reaches the best performance on this task (F0=1=92.8).",
        "There has been less work on identifying general noun phrases than on recognizing baseNPs.",
        "(Osborne, 1999) extended a definite clause grammar with rules induced by a learner that was based upon the maximum description length principle.",
        "He processed other parts of the Penn Treebank than we with an Fo=1 rate of about 60.",
        "Our earlier effort to process the CoNLL data set was performed in the same way as described in this paper but without using the combination method for baseNPs.",
        "We obtained an Fo=1 rate of 82.98 (CoNLL-99, 1999)."
      ]
    },
    {
      "heading": "5 Concluding remarks",
      "text": [
        "We have put forward a method for recognizing noun phrases by combining the results of a memory-based classifier applied to different representations of the data.",
        "We have examined different combination techniques and each of them performed significantly better than the best individual classifier.",
        "We have chosen to work with majority voting because it does not require tuning data and thus enables the individual classifiers to use all the training data.",
        "This approach was applied to three standard data sets for base noun phrase recognition and arbitrary noun phrase recognition.",
        "For all data sets majority voting improved the best result for that data set known to us.",
        "Varying data representations is not the only way for generating different classifiers for combination purposes.",
        "We have also tried dividing the training data in partitions (bagging) and working with artificial training data generated by a crossover-like operator borrowed from genetic algorithm theory.",
        "With our memory-based classifier applied to this data, we have been unable to generate a combination which improved the performance of its best member.",
        "Another approach would be to use different classification algorithms and combine the results.",
        "We are working on this but we are still to overcome the practical problems which prevent us from obtaining acceptable results with the other learning algorithms."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We would like to thank the members of the CNTS group in Antwerp, Belgium, the members of the ILK group in Tilburg, The Netherlands and three anonymous reviewers for valuable discussions and comments.",
        "This research was funded by the European TMR network Learning Computational Grammars'."
      ]
    }
  ]
}
