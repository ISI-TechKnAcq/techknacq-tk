{
  "info": {
    "authors": [
      "Roberto Navigli"
    ],
    "book": "Conference of the European Association for Computational Linguistics",
    "id": "acl-E06-1017",
    "title": "Experiments on the Validation of Sense Annotations Assisted by Lexical Chains",
    "url": "https://aclweb.org/anthology/E06-1017",
    "year": 2006
  },
  "references": [
    "acl-C04-1053",
    "acl-C96-1005",
    "acl-H93-1061",
    "acl-J91-1002",
    "acl-W02-0817",
    "acl-W04-0804",
    "acl-W04-0811",
    "acl-W04-0827",
    "acl-W04-0838",
    "acl-W04-0864"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "It is widely recognized that the annotation of texts with senses from a computational lexicon is a complex and often subjective task.",
        "We propose the use of lexical chains, specifically semantic interconnections, to support validators in the difficult task of assessing the quality of sense assignments.",
        "We provide a twofold experimental evaluation of our approach applied to the validation of manual annotations from the SemCor corpus, and we further assess the method on automatic annotations from the English all-words Sense-val 3 competition.",
        "Introduction Sense annotation is the task of assigning senses chosen from a computational lexicon to words in context.",
        "This is a task where both machines and humans find it difficult to reach an agreement.",
        "The problem depends on a variety of factors, ranging from the inherent subjectivity of the task to the granularity of sense discretization, coverage of the reference dictionary, etc.",
        "The problem of validation is even amplified when sense tags are collected through acquisition interfaces like the Open Mind Word Expert (Chklovski and Mihalcea, 2002), due to the unknown source of the contributions of possibly unskilled volunteers.",
        "Strategies like voting for automatic sense annotations and the use of inter-annotator agreement with adjudication for human sense assignments only partially solve the issue of disagreement.",
        "Especially when there is no clear preference towards a certain word sense, the final choice made by a judge can be subjective, if not arbitrary.",
        "This is a case where analysing the intrinsic structure of the reference lexicon is essential for producing a consistent decision.",
        "A lexicographer is indeed expected to review a number of related dictionary entries in order to adjudicate a sense coherently.",
        "This work can be tedious, time-consuming, and often incomplete, due to the complex structure of the resource.",
        "As a result, inconsistent choices can be made.",
        "In this paper, we present and evaluate a knowledge-based method for assisting the validation of both manual and automatic sense annotations.",
        "The paper is organized as follows: first, we introduce lexical chains and semantic interconnections (Section 1), we illustrate our method for the validation of sense annotations (Section 2), and we evaluate the approach applied to both manual and automatic annotations (Section 3).",
        "Finally, in Section 4 we present some conclusions."
      ]
    },
    {
      "heading": "1 Lexical Chains and Semantic Interconnections",
      "text": [
        "Semantic networks are a graphical notation developed to represent knowledge explicitly as a set of conceptual entities and their interrelationships.",
        "The availability of wide-coverage computational lexicons like WordNet (Fellbaum, 1998), as well as semantically annotated corpora like SemCor (Miller et al., 1993), has certainly contributed to the exploration and exploitation of semantic graphs for several tasks like the analysis of the lexical text cohesion (Morris and Hirst, 1991), word sense disambiguation (Agirre and Rigau, 1996; Mihalcea and Moldovan, 2001), ontology learning, etc.",
        "Lexical chains (Morris and Hirst, 1991), inspired by the notion of cohesion in discourse, are sequences of words w\\,..., wn in a text that represent the same topic, i.e. such that Wi is related to Wi+i by a lexico-semantic relation (e.g. hypernymy, meronymy, etc.).",
        "Subsequent works (e.g. Mihalcea and Moldovan (2001)) further develop this idea by providing knowledge-based approaches to Word Sense Disambiguation.",
        "Given a word context a = w\\,W2, ■ ■ ■ ,wn and a lexical knowledge base, these approaches tend to select those configurations of senses swi, sW2, sWnthat maximize the degree of mutual interconnection, according to a measure of connectivity, that IS Sw – arg ~m&X-Sw£Senses(w) f(sw,a), where / is a function of the lexical chains connecting sw to the senses chosen for a.",
        "Recently, a knowledge-based algorithm for Word Sense Disambiguation, called Structural Semantic Interconnections (SSI) (Navigli and Ve-lardi, 2005), has been shown to provide interesting insights into the choice of word senses by providing structural justifications in terms of a specific kind of lexical chains, called semantic interconnections.",
        "A semantic interconnection pattern is a relevant sequence of edges selected according to a context-free grammar, i.e. a path connecting a pair of word senses (dark nodes in Figure 1), possibly including a number of intermediate concepts (light nodes in Figure 1).",
        "The SSI algorithm exploits a lexical knowledge base, based on the WordNet lexicon and enriched with a number of relatedness relations, connecting pairs of related word senses.",
        "The enrichment is based on the acquisition of collocations from existing resources (like the Oxford Collocations, the Longman Language Activator, collocation web sites, etc.).",
        "Each collocation is mapped to the WordNet sense inventory in a semi-automatic manner (Navigli, 2005) and transformed into a relatedness edge.",
        "We choose the connectivity function / as the normalized sum of the inverse length of interconnections (i.e. the contribution of a single connection sw ^* sw, is length{8l^8vjl)) between swand the other senses chosen in context.",
        "Given the sense configuration swi, sW2,..., sWn that maximizes the degree of mutual interconnection, word w G a is assigned the word sense sw if the normalized sum of the contributions coming from the other senses sw> (w G a, w' ^ w) is over a fixed threshold.",
        "For example, if the context of words to be disambiguated is [ cross-v, street-n, intersection-n ], the senses chosen by SSI with respect to WordNet are: [ cross-v#l, street#2, intersection^ ], supported - among the others - by the pattern intersection#2 -> road#l <- thoroughfare#l <-street#2.",
        "An excerpt of the manually written context-free grammar encoding valid semantic interconnection patterns for the WordNet lexicon is reported in Table 1.",
        "The grammar allows to avoid the recognition of unwanted patterns causing a deep shift of meaning (e.g. universe#l > natural ob- jectffl -> objectffl -> commodity^!",
        "-> merchandiser1, or jobttl -> moneyffJ -> coinffl -> metal#l).",
        "For further details the reader can refer to the literature (e.g. Navigli and Velardi (2005)).",
        "In this paper, we aim at showing that knowledge-based, untrained WSD algorithms founded on the concept of lexical chains, and specifically on semantic interconnections, can help speed up the task of validating sense annotations.",
        "As illustrated in the following, semantic interconnections are an important requirement for this purpose in that the outcome of the algorithm applied to a sentence a can be visualized in terms of semantic graphs representing the patterns connecting the suggested senses."
      ]
    },
    {
      "heading": "2 Supporting Validation with Semantic Interconnection Patterns",
      "text": [
        "The task of validating sense annotations can be defined as follows: let w be a word in a sentence o, previously tagged by a set of annotators A – {ai, a2,an} each providing a sense for w, and let S = {s±, S2,sm} C Senses(w) be the set of senses chosen for w by the annotators in A, where Senses(w) is the subset of senses of w in the reference inventory (we adopt WordNet).",
        "A validator is asked to validate, that is to adjudicate a sense s E Senses(w) for a word w over the others.",
        "Notice that the annotators in A can be either human or automatic, depending upon the purpose of the exercise.",
        "Given a set of words with disagreement W C a, we apply SSI to W by taking into account for disambiguation only the senses in S (i.e. the set of senses selected by the annotators), and using as a fixed context the agreed senses chosen for the words in a \\ W. In the following subsections, we describe the application of our method to the validation of manual and automatic annotations, and we discuss cases of uncertain applicability.",
        "Consider the following sentence: (a) We crossed the street near the intersection All the occurrences of the phrase cross the street in the SemCor corpus are tagged with the first sense of street (defined in WordNet as a thoroughfare (usually including sidewalks) that is lined with buildings), but it is clear, from the definition of the second sense (the part of a thoroughfare between the sidewalks; the part of the thoroughfare on which vehicles travel; \"be careful crossing the street\"), that a pedestrian crosses that part of the thoroughfare between the sidewalks.",
        "Though questionable, this is a subtlety made explicit in the dictionary and reinforced by the usage example of sense #2 above.",
        "Suppose two annotators agreed on the senses of cross and intersection, but disagreed on the word street, choosing respectively the first and the second sense from the WordNet inventory.",
        "The application of the SSI algorithm to sentence (a) leads to the suggestion of the second sense as a solution to this disagreement.",
        "This suggestion is supported by a number of semantic interconnections according to the grammar in Table 1.",
        "Figure 1(a) shows some interconnections suggested by the algorithm.",
        "Semantic interconnections reflect the fine granularity of the inventory, as they are expressions of the lexical knowledge base from which they are extracted.",
        "In fact, the choice of streetj^l still produces good semantic interconnections, as illustrated in Figure 1(b), but the overall ranking of this sense selection, i.e. the degree of overall connectivity of the resulting graph, is smaller than that obtained for sireet#2.",
        "As a second example, consider the WordNet definition of motorcycle: (b) Motorcycle: a motor vehicle with two wheels and a strong frame In the Gloss Word Sense Disambiguation task at Senseval-3 (Litkowski, 2004), the human annotators assigned the first sense to the word frame (a structure supporting or containing something), unintentionally neglecting that the dictionary encodes a specific sense of frame concerning the structure of objects (e.g. vehicles, buildings, etc.).",
        "In fact, according to WordNet, a chassis#3 is a kind of frame#6 (the internal supporting structure that gives an artifact its shape), and is also part of a motor vehicle#l.",
        "While regular polysemy holds between sense #1 and #6, there is no justification for the former choice, as it does not refer to vehicles at all (as reflected by the lack of semantic interconnection patterns concerning/rarae#i).",
        "From these two real-world cases, it is evident that semantic interconnections can point at inconsistent, though acceptable, choices made by human annotators due, among others, to the line granularity of the sense inventory and to regular polysemy.",
        "Apart from tagging mistakes, most of the cases of disagreement between manual annotators is due to the fine granularity of the lexicon inventory.",
        "We recognize that subtle distinctions, like those encoded in WordNet, are rarely useful in any NLP application, but as a matter of fact WordNet is at the moment the de facto standard within the research community, as no other computational lexicon of that size and complexity is freely available.",
        "While the task of manual annotation is mostly restricted to lexicographers, the automatic annotation of texts (especially, web pages) is gaining a growing popularity in the Semantic Web vision (Berners-Lee, 1999).",
        "In order to perform automatic tagging, one or more word sense disambiguation systems are applied, resulting in a semantically-enhanced resource.",
        "Unfortunately, even when dealing with restricted sense inventories or selected domains, automated systems can make mistakes in the sense assignment, also due to the difficulty in training a supervised program with a sufficient number of annotated instances and again due to the fine granularity of the dictionary inventory.",
        "There are also cases in which an automatic dis-ambiguator chooses a partially justifiable, but incorrect interpretation for words in context.",
        "Consider for instance the sentence from the Senseval-3 English all-words competition: (c) The driver stopped swearing at them, turned on his heel and went back to his truck A partial interpretation of driver and heel can be provided in the golf domain (a heel#6 is part of a driver#5).",
        "This can be a reasonable choice for a word sense disambiguator, but the overall semantic graph exposes a poor structural quality.",
        "A different choice of senses pointed out by stronger semantic interconnections (driver as an operator of a vehicle and heel as the back part of the foot) provides a more interconnected structure (among others, driver'#1 -> motor vehicle#l {%nd°^ truck#l, turn-related – to , , , .",
        "v#l -> heel#2, etc.).",
        "It can happen that semantic interconnection patterns convey weak suggestions due to the lack of structure in the lexical knowledge base used to extract patterns like those in Table 1.",
        "In that case, the validator is expected to reject the possible suggestion and make a more reasonable choice.",
        "As a result, if no interesting suggestion is provided to the validator, it is less likely that the final choice will be inconsistent with the lexicon structure.",
        "A typical example is: (d) A payment was made last week.",
        "WordNet encodes two senses of payment: the sum of money paid (sense #1) and the act of paying money (sense #2).",
        "Such regular polysemy makes it hard to converge on a sense choice for payment in sentence (d).",
        "This difficulty is also manifested in the annotations of similar expressions involving make and payment within Sem-Cor.",
        "Furthermore, apart from the distinction between the act of doing the action and the amount of money paid, there are not many structural suggestions allowing to distinguish between the two senses.",
        "Semantic interconnections cannot help the validator here, but any choice will not violate the structural consistency of the lexicon."
      ]
    },
    {
      "heading": "3 Evaluation",
      "text": [
        "The objective of this section is to show that semantic interconnections constitute a good support for a validator in the detection of bad or inconsistent annotations.",
        "We assessed the method for both manual (Section 3.1) and automatic annotations (Section 3.2).",
        "In Section 3.3 we discuss the experiments.",
        "The evaluations are all based on standard test sets.",
        "We made two experiments for assessing the suggestions provided by SSI for validating manual annotations, both based on the semantically-tagged SemCor corpus (Miller et al., 1993).",
        "In a first experiment, 1,000 sentences were uniformly selected from the set of documents in the SemCor corpus.",
        "For each sentence a = w\\W2 ■ ■ • wn annotated in SemCor with the senses swlsW2...sWn (sWi £ Senses(wl),i £ {1,2,..., n}), we randomly identified a word Wi 6 a, and chose at random a different sense sWifor that word, that is sWi G Senses(wi) \\ {sWi}.",
        "In other words, we simulated in vitro a situation in which an annotator provides an appropriate sense and the other selects a different sense.",
        "The random factor guarantees the uniform distribution over the test set of all the possible degrees of disagreement between sense annotators (ranging from regular polysemy to homonymy).",
        "We applied SSI to the annotated sentences and evaluated the performance of the approach in suggesting the appropriate choice for the words with disagreement.",
        "We assessed both precision (the number of correct suggestions over the overall number of suggestions from SSI) and recall (the number of correct suggestions over the total number of words to be validated).",
        "The results are reported in Table 2 for nouns, adjectives, and verbs (we neglected adverbs as very few interconnections can be found for them).",
        "The experiment shows that evidences of inconsistency due to inappropriate annotations are provided with good precision (we fix the baseline as the chance, that is we have 50% of probability to provide the appropriate sense for each word).",
        "The overall Fl measure (calculated as |jq^r) is 59.18%.",
        "The improvement in precision over the baseline is statistically significant (p < 0.01).",
        "Notice that this test bed differs from the typical evaluation of Word Sense Disambiguation tasks, like the Senseval exercises, in that we are assessing words w whose sense inventory is restricted to the set of senses {sw,sw}.",
        "The low recall obtained for verbs, but especially for adjectives, is due to a lack of connectivity in the lexical knowledge base, when dealing with connections across different parts of speech.",
        "Therefore, we repeated the experiment with a number of variations in the initial test set, by simulating for each sentence a disagreement on:",
        "(a) the two most ambiguous words; (f3) the two least ambiguous words; (7) two words chosen at random; (8) three words chosen at random.",
        "The results, shown in Table 3, provide interesting insights.",
        "First, validating sense annotations of highly polysemous words is more advantageous.",
        "In fact, in the case (a) the two senses selected for each word convey meanings which are more likely to be distant than in low-polysemy words (case (/?)).",
        "As expected, the random choice strategy (7) provides intermediate results between (a) and 1/3).",
        "The second interesting remark is that when applying SSI to sentences with a number of disagreements per sentence greater than 1, the precision slightly decreases, and the recall increases, but both measures do not vary significantly (compare the figures in Table 2 with the results for (7) and {8) in Table 3).",
        "Finally, we studied how the sentence size affects the results of case (7).",
        "Figure 2 reports the results (similar figures can be obtained for the other cases).",
        "The figure highlights better, stable performances for a sentence size ranging between 8 and 12, which includes most of the sentences in our test set (754 sentences out of 1,000).",
        "In a second experiment, we applied semantic interconnections in vivo to a set of sentences that were used as a test set in the context of the MultiSemCor project, an English/Italian parallel version of the SemCor corpus (Bentivogli et al., 2004).",
        "In producing the test set, the lexicographers discovered a number of errors in the original SemCor annotations.",
        "Exploiting such divergences, we assessed the quality of the validation suggestions provided by SSI for each word with disagreement between the original SemCor annotations and those provided by the lexicographers working on MultiSemCor, assuming that the latters are the appropriate ones.",
        "The original test set consisted of 119 words, 14 of which excluded because of a reconciliation of the disagreements due to the conversion of the data from WordNet 1.6 and 2.0.",
        "The test set consisted therefore of 105 words, contained in a total of 82 sentences from 4 SemCor annotated texts.",
        "We report the results in Table 4 (the test set includes two adverbs).",
        "These figures are worse than those of the first experiment.",
        "This is due to the fact that in this case semantic interconnections have a minor impact, because the distinctions between the annotations from the original SemCor and those of MultiSemCor are often very subtle.",
        "Furthermore, the sample size is too small to provide a meaningful assessment.",
        "Our approach is still useful in a case like this, where a manual, visual inspection of the semantic interconnections can help the validator in accepting or discarding the suggestions provided, thus guaranteeing consistency with respect to the reference lexicon.",
        "For assessing semantic interconnections applied to the validation of automatic annotations, we chose the Senseval-3 corpus for the English all-words task (Snyder and Palmer, 2004).",
        "The task required WSD systems to provide a sense choice for a total of 2,081 content words in a set of 301 sentences from the fiction, news story, and editorial domains.",
        "For our experiments, we focused on the outcome of the three best-ranking systems - GAMBL (Decadt et al., 2004), SenseLearner (Mihalcea and Faruque, 2004), and Koc University (Yuret, 2004) - and selected the subset of the sentences including one or more words with disagreement between the systems and such that at least one system made the appropriate sense choice according to the manual tagging provided by the organizers.",
        "We excluded from our test set any word included in our extended stopwords (e.g., such, something, etc.",
        "), resulting in a final figure of 411 disagreed words in 197 sentences.",
        "The application of SSI to this test set led to the figures in Table 5 (the overall Fl measure was 52.51%).",
        "In the table, we compare our results with the chance baseline (calculated by taking into account the number of distinct answers given for each sentence by the three systems), the first sense heuristic (i.e. the choice of the most frequent sense in SemCor), and the best-performing Senseval system.",
        "The precision improvement over",
        "the baseline is statistically significant (p < 0.01).",
        "Although the improvements of 6.5% and 5.04%, respectively, over the first sense and the best Sen-seval system are not statistically significant, we remark that we are comparing our method with the best supervised approaches.",
        "Furthermore, the (even smaller) difference in performance between the first sense and the best senseval system is not statistically significant as well.",
        "An additional parameter to evaluate in this second experiment was the number of disagreements per sentence.",
        "The distribution of the 411 disagreed words over the sentences is illustrated in Figure 3.",
        "We noticed that several small-size sentences in the test set are very difficult to be disambiguated even for humans.",
        "Examples of such sentences are (content words in italic): that's what the man said; was the man drunk or crazy or both?",
        "; I'm just numb, etc.",
        "This observation led us to study the correctness of the suggestions provided by structural semantic interconnections for sentences whose size was over a fixed threshold.",
        "The results are shown in Table 6, when the threshold for the sentence size \\a\\ ranges between 3 and 7.",
        "Many lexicographic studies established that human annotators cannot distinguish well between too fine-grained senses (e.g. Edmonds and Kil-gariff (2002)).",
        "The ceiling of about 80% inter-annotator agreement (at least for English) was confirmed also in preparing the latest Senseval exercises.",
        "Assuming an average sentence size \\a\\ = 10 (a figure consistent with our experiments), we can reasonably suppose an average disagreement on two words in a, a case similar to (7), where our approach largely beats the baseline.",
        "Although we evaluated our method on all open-class parts of speech with the exclusion of adverbs, we remark that nouns are by far the most frequent case, like in the SemCor corpus (in our random selection of words, they occurred more than half of the times), or the most relevant instances (e.g. when typed as queries to be matched against pages previously tagged with automatic semantic annotations).",
        "Finally, the overall precision beats the baseline by many points in all the experiments, while the difference in recall is not statistically significant.",
        "This is a major feature of our approach, enabling precise justifications for sense choices in terms of semantic graphs from which the human validator can benefit in order to take the final decision."
      ]
    },
    {
      "heading": "4 Conclusions",
      "text": [
        "In this paper we discussed the use of semantic interconnections to support validators in the difficult task of assessing the quality of both manual and automatic sense assignments.",
        "The use of semantic interconnection patterns to support validation allows to smooth possible divergences between the annotators and to corroborate choices consistent with the lexical knowledge base.",
        "Furthermore, the method is independent of the adopted lexicon (i.e. WordNet), in that patterns can be derived from any sufficiently rich ontological resource.",
        "An interesting point in favour of our approach is that the validator can visually analyse the correctness of a sense choice in terms of its semantic interconnections with respect to the other word senses chosen in context.",
        "The method has been implemented as a visual tool available online, called Valido.",
        "The tool takes as input a corpus of documents whose sentences are tagged by one or more annotators with word senses from the Word-Net inventory.",
        "The user can browse the sentences, and adjudicate a choice over the others in case of disagreement among the annotators.",
        "The tool could be used in the future to collect new, consistent collocations that could grow the lexical knowledge base from which the semantic interconnection patterns are extracted, possibly in an iterative process.",
        "We are investigating this topic in an ongoing work.",
        "Moreover, the approach allows the validator to discover mistakes in the lexicon: for instance, the semantic graphs analysed in a number of experiments helped us find out that a Swiss canton#l is not a Chinese city (cantontfl) but a division of a country (canton#2), that a male horse should be a kind of horse, and so on.",
        "These inconsistencies of WordNet 2.0 were promptly reported to the resource maintainers, and most of them have been corrected in the latest version of the lexicon.",
        "Finally, we would like to point out the fact that, in the future, semantic interconnections could also be used during the annotation phase by taggers looking for suggestions based on the structure of the lexical knowledge base, with the result of improving the coherence and awareness in the decisions to be taken."
      ]
    },
    {
      "heading": "Acknolwedgements",
      "text": []
    }
  ]
}
