{
  "info": {
    "authors": [
      "Jun'ichi Kazama",
      "Stijn De Saeger",
      "Kow Kuroda",
      "Masaki Murata",
      "Kentaro Torisawa"
    ],
    "book": "ACL",
    "id": "acl-P10-1026",
    "title": "A Bayesian Method for Robust Estimation of Distributional Similarities",
    "url": "https://aclweb.org/anthology/P10-1026",
    "year": 2010
  },
  "references": [
    "acl-D09-1098",
    "acl-I08-1025",
    "acl-P06-1124",
    "acl-P08-1047",
    "acl-P09-1012",
    "acl-P90-1034",
    "acl-P94-1038",
    "acl-P97-1008",
    "acl-P98-2127"
  ],
  "sections": [
    {
      "text": [
        "Jun'ichi Kazama Stijn De Saeger Kow Kuroda Masaki Muratat Kentaro Torisawa",
        "Existing word similarity measures are not robust to data sparseness since they rely only on the point estimation of words' context profiles obtained from a limited amount of data.",
        "This paper proposes a Bayesian method for robust distributional word similarities.",
        "The method uses a distribution of context profiles obtained by Bayesian estimation and takes the expectation of a base similarity measure under that distribution.",
        "When the context profiles are multinomial distributions, the priors are Dirichlet, and the base measure is the Bhattacharyya coefficient, we can derive an analytical form that allows efficient calculation.",
        "For the task of word similarity estimation using a large amount of Web data in Japanese, we show that the proposed measure gives better accuracies than other well-known similarity measures."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "The semantic similarity of words is a longstanding topic in computational linguistics because it is theoretically intriguing and has many applications in the field.",
        "Many researchers have conducted studies based on the distributional hypothesis (Harris, 1954), which states that words that occur in the same contexts tend to have similar meanings.",
        "A number of semantic similarity measures have been proposed based on this hypothesis (Hindle, 1990; Grefenstette, 1994; Dagan et al., 1994; Dagan et al., 1995; Lin, 1998; Dagan et al., 1999).",
        "* The work was done while the author was at NICT.",
        "In general, most semantic similarity measures have the following form:",
        "where v(wi) is a vector that represents the contexts in which wi appears, which we call a context profile of wi.",
        "The function g is a function on these context profiles that is expected to produce good similarities.",
        "Each dimension of the vector corresponds to a context, fk, which is typically a neighboring word or a word having dependency relations with wi in a corpus.",
        "Its value, vk(wi), is typically a co-occurrence frequency c(wi, fk), a conditional probability p(fk|wi), or point-wise mutual information (PMI) between wi and fk, which are all calculated from a corpus.",
        "For g, various works have used the cosine, the Jaccard coefficient, or the Jensen-Shannon divergence is utilized, to name only a few measures.",
        "Previous studies have focused on how to devise good contexts and a good function g for semantic similarities.",
        "On the other hand, our approach in this paper is to estimate context profiles (v(wi)) robustly and thus to estimate the similarity robustly.",
        "The problem here is that v(wi) is computed from a corpus of limited size, and thus inevitably contains uncertainty and sparseness.",
        "The guiding intuition behind our method is as follows.",
        "All other things being equal, the similarity with a more frequent word should be larger, since it would be more reliable.",
        "For example, if p(fk|wi) and p(fk lw2) for two given words w1 and w2 are equal, but wi is more frequent, we would expect that sim(w0,w1) > sim(w0,w2).",
        "In the NLP field, data sparseness has been recognized as a serious problem and tackled in the context of language modeling and supervised machine learning.",
        "However, to our knowledge, there has been no study that seriously dealt with data sparseness in the context of semantic similarity calculation.",
        "The data sparseness problem is usually solved by smoothing, regularization, margin maximization and so on (Chen and Goodman, 1998; Chen and Rosenfeld, 2000; Cortes and Vap-nik, 1995).",
        "Recently, the Bayesian approach has emerged and achieved promising results with a clearer formulation (Teh, 2006; Mochihashi et al., 2009).",
        "In this paper, we apply the Bayesian framework to the calculation of distributional similarity.",
        "The method is straightforward: Instead of using the point estimation of v(wi), we first estimate the distribution of the context profile, p(v(wi)), by Bayesian estimation and then take the expectation of the original similarity under this distribution as follows:",
        "The uncertainty due to data sparseness is represented by p(v(wi)), and taking the expectation enables us to take this into account.",
        "The Bayesian estimation usually gives diverging distributions for infrequent observations and thus decreases the expectation value as expected.",
        "The Bayesian estimation and the expectation calculation in Eq.",
        "2 are generally difficult and usually require computationally expensive procedures.",
        "Since our motivation for this research is to calculate good semantic similarities for a large set of words (e.g., one million nouns) and apply them to a wide range of NLP tasks, such costs must be minimized.",
        "Our technical contribution in this paper is to show that in the case where the context profiles are multinomial distributions, the priors are Dirich-let, and the base similarity measure is the Bhat-tacharyya coefficient (Bhattacharyya, 1943), we can derive an analytical form for Eq.",
        "2, that enables efficient calculation (with some implementation tricks).",
        "In experiments, we estimate semantic similarities using a large amount of Web data in Japanese and show that the proposed measure gives better word similarities than a non-Bayesian Bhat-tacharyya coefficient or other well-known similarity measures such as Jensen-Shannon divergence and the cosine with PMI weights.",
        "The rest of the paper is organized as follows.",
        "In Section 2, we briefly introduce the Bayesian estimation and the Bhattacharyya coefficient.",
        "Section 3 proposes our new Bayesian Bhattacharyya coefficient for robust similarity calculation.",
        "Section 4 mentions some implementation issues and the solutions.",
        "Then, Section 5 reports the experimental results."
      ]
    },
    {
      "heading": "2. Background",
      "text": [
        "Assume that we estimate a probabilistic model for the observed data D, p(Dl<fi), which is parameterized with parameters In the maximum likelihood estimation (MLE), we find the point estimation = argmax^p(DK).",
        "For example, we estimate p(fklwi) as follows with MLE:",
        "On the other hand, the objective of the Bayesian estimation is to find the distribution of << given the observed data D, i.e., p(<filD), and use it in later processes.",
        "Using Bayes' rule, this can also be viewed as:",
        "( < ) is a prior distribution that represents the plausibility of each < based on the prior knowledge.",
        "In this paper, we consider the case where <fi is a multinomial distribution, i.e., k <k = 1 that models the process of choosing one out of K choices.",
        "Estimating a conditional probability distribution <k = p(fklwi) as a context profile for each wi falls into this case.",
        "In this paper, we also assume that the prior is the Dirichlet distribution, Dir (a).",
        "The Dirichlet distribution is defined as follows.",
        "r(.)",
        "is the Gamma function.",
        "The Dirichlet distribution is parametrized by hyperparameters ak(> 0).",
        "It is known that p(<filD) is also a Dirichlet distribution for this simplest case, and it can be analytically calculated as follows.",
        "where c(k) is the frequency of choice k in data D. For example, c(k) = c(wi, fk) in the estimation of p(fklwi).",
        "This is very simple: we just need to add the observed counts to the hyperparameters.",
        "When the context profiles are probability distributions, we usually utilize the measures on probability distributions such as the Jensen-Shannon (JS) divergence to calculate similarities (Dagan et al., 1994; Dagan et al., 1997).",
        "The JS divergence is defined as follows.",
        "where pavg = Pi+2P2 is a point-wise average of p\\ and p2 and KL(.)",
        "is the Kullback-Leibler divergence.",
        "Although we found that the JS divergence is a good measure, it is difficult to derive an efficient calculation of Eq.",
        "2, even in the Dirichlet prior case.",
        "In this study, we employ the Bhattacharyya coefficient (Bhattacharyya, 1943) (BC for short), which is defined as follows:",
        "The BC is also a similarity measure on probability distributions and is suitable for our purposes as we describe in the next section.",
        "Although BC has not been explored well in the literature on distributional word similarities, it is also a good similarity measure as the experiments show."
      ]
    },
    {
      "heading": "3. Method",
      "text": [
        "In this section, we show that if our base similarity measure is BC and the distributions under which we take the expectation are Dirichlet distributions, then Eq.",
        "2 also has an analytical form, allowing efficient calculation.",
        "Here, we calculate the following value given two Dirichlet distributions:",
        "Dir(pi\\a )Dir(p2\\ß )BC(pi,p2)dpidp2.",
        "After several derivation steps (see Appendix A), we obtain the following analytical solution for the above:",
        "where a0 = Y k ak and ß0 = Y k ßk.",
        "Note that with the Dirichlet prior, ak = ak + c(w\\, fk) and ßk = ßk + c(w2, fk), where ak and ßk are the hyperparameters of the priors of wi and w2, respectively.",
        "To put it all together, we can obtain a new Bayesian similarity measure on words, which can be calculated only from the hyperparameters for the Dirichlet prior, a and ß, and the observed counts c(wi, fk).",
        "It is written as follows.",
        "Ek c(wi,fk ) and bo",
        "We can see that BCb actually encodes our guiding intuition.",
        "Consider four words, wo, wi , w2, and w4, for which we have c(w0, f1) = 10, c(wi,fi) = 2, c(w2,fi) = 10, and c(w3, fi) = 20.",
        "They have counts only for the first dimension, i.e., they have the same context profile: p(fi\\wi) = 1.0, when we employ MLE.",
        "When K = 10, 000 and ak = 1.0, the Bayesian similarity between these words is calculated as",
        "BCb(wo,wi)",
        "We can see that similarities are different according to the number of observations, as expected.",
        "Note that the non-Bayesian BC will return the same value, 1.0, for all cases.",
        "Note also that BCb(w0,w0) = 0.78542 if we use Eq.",
        "8, meaning that the self-similarity might not be the maximum.",
        "This is conceptually strange, although not a serious problem since we hardly use sim(wi,wi) in practice.",
        "If we want to fix this, we can use the special definition: BCb(wi, wi) = 1.",
        "This is equivalent to using simb(wi,wi) = E[sim(wi, wi)]{p(v(Wi))j = 1 only for this case.",
        "where a0 =",
        "Yk c(w2, fk).",
        "We call this new measure the Bayesian Bhattacharyya coefficient (BCb for short).",
        "For simplicity, we assume ak = ßk = a in this paper."
      ]
    },
    {
      "heading": "4. Implementation Issues",
      "text": [
        "Although we have derived the analytical form (Eq.",
        "8), there are several problems in implementing robust and efficient calculations.",
        "First, the Gamma function in Eq.",
        "8 overflows when the argument is larger than 170.",
        "In such cases, a commonly used way is to work in the logarithmic space.",
        "In this study, we utilize the \"log Gamma\" function: luY(x), which returns the logarithm of the Gamma function directly without the overflow problem.",
        "Second, the calculation of the log Gamma function is heavier than operations such as simple multiplication, which is used in existing measures.",
        "In fact, the log Gamma function is implemented using an iterative algorithm such as the Lanczos method.",
        "In addition, according to Eq.",
        "8, it seems that we have to sum up the values for all k, because even if c(wi, fk) is zero the value inside the summation will not be zero.",
        "In the existing measures, it is often the case that we only need to sum up for k where c(wi, fk) > 0.",
        "Because c(wi, fk) is usually sparse, that technique speeds up the calculation of the existing measures drastically and makes it practical.",
        "In this study, the above problem is solved by pre-computing the required log Gamma values, assuming that we calculate similarities for a large set of words, and pre-computing default values for cases where c(wi, fk) = 0.",
        "The following values are precomputed once at the start-up time.",
        "For each word:",
        "For each k:",
        "(D) : exp(2(lnl>k + 2)).",
        "In the calculation of BCb(w\\,w2), we first assume that all c(wi, fk) = 0 and set the output variable to the default value.",
        "Then, we iterate over the sparse vectors c(w , fk) and c(w2, fk).",
        "If c(w , fk) > 0 and c(w2, fk) = 0 (and vice versa), we update the output variable just by adding (C).",
        "If c(w , fk) > 0 and c(w2, fk) > 0, we update the output value using (B), (D) and one additional exp(.)",
        "operation.",
        "With this implementation, we can make the computation of BCb practically as fast as using other measures."
      ]
    },
    {
      "heading": "5. Experiments",
      "text": [
        "We evaluated our method in the calculation ofsim-ilarities between nouns in Japanese.",
        "Because human evaluation of word similarities is very difficult and costly, we conducted automatic evaluation in the set expansion setting, following previous studies such as Pantel et al.",
        "(2009).",
        "Given a word set, which is expected to contain similar words, we assume that a good similarity measure should output, for each word in the set, the other words in the set as similar words.",
        "For given word sets, we can construct input-and-answers pairs, where the answers for each word are the other words in the set the word appears in.",
        "We output a ranked list of 500 similar words for each word using a given similarity measure and checked whether they are included in the answers.",
        "This setting could be seen as document retrieval, and we can use an evaluation measure such as the mean of the precision at top T (MP@T) or the mean average precision (MAP).",
        "For each input word, P@T (precision at top T) and AP (average precision) are defined as follows.",
        "AP = – Y2 à(wi e ans)P@i.",
        "ö(wi G ans) returns 1 if the output word wi is in the answers, and 0 otherwise.",
        "N is the number of outputs and R is the number of the answers.",
        "MP@T and MAP are the averages of these values over all input words.",
        "Dependency relations are used as context profiles as in Kazama and Torisawa (2008) and Kazama et al.",
        "(2009).",
        "From a large corpus of Japanese Web documents (Shinzato et al., 2008) (100 million documents), where each sentence has a dependency parse, we extracted noun-verb and noun-noun dependencies with relation types and then calculated their frequencies in the corpus.",
        "If a noun, n, depends on a word, w, with a relation, r, we collect a dependency pair, (n, (w, r)).",
        "That is, a context fk, is (w, r) here.",
        "For noun-verb dependencies, postpositions in Japanese represent relation types.",
        "For example, we extract a dependency relation (y^ï'y, ( Ml, § )) from the sentence below, where a postposition (wo)\" is used to mark the verb object.",
        "Note that we leave various auxiliary verb suffixes, such as \" tl?> (reru),\" which is for passiviza-tion, as a part of w, since these greatly change the type of n in the dependent position.",
        "As for noun-noun dependencies, we considered expressions of type \"n1 <?)",
        "n2\" (k \"n2 of m\") as dependencies (ni, (n2, )).",
        "We extracted about 470 million unique dependencies from the corpus, containing 31 million unique nouns (including compound nouns as determined by our filters) and 22 million unique contexts, fk.",
        "We sorted the nouns according to the number of unique co-occurring contexts and the contexts according to the number of unique co-occurring nouns, and then we selected the top one million nouns and 100,000 contexts.",
        "We used only 260 million dependency pairs that contained both the selected nouns and the selected contexts.",
        "We prepared three test sets as follows.",
        "Set \"A\" and \"B\": Thesaurus siblings We considered that words having a common hypernym (i.e., siblings) in a manually constructed thesaurus could constitute a similar word set.",
        "We extracted such sets from a Japanese dictionary, EDR (V3.0) (CRL, 2002), which contains concept hierarchies and the mapping between words and concepts.",
        "The dictionary contains 304,884 nouns.",
        "In all, 6,703 noun sibling sets were extracted with the average set size of 45.96.",
        "We randomly chose 200 sets each for sets \"A\" and \"B.\"",
        "Set \"A\" is a development set to tune the value of the hyperparameters and \"B\" is for the validation of the parameter tuning.",
        "Set \"C\": Closed sets Murata et al.",
        "(2004) constructed a dataset that contains several closed word sets such as the names of countries, rivers, sumo wrestlers, etc.",
        "We used all of the 45 sets that are marked as \"complete\" in the data, containing 12,827 unique words in total.",
        "Note that we do not deal with ambiguities in the construction of these sets as well as in the calculation of similarities.",
        "That is, a word can be contained in several sets, and the answers for such a word is the union of the words in the sets it belongs to (excluding the word itself).",
        "In addition, note that the words in these test sets are different from those of our one-million-word vocabulary.",
        "We filtered out the words that are not included in our vocabulary and removed the sets with size less than 2 after the filtering.",
        "Set \"A\" contained 3,740 words that are actually evaluated, with about 115 answers on average, and \"B\" contained 3,657 words with about 65 answers on average.",
        "Set \"C\" contained 8,853 words with about 1,700 answers on average.",
        "We compared our Bayesian Bhattacharyya similarity measure, BCb, with the following similarity measures.",
        "JS Jensen-Shannon divergence between p(fk\\w1) and p(fk\\w2) (Dagan et al., 1994; Dagan et al., 1999).",
        "PMI-cos The cosine of the context profile vectors, where the k-th dimension is the point-wise mutual information (PMI) between wi and fk defined as: PMI (wi, fk ) = log pwpffk) (Pantel and Lin, 2002; Pantel",
        "Cls-JS Kazama et al.",
        "(2009) proposed using the Jensen-Shannon divergence between hidden class distributions, p(c\\w1) and p(c\\w2), which are obtained by using an EM-based clustering of dependency relations with a model p(wi,fk) = EcP(wi\\c)p(fk\\c)p(c) (Kazama and Torisawa, 2008).",
        "In order to alleviate the effect of local minima of the EM clustering, they proposed averaging the similarities by several different clustering results, which can be obtained by using different initial parameters.",
        "In this study, we combined two clustering results (denoted as \"s1+s2\" in the results), each of which (\"s1\" and \"s2\") has 2,000 hidden classes.",
        "We included this method since clustering can be regarded as another way of treating data sparseness.",
        "BC The Bhattacharyya coefficient (Bhattacharyya, 1943) between p(fk \\w1) and p(fk\\w2).",
        "This is the baseline for BCb.",
        "BCa The Bhattacharyya coefficient with absolute discounting.",
        "In calculating p(fk \\wi), we subtract the discounting value, a, from c(wi, fk) and equally distribute the residual probability mass to the contexts whose frequency is zero.",
        "This is included as an example of naive smoothing methods.",
        "Since it is very costly to calculate the similarities with all of the other words (one million in our case), we used the following approximation method that exploits the sparseness of c(wi, fk).",
        "Similar methods were used in Pantel tel et al.",
        "(2009) as well.",
        "For a given word, wi, we sort the contexts in descending order according to c(wi, fk) and retrieve the top-L contexts.For each selected context, we sort the words in descending order according to c(wi, fk) and retrieve the top-M words (L = M = 1600).",
        "We merge all of the words above as candidate words and calculate the similarity only for the candidate words.",
        "Finally, the top 500 similar words are output.",
        "Note also that we used modified counts, log(c(wi, fk)) + 1, instead of raw counts, c(wi, fk), with the intention of alleviating the effect of strangely frequent dependencies, which can be found in the Web data.",
        "In preliminary experiments, we observed that this modification improves the quality of the top 500 similar words as reported in Terada et al.",
        "(2004) and Kazama et al.",
        "(2009).",
        "As for BCb, we assumed that all of the hyperparameters had the same value, i.e., ak = a.",
        "It is apparent that an excessively large a is not appropriate because it means ignoring observations.",
        "Therefore, a must be tuned.",
        "The discounting value of BCa is also tuned.",
        "Table 1 shows the results for Set A.",
        "The MAP and the MPs at the top 1, 5, 10, and 20 are shown for each similarity measure.",
        "As for BCb and BCa, the results for the tuned and several other values for a are shown.",
        "Figure 1 shows the parameter tuning for BCb with MAP as the y-axis (results for BCaare shown as well).",
        "Figure 2 shows the same results with MPs as the y-axis.",
        "The MAP and MPs showed a correlation here.",
        "From these results, we can see that BCb surely improves upon BC, with 6.6% improvement in MAP and 14.7% improvement in MP@1 when a = 0.0016.",
        "BCb achieved the best performance among the compared measures with this setting.",
        "The absolute discounting, BCa, improved upon BC as well, but the improvement was smaller than with BCb.",
        "Table 1 also shows the results for the case where we did not use the log-modified counts.",
        "We can see that this modification gives improvements (though slight or unclear for PMI-cos).",
        "Because tuning hyperparameters involves the possibility of overfitting, its robustness should be assessed.",
        "We checked whether the tuned a with Set A works well for Set B.",
        "The results are shown in Table 2.",
        "We can see that the best a (= 0.0016) found for Set A works well for Set B as well.",
        "That is, the tuning of a as above is not unrealistic in",
        "Measure",
        "MAP",
        "MP",
        "@1",
        "@5",
        "@10",
        "@20",
        "JS",
        "0.0299",
        "0.197",
        "0.122",
        "0.0990",
        "0.0792",
        "PMI-cos",
        "0.0332",
        "0.195",
        "0.124",
        "0.0993",
        "0.0798",
        "Cls-JS (s1)",
        "0.0319",
        "0.195",
        "0.122",
        "0.0988",
        "0.0796",
        "Cls-JS (s2)",
        "0.0295",
        "0.198",
        "0.122",
        "0.0981",
        "0.0786",
        "Cls-JS (s1+s2)",
        "0.0333",
        "0.206",
        "0.129",
        "0.103",
        "0.0841",
        "BC",
        "0.0334",
        "0.211",
        "0.131",
        "0.106",
        "0.0854",
        "BCf, (0.0002)",
        "0.0345",
        "0.223",
        "0.138",
        "0.109",
        "0.0873",
        "BCf, (0.0016)",
        "0.0356",
        "0.242",
        "0.148",
        "0.119",
        "0.0955",
        "BCf, (0.0032)",
        "0.0325",
        "0.223",
        "0.137",
        "0.111",
        "0.0895",
        "BCa (0.0016)",
        "0.0337",
        "0.212",
        "0.133",
        "0.107",
        "0.0863",
        "BCa (0.0362)",
        "0.0345",
        "0.221",
        "0.136",
        "0.110",
        "0.0890",
        "BCa (0.1)",
        "0.0324",
        "0.214",
        "0.128",
        "0.101",
        "0.0825",
        "without log(c(wi, fk)) + 1 modification",
        "JS",
        "0.0294",
        "0.197",
        "0.116",
        "0.0912",
        "0.0712",
        "PMI-cos",
        "0.0342",
        "0.197",
        "0.125",
        "0.0987",
        "0.0793",
        "BC",
        "0.0296",
        "0.201",
        "0.118",
        "0.0915",
        "0.0721",
        "a (log-scale)",
        "Figure 2: Tuning of a (MP).",
        "practice because it seems that we can tune it robustly using a small subset of the vocabulary as shown by this experiment.",
        "Next, we evaluated the measures on Set C, i.e., the closed set data.",
        "The results are shown in Table 3.",
        "For this set, we observed a tendency that is different from Sets A and B. Cls-JS showed a particularly good performance.",
        "BCb surely improves upon BC.",
        "For example, the improvement was 7.5% for MP@1.",
        "However, the improvement in MAP was slight, and MAP did not correlate well with MPs, unlike in the case of Sets A and",
        "B.",
        "We thought one possible reason is that the number of outputs, 500, for each word was not large enough to assess MAP values correctly because the average number of answers is 1,700 for this dataset.",
        "In fact, we could output more than 500 words if we ignored the cost of storage.",
        "Therefore, we also included the results for the case where L = M = 3600 and N = 2,000.",
        "Even with this setting, however, MAP did not correlate well with MPs.",
        "Although Cls-JS showed very good performance for Set C, note that the EM clustering is very time-consuming (Kazama and Torisawa, 2008), and it took about one week with 24 CPU cores to get one clustering result in our computing environment.",
        "On the other hand, the preparation for our method requires just an hour with a single core."
      ]
    },
    {
      "heading": "6. Discussion",
      "text": [
        "We should note that the improvement by using our method is just \"on average,\" as in many other NLP tasks, and observing clear qualitative change is relatively difficult, for example, by just showing examples of similar word lists here.",
        "Comparing the results of BCb and BC, Table 4 lists the numbers of improved, unchanged, and degraded words in terms of MP@20 for each evaluation set.",
        "As can be seen, there are a number of degraded words, although they are fewer than the improved words.",
        "Next, Figure 3 shows the averaged differences of observe that the advantage of BCb is lessened es-",
        "MP",
        "Measure",
        "MAP",
        "@1",
        "@5",
        "@10",
        "@20",
        "JS",
        "0.0265",
        "0.208",
        "0.116",
        "0.0855",
        "0.0627",
        "PMI-cos",
        "0.0283",
        "0.203",
        "0.116",
        "0.0871",
        "0.0660",
        "Cls-JS (s1+s2)",
        "0.0274",
        "0.194",
        "0.115",
        "0.0859",
        "0.0643",
        "BC",
        "0.0295",
        "0.223",
        "0.124",
        "0.0922",
        "0.0693",
        "BCf (0.0002)",
        "0.0301",
        "0.225",
        "0.128",
        "0.0958",
        "0.0718",
        "BCf (0.0016)",
        "0.0313",
        "0.246",
        "0.135",
        "0.103",
        "0.0758",
        "BCf (0.0032)",
        "0.0279",
        "0.228",
        "0.127",
        "0.0938",
        "0.0698",
        "BCa (0.0016)",
        "0.0297",
        "0.223",
        "0.125",
        "0.0934",
        "0.0700",
        "BCa (0.0362)",
        "0.0298",
        "0.223",
        "0.125",
        "0.0934",
        "0.0705",
        "BCa (0.01)",
        "0.0300",
        "0.224",
        "0.126",
        "0.0949",
        "0.0710",
        "Table 3: Performance on closed-sets (Set C).",
        "Measure",
        "MAP",
        "MP",
        "@1",
        "@5",
        "@10",
        "@20",
        "JS",
        "0.127",
        "0.607",
        "0.582",
        "0.566",
        "0.544",
        "PMI-cos",
        "0.124",
        "0.531",
        "0.519",
        "0.508",
        "0.493",
        "Cls-JS (s1)",
        "0.125",
        "0.589",
        "0.566",
        "0.548",
        "0.525",
        "Cls-JS (s2)",
        "0.137",
        "0.608",
        "0.592",
        "0.576",
        "0.554",
        "Cls-JS (s1+s2)",
        "0.152",
        "0.638",
        "0.617",
        "0.603",
        "0.583",
        "BC",
        "0.131",
        "0.602",
        "0.579",
        "0.565",
        "0.545",
        "BCf (0.0004)",
        "0.133",
        "0.636",
        "0.605",
        "0.587",
        "0.563",
        "BCf (0.0008)",
        "0.131",
        "0.647",
        "0.615",
        "0.594",
        "0.568",
        "BCf (0.0016)",
        "0.126",
        "0.644",
        "0.615",
        "0.593",
        "0.564",
        "BCf (0.0032)",
        "0.107",
        "0.573",
        "0.556",
        "0.529",
        "0.496",
        "L =",
        "M = 3200 and N = 2000",
        "JS",
        "0.165",
        "0.605",
        "0.580",
        "0.564",
        "0.543",
        "PMI-cos",
        "0.165",
        "0.530",
        "0.517",
        "0.507",
        "0.492",
        "Cls-JS (s1+s2)",
        "0.209",
        "0.639",
        "0.618",
        "0.603",
        "0.584",
        "BC",
        "0.168",
        "0.600",
        "0.577",
        "0.562",
        "0.542",
        "BCf (0.0004)",
        "0.170",
        "0.635",
        "0.604",
        "0.586",
        "0.562",
        "BCf (0.0008)",
        "0.168",
        "0.647",
        "0.615",
        "0.594",
        "0.568",
        "BCf (0.0016)",
        "0.161",
        "0.644",
        "0.615",
        "0.593",
        "0.564",
        "BCf (0.0032)",
        "0.140",
        "0.573",
        "0.556",
        "0.529",
        "0.496",
        "Table 4: The numbers of improved, unchanged, and degraded words in terms of MP@20 for each evaluation set.",
        "C).",
        "pecially for low-ID words (as expected) with on-average degradation.",
        "The improvement is \"on average\" in this sense as well.",
        "One might suspect that the answer words tended to be low-ID words, and the proposed method is simply biased towards low-ID words because of its nature.",
        "Then, the observed improvement is a trivial consequence.",
        "Table 5 lists some interesting statistics about the IDs.",
        "We can see that BCbsurely outputs more low-ID words than BC, and BC more than Cls-JS and JS.",
        "However, the average ID of the outputs of BC is already lower than the average ID of the answer words.",
        "Therefore, even if BCb preferred lower-ID words than BC, it should not have the effect of improving the accuracy.",
        "That is, the improvement by BCbis not superficial.",
        "From BC/BCb, we can also see that the IDs of the correct outputs did not become smaller compared to the IDs of the system outputs.",
        "Clearly, we need more analysis on what caused the improvement by the proposed method and how that affects the efficacy in real applications of similarity measures.",
        "The proposed Bayesian similarity measure outperformed the baseline Bhattacharyya coefficient",
        "Table 5: Statistics on IDs.",
        "(A): Avg.",
        "ID of answers.",
        "(B): Avg.",
        "ID of system outputs.",
        "(C): Avg.",
        "ID of correct system outputs.",
        "and other well-known similarity measures.",
        "As a smoothing method, it also outperformed a naive absolute discounting.",
        "Of course, we cannot say that the proposed method is better than any other sophisticated smoothing method at this point.",
        "However, as noted above, there has been no serious attempt to assess the effect of smoothing in the context of word similarity calculation.",
        "Recent studies have pointed out that the Bayesian framework derives state-of-the-art smoothing methods such as Kneser-Ney smoothing as a special case (Teh, 2006; Mochihashi et al., 2009).",
        "Consequently, it is reasonable to resort to the Bayesian framework.",
        "Conceptually, our method is equivalent to modifying p(fk\\ wi) taking the Bhattacharyya coefficient.",
        "However, the implication of this form has not yet been investigated, and so we leave it for future research.",
        "Our method is the simplest one as a Bayesian method.",
        "We did not employ any numerical optimization or sampling iterations, as in a more complete use of the Bayesian framework (Teh, 2006; Mochihashi et al., 2009).",
        "Instead, we used the obtained analytical form directly with the assumption that ak = a and a can be tuned directly by using a simple grid search with a small subset of the vocabulary as the development set.",
        "If substantial additional costs are allowed, we can fine-tune each ak using more complete Bayesian methods.",
        "We also leave this for future research.",
        "In terms of calculation procedure, BCb has the same form as other similarity measures, which is basically the same as the inner product of sparse vectors.",
        "Thus, it can be as fast as other similarity measures with some effort as we described in Section 4 when our aim is to calculate similarities between words in a fixed large vocabulary.",
        "For example, BCb took about 100 hours to calculate the top 500 similar nouns for all of the one million nouns (using 16 CPU cores), while JS took about 57 hours.",
        "We think this is an acceptable additional cost.",
        "# improved",
        "# unchanged",
        "# degraded",
        "SetA",
        "755",
        "2,585",
        "400",
        "SetB",
        "643",
        "2,610",
        "404",
        "SetC",
        "3,153",
        "3,962",
        "1,738",
        "Set A",
        "SetC",
        "(A)",
        "238,483",
        "255,248",
        "(B) (C)",
        "(B) (C)",
        "Cls-JS (s1+s2)",
        "JS",
        "BC",
        "BCf,(0.0016)",
        "282,098 176,706",
        "183,054 11,3442 162,758 98,433 55,915 54,786",
        "273,768 232,796 211,671 201,214 193,508 189,345 90,472 127,877",
        "BC/BCf,",
        "2.91 1.80",
        "2.14 1.48",
        "The limitation of our method is that it cannot be used efficiently with similarity measures other than the Bhattacharyya coefficient, although that choice seems good as shown in the experiments.",
        "For example, it seems difficult to use the Jensen-Shannon divergence as the base similarity because the analytical form cannot be derived.",
        "One way we are considering to give more flexibility to our method is to adjust ak depending on external knowledge such as the importance of a context (e.g., PMIs).",
        "In another direction, we will be able to use a \"weighted\" Bhattacharyya coefficient: E k ß(wi, fk)KW2, fkWvik x p2k, where the weights, ß(wi, fk), do not depend on pik, as the base similarity measure.",
        "The analytical form for it will be a weighted version of BCb.",
        "BCb can also be generalized to the case where the base similarity is BCd(pi,p2) = E k=i Pik x p^k, where d > 0.",
        "The Bayesian analytical form becomes as follows.",
        "See Appendix A for the derivation.",
        "However, we restricted ourselves to the case of d = 2 in this study.",
        "Finally, note that our BCb is different from the Bhattacharyya distance measure on Dirichlet distributions of the following form described in Rauber et al.",
        "(2008) in its motivation and analytical form:",
        "Ilk rK )\\fïïk",
        "Empirical and theoretical comparisons with this measure also form one of the future directions."
      ]
    },
    {
      "heading": "7. Conclusion",
      "text": [
        "We proposed a Bayesian method for robust distributional word similarities.",
        "Our method uses a distribution of context profiles obtained by Bayesian estimation and takes the expectation of a base similarity measure under that distribution.",
        "We showed that, in the case where the context profiles are multinomial distributions, the priors are Dirichlet, and the base measure is the Bhattacharyya coefficient, we can derive an analytical form, permitting efficient calculation.",
        "Experimental results show that the proposed measure gives better word similarities than a non-Bayesian Bhattacharyya coefficient, other well-known similarity measures such as Jensen-Shannon divergence and the cosine with PMI weights, and the Bhattacharyya coefficient with absolute discounting.",
        "Here, we give the analytical form for the generalized case (BCbd) in Section 6.",
        "Recall the following relation, which is used to derive the normalization factor of the Dirichlet distribution:",
        "Ja k r(ao)",
        "Z(a )Z(ß ) x Il t\" W. ^m-1^, tâktâk d<i>i d<j>2 .",
        "Using Eq.",
        "10, A in the above can be calculated as follows:",
        "E *2k/ tir-jQ fir"
      ]
    }
  ]
}
