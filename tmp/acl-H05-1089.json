{
  "info": {
    "authors": [
      "Ping Li",
      "Kenneth Ward Church"
    ],
    "book": "Human Language Technology Conference and Empirical Methods in Natural Language Processing",
    "id": "acl-H05-1089",
    "title": "Using Sketches to Estimate Associations",
    "url": "https://aclweb.org/anthology/H05-1089",
    "year": 2005
  },
  "references": [
    "acl-J90-1003",
    "acl-J93-1003",
    "acl-P05-1077",
    "acl-P89-1010",
    "acl-W04-3243"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We should not have to look at the entire corpus (e.g., the Web) to know if two words are associated or not.1 A powerful sampling technique called Sketches was originally introduced to remove duplicate Web pages.",
        "We generalize sketches to estimate contingency tables and associations, using a maximum likelihood estimator to find the most likely contingency table given the sample, the margins (document frequencies) and the size of the collection.",
        "Not unsurprisingly, computational work and statistical accuracy (variance or errors) depend on sampling rate, as will be shown both theoretically and empirically.",
        "Sampling methods become more and more important with larger and larger collections.",
        "At Web scale, sampling rates as low as 10-4 may suffice."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Word associations (co-occurrences) have a wide range of applications including: Speech Recognition, Optical Character Recognition and Information Retrieval (IR) (Church and Hanks, 1991; Dunning, 1993; Manning and Schütze, 1999).",
        "It is easy to compute association scores for a small corpus, but more challenging to compute lots of scores for lots of data (e.g. the Web), with billions of web pages (D) and millions of word types (V).",
        "For a small corpus, one could compute pairwise associations by multiplying the (0/1) term-by-document matrix with its transpose (Deerwester et al., 1999).",
        "But this is probably infeasible at Web scale.",
        "Approximations are often good enough.",
        "We should not have to look at every document to determine that two words are strongly associated.",
        "A number of sampling-based randomized algorithms have been implemented at Web scale (Broder, 1997; Charikar, 2002; Ravichandran et al., 2005).2 A conventional random sample is constructed by selecting D3 documents from a corpus of D documents.",
        "The (corpus) sampling rate is D3D.",
        "Of course, word distributions have long tails.",
        "There are a few high frequency words and many low frequency words.",
        "It would be convenient if the sampling rate could vary from word to word, unlike conventional sampling where the sampling rate is fixed across the vocabulary.",
        "In particular, in our experiments, we will impose a floor to make sure that the sample contains at least 20 documents for each term.",
        "(When working at Web scale, one might raise the floor somewhat to perhaps 104.)",
        "Sampling is obviously helpful at the top of the frequency range, but not necessarily at the bottom (especially if frequencies fall below the floor).",
        "The question is: how about “ordinary” words?",
        "To answer this question, we randomly picked 15 pages from a Learners’ dictionary (Hornby, 1989), and selected the first entry on each page.",
        "According to Google, there are 10 million pages/word (median value, aggregated over the 15 words), no where near the floor.",
        "Sampling can make it possible to work in memory, avoiding disk.",
        "At Web scale (D Pz� 10 billion pages), inverted indexes are large (1500 GBs/billion pages)3, probably too large for memory.",
        "But a sample is more manageable; the inverted index for a 10-4 sample of the entire web could fit in memory on a single PC (1.5 GB).",
        "first join can be reduced from 504,000 to 120,000, by starting with “Schwarzenegger & Austria” rather than the baseline (“Schwarzenegger & Terminator”).",
        "The standard practice of starting with the two least frequent terms is a good rule of thumb, but one can do better, given (estimates of) joint frequencies."
      ]
    },
    {
      "heading": "1.1 An Application: The Governator",
      "text": [
        "Google returns the top k hits, plus an estimate of how many hits there are.",
        "Table 1 shows the number of hits for four words and their pairwise combinations.",
        "Accurate estimates of associations would have applications in Database query planning (Garcia-Molina et al., 2002).",
        "Query optimizers construct a plan to minimize a cost function (e.g., intermediate writes).",
        "The optimizer could do better if it could estimate a table like Table 1.",
        "But efficiency is important.",
        "We certainly don’t want to spend more time optimizing the plan than executing it.",
        "Suppose the optimizer wanted to construct a plan for the query: “Governor Schwarzenegger Terminator Austria.” The standard solution starts with the two least frequent terms: “Schwarzenegger” and “Terminator.” That plan generates 504,000 intermediate writes after the first join.",
        "An improvement starts with “Schwarzenegger” with “Austria,” reducing the 504,000 down to 120,000.",
        "In addition to counting hits, Table 1 could also help find the top k pages.",
        "When joining the first pair of terms, we’d like to know how far down the ranking we should go.",
        "Accurate estimates of associations would help the optimizer make such decisions.",
        "It is desirable that estimates be consistent, as well as accurate.",
        "Google, for example, reports 6 million hits for “America, China, Britain,” and 23 million for “America, China, Britain, Japan.” Joint frequencies decrease monotonically: s c S =#- hits(s) > hits(S)."
      ]
    },
    {
      "heading": "1.2 Sampling and Estimation",
      "text": [
        "Two-way associations are often represented as two-way contingency tables (Figure 1(a)).",
        "Our task is to construct a sample contingency table (Figure 1(b)), and estimate 1(a) from 1(b).",
        "We will use a maximum likelihood estimator (MLE) to find the most likely contingency table, given the sample and various other constraints.",
        "We will propose a sampling procedure that bridges two popular choices: (A) sampling over documents and (B) sampling over postings.",
        "The estimation task is straightforward and well-understood for (A).",
        "As we consider more flexible sampling procedures such as (B), the estimation task becomes more challenging.",
        "Flexible sampling procedures are desirable.",
        "Many studies focus on rare words (Dunning, 1993; Moore, 2004); butterflies are more interesting than moths.",
        "The sampling rate can be adjusted on a word-byword basis with (B), but not with (A).",
        "The sampling rate determines the trade-off between computational work and statistical accuracy.",
        "We assume a standard inverted index.",
        "For each word x, there are a set of postings, X. X contains a set of document IDs, one for each document containing x.",
        "The size of postings, fx = IX1, corresponds to the margins of the contingency tables in Figure 1(a), also known as document frequencies in IR.",
        "The postings lists are approximated by sketches, skX, first introduced by Broder (1997) for removing duplicate web pages.",
        "Assuming that document IDs are random (e.g., achieved by a random permutation), we can compute skX, a random sample of",
        "X, by simply selecting the first few elements of X.",
        "In Section 3, we will propose using sketches to construct sample contingency tables.",
        "With this novel construction, the contingency table (and summary statistics based on the table) can be estimated using conventional statistical methods such as MLE."
      ]
    },
    {
      "heading": "2 Broder’s Sketch Algorithm",
      "text": [
        "One could randomly sample two postings and intersect the samples to estimate associations.",
        "The sketch technique introduced by Broder (1997) is a significant improvement, as demonstrated in Figure 2.",
        "Assume that each document in the corpus of size D is assigned a unique random ID between 1 and D. The postings for word x is a sorted list of fx doc IDs.",
        "The sketch, skX, is the first (smallest) sx doc IDs in X. Broder used MINs (Z) to denote the s smallest elements in the set, Z.",
        "Thus, skX = MINsx (X).",
        "Similarly, Y denotes the postings for word y, and skY denotes its sketch, MINsy (Y).",
        "Broder assumed sx=sy=s.",
        "Broder defined resemblance (R) and sample re",
        "Broder (1997) proved that Rs is an unbiased estimator of R. One could use Rs to estimate a but he didn’t do that, and it is not recommended.4 Sketches were designed to improve the coverage of a, as illustrated by Monte Carlo simulation in Figure 2.",
        "The figure plots, E (as ), percentage of intersections, as a function of (postings) sampling rate, sf, where fx = fy = f, sx = sy = s. The solid lines (sketches), E (as) ≈ sf ,are above the dashed curve a (random sampling), E (a) = f2 .",
        "The difference is particularly important at low sampling rates."
      ]
    },
    {
      "heading": "3 Generalizing Sketches: R → Tables",
      "text": [
        "Sketches were first proposed for estimating resemblance (R).",
        "This section generalizes the method to construct sample contingency tables, from which we can estimate associations: R, LLR, cosine, etc.",
        "4 There are at least three problems with estimating a from Rs.",
        "First, the estimate is biased.",
        "Secondly, this estimate uses just s of the 2 × s samples; larger samples → smaller errors.",
        "Thirdly, we would rather not impose the restriction: sx = sy.",
        "Recall that the doc IDs span the integers from 1 to D with no gaps.",
        "When we compare two sketches, skX and skY, we have effectively looked at Ds = min{skX(sx), skY(sy)} documents, where skX(j) is the jth smallest element in skX.",
        "The following construction generates the sample contingency table, as, bs, cs, ds (as in Figure 1(b)).",
        "The example shown in Figure 3 may help explain the procedure.",
        "Given the sample contingency table, we are now ready to estimate the contingency table.",
        "It is sufficient to estimate a, since the rest of the table can be determined from fx, fy and D. For practical applications, we recommend the convenient closed-form approximation (8) in Section 5.1.",
        "4 Margin-Free (MF) Baseline Before considering the proposed MLE method, we introduce a baseline estimator that will not work as well because it does not take advantage of the margins.",
        "The baseline is the multivariate hypergeometric model, usually simplified as a multinomial by as-sum ing “sample-with-replacement.” The sample expectations are (Siegrist, 1997),",
        "as= 2 bs= 5 cs= 3 to randomly sampling Ds documents from the corpus.",
        "Based on this key observation and Figure 3, conditional on Ds, P(as, bs, cs, dsIDs; a) is the PMF of a two-way sample contingency table.",
        "We factor the full likelihood into:",
        "(larger shaded box), are used to construct a sample contingency table: as, bs, cs, ds.",
        "skX consists of the first sx = 7 doc IDs in X, the postings for word x.",
        "Similarly, skY consists of the first sy = 7 doc IDs in Y, the postings for word y.",
        "There are 11 doc IDs in both X and Y, and a = 5 doc IDs in the intersection: 14, 15, 19, 24, 281.",
        "(a) shows that Ds = min(18, 21) = 18.",
        "Doc IDs 19 and 21 are excluded because we cannot determine if they are in the intersection or not, without looking outside the box.",
        "As it turns out, 19 is in the intersection and 21 is not.",
        "(b) enumerates the Ds = 18 documents, showing which documents contain x (small circles) and which contain y (small squares).",
        "Both procedures, (a) and (b), produce the same sample contingency table: as = 2, bs = 5, cs = 3 and ds = 8.",
        "The margin-free estimator and its variance are",
        "For the multinomial simplification, we have where “r” indicates “sample-with-replacement.” The term D Ds D D s is often called the “finite-sample correction factor” (Siegrist, 1997)."
      ]
    },
    {
      "heading": "5 The Proposed MLE Method",
      "text": [
        "The task is to estimate the contingency table from the samples, the margins and D. We would like to use a maximum likelihood estimator for the most probable a, which maximizes the (full) likelihood (probability mass function, PMF) P(as, bs, cs, ds; a).",
        "Unfortunately, we do not know the exact expression for P(as, bs, cs, ds; a), but we do know the conditional probability P(as, bs, cs, ds IDs; a).",
        "Since the doc IDs are uniformly random, sampling the first Ds contiguous documents is statistically equivalent",
        "pect a strong dependency of Ds on a, we maximize the partial likelihood instead, and assume that is good enough.",
        "An example of partial likelihood is the Cox proportional hazards model in survival analysis (Venables and Ripley, 2002, Section 13.3) .",
        "Our partial likelihood is",
        "where (m m,( ) = n !m)!.“ ∝” is “proportional to.” We now derive an MLE for (4), a result that was not previously known, to the best of our knowledge.",
        "Let ˆaMLE maximizes log P (as, bs, cs, dsIDs; a):",
        "Since the second derivative, ∂2logP(a3,b32c3,d3�D3;a) Si , ∂a is negative, the log likelihood function is concave, hence has a unique maximum.",
        "One could numerically solve (5) for ∂ log P(as,as s,ds |Ds;a) = 0.",
        "However, we derive the exact solution using the following updating formula from (4): rom (4):"
      ]
    },
    {
      "heading": "5.1 A Convenient Practical Approximation",
      "text": [
        "Rather than solving the cubic equation for the exact MLE, the following approximation may be more convenient.",
        "Assume we sample nx = as + bs from X and obtain as co-occurrences without knowledge of the samples from Y.",
        "Further assuming “sample-with-replacement,” as is then binomially distributed, as – Binom(nx, afx ).",
        "Similarly, assume as – Binom(ny, afy ).",
        "Under these assumptions, the PMF of as is a product of two binomial PMFs:",
        "Setting the first derivative of the logarithm of (7) to be zero, we obtain 2as – f bs a – f� s a = 0, which is quadratic in a and has a solution:",
        "Section 6 shows that ˆaMLE,a is very close to ˆaMLE."
      ]
    },
    {
      "heading": "5.2 Theoretical Evaluation: Bias and Variance",
      "text": [
        "How good are the estimates?",
        "A popular metric is mean square error (MSE): MSE(ˆa) = E (ˆa – a)2 = Var (ˆa) +Bias2 (ˆa).",
        "If aˆ is unbiased, MSE(ˆa) =Var (ˆa) = SE2 (ˆa), where SE is the standard error.",
        "Here all expectations are conditional on Ds.",
        "Large sample theory (Lehmann and Casella, 1998, Chapter 6) says that, under “sample-with-replacement,” ˆaMLE is asymptotically unbiased and converges to Normal with mean a and variance I1), where I(a), the Fisher Information, is",
        "We plug (1) from the margin-free model into (11) as an approximation, to obtain",
        "which is 1(a) multiplied by sample correction factor,” to consider “sample-without-replacement.” We can see that Var (ˆaMLE) is less than Var (ˆaMF) in (2).",
        "In addition, ˆaMLE is asymptotically unbiased while ˆaMF is no longer unbiased under margin constraints.",
        "Therefore, we expect ˆaMLE has smaller MSE than ˆaMF.",
        "In other words, the proposed MLE method is more accurate than the MF baseline, in terms of variance, bias and mean square error.",
        "If we know the margins, we ought to use them."
      ]
    },
    {
      "heading": "5.3 Unconditional Bias and Variance",
      "text": [
        "The unconditional variance is useful because often we would like to estimate the errors before knowing Ds (e.g., for choosing sample sizes).",
        "To compute the unconditional variance of ˆaMLE, we should replace D3 with E ( D3 ) in (12).",
        "We resort to an approximation for E (D ) .",
        "Note that skX(sx) is the order statistics of a discrete random variable (Siegrist, 1997) with expectation",
        "Replacing the inequalities with equalities underestimates the variance, but only slightly."
      ]
    },
    {
      "heading": "5.4 Smoothing",
      "text": [
        "Although not a major emphasis here, our evaluations will show that ˆaMLE+S, a smoothed version of the proposed MLE method, is effective, especially at low sampling rates.",
        "ˆaMLE+S uses “add-one” smoothing.",
        "Given that such a simple method is as effective as it is, it would be worth considering more sophisticated methods such as Good-Turing."
      ]
    },
    {
      "heading": "5.5 How Many Samples Are Sufficient?",
      "text": [
        "The answer depends on the trade-off between computation and estimation errors.",
        "One simple rule is to sample “2%.” (12) implies that the standard error is proportional to VD/Ds −1.",
        "Figure 4(a) plots VD/Ds – 1 as a function of sampling rate, Ds/D, indicating a “elbow” about 2%.",
        "However, 2% is too large for high frequency words.",
        "A more reasonable metric is the “coefficient of variation,” cv = SE(ˆa) a .At Web scale (10 billion pages), we expect that a very small sampling rate such as 10-4 or 10-5 will suffice to achieve a reasonable cv (e.g., 0.5).",
        "See Figure 4(b)."
      ]
    },
    {
      "heading": "6 Evaluation",
      "text": [
        "Two sets of experiments were run on a collection of D = 216 web pages, provided by MSN.",
        "The first experiment considered 4 English words shown in Table 2, and the second experiment considers 968 English words with mean df = 2135 and median df = 1135.",
        "They form 468,028 word pairs, with mean co-occurrences = 188 and median = 74."
      ]
    },
    {
      "heading": "6.1 Small Dataset Monte Carlo Experiment",
      "text": [
        "Figure 5 evaluates the various estimate methods by MSE over a wide range of sampling rates.",
        "Doc IDs",
        "but after that there are diminishing returns.",
        "(b): An analysis based on cv = SE a = 0.5 suggests that we can get away with much lower sampling rates.",
        "The three curves plot the critical value for the sampling rate, DsD , asa function of corpus size, D. At Web scale, D Pz� 1010, sampling rates above 10-3 to 10-5 satisfy cv < 0.5, at least for these settings of fx, fy and a.",
        "The settings were chosen to simulate “ordinary” words.",
        "The three curves correspond to three choices of fx: D/100, D/1000, and D/10,000.",
        "fy = fx/10, a = fy/20.",
        "SE is based on (12).",
        "were randomly permuted 105 times.",
        "For each permutation we constructed sketches from the inverted index at a series of sampling rates.",
        "The figure shows that the proposed method, ˆaMLE, is considerably better (by 20% – 40%) than the margin-free baseline, ˆaMF.",
        "Smoothing is effective at low sampling rates.",
        "The recommended approximation, ˆaMLE,a, is remarkably close to the exact solution.",
        "Figure 6 shows agreement between the theoretical and empirical unconditional variances.",
        "Smoothing reduces variances, at low sampling rates.",
        "We used the empirical E � DS ) to compute the theoretical variances.",
        "The approximation, max (sxfx , 3 ) , is"
      ]
    },
    {
      "heading": "6.2 Large Dataset Experiment",
      "text": [
        "The large experiment considers 968 English words (468,028 pairs) over a range of sampling rates.",
        "A floor of 20 was imposed on sample sizes.",
        "As reported in Figure 8, the large experiment confirms once again that proposed method, ˆaMLE, is considerably better than the margin-free baseline (by",
        "a .",
        "The recommended approximation, ˆaMLE,a, is close to ˆaMLE.",
        "Smoothing, ˆaMLE+S, is effective at low sampling rates.",
        "All methods are better than assuming independence (IND).",
        "15% - 30%).",
        "The recommended approximation, ˆaMLE,a, is close to ˆaMLE.",
        "Smoothing, ˆaMLE+S helps at low sampling rates."
      ]
    },
    {
      "heading": "6.3 Rank Retrieval: Top k Associated Pairs",
      "text": [
        "We computed a gold standard similarity cosine ranking of the 468,028 pairs using a 100% sample: cos = a .",
        "We then compared the gold standard to rank-√fx fy ings based on smaller samples.",
        "Figure 9(a) compares the two lists in terms of agreement in the top k. For 3 < k < 200, with a sampling rate of 0.005, the agreement is consistently 70% or higher.",
        "Increasing sampling rate, increases agreement.",
        "The same comparisons are evaluated in terms of precision and recall in Figure 9(b), by fixing the top 1% of the gold standard list but varying the top percentages of the sample list.",
        "Again, increasing sampling rate, increases agreement."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "We proposed a novel sketch-based procedure for constructing sample contingency tables.",
        "The method bridges two popular choices: (A) sampling over documents and (B) sampling over postings.",
        "Well-understood maximum likelihood estimation (MLE) techniques can be applied to sketches (or to traditional samples) to estimate word associations.",
        "We derived an exact cubic solution, ˆaMLE, as well as a quadratic approximation, ˆaMLE,a.",
        "The approximation is recommended because it is close to the exact solution, and easy to compute.",
        "The proposed MLE methods were compared empirically and theoretically to a margin-free (MF) baseline, finding large improvements.",
        "When we know the margins, we ought to use them.",
        "Sample-based methods (MLE & MF) are often better than sample-free methods.",
        "Associations are often estimated without samples.",
        "It is popular to assume independence: (Garcia-Molina et al., 2002, Chapter 16.4), i.e., aˆ ,, ffD .",
        "Independence led to large errors in our experiments.",
        "Not unsurprisingly, there is a trade-off between computational work (space and time) and statistical",
        "errors (divided by the mean co-occurrences, 188).",
        "All curves are averaged over three permutations.",
        "The proposed MLE and the recommended approximation are very close and both are significantly better than the margin-free (MF) baseline.",
        "Smoothing, ˆ6MLE+S, helps at low sampling rates.",
        "All estimators do better than assuming independence.",
        "accuracy (variance or errors); reducing the sampling rate saves work, but costs accuracy.",
        "We derived formulas for variance, showing precisely how accuracy depends on sampling rate.",
        "Sampling methods become more and more important with larger and larger collections.",
        "At Web scale, sampling rates as low as 10-4 may suffice for “ordinary” words.",
        "We have recently generalized the sampling algorithm and estimation method to multi-way associations; see (Li and Church, 2005)."
      ]
    }
  ]
}
