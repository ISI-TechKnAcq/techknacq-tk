{
  "info": {
    "authors": [
      "Guodong Zhou"
    ],
    "book": "SIGHAN Workshop on Chinese Language Processing",
    "id": "acl-W03-1731",
    "title": "Chunking-Based Chinese Word Tokenization",
    "url": "https://aclweb.org/anthology/W03-1731",
    "year": 2003
  },
  "references": [
    "acl-W00-1309"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "This paper introduces a Chinese word tokenization system through HMM-based chunking.",
        "Experiments show that such a system can well deal with the unknown word problem in Chinese word tokenization.",
        "The second term in (2-1) is the mutual information between T\" and G1\" .",
        "In order to simplify the computation of this term, we assume mutual information independence (2-2):"
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Word Tokenization is regarded as one of major bottlenecks in Chinese Language Processing.",
        "Normally, word tokenization is implemented through word segmentation in Chinese Language Processing literature.",
        "This is also affected in the title of this competition.",
        "There exists two major problems in Chinese word segmentation: ambiguity and unknown word detection.",
        "While ngarm modeling and/or word co-ocurrence has been successfully applied to deal with ambiguity problem, unknown word detection has become major bottleneck in word tokenization.",
        "This paper proposes a HMM-based chunking scheme to cope with unkown words in Chinese word tokenization.",
        "The unknown word detection is re-casted as chunking several words (single-character word or multi-character word) together to form a new word."
      ]
    },
    {
      "heading": "2 HMM-based Chunking",
      "text": []
    },
    {
      "heading": "2.1 HMM",
      "text": [
        "Given an input sequence G; = 9192 • • • g\" , the goal of Chunking is to find a stochastic optimal tag sequence T1\" = t1t2 t\" that maximizes (Zhou and Su 2000) (2-1) That is, an individual tag is only dependent on the token sequence G\"1 and independent on other tags in the tag sequence T\" .",
        "This assumption is reasonable because the dependence among the tags in the tag sequence T1\" has already been captured by the first term in equation (2-1).",
        "Applying it to equation (2-1), we have (2-3): From equation (2-3), we can see that: The first term can be computed by applying chain rules.",
        "In ngram modeling, each tag is assumed to be probabilistically dependent on the N-1 previous tags.",
        "• The second term is the summation of log probabilities of all the individual tags.",
        "• The third term corresponds to the “lexical” component (dictionary) of the tagger.",
        "We will not discuss either the first or the second term further in this paper because ngram modeling has been well studied in the literature.",
        "We will focus n on the third term ∑ log Rt; |G1n )."
      ]
    },
    {
      "heading": "2.2 Chinese Word Tokenization",
      "text": [
        "Given the previous HMM, for Chinese word tokenization, we have (Zhou and Su 2002): English characters/Chinese digits/Chinesenumbers).",
        "o Word Formation Pattern(P): Because of the limited number of boundary and word categories, the word formation pattern is added into the structural chunk tag to represent more accurate models."
      ]
    },
    {
      "heading": "3 Context-dependent Lexicons",
      "text": [
        "• g; =< p;, w; >;W1n = w1w2 • • • wn is the word",
        "sequence; P1 = P1P2 Pn is the word formation pattern sequence and p; is the word formation pattern of w; .",
        "Here p; consists of: o The percentage of w; occurring as a whole word (round to 10%) o The percentage of w; occurring at the beginning of other words (round to 10%) o The percentage of w; occurring at the end of other words (round to 10%) o The length of w; o The occurring frequence feature, which is set to max(log(Frequence), 9 ).",
        "• tag t; : Here, a word is regarded as a chunk (called \"Word-Chunk\") and the tags are used to bracket and differentiate various types of Word-chunks.",
        "Chinese word tokenization can be regarded as a bracketing process while differentiation of different word types can help the bracketing process.",
        "For convenience, here the tag used in Chinese word tokenization is called “Word-chunk tag”.",
        "The Word-chunk tag is structural and consists of three parts:",
        "o Boundary category (B): it is a set of four values: 0,1,2,3, where 0 means that current word is a whole entity and 1/2/3 means that current word is at the beginning/in the middle/at the end of a word.",
        "o Word category (W): used to denote the class of the word.",
        "In our system, word is classified into two types: pure Chinese word type and mixed word type (for example, including The major problem with Chunking-based Chinese word tokenization is how to effectively approximate P(t; /Gn .",
        "This can be done by adding lexical entries with more contextual information into the lexicon Φ.",
        "In the following, we will discuss five context-dependent lexicons which consider different contextual information."
      ]
    },
    {
      "heading": "3.1 Context of current word formation pattern and current word",
      "text": [
        "Here, we assume: where = {pwpw ∃C } + {p._ p.∃C} and n w. is a word formation pattern and word pair existing in the training data C ."
      ]
    },
    {
      "heading": "3.2 Context of previous word formation pattern and current word formation pattern",
      "text": [
        "Here, we assume : where = { p - p p - p∃C} + {p p∃C} and is a pair of previous word formation pattern and current word formation pattern existing in the training data C .",
        "accuracy by merging all the above context-dependent lexicons in a single lexicon.",
        "3.3 Context of previous word formation pattern, previous word and current word formation pattern Here, we assume : where",
        "training corpus."
      ]
    },
    {
      "heading": "3.4 Context of previous word formation pattern,",
      "text": [
        "current word formation pattern and current word Here, we assume : where p;_1 p; w; is a triple pattern."
      ]
    },
    {
      "heading": "3.5 Context of previous word formation pattern,",
      "text": [
        "previous word, current word formation pattern and current word Here, the context of previous word formation pattern, previous word, current word formation pattern and current word is used as a lexical entry to determine the current structural chunk tag and (D _ + {p;, p;3C} , where p;_1w;_1p;w; is a pattern existing in the training corpus.",
        "Due to memory limitation, only lexical entries which occurs at least 3 times are kept."
      ]
    },
    {
      "heading": "4 Error-Driven Learning",
      "text": [
        "In order to reduce the size of lexicon effectively, an error-driven learning approach is adopted to examine the effectiveness of lexical entries and make it possible to further improve the chunking For a new lexical entry e; , the effectiveness is measured by the reduction in error which results from adding the lexical entry to the lexicon : .",
        "Here, is the chunking error number of the lexical entry e; for the old lexicon (D and is the chunking error number of the lexical entry e; for the new lexicon (D + 4(D where e; e 4(D ( 4(D is the list of new lexical entries added to the old lexicon (D ).",
        "If F(D (e;) > 0, we define the lexical entry e; as positive for lexicon (D .",
        "Otherwise, the lexical entry e; is negative for lexicon (D ."
      ]
    },
    {
      "heading": "5 Implementation",
      "text": [
        "In training process, only the words occurs at least 5 times are kept in the training corpus and in the word table while those less-freqently occurred words are separated into short words (most of such short words are single-character words) to simulate the chunking.",
        "That is, those less-frequently words are regarded as chunked from several short words.",
        "In word tokenization process, the Chunking-based Chinese word tokenization can be implemented as follows:",
        "1) Given an input sentence, a lattice of word and word formation pattern pair is generated by skimming the sentence from left-to-right, looking up the word table to determine all the possible words, and determining the word formation pattern for each possible word.",
        "2) Viterbi algorithm is applied to decode the lattice to find the most possible tag sequence.",
        "where"
      ]
    },
    {
      "heading": "6 Experimental Results",
      "text": [
        "Table 1 shows the performance of our chunking-based Chinese word tokenization in the competition."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "This paper proposes a HMM-based chunking scheme to cope with the unkown words in Chinese word tokenization.",
        "In the meantime, error-driven learning is applied to effectively incorporate various context-dependent information.",
        "Experiments show that such a system can well deal with the unknown word problem in Chinese word tokenization."
      ]
    }
  ]
}
