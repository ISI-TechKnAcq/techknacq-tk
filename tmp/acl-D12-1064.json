{
  "info": {
    "authors": [
      "Sze-Meng Jojo Wong",
      "Mark Dras",
      "Mark Johnson"
    ],
    "book": "EMNLP",
    "id": "acl-D12-1064",
    "title": "Exploring Adaptor Grammars for Native Language Identification",
    "url": "https://aclweb.org/anthology/D12-1064",
    "year": 2012
  },
  "references": [
    "acl-C08-1118",
    "acl-D07-1072",
    "acl-D10-1028",
    "acl-D11-1131",
    "acl-D11-1148",
    "acl-P05-1022",
    "acl-P10-1117",
    "acl-W07-0602"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "The task of inferring the native language of an author based on texts written in a second language has generally been tackled as a classification problem, typically using as features a mix of n-grams over characters and part of speech tags (for small and fixed n) and un-igram function words.",
        "To capture arbitrarily long n-grams that syntax-based approaches have suggested are useful, adaptor grammars have some promise.",
        "In this work we investigate their extension to identifying n-gram collocations of arbitrary length over a mix of PoS tags and words, using both maxent and induced syntactic language model approaches to classification.",
        "After presenting a new, simple baseline, we show that learned collocations used as features in a maxent model perform better still, but that the story is more mixed for the syntactic language model."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "The task of inferring the native language of an author based on texts written in a second language ?",
        "native language identification (NLI) ?",
        "has, since the seminal work of Koppel et al2005), been primarily tackled as a text classification task using supervised machine learning techniques.",
        "Lexical features, such as function words, character n-grams, and part-of-speech (PoS) n-grams, have been proven to be useful in NLI (Koppel et al2005; Tsur and Rappoport, 2007; Estival et al2007).",
        "The recent work of Wong and Dras (2011), motivated by ideas from Second Language Acquisition (SLA), has shown that syntactic features ?",
        "potentially capturing syntactic errors characteristic of a particular native language ?",
        "improve performance over purely lexical ones.",
        "PoS n-grams can be leveraged to characterise surface syntactic structures: in Koppel et al2005), for example, ungrammatical structures were approximated by rare PoS bigrams.",
        "For the purpose of NLI, small n-gram sizes like bigram or trigram might not suffice to capture sequences that are characteristic of a particular native language.",
        "On the other hand, an attempt to represent these with larger n-grams would not just lead to feature sparsity problems, but also computational efficiency issues.",
        "Some form of feature selection should then come into play.",
        "Adaptor grammars (Johnson, 2010), a hierarchical non-parametric extension of PCFGs (and also interpretable as an extension of LDA-based topic models), hold out some promise here.",
        "In that initial work, Johnson's model learnt collocations of arbitrary length such as gradient descent and cost function, under a topic associated with machine learning.",
        "Hardisty et al2010) applied this idea to perspective classification, learning collocations such as pales-tinian violence and palestinian freedom, the use of which as features was demonstrated to help the classification of texts from the Bitter Lemons corpus as either Palestinian or Israeli perspective.",
        "Typically in NLI and other authorship attribution tasks, the feature sets exclude content words, to avoid unfair cues due to potentially different domains of discourse.",
        "In our context, then, what we are interested in are ?quasi-syntactic collocations?",
        "of either pure PoS (e.g. NN IN NN) or a mixture of PoS with function words (e.g. NN of NN).",
        "The particular question of interest for this paper, then, is to",
        "investigate whether the power of adaptor grammars to discover collocations ?",
        "specifically, ones of arbitrary length that are useful for classification ?",
        "extends to features beyond the purely lexical.",
        "We examine two different approaches in this paper.",
        "We first utilise adaptor grammars for discovery of high performing ?quasi-syntactic collocations?",
        "of arbitrary length as mentioned above and use them as classification features in a conventional maximum entropy (maxent) model for identifying the author's native language.",
        "In the second approach, we adopt a grammar induction technique to learn a grammar-based language model in a Bayesian setting.",
        "The grammar learned can then be used to infer the most probable native language that a given text written in a second language is associated with.",
        "The latter approach is actually closer to the work of Hardisty et al2010) using adaptor grammars for perspective modeling, which inspired our general approach.",
        "This alternative approach is also similar in nature to the work of Bo?rschinger et al2011) in which grounded learning of semantic parsers was reduced to a grammatical inference task.",
        "The structure of the paper is as follows.",
        "In Section 2, we review the existing work of NLI as well as the mechanics of adaptor grammars along with their applications to classification.",
        "Section 3 details the supervised maxent classification of NLI with collocation (n-gram) features discovered by adaptor grammars.",
        "The language model-based classifier is described in Section 4.",
        "Finally, we present a discussion in Section 5 and follow with concluding remarks."
      ]
    },
    {
      "heading": "2 Related Work 2.1 Native Language Identification",
      "text": [
        "Most of the existing research treats the task of native language identification as a form of text classification deploying supervised machine learning approaches.",
        "The earliest notable work in this classification paradigm is that of Koppel et al2005) using as features function words, character n-grams, and PoS bigrams, together with some spelling errors.",
        "Their experiments were conducted on English essays written by authors whose native language one of Bulgarian, Czech, French, Russian, or Spanish.",
        "The corpus used is the first version of International Corpus of Learner English (ICLE).",
        "Apart from investigating lexical features, syntactic features (errors in particular) were highlighted by Koppel et al2005) as potentially useful features, but they only explored this by characterising ungrammatical structures with rare PoS bigrams: they chose 250 rare bigrams from the Brown corpus.",
        "Features for this task can include content words or not: Koppel et al2009), in reviewing work in the general area of authorship attribution (including NLI), discuss the (perhaps unreasonable) advantage that content word features can provide, and comment that consequently they ?are careful .",
        ".",
        ".",
        "to distinguish results that exploit content-based features from those that do not?.",
        "We will not be using content words as features; we therefore note only approaches to NLI that similarly do not use them.",
        "Following Koppel et al2005), Tsur and Rap-poport (2007) replicated their work and hypothesised that word choices in second language writing is highly influenced by the frequency of native language syllables.",
        "They investigated this through measuring classification performance with only character bigrams as features.",
        "Estival et al2007) tackled the broader task of developing profiles of authors, including native language and various other demographic and psychometric author traits, across a smaller set of languages (English, Spanish and Arabic).",
        "To this end, they deployed various lexical and document structure features.",
        "Wong and Dras (2011), starting from the Koppel et al2005) approach, explored the usefulness of syntactic features in a broader sense in which they characterised syntactic errors with cross sections of parse trees obtained from statistical parsers, both horizontal slices of the parse trees in the form of CFG production rules, and the feature schemata used in discriminative parse reranking (Charniak and Johnson, 2005); they also found that using the top 200 PoS bigrams helped.",
        "Their results on the second version of the ICLE corpus, across seven languages (those of Koppel et alplus two Oriental languages, Chinese and Japanese) demonstrated that syntactic features of these kinds lead to significantly better performance than the Koppel et aleatures alone, with a top accuracy (on 5-fold cross-validation) of 77.75%.",
        "Subsequently, Wong et al2011) explored Bayesian topic modeling (Blei et al2003; Griffiths and Steyvers, 2004) as a form of feature dimensionality reduction technique to discover coherent latent factors (?topics?)",
        "that might capture predictive features for individual native languages.",
        "Their topics, rather than the typical word n-grams, consisted of bigrams over (only) PoS.",
        "However, while there was some evidence of topic cluster coherence, this did not improve classification performance.",
        "The work of the present paper differs in that it uses Bayesian techniques to discover collocations of arbitrary length for use in classification, over a mix of both PoS and function words, rather than for use as feature dimensionality reduction."
      ]
    },
    {
      "heading": "2.2 Adaptor Grammars",
      "text": [
        "Adaptor Grammars are a non-parametric extension to PCFGs that are associated with a Bayesian inference procedure.",
        "Here we provide an informal introduction to Adaptor Grammars; Johnson et al. (2007) provide a definition of Adaptor Grammars as a hierarchy of mixtures of Dirichlet (or 2-parameter Poisson-Dirichlet) Processes to which the reader should turn for further details.",
        "Adaptor Grammars can be viewed as extending PCFGs by permitting the grammar to contain an unbounded number of productions; they are non-parametric in the sense that the particular productions used to analyse a corpus depends on the corpus itself.",
        "Because the set of possible productions is unbounded, they cannot be specified by simply enumerating them, as is standard with PCFGs.",
        "Instead, the productions used in an adaptor grammar are specified indirectly using a base grammar: the subtrees of the base grammar's ?adapted non-terminals?",
        "serve as the possible productions of the adaptor grammar (Johnson et al2007), much in the way that subtrees function as productions in Tree Substitution Grammars .1 Another way to view Adaptor Grammars is that they relax the independence assumptions associated with PCFGs.",
        "In a PCFG productions are generated independently conditioned on the parent non-terminal, while in an Adaptor Grammar the probability of generating a subtree rooted in an adapted 1For computational efficiency reasons Adaptor Grammars require the subtrees to completely expand to terminals.",
        "The Fragment Grammars of O?Donnell (2011) lift this restriction.",
        "non-terminal is roughly proportional to the number of times it has been previously generated (a certain amount of mass is reserved to generate ?new?",
        "subtrees).",
        "This means that the distribution generated by an Adaptor Grammar ?adapts?",
        "based on the corpus being generated.",
        "Adaptor Grammars are specified by a PCFG G, plus a subset of G's non-terminals that are called the adapted non-terminals, as well as a discount parameter aA, where 0 ?",
        "aA < 1 and a concentration parameter bA, where b > ?a, for each adapted non-terminal A.",
        "An adaptor grammar defines a two-parameter Poisson-Dirichlet Process for each adapted non-terminal A governed by the parameters aA and bA.",
        "For computational purposes it is convenient to integrate out the Poisson-Dirichlet Process, resulting in a predictive distribution specified by a Pitman-Yor Process (PYP).",
        "A PYP can be understood in terms of a ?Chinese Restaurant?",
        "metaphor in which ?customers?",
        "(observations) are seated at ?tables?, each of which is labelled with a sample from a ?base distribution?",
        "(Pitman and Yor, 1997).",
        "In an Adaptor Grammar, unadapted non-terminals expand just as they do in a PCFG; a production r expanding the non-terminal is selected according to the multinomial distribution ?r over productions specified in the grammar.",
        "Each adapted non-terminalA is associated with its own Chinese Restaurant, where the tables are labelled with subtrees generated by the grammar rooted in A.",
        "In the Chinese Restaurant metaphor, the customers are expansions of A, each table corresponds to a particular subtree expanding A, and the PCFG specifies the base distribution for each of the adapted non-terminals.",
        "An adapted non-terminal A expands as follows.",
        "A expands to a subtree t with probability proportional to nt, where nt is the number of times t has been previously generated.",
        "In addition, A expands using a PCFG rule r expanding A with probability proportional to (mA aA + bA) ?r, where mA is the number of subtrees expanding A (i.e., the number of tables in A's restaurant).",
        "Because the underlying Pitman-Yor Processes have a ?rich get richer?",
        "property, they generate power-law distributions over the subtrees for adapted non-terminals.",
        "With the ability to rewrite non-terminals to entire subtrees, adaptor grammars have been used to extend unigram-based LDA topic models (Johnson, 2010).",
        "This allows topic models to capture sequences of words with abitrary length rather than just unigrams of word.",
        "It has also been shown that it is crucial to go beyond the bag-of-words assumption as topical collocations capture more meaning information and represent more interpretable topics (Wang et al2007).",
        "Taking the PCFG formulation for the LDA topic models, it can be modified such that each topic Topici generates sequences of words by adapting each of the Topici non-terminals (usually indicated with an underline in an adaptor grammar).",
        "The overall schema for capturing topical collocations with an adaptor grammar is as follows:",
        "There is a non-grammar-based approach to finding topical collocations as demonstrated by Wang et al.",
        "(2007).",
        "Both of these approaches learned useful collocations: for instance, as mentioned in Section 1, Johnson (2010) found collocations such gradient descent and cost function associated with the topic of machine learning; Wang et al2007) found the topic of human receptive system comprises of collocations such as visual cortext and motion detector.",
        "Adaptor grammars have also been deployed as a form of feature selection in discovering useful collocations for perspective classification.",
        "Hardisty et al. (2010) argued that indicators of perspectives are often beyond the length of bigrams and demonstrated that the use of the adaptor grammar inferred n-grams of arbitrary length as features establishes the start-of-the-art performance for perspective classification on the Bitter Lemons corpus, depicting two different perspectives of Israeli and Pelestinian.",
        "We are adopting a similar approach in this paper for classifying texts with respect to the author's native language; but the key difference with Hardisty et al. (2010)'s approach is that our focus is on collocations that mix PoS and lexical elements, rather than being purely lexical."
      ]
    },
    {
      "heading": "3 Maxent Classification",
      "text": [
        "In this section, we first explain the procedures taken to set up the conventional supervised classification task for NLI through the deployment of adaptor grammars for discovery of ?quasi-syntactic collocations?",
        "of arbitrary length.",
        "We then present the classification results attained based on these selected sets of n-gram features.",
        "In all of our experiments, we investigate two sets of collocations: pure PoS and a mixture of PoS and function words.",
        "The idea of examining the latter set is motivated by the results of Wong and Dras (2011) where inclusion of parse production rules lexicalised with function words as features had shown to improve the classification performance relative to unlexicalised ones."
      ]
    },
    {
      "heading": "3.1 Experimental Setup",
      "text": [
        "The classification experiments are conducted on the second version of ICLE (Granger et al2009).2 Following our earlier NLI work in Wong and Dras (2011), our data set consists of 490 texts written in English by authors of seven different native language groups: Bulgarian, Czech, French, Russian, Spanish, Chinese, and Japanese.",
        "Each native language contributes 70 out of the 490 texts.",
        "As we are using a relative small data set, we perform k-fold cross-validation, choosing k = 5.",
        "classification We derive two adaptor grammars for the maxent classification setting, where each is associated with a different set of vocabulary (i.e. either pure PoS or the mixture of PoS and function words).",
        "We use",
        "out (personal communication) that there is a subtle issue with ICLE that could have an impact on the classification performance of NLI tasks; in particular, when character n-grams are used as features, some special characters used in some ICLE texts might affect performance.",
        "For our case, this should not be of much issue since they will not appear in our collocations.",
        "the grammar of Johnson (2010) as presented in Section 2.2.2, except that the vocabulary differs: either w ?",
        "Vpos or w ?",
        "Vpos+fw.",
        "For Vpos, there are 119 distinct PoS tags based on the Brown tagset.",
        "Vpos+fw is extended with 398 function words as per Wong and Dras (2011).",
        "m = 490 is the number of documents, and t = 25 the number of topics (chosen as the best performing one from Wong et al2011)).",
        "Rules of the form Docj ?",
        "Docj Topici that encode the possible topics that are associated with a document j are given similar ?",
        "priors as used in LDA (?",
        "= 5/t where t = 25 in our experiments).",
        "Likewise, similar ?",
        "priors from LDA are placed on the adapted rules expanding from Topici ?",
        "Words, representing the possible sequences of words that each topic comprises (?",
        "= 0.01).3 The inference algorithm for the adaptor grammars are based on the Markov Chain Monte Carlo technique made available online by Johnson (2010).4 3.1.3 Classification models with n-gram features Based on the two adaptor grammars inferred, the resulting collocations (n-grams) are extracted as features for the classification task of identifying authors?",
        "native language.",
        "These n-grams found by the adaptor grammars are only a (not necessarily proper) subset of those n-grams that are strongly characteristic of a particular native language.",
        "In principle, one could find all strongly characteristic n-grams by enumerating all the possible instances of n-grams up to a given length if the vocabulary is of a small enough closed set, such as for PoS tags, but this is infeasible when the set is extended to PoS plus function words.",
        "The use of adaptor grammars here can be viewed as a form of feature selection, as in Hardisty et al2010).",
        "Baseline models To serve as a baseline, we take the commonly used PoS bigrams as per the previous work of NLI (Koppel et al2005).",
        "A set of 200 PoS bigrams is selected in two ways: the 200 most frequent in the training data (as in Wong and Dras (2011)) and the 200 with the highest informa",
        "science.mq.edu.au/?mjohnson/Software.htm.",
        "ated in other work).",
        "Enumerated n-gram models Here, we enumerate all the possible n-grams up to a fixed length and select the best of these according to IG, as a generalisation of the baseline.",
        "The first motivation for this feature set is that, in a sense, this should give a rough upper bound for the adaptor grammar's PoS-alone n-grams, as these latter should most often be a subset of the former.",
        "The second motivation is that it gives a robust comparison for the mixed PoS and function word n-grams, where it is infeasible to enumerate all of them.",
        "ENUM-POS We enumerate all possible n-grams up to the length of 5, and select those that actually occur (i.e. of the",
        "this is 218,042 based on the average of 5 folds).",
        "We look at the top n-grams up to length 5 selected by IG: the top 2,800 and the top 6,500 (for comparability with adaptor grammar feature sets, below), as well as the top 10,000 and the top 20,000 (to study the effect of larger feature space).",
        "Adaptor grammar n-gram models The classification features are the two sets of selected collocations inferred by the adaptor grammars which are the main interest of this paper.",
        "AG-POS This first set of the adaptor grammar-inferred features comprise of pure PoS n-grams (i.e. Vpos).",
        "The largest length of n-gram found is 17, but about 97% of the collocations are of length between 2 to 5.",
        "We investigate three variants of this feature set: top 200 n-grams of all lengths (based on IG), all n-grams of all lengths (n = 2, 795 on average), and all n-grams up to the length of 5 (n = 2, 710 on average).",
        "AG-POS+FW This second set of the adaptor grammar-inferred features are mixtures of PoS and function words (i.e. Vpos+fw).",
        "The largest length of n-gram found for this set is 19 and the total number of different collocations found is much higher.",
        "For the purpose of comparability with the first set of adaptor grammar features, we investigate the following five variants for this feature set: top 200 n-grams of all lengths, all n-grams of all lengths (n = 6, 490 on average), all n-grams up to the length of 5 (n = 6, 417 on average), top 2,800 n-grams of all different lengths,",
        "ture sets (with 5-fold cross validation).",
        "and top 2,800 n-grams up to the length of 5.",
        "(All the selections are based on IG).",
        "In our models, all feature values are of binary type.",
        "For the classifier, we employ a maximum entropy (MaxEnt) machine learner ?",
        "MegaM (fifth release) by Hal Daume?",
        "III.5"
      ]
    },
    {
      "heading": "3.2 Classification results",
      "text": [
        "Table 1 presents all the classification results for the individual feature sets, along with the baselines.",
        "On the whole, both sets of the collocations inferred by the adaptor grammars perform better than the two baselines.",
        "We make the following observations: ?",
        "Regarding ENUM-POS as a (rough) upper bound, the adaptor grammar AG-POS with a comparable number of features performs almost as well.",
        "However, because it is possible to enumerate many more n-grams than are found during the sampling process, ENUM-POS opens up a gap over AG-POS of around 4%.",
        "?",
        "Collocations with a mix of PoS and function words do in fact lead to higher accuracy as compared to those of pure PoS (except for the top 200 n-grams); for instance, compare the 2,800 n-grams up to length 5 from the two corresponding sets (71.84 vs. 68.57).",
        "ture sets (with 5-fold cross validation).",
        "aFeatures from the two sets are selected based on the overall top 3700 with highest IG; bfeatures from the two sets are just linearly concatenated.",
        "words (AG-POS+FW) in general perform better than our rough upper bound of PoS collocations, i.e. the enumerated PoS n-grams (ENUM-POS): the overall best results of the two feature sets are 74.49 and 72.44 respectively.",
        "Given that the AG-POS+FW n-grams are capturing different sorts of document characteristics, they could potentially usefully be combined with the PoS-alone features.",
        "We thus combined them with both AG-POS and ENUM-POS feature sets, and the classification results are presented in Table 2.",
        "We tried two ways of integrating the feature sets: one way is to take the overall top 2,800 of the two sets based on IG; the other way is to just combine the two sets of features by concatenation of feature vectors (as indicated by a and b respectively in the result table).",
        "For comparability purposes, we considered only n-grams up to the length of 5.",
        "A baseline approach to this is just to add in function words as unigram features by feature vector concatenation, giving two further models, AG-POS [all ?",
        "5-gram] & FW and ENUM-POS [top2800 ?",
        "5-gram] & FW.",
        "Overall, the classification accuracies attained by the combined feature sets are higher than the individual feature sets.",
        "The best performing of all the models is achieved by combining the mixed PoS and function word collocations with the adaptor grammar-inferred PoS, producing the best accuracy thus far of 75.71.",
        "This demonstrates that features inferred by adaptor grammars do capture some useful information and function words are playing a role.",
        "The way of integrating the two feature sets has different effects on the types of combination.",
        "As seen in Table 2, method a works better for the com",
        "bination of the two adaptor grammar feature sets; whereas method b works better for combining adaptor grammar features with enumerated n-gram features.",
        "Using adaptor grammar collocations also outperforms the alternative baseline of adding in function words as unigrams.",
        "For instance, the best performing combined feature set of both AG-POS and AG-POS+FW does result in higher accuracy as compared to the two alternative baseline models, comparing 75.71 with 72.04 (and 75.71 with 73.67).",
        "This demonstrates that our more general PoS plus function word collocations derived from adaptor grammars are indeed useful, and supports the argument of Wang et al2007) that they are a useful technique for looking into features beyond just the bag of words."
      ]
    },
    {
      "heading": "4 Language Model-based Classification",
      "text": [
        "In this section, we take a language modeling approach to native language identification; the idea here is to adopt grammatical inference to learn a grammar-based language model to represent the texts written by non-English native users.",
        "The grammar learned is then used to predict the most probable native language that a document (a sentence) is associated with.",
        "In a sense, we are using a parser-based language model to rank the documents with respect to native language.",
        "We draw on the work of Bo?rschinger et al.",
        "(2011) for this section.",
        "In that work, the task was grounded learning of a semantic parser.",
        "Training examples there consisted of natural language strings (descriptions of a robot soccer game) and a set of candidate meanings (actions in the robot soccer game world) for the string; each was tagged with a context identifier reflecting the actual action of the game.",
        "A grammar was then induced that would parse the examples, and was used on test data (where the context identifier was absent) to predict the context.",
        "We take a similar approach to developing an grammatical induction technique, although where they used a standard LDA topic model-based PCFG, we use an adaptor grammar.",
        "We expect that the results will likely to be lower than for the discriminative approach of Section 3.",
        "However, the approach is of interest for a few reasons: because, whereas the adaptor grammar plays an ancillary, feature selection role in Section 3, here the feature selection is an organic part of the approach as per the actual implementation of Hardisty et al2010); because adaptor grammars can potentially be extended in a natural way with unlabelled data; and because, for the purposes of this paper, it constitutes a second, quite different way to evaluate the use of n-gram collocations."
      ]
    },
    {
      "heading": "4.1 Language Models",
      "text": [
        "We derive two adaptor grammar-based language models.",
        "One consists of only unigrams and bigrams, and the other finds n-gram collocations, in both cases over either PoS or the mix of PoS and function words.",
        "The assumption that we make is that each document (each sentence) is a mixture of two sets of topics: one is the native language-specific topic (i.e. characteristic of the native language) and the other is the generic topic (i.e. characteristic of the second language ?",
        "English in our case).",
        "The generic topic is thus shared across all languages, and will behave quite differently from a language-specific topic, which is not shared.",
        "In other words, there are eight topics, representing seven native language groups that are of interest (Bulgarian, Czech, French, Russian, Spanish, Chinese, and Japanese) and the second language English itself.6 Bigram models The following rule schema is applicable to both vocabulary types of PoS and the mixture of PoS and function words.",
        "N-gram models The grammar is the same as the above with the exception that the non-terminal Words is now rewritten as follows in order to 6We could just induce a regular PCFG here, rather than an adaptor grammar, by taking as terminals all pairs of PoS tags.",
        "We use the adaptor grammar formulation for comparability.",
        "capture n-gram collocations of arbitrary length."
      ]
    },
    {
      "heading": "Words?Words Word Words?Word",
      "text": [
        "It should be noted that the two grammars above can in theory be applied to an entire document or on individual sentences.",
        "For this present work, we work on the sentence level as the runtime of the current implementation of the adaptor grammars grows proportional to the cube of the sentence length.",
        "For each grammar we try both sparse and uniform Dirichlet priors (?",
        "= {0.01, 0.1, 1.0}).",
        "The sparse priors encourage only a minority of the rules to be associated with high probabilities."
      ]
    },
    {
      "heading": "4.2 Training and Evaluation",
      "text": [
        "As we are using the same data set as per the previous approach, we perform 5-fold cross validation as well.",
        "However, the training for each fold is conducted with a different grammar consisting of only the vocabulary that occur in each training fold.",
        "The reason is that we are now having a form of supervised topic models where the learning process is guided by the native languages.",
        "Hence, each of the training sentences are prefixed with the (native) language identifiers lang, as seen in the Root rules of the grammar presented above.",
        "To evaluate the grammars learned, as in Bo?rschinger et al2011) we need to slightly modify the grammars above by removing the language identifiers ( lang) from theRoot rules and then parse the unlabeled sentences using a publicly available CKY parser.",
        "The predicted native language is inferred from the parse output by reading off the langTopics that the Root is rewritten to.",
        "We take that as the most probable native language for a particular test sentence.",
        "At the document level, we select as the class the language predicted for the largest number of sentences in that document."
      ]
    },
    {
      "heading": "4.3 Parsing Results",
      "text": [
        "Tables 3 and 4 present the parsing results at the sentence level and the document level, respectively.",
        "On the whole, the results at the sentence level are much poorer as compared to those at the document level.",
        "In light of the results of Section 3.2, it is surprising",
        "based on parsing (at the document level).",
        "that bigram models appear to perform better than n-gram models for both types of vocabulary, with the exception of AG-POS+FW at the document level.",
        "In fact, one would expect n-gram models to perform better in general as it is a generalisation that would contain all the potential bigrams.",
        "Nonetheless, the language models over the mixture of PoS and function words appear to be a more suitable representative of our learner corpus as compared to those over purely PoS, confirming the usefulness of integrated function words for the NLI classification task.",
        "It should also be noted that sparse priors generally appear to be more appriopriate; except that for AG-POS+FW n-grams, uniform priors are indeed better and resulted in the highest parsing result of 50.15.",
        "(Although all the parsing results are much weaker as compared to the results presented in Section 3.2, they are all higher than the majority baseline of 14.29% i.e. 70/490)."
      ]
    },
    {
      "heading": "5 Discussion",
      "text": [
        "Here we take a closer look at how well each approach does in identifying the individual native languages.",
        "The confusion matrix for the best model of two approaches are presented in Table 5 and Table 6.",
        "Both approaches perform reasonably well for the two Oriental languages (Chinese in particular); this is not a major surprise, as the two languages are not part of the language family that the rest of the languages come from (i.e. Indo-European).",
        "Under the supervised maxent classification, misclassifications largely are observed in the Romance ones (French and Spanish) as well as Russian; for the language model-based approach, Bulgarian is identi",
        "performing model under language modeling setting (BL:Bulgarian, CZ:Czech, RU:Russian, FR:French, SP:Spanish, CN:Chinese, JP:Japanese).",
        "fied poorly, and Spanish moreso.",
        "However, the latter approach appears to be better in identifying Czech.",
        "On the whole, the maxent approach results in much fewer misclassifications compared to its counterpart.",
        "In fact, there is a subtle difference in the experimental setting of the models derived from the two approaches with respect to the adaptor grammar: the number of topics.",
        "Under the maxent setting, the number of topics t was set to 25, while we restricted the models with the language modeling approach to only eight topics (seven for the individual native languages and one for the common second language, English).",
        "Looking more deeply into the topics themselves reveals that there appears to be at least two out of the 25 topics (from the supervised models) associated with n-grams that are indicative of the native languages, taking Chinese and Japanese as examples (see the associated topics in Table 7).8 Perhaps associating each native language with only one generalised topic is not sufficient.",
        "Furthermore, the distribution of n-grams among the topics (i.e. subtrees of collocations derived from the adaptor grammars) are quite different between the two approaches although the total num8Taking the examples from Wong et al2011) as reference, we found similar n-grams that are indicative of Japanese and Chinese.",
        "the 25 topics representative of Japanese and Chinese (under maxent setting).",
        "N-grams of pronoun with verb are found at the upper end of Topic2 and Topic23 reflecting the frequent usage of Japanese; n-grams of noun are top n-grams under Topic9 and Topic17 indicating Chinese's common error of determiner-noun disagreement.",
        "ber of n-grams inferred by each approach is about the same.",
        "For the language modeling ones, a high number of n-grams were associated with the generic topic nullTopic9 and each language-specific topic langTopic has a lower number of n-grams relative to bi-grams (Table 8) associated with it.",
        "For the maxent models, in contrast, the majority of the topics were associated with a higher number of n-grams (Table 9).",
        "The smaller number of n-grams to be used as features ?",
        "and the fact that their extra length means that they will occur more sparsely in the documents ?",
        "seems to be the core of the problem.",
        "Nonetheless, the language models inferred discover relevant n-grams that are representative of individual native languages.",
        "For instance, the bigram NN NN, which Wong and Dras (2011) claim may reflect the error of determiner-noun disagreement commonly found amongst Chinese learners, was found under the Chinese topic at the top-2 position with a probability of 0.052 as compared to the other languages at the probability range of 0.0005- 0.003.",
        "Similarly, one example for Japanese, the mixture bigram PPSS think, indicating frequent usage of pronouns within Japanese was seen under the Japanese topic at the top-9 position with a probability of 0.025 in relation to other languages within the range of 0.0002-0.006: this phenomenon as char-9This is quite plausible as there should be quite a number of structures that are representative of native English speakers that are shared by non-native speakers."
      ]
    },
    {
      "heading": "Languages Excerpts from ICLE",
      "text": [
        "Chinese ... the overpopulation problem in urban area ... ...",
        "The development of country park can directly ... ... when it comes to urban renewal project ... ... As developing new town in ... ... and reserve some country park as ... Japanese ...",
        "I think many people will ... ...",
        "I think governments should not ... ...",
        "I think culture is the most significant ... ...",
        "I think the state should not ... ...",
        "I really think we must live ... Table 10: Excerpts from ICLE illustrating the common phenomena observed amongst Chinese and Japanese.",
        "acteristic of Japanese speakers has also been noted for different corpora by Ishikawa (2011).",
        "(Note that this collocation as well as its pure PoS counterpart PPSS VB are amongst the top n-grams discovered under the maxent setting as seen in Table 7.)",
        "Table 10 presents some excerpts extracted from the corpus that illustrate these two common phenomena.",
        "To investigate further the issue associated with the number of topics under the language modeling setting, we attempted to extend the adaptor grammar with three additional topics that represent the language family of the seven native languages of interest: Slavic, Romance, and Oriental.",
        "(The resulting grammar is presented as below.)",
        "However, the parsing result does not improve over the initial setting with eight topics in total.",
        "Root?",
        "lang langTopics langTopics?",
        "langTopics langTopic langTopics?",
        "langTopics familyTopic langTopics?",
        "langTopics nullTopic"
      ]
    },
    {
      "heading": "6 Conclusion and Future Work",
      "text": [
        "This paper has shown that the extension of adaptor grammars to discovering collocations beyond the lexical, in particular a mix of PoS tags and function words, can produce features useful in the NLI classification problem.",
        "More specifically, when added to a new baseline presented in this paper, the combined feature set of both types of adaptor grammar inferred collocations produces the best result in the context of using n-grams for NLI.",
        "The usefulness of the collocations does vary, however, with the technique used for classification.",
        "Future work will involve a broader exploration of the parameter space of the adaptor grammars, in particular the number of topics and the value of ?",
        "; a look at other non-parametric extensions of PCFGs, such as infinite PCFGs (Liang et al2007) for finding a set of non-terminals permitting more fine-grained topics; and an investigation of how the approach can be extended to semi-supervised learning to take advantage of the vast quantity of texts with errors available on the Web."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to acknowledge the support of ARC Linkage Grant LP0776267.",
        "We also thank the anonymous reviewers for useful feedback.",
        "Much gratitude is due to Benjamin Bo?rschinger for his help with the language modeling implementation."
      ]
    }
  ]
}
