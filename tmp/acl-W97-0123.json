{
  "info": {
    "authors": [
      "Takehito Utsuro",
      "Takashi Miyata"
    ],
    "book": "Workshop on Very Large Corpora",
    "id": "acl-W97-0123",
    "title": "Maximum Entropy Model Learning of Subcategorization Preference",
    "url": "https://aclweb.org/anthology/W97-0123",
    "year": 1997
  },
  "references": [
    "acl-A97-1053",
    "acl-C96-1004",
    "acl-H93-1054",
    "acl-J96-1002",
    "acl-P93-1005",
    "acl-P95-1037",
    "acl-P96-1025"
  ],
  "sections": [
    {
      "heading": "URL: http : //cactus .aist -n.ara.ac . jp/staff/ut surahome-e.html Abstract",
      "text": [
        "This paper proposes a novel method for learning probabilistic models of subcategorization preference of verbs.",
        "Especially, we propose to consider the issues of case dependencies and noun class generalization in a uniform way.",
        "We adopt the maximum entropy model learning method and apply it to the task of model learning of subcategorization preference.",
        "Case dependencies and noun class generalization are represented as features in the maximum entropy approach.",
        "The feature selection facility of the maximum entropy model learning makes it possible to find optimal case dependencies and optimal noun class generalization levels.",
        "We describe the results of the experiment on learning probabilistic models of subcategorization preference from the EDR Japanese bracketed corpus.",
        "We also evaluated the performance of the selected features and their estimated parameters in the subcategorization preference task."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In corpus-based NLP, extraction of linguistic knowledge such as lexical/semantic collocation is one of the most important issues and has been intensively studied in recent years.",
        "In those research, extracted lexical/semantic collocation is especially useful in terms of ranking parses in syntactic analysis as well as automatic construction of lexicon for NLP.",
        "For example, in the context of syntactic disambiguation, Black (1993) and Magerraan (1995) proposed statistical parsing models based-on decision-tree learning techniques, which incorporated not only syntactic but also lexical/semantic information in the decision-trees.",
        "As lexical/semantic information, Black (1993) used about 50 semantic categories, while Magerman (1995) used lexical forms of words.",
        "Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between headwords in the parse tree.",
        "In those works, lexical/semantic collocation are used for ranking parses in syntactic analysis.",
        "They put an assumption that syntactic and lexical/semantic features are dependent on each other.",
        "In their models, syntactic and lexical/semantic features are combined together, and this causes each parameter to depend on both syntactic and lexical/semantic features.",
        "On the other hand, in the context of automatic lexicon construction, the emphasis is mainly on the extraction of lexical/semantic collocational knowledge of specific words rather than its use in sentence parsing.",
        "For example, Haruno (1995) applied an information-theoretic data compression technique to corpus-based case frame learning, and proposed a method of finding case frames of verbs as compressed representation of verb-noun collocational data in corpus.",
        "The work concentrated on the extraction of declarative representation of case frames and did not consider their performance in sentence parsing.",
        "As in the case of the models of Black (1993), Magerman (1995), and Collins (1996), this paper proposes a method of utilizing lexical/semantic features for the purpose of applying them to ranking parses in syntactic analysis.",
        "However, unlike the models of Black (1993), Magerman (1995), and Collins (1996), we put an assumption that syntactic and lexical/semantic features are independent.",
        "Then, we focus on extracting lexical/semantic collocational knowledge of verbs which is useful in syntactic analysis.",
        "More specifically, we propose a novel method for learning a probabilistic model of subcategorization preference of verbs.",
        "In general, when learning lexical/semantic collocational knowledge of verbs from corpus, it is necessary to consider the following two issues:",
        "1) Case dependencies 2) Noun class generalization",
        "When considering 1), we have to decide which cases are dependent on each other and which cases are optional and independent of other cases.",
        "When considering 2), we have to decide which superordinate class generates each observed leaf class in the verb-noun collocation.",
        "So far, there exist several researches which worked on these two issues in learning collocational knowledge of verbs and also evaluated the results in terms of syntactic disambiguation.",
        "Resnik (1993) and Li and Abe (1995) studied how to find an optimal abstraction level of an argument noun in a tree-structured thesaurus.",
        "Although they evaluated the obtained abstraction level of the argument noun by its performance in syntactic disambiguation, their works are limited to only one argument.",
        "Li and Abe (1996) also studied a method for learning dependencies between case slots and evaluated the discovered dependencies in the syntactic disambiguation task.",
        "They first obtained optimal abstraction levels of the argument nouns by the method in Li and Abe (1995), and then tried to discover dependencies between the class-based case slots.",
        "They reported that dependencies were discovered only at the slot-level and not at the class-leveL Compared with those previous works, this paper proposes to consider the above two issues in a uniform way.",
        "First, we introduce a model of generating a collocation of a verb and argument/adjunct nouns and then view the model as a probabilistic model.",
        "As a model learning method, we adopt the maximum entropy model learning method (Della Pietra, Della Pietra, and Lafferty, 1997; Berger, Della Pietra, and Della Pietra, 1996) and apply it to the task of model learning of subcategorization preference.",
        "Case dependencies and noun class generalization are represented as features in the maximum entropy approach.",
        "In the maximum entropy approach, features are allowed to have overlap and this is quite advantageous when we consider case dependencies and noun class generalization in parameter estimation.",
        "The feature selection facility of the maximum entropy model learning method also makes it possible to find optimal set of features, i.e, optimal case dependencies and optimal noun class generalization levels.",
        "We introduce several different models according to the difference of case dependencies.",
        "We describe the results of the experiment on learning models of subcategorization preference from the EDR Japanese bracketed corpus (EDR, 1995).",
        "We also evaluate the performance of the selected features and their estimated parameters in the subcategorization preference task."
      ]
    },
    {
      "heading": "2 A Model of Generating a Verb-Noun Collocation from Subcategorization Frame(s)",
      "text": [
        "This section introduces a model of generating a verb-noun collocation from subcategorization frame (s) ."
      ]
    },
    {
      "heading": "2.1 Data Structure",
      "text": [
        "Verb-noun collocation is a data structure for the collocation of a verb and all of its argument/adjunct nouns.",
        "A verb-noun collocation e is represented by a feature structure which consists of the verb v and all the pairs of co-occurring case-markers p and thesaurus classes c of case-marked nouns:"
      ]
    },
    {
      "heading": "Pk Ck",
      "text": [
        "We assume that a thesaurus is a tree-structured type hierarchy in which each node represents a semantic class, and each thesaurus class , chin a verb-noun collocation is a leaf class.",
        "We also introduce -<e as the superordinate-subordinate relation of classes in a thesaurus: c1 c2 means that c1 is subordinate to c2- 1"
      ]
    },
    {
      "heading": "2.1.2 Subcategorization Frame",
      "text": [
        "A subcategorization frame s is represented by a feature structure which consists of a verb v and the pairs of case-markers p and sense restriction c of case-marked argument/adjunct nouns:",
        "Sense restriction ,c of case-marked argument/adjunct nouns are represented by classes at arbitrary levels of the thesaurus.",
        "A subcategorization frame a can be divided into two parts: one is the verbal part ay containing the verb v while the other is the nominal part sp containing all the pairs of case-markers p and sense restriction c of case-marked nouns."
      ]
    },
    {
      "heading": "2.1.3 Subsumption Relation",
      "text": [
        "We introduce subsumption relation -.<3.f of a verb-noun collocation e and a subcategorizatiora frame s: e s iff.",
        "for each case-marker pi in s and its noun class csi, there exists the same case-marker pi in e and its noun class cei is subordinate to cei, i.e. cei The subsumption relation --<sf is applicable also as a subsumption relation of two subcategorization frames."
      ]
    },
    {
      "heading": "2.2 Generating a Verb-Noun Collocation from Subcategorization Frame(s)",
      "text": [
        "Next, let us consider modeling the generation of a verb-noun collocation from a subcategorization frame.",
        "Especially, we describe the basic idea of incorporating case dependencies and noun class generalization into the model of generating a verb-noun collocation from a subcategorization frame."
      ]
    },
    {
      "heading": "Suppose a verb-noun collocation e is given as:",
      "text": [
        "pred :",
        "• 41"
      ]
    },
    {
      "heading": "C =",
      "text": []
    },
    {
      "heading": "Pk Cek",
      "text": [
        "'Although we ignore sense ambiguities of case-marked nouns in the definitions of this section, in the current implementation, we deal with sense ambiguities of case-marked nouns by deciding that a class c is superordinate to an ambiguous leaf class C1 if c is superordinate to at least one of the possible unambiguous classes of",
        "Then, we consider a subcategorization frame s which can generate e and assume that s subsumes e:"
      ]
    },
    {
      "heading": "e – Th<sf",
      "text": [
        "We denote the generation of the verb-noun collocation e from the subcategorization frame s as:"
      ]
    },
    {
      "heading": "s e (4) 2.2.1 Case Dependencies When considering a subcategorization frame which can generate a verb-noun collocation e, there",
      "text": [
        "are several possibilities of the case dependencies in the subcategorization frame.",
        "For example, consider the following example: Example 1 Kodomo-ga kouen-de juusu-wo nomu."
      ]
    },
    {
      "heading": "child-NOM park-at juice-ACC drink (A child drinks juice at the park.)",
      "text": [
        "The verb-noun collocation is represented as a feature structure e below:"
      ]
    },
    {
      "heading": "In this feature structure e, cc, ep, and ci represent the leaf classes (in the thesaurus) of the nouns",
      "text": [
        "\"kodomo(child)\", lconen(park)\", and \"juusu(juice)\".",
        "Next, we assume that the concepts 'human\", \"place', and 'beverage\" are superordinate to \"kodomo(child)\", louen(park)\", and ajuusu(juice)\", respectively, and introduce the corresponding classes chum, cpic, and cbc„.",
        "Then, the following superordinate-subordinate relations hold:"
      ]
    },
    {
      "heading": "Cc -Lic chum, op --5.c Cptc) Cj 7-4c Cbev",
      "text": [
        "Allowing these superordinate classes as sense restriction in subcategorization frames, let us consider several patterns of sub categorization frames each of which can generate the verb-noun collocation e. Those patterns of subcategorization frames vary according to the dependencies of cases within them.",
        "If the three cases \"ga(NOM)\", \"tvo(ACC)\", and 'de(at)\" are dependent on each other and it is not possible to find any division into several independent subcategorization frames, e can be regarded as generated from a subcategorization frame containing all of the three cases:",
        "Otherwise, if only the two cases \"ga(VOM)\" and \"wo(A CC)\" are dependent on each other and the \"de(at)\" case is independent of those two cases, e can be regarded as generated from the following two sub categorization frames independently: [pred : nomu ga : Chum --1. e , wo : cbev Otherwise, if all the three cases \"ga(NOM) \", qtvo(A CC)\", and 'cie(at)\" are independent of each other, e can be regarded as generated from the following three subcategorization frames independently, each of which contains only one case:",
        "In the similar way, when considering a subcategorizat ion frame which can generate a verb-noun collocation e, there are several possibilities of the noun class generalization levels as the sense restrictions of the case-marked nouns.",
        "For example, let us again consider Example 1.",
        "We assume that the concepts \"animal\" and 'liquid\" are superordinate to \"human\" and \"beverage\", respectively, and introduce the corresponding classes cani and clic,.",
        "Then, the following superordinate-subordinate relations hold: chum -4.",
        "Coati, Cbet, c gig If we additionally allow these superordinate classes as sense restriction in subcategorization frames, we can consider several additional patterns of subcategorization frames which can generate the verb-noun collocation e, along with those patterns described in the previous section.",
        "Suppose that only the two cases \"ga(NOM)\" and \"?.00(A CO\" are dependent on each other and the \"de(at)\" case is independent of those two cases as in the formula (7).",
        "Since the leaf class ce ( \"child\") can be generated from either chum or cani, and also the leaf class ci (\"juice\") can be generated from either cbe,, or cal, e can be regarded as generated according to either of the four formulas (the left-side formula of) (7) and (9): {pred:nornu e [pred: mama pred: 71,0MU (9) ga: cond.",
        "ga Chum ga cani e WO: Chet, WO:Q4"
      ]
    },
    {
      "heading": "2.3 Case Dependencies and the Design of the Generation Models",
      "text": [
        "As we described in the previous section, there are several possibilities of the case dependencies in a verb-noun collocation, and this results in the differences of the sub categorization frames which can generate the given verb-noun collocation.",
        "According to the different assumptions on the case dependencies, we can design several different models of generating a verb-noun collocation from subcategorization frame(s).",
        "First, we put no assumption on the case dependencies in the given verb-noun collocation e, and assume that any subcategorization frame s which subsumes e can generate e. e With this requirement, the subcategorization frame s does not have to have all the cases in e, but has to have only some part of the cases in e. We call the model satisfying this requirement the partial-frame model.",
        "All the examples of the formulas (6) and (9) satisfy this requirement and can be regarded as examples of the partial-frame model.",
        "Next, in addition to the requirement that s subsumes e, we put another assumption that all the cases in the given verb-noun collocation e are dependent on each other and that a subcategorization frame s which can generate e should have exactly the same cases as e has:",
        "C p red: v 8= [pIPT ed.",
        "ci.",
        ": iv (10) • : Pk : d k 7 • : ek",
        "We call the model satisfying this requirement as the one-frame model.",
        "For example, supposing that the verb-noun collocation e in the equation (5) is given, the example in the formula (6) satisfies this requirement."
      ]
    },
    {
      "heading": "2.3.3 Independent-Case Model",
      "text": [
        "In addition to the requirement that s subsumes e, we can also put an assumption that all the cases in the given verb-noun collocation e are independent of each other and that a subcategorization frame s which has only one case of e can generate e:",
        "We call the model satisfying this requirement as the independent-case model.",
        "For example, supposing that the verb-noun collocation e in the equation (5) is given, the examples in the formula (8) satisfy this requirement."
      ]
    },
    {
      "heading": "2.3.4 Independent-Frame Model",
      "text": []
    },
    {
      "heading": "As can be seen in the definitions of the above three models, the basic idea of defining the model of generating a verb-noun collocation from subcategorization frame(s) lies in identifying the dependencies of the cases in the given verb-noun collocation and expressing the dependencies within",
      "text": [
        "a subcategorization frame.",
        "Here, we briefly show a method of statistically identifying the dependencies of the cases in verb-noun collocations from corpus.",
        "Then, by incorporating the identified case dependencies into the generation model, we introduce a model of generating a verb-noun collocation from a tuple of independent partial subcategorization frames.",
        "We call this model as the independent-frame model."
      ]
    },
    {
      "heading": "Partial Subcategorization Frame",
      "text": [
        "Suppose a verb-noun collocation e is given as in the formula (10) and a subcategorization frame s satisfies the requirement of the one-frame model in section 2.3.2, i.e., as in the formula (10), s has exactly the same case-markers as e has, and s subsumes e. Then, we define a partial subcategorization frame si of s as a subcategorization frame which has the same verb v as s as well as some of the case-markers of s and their semantic classes.",
        "Then, we can find a division of s into a tuple (Si, , sn) of partial subcategorization frames of s, where any pair si and (i i') do not have common case-markers and the unification si A - A s„ of all the partial subcategorization frames equals to s:",
        "Then, we introduce a parameter a (0 < a <1) for relaxing the constraint of independence.",
        "Partial subcategorization frames si, , snare judged as independent if, for every subset , sij of j of these partial subcategorization frames (j = 2, ... , n), the following inequalities hold:",
        "This definition of independence judgment means that the condition on independence judgment becomes weaker as a decreases, while it becomes more strict as a increases.",
        "Generation from Independent Partial Subcategorization Frames Now, we denote the generation of e from a tuple (Si, sn) of independent partial subcategorization frames of s as below:",
        "For example, suppose that a verb-noun collocation e is given as in the formula (5) in section 2.2.1.",
        "If the three cases in e are dependent on each other as in the generation of e in the formula (6), the generation of e is denoted as below in the case of the independent-frame model:"
      ]
    },
    {
      "heading": "3 Maximum Entropy Modeling",
      "text": [
        "This section gives a formal description of maximum entropy modeling (Della Pietra, Della Pietra, and Lafferty, 1997; Berger, Della Pietra, and Della Pietra, 1996)."
      ]
    },
    {
      "heading": "3.1 The Maximum Entropy Principle",
      "text": [
        "We consider a random process that produces an output value y, a member of a finite set Y.",
        "In generating y, the process may be influenced by some contextual information x, a member of a finite set X.",
        "Our task is to construct a stochastic model that accurately represents the behavior of the random process.",
        "Such a model is a method of estimating the conditional probability that, given a context x, the process will output y.",
        "We denote by p(y I x) the probability that the model assigns to y in context m. We also denote by P the set of all conditional probability distributions.",
        "Thus a model p(y I x) is an element of P. To study the process, we observe the behavior of the random process by collecting a large number of samples of the event (x, y).",
        "We can summarize the training sample in terms of its empirical probability distribution 23, defined by:",
        "where freq(x, y) is the number of times that the pair (x, y) occurs in the sample.",
        "Next, in order to express certain features of the whole event (x, y), a binary-valued indicator function is introduced and called a feature function.",
        "Usually, we suppose that there exists a large collection .7 of candidate features, and include in the model only a subset $ of the full set of candidate features 2.",
        "We call S the set of active features.",
        "The choice of S must capture as much information about the random process as possible, yet only include features whose expected values can be reliably estimated.",
        "In this section and the next section, we assume that the set S of active features can be found in some way.",
        "How to find S will be described in section 3.3.",
        "Now, we assume that S contains n feature functions.",
        "For each feature fi(E 8), the sets Vzi and V* will be given for indicating the sets of the values of x and y for that feature.",
        "According to those sets, each feature function A will be defined as follows:",
        "When we discover a feature that we feel is useful, we can acknowledge its importance by requiring that our model accord with the feature's empirical distribution.",
        "In maximum entropy modeling approach, this is done by constraining that the expected value of each f with respect to the model p(yI x) (left-hand side) be the same as that of fi in the training sample (right-hand side):",
        "This requirement is called a constraint equation.",
        "This requirement means that we would like p to lie in the subset of P. Then, among the possible models p, the philosophy of the maximum entropy modeling approach is that we should select the most uniform distribution.",
        "A mathematical measure of the uniformity of a conditional distribution p(y x) is provided by the conditional entropy:",
        "Now, we present the principle of maximum entropy:"
      ]
    },
    {
      "heading": "Maximum Entropy Principle",
      "text": [
        "To select a model from a set of allowed probability distributions, choose the model p,,, with maximum entropy H (p):"
      ]
    },
    {
      "heading": "3.2 Parameter Estimation",
      "text": [
        "It can be shown that there always exists a unique model p. with maximum entropy in any constrained set.",
        "According to Della Pietra, Della Pietra, and Lafferty (1997) and Berger, Della Pietra, and Della Pietra (1996), the solution can be found as the following p), (y I x) of the form of the exponential family:",
        "where a parameter Ai is introduced for each feature Della Pietra, Della Pietra, and Lafferty (1997) and Berger, Della Pietra, and Della Pietra (1996) also presented an optimization method of estimating the parameter values A*i that maximize the entropy, which is called Improved Iterative Scaling (IS) algorithm."
      ]
    },
    {
      "heading": "3.3 Feature Selection",
      "text": [
        "Given the fall set .F of candidate features, this section outlines how to select an appropriate subset S of active features.",
        "The feature selection process is an incremental procedure that builds up S by successively adding features_ At each step, we select the candidate feature which, when adjoined to the set of active features 8, produces the greatest increase in log-likelihood of the training sample:3"
      ]
    },
    {
      "heading": "4 Maximum Entropy Model Learning of Subcategorization Preference",
      "text": [
        "This section describes how to apply the maximum entropy modeling approach to the task of model learning of subcategorization preference."
      ]
    },
    {
      "heading": "4.1 Events",
      "text": [
        "In our task of model learning of subcategorization preference, each event (x, y) in the training sample is a verb-noun collocation e, which is defined as in the formula (1).",
        "As well as a subcategorization frame, a verb-noun collocation e can be divided into two parts: one is the verbal part et, containing the verb v while the other is the nominal part ep containing all the pairs of case-markers p and thesaurus leaf classes c of case-marked nouns:",
        "Then, we define the context x of an event (x, y) as the verb v and the output y as the nominal part ep of e and each event in the training sample is denoted as (v, ep):"
      ]
    },
    {
      "heading": "4.2 Features",
      "text": [
        "Each (partial) subcategorization frame is represented as a feature in the maximum entropy modeling approach.",
        "In the case of the partial-frame/one-frame/independent-case models in the sections 2.3.1 2.3.3, a binary-valued feature function fs(v, ep) is defined for each subcategorization frame s. In the case of the independent-frame model in section 2.3.4, a binary-valued feature function hi (v, ep) is defined for each partial subcategorization frames si in the tuple of the formula (14).",
        "Each feature function f has its own parameter A, which is also the parameter of the corresponding (partial) subcategorization frame.",
        "According to the possible variations of case dependencies and noun class generalization, we consider every possible patterns of subcategorization frames which can generate a verb-noun collocation, and then construct the full set .F of candidate features.",
        "In the following, we give formal definitions of the features in each of the partial-frame/oneframe/independent-case/independent-frame models which we introduced in section 2.3.",
        "Each feature function corresponds to a subcategorization frame s. For each subcategorization frame s, a binary-valued feature function fs(v, ep) is defined to be true if and only if the given verb-noun collocation e is subsumed by s:"
      ]
    },
    {
      "heading": "4.2.2 One-Frame Model",
      "text": [
        "Each feature function correspondi-to a subcategorization frame s which has exactly the same cases as the given verb-noun collocation e has.",
        "For each subcategorization frame s, a binary-valued feature function 19(v, er) is defined to be true if and only if the given verb-noun collocation e has exactly the same cases as $ has and is also subsumed by s:"
      ]
    },
    {
      "heading": "4.2.3 Independent-Case Model",
      "text": [
        "Each feature function corresponds to a subcategorization frame s which has only one case of the given verb-noun collocation e. For each subcategorization frame s which has only one case, a binary-valued feature function fs(v, ep) is defined to be true if and only if the given verb-noun collocation e has the same case and is also subsumed by s:",
        "Each feature function corresponds to a partial subcategorization frames si in the tuple of independent partial subcategorization frames which can generate the given verb-noun collocation.",
        "First, for the given verb-noun collocation e, tuples of independent partial subcategorization frames which can generate e are collected into the set SF(e) as below:4 5",
        "Then, for each partial subcategorization frame s, a binary-valued feature function Is(v, , el?)",
        "is defined to be true if and only if at least one element of the set SF(e) is a tuple (Si,., s, • - sn) that contains s"
      ]
    },
    {
      "heading": "4.3 Parameter Estimation",
      "text": [
        "Let E be the training corpus consisting of training events of the form (v, ep).",
        "Let Y be the full set of candidate features each element of which corresponds to a possible subcategorization frame.",
        "Then, given the empirical distribution 23(v, ep) of the training sample, the set S(C ./) of active features is found according to the feature selection algorithm in section 3.3, and the parameters of subcategorization frames are estimated according to US Algorithm(Della Pietra, Della Pietra, and Lafferty, 1997; Berger, Della Pietra, and Della Pietra, 1996).",
        "Finally, the conditional probability distribution ps(ep 1 v) is estimated."
      ]
    },
    {
      "heading": "4.4 Subcategorization Preference in Parsing a Sentence",
      "text": [
        "Suppose that, after estimating parameters of subcategorization preference from the training corpus E of verb-noun collocations, we obtain the set S of active features and the model ps(ep 1 v) incorporating these features.",
        "Now, we describe how to rank parse trees of a given input sentence according to the estimated parameters of subcategorization preference of verbs.",
        "4More precisely, for a tuple (si , , sn) of independent partial subcategorization frames to be included in the set SF(e), the following requirement has to be satisfied: it is not possible to divide any of the partial frames Si,., sn into more than one frame and to construct a finer-grained tuple (4,...,s,c,...,s'n+k) of independent partial subcategorization frames.",
        "5When applying the learned probabilistic model to the held-out test event e\", independence of the partial subcategorization frames are judged using the probabilities of partial subcategorization frames estimated from the training data (as described in section 2.3.4), then the set SF(e3) is constructed."
      ]
    },
    {
      "heading": "4.4.1 Basic Model",
      "text": [
        "Let w be the given input sentence, T(w) be the set of parse trees of w, t be a parse tree in T(w), E(t) be the set of verb-noun collocations contained in t. Then, each parse tree is assigned the product of all the conditional probabilities ps /epts k I v) of verb-noun collocations (v, epts) within it, which is denoted by OW:"
      ]
    },
    {
      "heading": "4.4.2 Heuristics of Case Covering",
      "text": [
        "Along with the estimated conditional probabilities ps(_pts z v) and the basic model above, we consider a heuristics concerning covering of the cases of verb-noun collocations as below and evaluate their effectiveness in the experiments of the next section.",
        "Let (v, etps) be a test event which is not included in the training corpus E (i.e., (v, er) E).",
        "Subcategorization preference of test events is determined according to whether each case p (and the leaf class marked by p) of epts is covered by at least one feature in S. More formally, we introduce case covering relation -<,.„ of a verb-noun collocation (v, ep) and a feature set S: (v, ep) S iff.",
        "for each case p (and the leaf class q marked by p) of ep, at least one subcategorization frame corresponding to a feature in S has the same case p and its sense restriction c, subsumes Cl, i.e. q cs According to this factor, (v1, epi) is preferred to (v2, ep2) if and only if the following condition holds: (vi,epi) cv S, (v2, e2) S"
      ]
    },
    {
      "heading": "Ranking Parse Trees",
      "text": [
        "This heuristics can be also incorporated into ranking parse trees of a given input sentence.",
        "Let w be the given input sentence, T(w) be the set of parse trees of w, t be a parse tree in T(w), E(t) be the set of verb-noun collocations contained in t. Let 4(0 (C E(t)) be the set of verb-noun collocations (v, ep) for which (v, ep) S holds, and Es (t) (C E(t)) be the set of verb-noun collocations (v, es,) for which (v, es,) -<c„ S does not hold.",
        "Then, subcategorization preference of parse trees is determined as follows.",
        "t1 is preferred to t2 if and only if one of the following conditions (1) (iii) holds:"
      ]
    },
    {
      "heading": "5 Experiments and Evaluation",
      "text": []
    },
    {
      "heading": "5.1 Corpus and Thesaurus",
      "text": [
        "As the training and test corpus, we used the EDR Japanese bracketed corpus (EDR, 1995), which contains about 210,000 sentences collected from newspaper and magazine articles.",
        "From the EDR corpus, we extracted 153,014 verb-noun collocations of 835 verbs which appear more than 50 times",
        "in the corpus.",
        "These verb-noun collocations contain about 270 case-markers.",
        "We constructed the training set E from these 153,014 verb-noun collocations.",
        "We used `Bunrui Goi Hyou'(BGH) (NLR1, 1993) as the Japanese thesaurus.",
        "BGH has a six-layered abstraction hierarchy and more than 60,000 words are assigned at the leaves and its nominal part contains about 45,000 words.",
        "Five classes are allocated at the next level from the root node."
      ]
    },
    {
      "heading": "5.2 Feature Selection and Parameter Estimation",
      "text": [
        "We conduct the feature selection procedure in section 3.3 and the parameter estimation procedure in section 3.2 under the following conditions: i) we limit the noun class generalization level of each feature to those which are above the level 5 from the root node in the thesaurus, ii) since verbs are independent of each other in our model learning framework, we collect verb-noun collocations of one verb into a training data set and conduct the model learning procedure for each verb separately.",
        "For each verb, the size of the training data set is about 200 500.",
        "The size of the set of candidate features varies according to the models: 200 400 for independent-case model, 500 1,300 for one-frame/independent-frame(independence parameter a = 0.5/0.9) models, and 650 1,550 for partial-frame model.",
        "In the independent-case model, each feature corresponds to a subcategorization frame with only one case, while in the one-frame/independent/frame/partialframe models, each feature corresponds to a subcategorization frame with any number of cases.",
        "This is why the size of the set of candidate features is much smaller in the independent-case model than in other models.",
        "In the one-frame/independent-frame models, more restrictions are put on the definition of features than in the partial-frame model, and the sizes of the sets of candidate features are relatively smaller.",
        "Examples of Selected Features For a Japanese verb akau(buy, incur)\", Table 1 shows examples of the selected features for the independent-frame model (independence parameter a = 0.9).",
        "In the table, first 10 selected features, as well as first 5 selected features corresponding to (partial) subcategorization frames with more than one cases, are shown.",
        "In the tables, each feature is represented as the corresponding (partial) subcategorization frame which consists of pairs of a case-marking particle and the noun class restriction of the case.",
        "Each noun class restriction is represented as a Japanese noun class of BGH thesaurus.",
        "Noun classes of BGH thesaurus are represented as numerical codes, in which each",
        "digit denotes the choice of the branch in the thesaurus.",
        "The classes starting with '11', '12', '13', '14', and '15' are subordinate to abstract-relations, agents-of-human-activities, human-activities, products and natural-objects-and-natural-phenomena, respectively.",
        "Each table consists of the order of the feature, the feature itself (which is represented as a (partial) subcategorization frame), noun class descriptions or example nouns in the (partial) subcategorization frames, and the number of the training verb-noun collocations for which the feature function returns true.",
        "Since about 75% of the verb-noun collocations in the training set have only one case-marked noun, all of the first 10 selected features have only one cases in both of the independent-frame/partialframe models.",
        "However, the two models are different in the orders of the first 5 selected features with more than one cases.",
        "In the partial-frame model, those 5 features have much superior orders than in the independent-frame model.",
        "In the partial-frame model, less restrictions are put on the definitions of features than in the independent-frame model.",
        "Therefore, in the partial-frame model, the feature functions corresponding to (partial) subcategorization frames with more than one cases tend to return true for more verb-noun collocations than in the independent-frame model."
      ]
    },
    {
      "heading": "5.3 Evaluation of Sub categorization Preference",
      "text": [
        "We evaluate the performance of the selected features and their estimated parameters in the following subcategorization preference task.",
        "Suppose that the following word sequence represents a verb-final Japanese sentence with a subordinate clause, where Ni,.",
        ", N2k are nouns, px, • • - ,P2k are case-marking post-positional particles, and v1, v2 are verbs, and the first verb v1 is the head verb of the subordinate clause.",
        " – N2k-P2k-v2 We consider the subcategorization ambiguity of the post-positional phrase N.-p: i.e, whether Nx-px is subcategorized for by v1 or v2.",
        "We use held-out verb-noun collocations of the verbs v1 and v2 which are not used in the training.",
        "They are like those verb-noun collocations in the left side below.",
        "Next, we generate erroneous verb-noun collocations of v1 and v2 as those in the right side below, by choosing a case element pz:Nz at random and moving it from v1 to v2.",
        "Then, we compare the products OW (in the equation (26)) of the conditional probabilities of the constituent verb-noun collocations between the correct and the erroneous pairs, and calculate the rate of selecting the correct pair.",
        "We measure the following three types of precisions: i) the precision rb of the basic model in section 4.4.1, ii) the precision rh when incorporating the heuristics in section 4.4.2, iii) the precision rc of those verb-noun collocations which satisfy the case covering relation with the set S of active features, i.e., this means that we collect verb-noun collocations (vi, epi) and (v2, e2) of the verbs viand v2 which satisfy the case covering relation (v1, epi), (v2, ep2) cvS, and calculate the precision rc."
      ]
    },
    {
      "heading": "5.3.2 Results",
      "text": [
        "For the independent-frame model, we examined two different values of the independence parameter a, i.e., a = 0.5 as a weak condition on independence judgment and a = 0.9 as a strict condition on independence judgment.",
        "Figure 1 (d) shows the changes of the precisions rb, rh, and rc as well as the case-coverage of the test data during the training for the independent-frame model (the independence parameter a = 0.9).",
        "Both of the precisions r, and rh of the independent-frame model are higher than those of any other models.",
        "On the other hand, the case-coverage of the independent-frame model (as well as the that of one-frame model) is much lower than that of the partial-frame/independent-case models.",
        "The decrease of the case-coverage in the independent-frame/one-frame models is caused by the overfitting to the training data.6 In the case of the independent-frame model, precisions decrease in the order of 7.0, rh, and rb.",
        "This means that the independent-frame model performs well in the task of subcategorization preference when the verb-noun collocations satisfy the case covering relation with the set S of active features.",
        "When the verb-noun collocations do not satisfy the case covering relation, we have to use the heuristics of case covering in section 4.4.2 and then the precision of subcategorization preference decreases.",
        "If we do not care whether the verb-noun collocations satisfy the case covering relation and do not use the heuristics of case covering, this means that we use the basic model in 6 The reason why the overfitting to the training data occurs in the independent-frame/one-frame models can be explained by comparing the effects of the two values of the independence parameter a in the independent model.",
        "When a equals to 0.9, both 7., and rh are slightly higher than when a equals to 0.5.",
        "Especially, when the number of selected features are less than 300, rc is much higher when a equals to 0.9 than when a equals to 0.5, although the case-coverage of the test data is much lower.",
        "When the condition on independence judgment becomes more strict, the cases in the training data are judged as dependent on each other more often and then this causes the estimated model to overfit to the training data.",
        "In the case of the independent-frame model, overfit to the training data seems to result in higher performance in subcategorization preference task, although the case-coverage of the test data is caused to become lower.",
        "o",
        "section 4.4.1 and it performs worst as indicated by the precision rb"
      ]
    },
    {
      "heading": "6 Conclusion",
      "text": [
        "This paper proposed a novel method for learning probabilistic models of subcategorization preference of verbs.",
        "We proposed to consider the issues of case dependencies and noun class generalization in a uniform way.",
        "We adopted the maximum entropy model learning method and applied it to the task of model learning of subcategorization preference.7 We described the results of the experiment on learning the models of subcategorization preference from the EDR Japanese bracketed corpus.",
        "We evaluated the performance of the selected features and their estimated parameters in the subcategorization preference task.",
        "In this evaluation task, the independent-frame model with the independence parameter a = 0.9 performed best in the precision when incorporating the heuristics of case-covering, as well as in the precision of case-covered test events.",
        "As for further issues, it is important to improve the case-coverage of the independent-frame model without decreasing the precision of subcategorization preference.",
        "For this purpose, we have already invented a new feature selection algorithm which meets the above requirement on preserving high case-coverage with a relatively small number of active features.8 We will report the details of applying this new algorithm to the task of model learning of subcategorization preference in the near future."
      ]
    }
  ]
}
