{
  "info": {
    "authors": [
      "Sanda M. Harabagiu",
      "Razvan C. Bunescu",
      "Steven J. Maiorano"
    ],
    "book": "Meeting of the North American Association for Computational Linguistics",
    "id": "acl-N01-1008",
    "title": "Text and Knowledge Mining for Coreference Resolution",
    "url": "https://aclweb.org/anthology/N01-1008",
    "year": 2001
  },
  "references": [
    "acl-C96-1021",
    "acl-J94-4002",
    "acl-J95-2003",
    "acl-M95-1005",
    "acl-P87-1022",
    "acl-P88-1014",
    "acl-P98-2143",
    "acl-P99-1032",
    "acl-W97-1306",
    "acl-W97-1307",
    "acl-W99-0611"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Traditionally coreference is resolved by satisfying a combination of salience, syntactic, semantic and discourse constraints.",
        "The acquisition of such knowledge is time-consuming, difficult and error-prone.",
        "Therefore, we present a knowledge-minimalist methodology of mining coreference rules from annotated text corpora.",
        "Semantic consistency evidence, which is a form of knowledge required by coreference, is easily retrieved from WordNet.",
        "Additional consistency knowledge is discovered by a meta-bootstrapping algorithm applied to unlabeled texts.",
        "1 Background Reference resolution is an important task for discourse or dialogue processing systems since identity relations between anaphoric textual entities and their antecedents is a prerequisite to the understanding of text or conversation.",
        "Traditionally, coreference resolution has been performed by combining linguistic and cognitive knowledge of language.",
        "Linguistic information is provided mostly by syntactic and semantic modeling of language whereas cognitive information is incorporated in computational models of discourse.",
        "Computational methods based on linguistic and connitive information were presented in (Hobbs 1978), (Lappin and Le-ass 1994), (Brennan et al.1987), (Grosz et a1.1995) and (Webber 1988).",
        "The acquisition of extensive linguistic and discourse knowledge necessary for resolving coreference is time consuming, difficult and error-prone.",
        "Neverthless, recent results show that knowledge-poor, empirical methods perform with amazing accuracy on certain forms of coreference (cf. (Mitkov 1998) (Kennedy and Bogu-raev 1996) (Kameyama 1997)).",
        "For example, COG-NIAC (Baldwin 1997), a system based on just seven ordered heuristics, generates high-precision resolution (over 90%) for some cases of pronominal reference.",
        "In our work, we approached the coreference resolution problem by trying to determine how much more knowledge is required to supplement the above-mentioned knowledge-poor methods and how to derive that knowledge.",
        "To this end we (1) analyze the data to find what types of anaphor-antecedent pairs are most popular in real-world texts; (2) devise knowledge-minimalist rules for handling the majority of those popular cases; and (3) discover what supplementary knowledge is needed for remaining, more difficult cases.",
        "To analyze coreference data we use a corpus of annotated texts.",
        "To devise minimalist coreference resolution rules we consider (1) strong indicators of cohesion, such as repetitions, name aliases or apposi-tions; and (2) gender, number and class agreements.",
        "WordNet (Miller 1995), the vast semantic knowledge base, provides suplementary knowledge in the form of semantic consistency between coreferring nouns.",
        "Additional semantic consistency knowledge is generated by a bootstrapping mechanism when our coreference resolution system, COCKTAIL', processes new texts.",
        "This bootstrapping mechanism inspired by the technique presented in (Riloff and Jones 1999) targets one of the most problematic forms of knowledge needed for coreference resolution: the semantic consistency of corefering nominals.",
        "The rest of the paper is organized as follows.",
        "Section 2 discusses our text mining methodology for analysing the data and devising knowledge-minimalist rules for resolving the most popular coreference cases.",
        "Section 3 presents the knowledge-mining components of COCKTAIL that use WordNet for deriving semantic consistency as well as gender information.",
        "Section 4 presents an entropy-based method for optimally combining coreference rules and Section 5 presents the bootstrapping mechanism.",
        "Section 6 reports and discusses the experimental results while Section 7 summarizes the conclusions.",
        "'COCKTAIL is a pun On Coc NIAC, because COCKTAIL uses multiple coreference resolution rules corresponding to different forms of coreference, blended together in a single system."
      ]
    },
    {
      "heading": "2 Text Mining for Coreference Resolution",
      "text": [
        "Information used for categorizing coreference resolution cases was mined from 30 documents manually annotated with SGML-coreference tags.",
        "The annotations contain information needed to establish a coreference link between an explicitly marked pair of noun phrases from the text.",
        "These 30 texts, used for training our procedure, constitute half of the 60 documents annotated for coreference and made available during the MUC-6 and MUC-7 Message Understanding Conferences (MUC)2.",
        "The remaining 30 documents were used for testing our coreference procedure and bootstrapping supplemental knowledge.",
        "In order to generate the massive amount of data essential to our text mining approach, we expand the annotated tags from the training corpus.",
        "The expansion techniques make use of the properties of coreference relations.",
        "Due to the transitivity of coreference relations, any k coreference relations having at least one common argument generate k + 1 coreferring expressions.",
        "The text position determines an order among coreferring expressions.",
        "A coreference structure is created when a set of coreferring expressions are connected in an oriented graph, such that each node is related only to one of its preceding nodes.",
        "In turn, a coreference chain is the coreference structure in which every node is connected to its immediately preceding node.",
        "Clearly, multiple coreference structures for the same set of coreferring expressions can be mapped in a single coreference chain.",
        "As an example, both coreference structures illustrated in Figure 1(a) and (c) are cast into the coreference chain illustrated in Figure 1(b).",
        "2(Hirschman et al.1998) describes the annotation procedure for the MUC texts that results in an inter-annotator agreement of 85%, similar to the results presented in (Wiebe et al.",
        "1999) for the annotations of the gold-data set of subjectivity classifications.",
        "The same coreference data set was used in the development of several other robust coreference resolution algorithms (cf. (Kameyama 1997) (Cardie and Wagstaff 1999)).",
        "The corefrence chain illustrated in Figure 1(b) is generated from the relations {Rl, R2i R3, R4}.",
        "If it is the case that relations R3 and R4 are more difficult to identify automatically, since they rely on knowledge that is not easily available, the same set of coreferring expressions can be generated by the relations {Rl, R2i R5, R6}, as illustrated in Figure 1(a).",
        "If relations R2 and R3 are difficult to identify automatically, the same set of coreferring expressions can be generated by the relations {Rl, R4i R7, R8 1, as ilustrated in Figure 1(c).",
        "From this example, we can see that if all the possible coreference relations were available, we could mine minimalist coreference knowledge that relies on relatively simpler syntactic and semantic information.",
        "To find out how many possible coreference relations we can generate from the tags of the training documents, we first compute the number of coreference structures that can be derived.",
        "Given a set of coreferring expressions ni, n2, ... n1+13, if each node nk (1<k<l) is connected to any of the k – 1 nodes preceding it in the document, we can generate I x 2 x ... x (l – k)... x l – 1= l!",
        " – 1 coreference structures.",
        "The number of new coreference relations highlighted by these structures is computed by the recursive equation nl ew = nnew +1 – 2.",
        "The solution of this equation is nnew = 1 + 2 + 3 + ... + (l – 2) (1-1)(1-2) We can thus hypothesize that it is possible �to generate, from a set of set of 30 annotated texts, new relations that are an order of magnitude more numerous than the original annotated ones.",
        "This hypothesis is confirmed by the data listed in Table 1.",
        "Such a large number of new annotations cannot be derived manually and therefore we devised an automatic annotation procedure, called AUToTAG-CoREF4.",
        "The algorithm for annotation is as follows:",
        "1.",
        "For every coreference chain CC containing more than 2 expressions, e.g.CC={E(1),E(2),...,E(n)} 2.",
        "For (k=n; kz2' k=k-1) 3. if (Relation (E(k),E(k+1)) :� Apposition) 4.",
        "For (i=k+1; i < n + 1; i=i+1) 5.",
        "Add Annotated-Relation(E(k-1),E(i));",
        "Notice that apposition is not included in the above algorithm.",
        "AUToTAG-CoREF considers the coreference established by appositions as a special case.",
        "Because appositions can be detected fairly reliably, no other coreference annotation sourced at an appositive expression needs to be generated.",
        "Thus, for a coreference chain of length l + 1, any apposition will reduce the number of new links by",
        "ber of new relations, as shown in Table 1, but it also changes the distribution of coreference relations.",
        "For example, in the original annotations, 18.4% of the links connect anaphors to proper nouns whereas the new relations changed this percentage to 29.1%.",
        "Furthermore, the number of relations connecting pairs of common nouns decreased from 31.5% to almost 10.2%.",
        "These observations show that the distribution of coreference data changes when new coreference relations are added.",
        "Once we have the expanded coreference data set that we were aiming for, we proceed with following three-step method for data analysis and derivation of coreference rules:",
        "1.",
        "Find the coreference knowledge satisfied by the largest number of anaphor-antecedent pairs.",
        "We have observed that most of the coreference relations with a Proper Noun antecedent involve a repetition of the anaphor, a name alias, or a very similar expression.",
        "Also many relations satisfy agreements in number, gender and semantic class between the anaphor and its antecedent.",
        "Fewer relations involve semantic knowledge, such as synonymy.",
        "2.",
        "Develop coreference rules from the above observed knowledge.",
        "We formalize the left-hand side of each coreference rule as a conjunct of the conditions that must be satisfied by the anaphor and its candidate antecedent.",
        "The right-hand side is always implemented by the function Cast_in_ Chain (Antecedent,Anaphor).",
        "The role",
        "of Cast-in-Chain is to check whether the antecedent already belongs to a coreference chain, and in this case a coreference relation is cast between the anaphor and the the closest textual expression from the coreference chain.",
        "The distance between two noun phrases NP, and NP2 is measured by the function Surface-Distance, which counts the number of NPs when scanning the text in the following way: (a) if both NPs are in the same sentence, a function called Surface_Search5 scans the sentence starting at NP2, going towards NP, in a right-to-left order;",
        "(Kameyama 1997)) and it is reported to model successfully the parse tree search proposed in (Hobbs 1978).",
        "(b) otherwise, Surface-Search scans the sentence containing NP2 in a right-to-left order and then all the previous sentences in the normal left-to-right order, until it reaches NP1.",
        "3.",
        "Filter out the annotations that can be",
        "identified by the current coreference rules.",
        "Whenever the conditions of a rule are satisfied, an antecedent for the anaphor is identified.",
        "Therefore no other coreference annotations sourced at the same anaphor are needed and they can be filtered out.",
        "After each filter is applied, the remaining annotations represent relations that are not identified by the current set of coreference rules.",
        "This tells us what kind of relations are more difficult to identify and, therefore, necessitate more than the minimalist amount of knowledge incorporated into the current set of coreference rules.",
        "Figure 2(a) illustrates the original annotations as opposed to the annotations expanded by AUToTAG-CoREF, represented in Figure 2(b).",
        "Figure 2(c) shows the remaining relations when coreference relations R1 and R2 from Figure 2(b) were identified by two different rules."
      ]
    },
    {
      "heading": "2.1 Coreference rules",
      "text": [
        "Coreference knowledge is implemented as (1) the conditions from the left-hand side of a set of coreference rules; and also as (2) the conditions of the filters that eliminate the annotations discovered by the current set of rules.",
        "Since all rules have the same right-hand side, the are differentiated only by the conditions implemented.",
        "Consequently, coreference rules can be described by the three filters applied to the coreference data, as they implement the same conditions.",
        "Figure 3 illustrates three coreference rules,",
        "the first one recognizes pronoun repetitions, the second one identifies coreference due to appositions and the third one is based on name alias identification.",
        "They correspond to the conditions applied by the first filter.",
        "Filter 1 The first filter implements strong cohesion indicators, thus imposing high confidence in the coreference rules:",
        "1) repetitions of the same expression (pronoun or nominal); 2) appositions and arguments of the same copulative verb (e.g. be, become, make); 3) name alias recognitions, comprising acronyms or forms of addressing people (e.g. Bill Clinton and Mr. Clinton); 4) the anaphor and the antecedent share the same head and have compatible adjuncts.",
        "The applications of the coreference rules determined by the first filter eliminates 83% of the exapnded annotations on the MUC-6 corpus, leaving only 1871 relations from the initial 11,690, whereas on the MUC-7 corpus, a reduction of 82% takes place, leaving only 2834 links from a total of 15,858 initial expanded relations.",
        "Filter 2 The second filter uses weaker indicators of coreference, but makes the salience factor more relevant.",
        "Unlike filter 1, where the four conditions are applied disjunctively, the second filter imposes three conditions that must be satisfied concurently.",
        "The conditions are:",
        "1) The anaphor and the antecedent share the same semantic category.",
        "A named entity tagger provides the semantic category information for proper names whereas WordNet defines the category of common nouns.",
        "2) The anaphor and the antecedent agree in number, gender and person.",
        "The number information is provided by the part-of-speech tagger, the gender information is mined from WordNet with a method described in the next section.",
        "3) No other text expression satisfying the above two conditions is at a smaller Surface-Distance from the anaphor.",
        "The only semantic category considered by this filter is the PERSON category, recognized by (a) a named entity recognizer; (b) as a hyponym of the WordNet synset {person, individual}, or (c) as personal pronouns.",
        "Only the non-personal pronouns can corefer with any other noun category.",
        "An exception to the first condition is implemented in our system, by allowing the pronoun they to corefer also with any hyponym of the synset {social group}, comprising such nouns as police, army or school.",
        "The second filter reduces the number of unresolved relations to 521 in the MUC-6 data and to 767 in the MUC-7 data.",
        "Filter 3 The third filter applies only to pairs of coreferring common nouns that are not PERSONS.",
        "The filter tests the semantic consistency of the coreference relation by using lexico-semantic information available from WordNet, as described in the next section.",
        "From the performance evaluation of other coreference resolution systems (e.g. (Kameyama 1997)), we know that more than 30% of the missed coreref-erence links are due to the lack of semantic consistency information between an anaphoric noun and its antecedent noun.",
        "After this filter is applied, when comparing the number of original coreference annotations against the remaining annotations, we obtained a maximum recall of 91.3% for the MUC-6 data and of 88.7% for the MUC-7 data."
      ]
    },
    {
      "heading": "3 Knowledge Mining for Coreference Resolution",
      "text": [
        "WordNet is used to acquire gender information for the agreement conditions and to mine patterns of semantic consistency between pairs of nouns.",
        "Acquiring gender information from WordNet We formalize the gender information through an expression G, which may be either an atomic expression, representing one of the gender attributes of a nominal, or a disjunct of two or three of them, as illustrated in Table 2.",
        "The gender attributes may have the values:",
        "• m for masculine nouns; • f for feminine nouns; and • n for all the nouns that either are not from the PERSON category or are polysemous6 and at least one of the senses does not belong to the PERSON category.",
        "Gender attributes are assigned by the two following heuristics: Heuristic 1 If a collocation four a WordNet synset contains the word male, the expression G for the whole synnet is m. If the collocation contains the words female or woman, G= f .",
        "Heuristic 2 Consider the first four words from the synset gloss.",
        "If any of the gloss words have been assigned gender information, propagate the same information to the defined synset as well.",
        "Each hyponym of the concept {person, individual, human}, categorized as PERSON has expression G initialized to f V m, since all lexemes represent persons, that can be either males or females.",
        "Whenever one of the two heuristics previously defined can be applied at any node S from this subhierarchy, three operations take place: r> Operation 1: We update G with the new expression brough forward by the heuristic.",
        "r> Operation 2: We propagate all the expression to the hyponyms of S; r> Operation 3: We revisit the whole PERSON subhierarchy, in search for concepts D that are defined with glosses that use any of the words from synset S or any word from any of its hyponyms.",
        "Whenever we find such a word, we update its G expression to G(S).",
        "We also note that many words are polyse-mous, thus a word w may have multiple senses under the PERSON sub-hierarchy and moreover, each sense might have a different G expression.",
        "In this case, all words from the synsets containing w receive the disjunct of the gender attributes corresponding to each sense of w. Mining semantic information from WordNet We used the WordNet knowledge base to mine patterns of WordNet paths that connect pairs of coreferring nouns from the annotated chains.",
        "The paths are combinations of any of the following WordNet",
        "relations: • SYNONYM connecting all elements of a synset; • Is-A connecting nouns and verbs from the same hierarchies.",
        "We also consider the reversed Is-A relation, denote RIs-A; • GLOSS connecting any element of a synset with the genus of its glossed definition.",
        "We also consider its reverse relation, named DEFINES; • IN-GLOSS connecting any element of a synset with one of the first four words of its glossed definition.",
        "We also consider its reversed relation, named IN-DEFINITION • HAS-PART connecting a concept to its meronyms.",
        "We also consider the reversed IS-PART relation; • MORPHO-DERIVATION connecting a word to its morphological derivations.",
        "• COLLIDE-SENSE connecting several senses of the same word.",
        "To determine the confidence of the path we consider three factors: *Factor fl has only two values.",
        "It is set to 1 when another coreference chain contains elements in the same NPs as the anaphor and the aneecedent.",
        "For example, if NP, is \" the professor's son\" and NP2 is \"his father\", the semantic consistency between father and professor is more likely, given that his and son corefer.",
        "Otherwise, fl is set to 0.",
        "*Factor f2 favors (a) relations that are considered \"stronger\" (e.g. SYNONYMY, GLOSS); and (b) shorter paths.",
        "For this purpose we assign the following weights to each relation considered:",
        "assume that whenever at least two relations of the same kind repeat, we should consider the sequence of relations equivalent to a single relation, having the weight devided by the length of the sequence.",
        "If we denote by nrrea the number of different relation types encountered in a path, and denotes the number of links of type rel in a sequence, then we define f2 with the formula:"
      ]
    },
    {
      "heading": "1 w rel",
      "text": [
        "� � �nrrel reiCPath nrsame rel *Factor f3 is a semantic measure operating on a conceptual space.",
        "When searching for a lexico-semantic path, a search space SS is created, which contains all WordNet content words that can be reached from the candidate antecedent or the anaphor in at most five combinations of the seven relations used by the third filter.",
        "We denote by N the total number of nouns and verbs in the search space.",
        "C represents the number of nouns and verbs that can be reached by both nominals.",
        "In addition nrtotal is the number of concepts along all paths established, whereas nrPo,th is the number of concepts along the path with the best scoring fz.",
        "The formula computing f3, inspired by Salton and Buckley's tf-idf weighting scheme (Salton and Buckley 1988), is:",
        "The confidence measure of the path, denoted by R, combines all three factors in a way similar to van Rijsbergen's E-Formula (van Rijsbergen 1979), used for evaluating the performance of Information Retrieval systems.",
        "The formula that computes the confidence value of a paths, R, is:",
        "The selection of the value b plays an important role in the overall performance of COCKTAIL.",
        "Since we are more interested in the precision of the lexico-semantic path than in the recall of all possible paths, we select b = 2.7.",
        "Table 3 lists some of the patterns determined on the training data and their confidence factors.",
        "The order in which coreference rules are applied is very important, since sometimes, for the same anaphor, different antecedents are indicated by different coreference rules.",
        "One solution is to use the same order in which coreference rules have been devised.",
        "This order gives preference to proper noun antecedents over pronominal antecedents or common noun antecedents.",
        "Such an order determines what rule should be applied when several candidate antecedents are identified.",
        "An alternative is not to use a predefined order, but to find for each anaphor what rule should be applied such that the resulting coreference chains are as precise as possible.",
        "For this purpose, for each rule Ri from the set of coreference rules R ={R1i R2, ..., RnI we compute p, the number of times when the application of rule Ri in the training corpus is correct, and n, the number of times when the application of rule Ri was not correct.",
        "This allows us to define the confidence of using Ri for establishing coreference between two noun phrases NPR and NPk by using the formula:",
        "where the entropy measure is defined as:",
        "The rationale for rel(R27 NPR, NPk) is given by the fact that the entropy indicates how much information is still needed to establish the coreference between NPR and NPk with certainty.",
        "As illustrated in Figure 4, if p+ (Ri) = +n then the closer p+ (Ri)",
        "is to 1, the more confidence we have in the coreference relation between NPR and NPk.",
        "Similarly, the closer p+ (Ri) is to 0, the more confident we are that NPR and NPk do not corefer.",
        "The confidence measure of each rule is used in determining the most precise coreference chains spanning a text.",
        "Given a text T we consider all its referential expressions RS(7-)={NP1, NPzi ..., NPT}, a subset of the text noun phrases.",
        "To derive the coreference chains spanning the elements from RS(T) we use the set of coreference rules R. A given application of the rules from R generates a partition on R£(T).",
        "Each partition is a set of coreference chains Par={CCQr} such that each NPR E RS(T) belongs to one and only one of the coreference chains CCPar.",
        "Each partition corresponds a possible combination of coreference chains spanning RS(T).",
        "If P(RS) are all the possible partitions on RS(T), our goal is to find the best partition, i.e. the partition that contains all the correct coreference chains established on RS(T).",
        "If every partition Par E P(RS) is assigned a measure m(Par, R) which computes the likelihood that Par contains all the correct coreference links from the text T, established by the rules from R, then the best partition is given by:",
        "in which m(Par, R) is defined by the sum between two factors:",
        "each coreference chain from Par.",
        "Formally it is defined as a sum ranging over all pairs of referential expressions that belong to the same coreference chain",
        "(2) m – (P, R) indicates the discrimination among all the coreference chains from Par.",
        "Formally it is defined as a sum ranging over all pairs of referential expressions that belong to different coreference"
      ]
    },
    {
      "heading": "Learning method",
      "text": [
        "At training time, for each Ri from R we compute the entropy(RZ) on the training corpus.",
        "At testing time, given a new, test text TT and its referential expressions RE(TT) ={ NP, NPZ, ..., NPT }, we find the best partition by using a local search algorithm, namely by applying hillclimbing to the values of the m(Par', R) measure for each possible partition of TT.",
        "The initial partition is",
        "t distinct coreference chains, each containing a single referential expression.",
        "The other partitions are generated in a recursive manner.",
        "For each partition Pare, with 1 < i < 2t, the new partitions Park+I are generated by combining any pairs of chains from Par'.",
        "If Par' has k chains, then k(k + 1)/2 new partitions can be generated and ordered according to their m measures.",
        "At each step of the process of generating a new partition, the hillclimbing algorithm selects the best partition Par's+I, having the highest m score.",
        "When m(Par'+I, R) < m(Par', R) the search terminates, since the maximum of the m measure has been reached.",
        "However, this is often a local maximum.",
        "To avoid local maxima, instead of selecting only the best scoring partition, we consider all the first p paritions, where p is called the patience of hillclimbing.",
        "In our case, we chose p = 5."
      ]
    },
    {
      "heading": "5 Bootstrapping for Coreference Resolution",
      "text": [
        "We considered bootstrapping, the new machine learning technique presented in (Riloff and Jones 1999), as an ideal vehicle for enhancing the semantic consistency constraints between common nouns.",
        "Boos-trapping is known to operate on unlabeled data by using only some knowledge seeds.",
        "The rules implemented in COCKTAIL do not capture all the coreference patterns, but they are fairly precise, thus they can be viewed as the knowledge seeds for bootstrapping.",
        "When applied to new, unlabeled texts, the coreference rules from COCKTAIL discover new pairs of common nouns that might corefer.",
        "For example, Figure 5(a) illustrates the application of three coreference rules on a new text.",
        "Two anaphors CNI and CNZ are common nouns and no semantic consistency information accounts for their coreference.",
        "However, if the antecedent of anaphor CNI is sought, coreference rule RI indicates expression A to be the antecedent.",
        "Similarly, the antecedent of anaphor CNZ is discovered by rule RZ as being expression B, which corefers with A, because of rule R3.",
        "When the coreference chain is built, expression CNI is directly linked to expression CNZ, thus enabling new semantic consistency information discovered from WordNet paths.",
        "Figure 5(b) illustrates one of the paths that were discovered and its corresponding coreference rule.",
        "As a rule of thumb, we do not consider a new coreference rule based on semantic consistency information unless coverage in the data warrants it.",
        "At training time, we selected N path patterns because the majority of the paths matching these patterns had the relevance larger than a threshold, tR.",
        "Whenever new semantic consistency is uncovered by a path Path,,,,,, its relevance R(Path,,,eu,) must be larger than the threshold tR.",
        "However, the new paths might not have been encontered in the training data and still encode relevant semantic consistency information.",
        "To account for this case, a new value for the t is selected, which determines a different"
      ]
    },
    {
      "heading": "R",
      "text": [
        "entropy for each coreference rule based on semantic consistency constraints.",
        "Consequently, a different set of coreference chains is generated for each training text, thus changing both the precision and the recall of COCKTAIL.",
        "This mechanism of discovering and adding new rules to the set of coreference rules enables the following bootstrapping algorithm: *Generate all candidate Paths from new texts"
      ]
    },
    {
      "heading": "MUTUAL BOOTSTRAPPING LOOP",
      "text": [
        "1.",
        "Score all candidate paths by their relevance 2.",
        "Add the best candidates and encode them as rules 3.",
        "Adjust the relevance threshold 4.",
        "Goto step 1 if the F-measure did not degrade",
        "under MIN-Performance (Riloff and Jones 1999) note that the performance of the mutual bootstrapping algorithm can deteriorate rapidly if erroneous rules are entered.",
        "To make the algorithm more robust we use the same solution by introducing a second level of bootrapping.",
        "The outer level, called meta-bootstrapping identifies the most reliable k rules based on semantic consistency and discard all the others before restarting the mutual bootstrapping loop again.",
        "In our experiments we have retained only those rules for which the new performance, given by the F-measure was larger than the median of the past four loops.",
        "The formula for the van Rijsbergen's F-measure combines the precision P with the recall R in F = 2 P+R"
      ]
    },
    {
      "heading": "6 Evaluation",
      "text": [
        "To measure the performance of COCKTAIL we have trained the system on 30 MUC-6 and MUC-7 texts and tested it on the remaining 30 documents.",
        "We computed the precision, the recall and the F-measure.",
        "The performance measures have been obtained automatically using the MUC-6 coreference scoring program (Vilain et al.",
        "1995).",
        "Table 4 lists the results.",
        "cision but poor recall.",
        "By combining the rules with the entropy-based measure, we obtained further enhancement in precision, but the recall dropped.",
        "The application of the bootstrapping methodology determined an enhancement of recall, and thus of the F-measure.",
        "In the future we intend to compare the overall effect of rules that recognize referential expressions on the overall performance of the system."
      ]
    },
    {
      "heading": "7 Conclusion",
      "text": [
        "We have introduced a new data-driven method for coreference resolution, implemented in the COCKTAIL system.",
        "Unlike other knowledge-poor methods for coreference resolution (Baldwin 1997) (Mitkov 1998), COCKTAIL filters its most performant rules through massive training data, generated by its AUToTAC-COREF component.",
        "Furthermore, by using an entropy-based method we determine the best partition of corefering expressions in coreference chains.",
        "New rules are learned by applying a bootstrapping methodology that uncovers additional semantic consistency data."
      ]
    }
  ]
}
