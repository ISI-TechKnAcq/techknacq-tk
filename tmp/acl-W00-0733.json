{
  "info": {
    "authors": [
      "Erik F. Tjong Kim Sang"
    ],
    "book": "Conference on Computational Natural Language Learning and of the Second Learning Language in Logic Workshop CoNLL and LLL",
    "id": "acl-W00-0733",
    "title": "Text Chunking by System Combination",
    "url": "https://aclweb.org/anthology/W00-0733",
    "year": 2000
  },
  "references": [
    "acl-A00-2007",
    "acl-P98-1081",
    "acl-W00-0726",
    "acl-W95-0107",
    "acl-W99-0629"
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": [
        "We will apply a system-internal combination of memory-based learning classifiers to the CoNLL-2000 shared task: finding base chunks.",
        "Apart from testing different combination methods, we will also examine if dividing the chunking process in a boundary recognition phase and a type identification phase would aid performance."
      ]
    },
    {
      "heading": "2 Approach",
      "text": [
        "Tjong Kim Sang (2000) describes how a system-internal combination of memory-based learners can be used for base noun phrase (baseNP) recognition.",
        "The idea is to generate different chunking models by using different chunk representations.",
        "Chunks can be represented with bracket structures but alternatively one can use a tagging representation which classifies words as being inside a chunk (I), outside a chunk (0) or at a chunk boundary (B) (Ramshaw and Marcus, 1995).",
        "There are four variants of this representation.",
        "The B tags can be used for the first word of chunks that immediately follow another chunk (the IOB1 representation) or they can be used for every chunk-initial word (I0B2).",
        "Alternatively an E tag can be used for labeling the final word of a chunk immediately preceding another chunk (IOE1) or it can be used for every chunk-final word (10E2).",
        "Bracket structures can also be represented as tagging structures by using two streams of tags which define whether words start a chunk or not (0) or whether words are at the end of a chunk or not (C).",
        "We need both for encoding the phrase structure and hence we will treat the two tag streams as a single representation (0+C).",
        "A combination of baseNP classifiers that use the five representation performs better than any of the included systems (Tjong Kim Sang, 2000).",
        "We will apply such a classifier combination to the CoNLL-2000 shared task.",
        "The individual classifiers will use the memory-based learning algorithm IB1-1G (Daelemans et al., 1999) for determining the most probable tag for each word.",
        "In memory-based learning the training data is stored and a new item is classified by the most frequent classification among training items which are closest to this new item.",
        "Data items are represented as sets of feature-value pairs.",
        "Features receive weights which are based on the amount of information they provide for classifying the training data (Daelemans et al., 1999).",
        "We will evaluate nine different methods for combining the output of our five chunkers (Van Halteren et al., 1998).",
        "Five are so-called voting methods.",
        "They assign weights to the output of the individual systems and use these weights to determine the most probable output tag.",
        "Since the classifiers generate different output formats, all classifier output has been converted to the O and the C representations.",
        "The most simple voting method assigns uniform weights and picks the tag that occurs most often (Majority).",
        "A more advanced method is to use as a weight the accuracy of the classifier on some held-out part of the training data, the tuning data (Tot-Precision).",
        "One can also use the precision obtained by a classifier for a specific output value as a weight (TagPrecision).",
        "Alternatively, we use as a weight a combination of the precision score for the output tag in combination with the recall score for competing tags (Precision-Recall).",
        "The most advanced voting method examines output values of pairs of classifiers and assigns weights to tags based on how often they appear with this pair in the tuning data (Tag-Pair, Van Halteren et al., (1998)).",
        "Apart from these voting methods we have also applied two memory-based learners to the output of the five chunkers: IB1-IG and IGTREE, a decision tree variant of m 1-1G (Daelemans et al., 1999).",
        "This approach is called classifier stacking.",
        "Like with the voting algorithms, we have tested these meta-classifiers with the output of the first classification stage.",
        "Unlike the voting algorithms, the classifiers do not require a uniform input.",
        "Therefore we have tested if their performance can be improved by supplying them with information about the input of the first classification stage.",
        "For this purpose we have used the part-of-speech tag of the current word as compressed representation of the first stage input (Van Halteren et al., 1998).",
        "The combination methods will generate a list of open brackets and a list of close brackets.",
        "We have converted these to phrases by only using brackets which could be matched with the closest matching candidate and ignoring the others.",
        "For example, in the structure [Np a [NP b ]NP [vp c ]pp d ]vp, we would accept [NP b ]NP as a noun phrase and ignore all other brackets since they cannot be matched with their closest candidate for a pair, either because of type inconsistencies or because there was some other bracket in between them.",
        "We will examine three processing strategies in order to test our hypothesis that chunking performance can be increased by making a distinction between finding chunk boundaries and identifying chunk types.",
        "The first is the single-pass method.",
        "Here each individual classifier attempts to find the correct chunk tag for each word in one step.",
        "A variant of this is the double-pass method.",
        "It processes the data twice: first it searches for chunks boundaries and then it attempts to identify the types of the chunks found.",
        "The third processing method is the n-pass method.",
        "It contains as many passes as there are different chunk types.",
        "In each pass, it attempts to find chunks of a single type.",
        "In case a word is classified as belonging to more than one chunk type, preference will be given to the chunk type that occurs most often in the training data.",
        "We expect the n-pass method to outperform the other two methods.",
        "However, we are not sure if the performance difference will be large enough to compensate for the extra computation that is required for this processing method."
      ]
    },
    {
      "heading": "3 Results",
      "text": [
        "In order to find out which of the three processing methods and which of the nine combination methods performs best, we have applied them to the training data of the CoNLL-2000 shared task (Tjong Kim Sang and Buchholz, 2000) in a 10-fold cross-validation experiment (Weiss and Kulikowski, 1991).",
        "For the single-pass method, we trained IB1-IG classifiers to produce the most likely output tags for the five data representations.",
        "In the input of the classifiers a word was represented as itself, its part-of-speech tag and a context of four left and four right word/partof-speech tag pairs.",
        "For the four JO representations we used a second phase with a limited input context (3) but with additionally the two previous and the two next chunk tags predicted by the first phase.",
        "The classifier output was converted to the 0 representation (open brackets) and the C representation (close brackets) and the results were combined with the nine combination methods.",
        "In the double-pass method finding the most likely tag for each word was split in finding chunk boundaries and assigning types to the chunks.",
        "The n-pass method divided this process into eleven passes each of which recognized one chunk type.",
        "For each processing strategy, all combination results were better than those obtained with the five individual classifiers.",
        "The differences between combination results within each processing strategy were small and between the three strategies the best results were not far apart: the best Fo=1 rates were 92.40 (single-pass), 92.35 (double-pass) and 92.75 (n-pass).",
        "Since the three processing methods reach a similar performances, we can choose any of them for our remaining experiments.",
        "The n-pass method performed best but it has the disadvantage of needing as many passes as there are chunk types.",
        "This will require a lot of computation.",
        "The single-pass method was second-best but in order to obtain good results with this method, we would need to use a stacked classifier because those performed better (F0=1.92.40) than the voting methods (F0=1=91.98).",
        "This stacked classifier requires preprocessed combinator training data which can be obtained by processing the original train",
        "ing data with 10-fold cross-validation.",
        "Again this will require a lot of work for new data sets.",
        "We have chosen for the double-pass method because in this processing strategy it is possible to obtain good results with majority voting.",
        "The advantage of using majority voting is that it does not require extra preprocessed combinator training data so by using it we avoid the extra computation required for generating this data.",
        "We have applied the double-pass method with majority voting to the CoNLL2000 test data while using the complete training data.",
        "The results can be found in table 1.",
        "The recognition method performs well for the most frequently occurring chunk types (NP, VP and PP) and worse for the other seven (the test data did not contain UCP chunks).",
        "The recognition rate for NP chunks (F0=1=93.23) is close to the result for a related standard baseNP data set obtained by Tjong Kim Sang (2000) (93.26).",
        "Our method outperforms the results mentioned in Buchholz et al.",
        "(1999) in four of the five cases (ADJP, NP, PP and VP); only for ADVP chunks it performs slightly worse.",
        "This is surprising given that Buchholz et al.",
        "(1999) used 956696 tokens of training data and we have used only 211727 (78% less)."
      ]
    },
    {
      "heading": "4 Concluding Remarks",
      "text": [
        "We have evaluated three methods for recognizing non-recursive non-overlapping text chunks of arbitrary syntactical categories.",
        "In each method a memory-based learner was trained to recognize chunks represented in five different ways.",
        "We have examined nine different methods for combining the five results.",
        "A 10- fold cross-validation experiment on the training data of the CoNLL-2000 shared task revealed that (1) the combined results were better than the individual results, (2) the combination methods perform equally well and (3) the best performances of the three processing methods were similar.",
        "We have selected the double-pass method with majority voting for processing the CoNLL-2000 shared task data.",
        "This method outperformed an earlier text chunking study for most chunk types, despite the fact that it used about 80% less training data."
      ]
    }
  ]
}
