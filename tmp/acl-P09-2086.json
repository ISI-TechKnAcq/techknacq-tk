{
  "info": {
    "authors": [
      "Taro Watanabe",
      "Hajime Tsukada",
      "Hideki Isozaki"
    ],
    "book": "ACL-IJCNLP: Short Papers",
    "id": "acl-P09-2086",
    "title": "A Succinct N-gram Language Model",
    "url": "https://aclweb.org/anthology/P09-2086",
    "year": 2009
  },
  "references": [
    "acl-D07-1021",
    "acl-D07-1090",
    "acl-P08-1058",
    "acl-W07-0712"
  ],
  "sections": [
    {
      "text": [
        "Taro Watanabe Hajime Tsukada Hideki Isozaki",
        "Efficient processing of tera-scale text data is an important research topic.",
        "This paper proposes lossless compression of N-gram language models based on LOUDS, a succinct data structure.",
        "LOUDS succinctly represents a trie with M nodes as a 2M + 1 bit string.",
        "We compress it further for the iV-gram language model structure.",
        "We also use 'variable length coding' and 'block-wise compression' to compress values associated with nodes.",
        "Experimental results for three large-scale iV-gram compression tasks achieved a significant compression rate without any loss."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "There has been an increase in available iV-gram data and a large amount of web-scaled iV-gram data has been successfully deployed in statistical machine translation.",
        "However, we need either a machine with hundreds of gigabytes of memory or a large computer cluster to handle them.",
        "Either pruning (Stolcke, 1998; Church et al., 2007) or lossy randomizing approaches (Talbot and Brants, 2008) may result in a compact representation for the application run-time.",
        "However, the lossy approaches may reduce accuracy, and tuning is necessary.",
        "A lossless approach is obviously better than a lossy one if other conditions are the same.",
        "In addtion, a lossless approach can easly combined with pruning.",
        "Therefore, lossless representation of iV-gram is a key issue even for lossy approaches.",
        "Raj and Whittaker (2003) showed a general N-gram language model structure and introduced a lossless algorithm that compressed a sorted integer vector by recursively shifting a certain number of bits and by emitting index-value inverted vectors.",
        "However, we need more compact representation.",
        "In this work, we propose a succinct way to represent the iV-gram language model structure based on LOUDS (Jacobson, 1989; Delpratt et al., 2006).",
        "It was first introduced by Jacobson (1989) and requires only a small space close to the information-theoretic lower bound.",
        "For an M node ordinal trie, its information-theoretical lower bound is 2M - 0{\\gM) bits (\\g(x) = log2(a;))",
        "probability pointer word la bacK-orr",
        "and LOUDS succinctly represents it by a 2M + 1 bit string.",
        "The space is further reduced by considering the iV-gram structure.",
        "We also use variable length coding and block-wise compression to compress the values associated with each node, such as word ids, probabilities or counts.",
        "We experimented with English Web IT 5-gram from LDC consisting of 25 GB of gzipped raw text iV-gram counts.",
        "By using 8-bit floating point quantization , N-gram language models are compressed into 10 GB, which is comparable to a lossy representation (Talbot and Brants, 2008)."
      ]
    },
    {
      "heading": "2. iV gram Language Model",
      "text": [
        "if w™ exists.",
        "exists.",
        "otherwise.",
        "a(wi) and are smoothed probabilities and back-off coefficients, respectively.",
        "The iV-grams are stored in a trie structure as shown in Figure 1. iV-grams of different orders are stored in different tables and each row corresponds to a particular w™, consisting of a word id for wn, a(wi), f3(wi) and a pointer to the first position of the succeeding (n + 1)-grams that share the same prefix w™.",
        "The succeeding (n + l)-grams are stored in a contiguous region and sorted by the word id of wn+\\.",
        "The boundary of the region is determined by the pointer of the next iV-gram in the",
        "We assume a back-off iV-gram language model in which the conditional probability Pr(wn\\w'1~l) cursively computed as follows.",
        "(b) Corresponding LOUDS bit string",
        "(d) Corresponding iV-gram optimized LOUDS bit string (c) Trie structure for iV-gram",
        "row.",
        "When an TV-gram is traversed, binary search is performed TV times.",
        "If each word id corresponds to its node position in the unigram table, we can remove the word ids for the first order.",
        "Our implementation merges across different orders of TV-grams, then separates into multiple tables such as word ids, smoothed probabilities, back-off coefficients, and pointers.",
        "The starting positions of different orders are memorized to allow access to arbitrary orders.",
        "To store TV-gram counts, we use three tables for word ids, counts and pointers.",
        "We share the same tables for word ids and pointers with additional probability and back-off coefficient tables.",
        "To support distributed computation (Brants et al., 2007), we further split the TV-gram data into \"shards\" by hash values of the first bigram.",
        "Unigram data are shared across shards for efficiency."
      ]
    },
    {
      "heading": "3. Succinct TV-gram Structure",
      "text": [
        "The table of pointers described in the previous section represents a trie.",
        "We use a succinct data structure LOUDS (Jacobson, 1989; Delpratt et al., 2006) for compact representation of the trie.",
        "For an M node ordinal trie, there exist 2M+1 (2iMl~) different tries.",
        "Therefore, its information-theoretical lower bound is",
        "0{\\gM) bits.",
        "M nodes as a",
        "The LOUDS bit string is constructed as follows.",
        "Starting from the root node, we traverse a trie in level order.",
        "For each node with d > 0 children, the bit string ld0 is emitted.",
        "In addition, 10 is prefixed to the bit string emitted by an imaginary super-root node pointing to the root node.",
        "Figure 2(a) shows an example trie structure.",
        "The nodes are numbered in level order, and from left to right.",
        "The corresponding LOUDS bit string is shown in Figure 2(b).",
        "Since the root node 0 has four child nodes, it emits four Is followed by 0, which marks the end of the node.",
        "Before the root node, we assume an imaginary super root node emits 10 for its only child, i.e., the root node.",
        "After the root node, its first child or node 1 follows.",
        "Since (M + l)0s and Mis are emitted for a trie with M nodes, LOUDS occupies 2M + 1 bits.",
        "We define a basic operation on the bit string.",
        "seli(i) returns the position of the i-th 1.",
        "We can also define similar operations over zero bit strings, selo(i).",
        "Given sel^,, we define two operations for a node x. parent(a;) gives x's parent node and firstch(a;) gives x's first child node:",
        "parent(a;) = sell (a; + 1) – x -firstch(a;) = se\\o(x + 1) – x.",
        "To test whether a child node exists, we simply check firstch(a;) / firstch(a; + 1).",
        "Similarly, the child node range is determined by [firstch(a;), firstch(a; + 1)).",
        "We propose removing redundant bits from the baseline LOUDS representation assuming TV-gram structures.",
        "Since we do not store any information in the root node, we can safely remove the root so that the imaginary super-root node directly points to unigram nodes.",
        "The node ids are renumbered and the first unigram is 0.",
        "In this way, 2 bits are saved.",
        "The TV-gram data structure has a fixed depth TV and takes a flat structure.",
        "Since the highest order TV-grams have no child nodes, they emit in the tail of the bit stream, where J\\fn stands for the number of n-grams.",
        "By memorizing the starting position of the highest order TV-grams, we can completely remove A/jv bits.",
        "The imaginary super-root emits 1^0 at the beginning of the bit stream.",
        "By memorizing the bigram starting position, we can remove the N\\ + l bits.",
        "Finally, parent(a;) and firstch(a;) are rewritten as",
        "2M+1 v M LOUDS represents a trie with",
        "2M + 0{M) bit string.",
        "node id",
        "0",
        "1",
        "2",
        "3",
        "4",
        "5",
        "6",
        "7",
        "8",
        "9",
        "10",
        "11",
        "12",
        "13",
        "14",
        "15",
        "bit position",
        "01",
        "23456",
        "78910",
        "111213",
        "14",
        "15 16",
        "171819",
        "20",
        "2122",
        "23",
        "24 25 26",
        "27",
        "28",
        "29",
        "30",
        "31",
        "32",
        "LOUDS bit",
        "10",
        "11110",
        "1110",
        "1 1 0",
        "0",
        "1 0",
        "1 1 0",
        "0",
        "1 0",
        "0",
        "1 1 0",
        "0",
        "0",
        "0",
        "0",
        "0",
        "0",
        "node id",
        "0",
        "1",
        "2",
        "3",
        "4",
        "5",
        "6",
        "7",
        "8",
        "9",
        "bit position",
        "0 123",
        "45 6",
        "7",
        "89",
        "10 11 12",
        "13",
        "14 15",
        "16",
        "17 18 19",
        "20",
        "LOUDS bit",
        "1110",
        "1 10",
        "0",
        "1 0",
        "1 1 0",
        "0",
        "1 0",
        "0",
        "1 1 0",
        "0",
        "follows:",
        "Figure 2(c) shows the TV-gram optimized trie structure (TV = 3) from Figure 2 with Wi = 4 and 7V3 = 5.",
        "The parent of node 8 is found by seh (8+1-4) = 5 and 5+4-8 = 1.",
        "The first child is located by sel0(8) = 16 and 16+4+1-8 = 13.",
        "When accessing the TV-gram data structure, selb(i) operations are used extensively.",
        "We use an auxiliary dictionary structure proposed by Kim et al.",
        "(2005) and Jacobson (1989) that supports an efficient seli(i) (selo(i)) with the dictionary.",
        "We omit the details due to lack of space.",
        "The above method compactly represents pointers, but not associated values, such as word ids or counts.",
        "Raj and Whittaker (2003) proposed integer compression on each range of the word id sequence that shared the same TV-gram prefix.",
        "Here, we introduce a simple but more effective variable length coding for integer sequences of word ids and counts.",
        "The basic idea comes from encoding each integer by the smallest number of required bytes.",
        "Specifically, an integer within the range of 0 to 255 is coded as a 1-byte integer, the integers within the range of 256 to 65,535 are stored as 2-byte integers, and so on.",
        "We use an additional bit vector to indicate the boundary of the byte sequences.",
        "Figure 3 presents an example integer sequence, 52, 156, 260 and 364 with coded integers in hex decimals with boundary bits.",
        "In spite of the length variability, the system can directly access a value at index % as bytes in [seli(i) + 1, seli(i + 1) + 1) by the efficient seh operation assuming that seli(0) yields – 1.",
        "For example, the value 260 at index 2 in Figure 3 is mapped onto the byte range of [sell (2) + l,seli(3) + l) = [2,4).",
        "We further compress every 8K-byte data block of all tables in TV-grams by using a generic compression library, z 1 ib, employed in UNIX gz ip.",
        "We treat a sequence of 4-byte floats in the probability table as a byte stream, and compress every 8K-byte block.",
        "To facilitate random access to the compressed block, we keep track of the compressed block's starting offsets.",
        "Since the offsets are in sorted order, we can apply sorted integer compression (Raj and Whittaker, 2003).",
        "Since TV-gram language model access preserves some locality, TV-gram with block compression is still practical enough to be usable in our system."
      ]
    },
    {
      "heading": "4. Experiments",
      "text": [
        "We applied the proposed representation to 5-gram trained by \"English Gigaword 3rd Edition,\" \"English Web 1T 5-gram\" from LDC, and \"Japanese Web 1T 7-gram\" from GSK.",
        "Since their tendencies are the same, we only report in this paper the results on English Web IT 5-gram, where the size of the count data in gzipped raw text format is 25GB, the number of N-grams is 3.8G, the vocabulary size is 13.6M words, and the number of the highest order N-grams is 1.2G.",
        "We implemented an TV-gram indexer/estimator using MPI inspired by the MapReduce implementation of TV-gram language model indexing/estimation pipeline (Brants et al., 2007).",
        "Table 1 summarizes the overall results.",
        "We show the initial indexed counts and the final language model size by differentiating compression strategies for the pointers, namely the 4-byte raw value (Trie), the sorted integer compression (Integer) and our succinct representation (Succinct).",
        "The \"block\" indicates block compression.",
        "For the sake of implementation simplicity, the sorted integer compression used a fixed 8-bit shift amount, although the original paper proposed recursively determined optimum shift amounts (Raj and Whittaker, 2003).",
        "8-bit quantization was performed for probabilities and back-off coefficients using a simple binning approach (Federico and Cettolo, 2007).",
        "TV-gram counts were reduced from 23.59GB to 10.57GB by our succinct representation with block compression.",
        "TV-gram language models of 42.65GB were compressed to 18.37GB.",
        "Finally, the 8-bit quantized TV-gram language models are represented by 9.83 GB of space.",
        "Table 2 shows the compression ratio for the pointer table alone.",
        "Block compression employed on raw 4-byte pointers attained a large reduction that was almost comparable to sorted integer compression.",
        "Since large pointer value tables are sorted, even a generic compression algorithm could achieve better compression.",
        "Using our succinct representation, 2.4 bits are required for each TV-gram.",
        "By using the \"flat\" trie structure, we approach closer to its information-theoretic lower bound beyond the LOUDS baseline.",
        "With block compression, we achieved 1.8 bits per TV-gram.",
        "Table 3 shows the effect of variable length coding and block compression for the word ids, counts, probabilities and back-off coefficients.",
        "After variable-length coding, the word id is almost half its original size.",
        "We assign a word id for each",
        "integer seq.",
        "52",
        "156",
        "260",
        "364",
        "coding",
        "0x34",
        "0x9c",
        "0x01 0x04",
        "0x01 0x6c",
        "boundary",
        "1",
        "1",
        "0 1",
        "0 1",
        "Table 1 : Summary of iV-gram compression word according to its reverse sorted order of frequency.",
        "Therefore, highly frequent words are assigned smaller values, which in turn occupies less space in our variable length coding.",
        "With block compression, we achieved further 1 GB reduction in space.",
        "Since the word id sequence preserves local ordering for a certain range, even a generic compression algorithm is effective.",
        "The most frequently observed count in TV-gram data is one.",
        "Therefore, we can reduce the space by the variable length coding.",
        "Large compression rates are achieved for both probabilities and backoff coefficients."
      ]
    },
    {
      "heading": "5. Conclusion",
      "text": [
        "We provided a succinct representation of the TV-gram language model without any loss.",
        "Our method approaches closer to the information-theoretic lower bound beyond the LOUDS baseline.",
        "Experimental results showed our succinct representation drastically reduces the space for the pointers compared to the sorted integer compression approach.",
        "Furthermore, the space of TV-grams was significantly reduced by variable",
        "Table 3 : Effects of block compression length coding and block compression.",
        "A large amount of TV-gram data is reduced from unin-dexed gzipped 25 GB text counts to 10 GB of indexed language models.",
        "Our representation is practical enough though we did not experimentally investigate the runtime efficiency in this paper.",
        "The proposed representation enables us to utilize a web-scaled TV-gram in our MT competition system (Watanabe et al., 2008).",
        "Our succinct representation will encourage new research on web-scaled TV-gram data without requiring a larger computer cluster or hundreds of gigabytes of memory."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank Daisuke Okanohara for his open source implementation and extensive documentation of LOUDS, which helped our original coding.",
        "w/o block w/ block",
        "Counts Trie",
        "Integer Succinct",
        "23.59 GB 12.21 GB 14.59 GB 11.18 GB 12.62 GB 10.57 GB",
        "Language Trie model Integer Succinct",
        "42.65 GB 20.01 GB 33.65 GB 18.98 GB 31.67 GB 18.37 GB",
        "Quantized Trie language Integer model Succinct",
        "24.73 GB 11.47 GB 15.73 GB 10.44 GB 13.75 GB 9.83 GB",
        "total",
        "per iV -gram",
        "4-byte Pointer +block compression",
        "12.04 GB 2.42 GB",
        "27.24 bits 5.48 bits",
        "Sorted Integer +block compression",
        "3.04 GB 1.39 GB",
        "6.87 bits 3.15 bits",
        "Succinct",
        "+block compression",
        "1.06 GB 0.78 GB",
        "2.40 bits 1.76 bits",
        "total",
        "per iV -gram",
        "word id size (4 bytes)",
        "+variable length +block compression",
        "14.09 GB 6.72 GB 5.57 GB",
        "31.89 bits 15.20 bits 12.60 bits",
        "count size (8 bytes)",
        "+variable length +block compression",
        "28.28 GB 4.85 GB 4.22 GB",
        "64.00 bits 10.96 bits 9.56 bits",
        "probability size (4 bytes) +block compression",
        "8-bit quantization",
        "+block compression",
        "14.14 GB 9.55 GB 3.54 GB 2.64 GB",
        "32.00 bits 21.61 bits 8.00 bits 5.97 bits",
        "backoff size (4 bytes) +block compression",
        "8-bit quantization",
        "+block compression",
        "9.76 GB 2.48 GB 2.44 GB 0.85 GB",
        "22.08 bits 5.61 bits 5.52 bits 1.92 bits"
      ]
    }
  ]
}
