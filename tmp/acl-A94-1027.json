{
  "info": {
    "authors": [
      "Makoto Iwayama",
      "Takenobu Tokunaga"
    ],
    "book": "Applied Natural Language Processing Conference",
    "id": "acl-A94-1027",
    "title": "A Probabilistic Model for Text Categorization: Based on a Single Random Variable With Multiple Values",
    "url": "https://aclweb.org/anthology/A94-1027",
    "year": 1994
  },
  "references": [
    "acl-A92-1018"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "Text categorization is the classification of documents with respect to a set of predefined categories.",
        "In this paper, we propose a new probabilistic model for text categorization, that is based on a Single random Variable with Multiple Values (SVMV).",
        "Compared to previous probabilistic models, our model has the following advantages; 1) it considers within-document term frequencies, 2) considers term weighting for target documents, and 3) is less affected by having insufficient training cases.",
        "We verify our model's superiority over the others in the task of categorizing news articles from the \"Wall Street Journal\"."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Text categorization is the classification of documents with respect to a set of predefined categories.",
        "As an example, let us take a look at the following article from the \"Wall Street Journal\" (1989/11/2).",
        "McDermott International, Inc. said its Babcock & Wilcox unit completed the sale of its Bailey Controls Operations to Finmeccanica S.p.A for $295 million.",
        "Finmeccanica is an Italian state-owned holding company with interests in the mechanical engineering industry.",
        "Bailey Controls, based in Wickliffe, Ohio, makes computerized industrial controls systems.",
        "It employs 2,700 people and has annual revenue of about $370 million.",
        "Two categories (topics) are manually assigned to this article; \"TENDER OFFERS, MERGERS, ACQUISITIONS (TNM)\" and \"COMPUTERS AND INFORMATION TECHNOLOGY (CPR).\" While there may be certain rules or standards for categorization, it is very difficult for human experts to assign categories consistently and efficiently to large numbers of daily incoming documents.",
        "The purpose of this paper is to propose a new probabilistic model for automatic text categorization.",
        "While many text categorization models have been proposed so far, in this paper, we concentrate on the probabilistic models (Robertson and Sparck Jones, 1976; Kwok, 1990; Fuhr, 1989; Lewis, 1992; Croft, 1981; Wong and Yao, 1989; Yu et al., 1989) because these models have solid formal grounding in probability theory.",
        "Section 2 quickly reviews the probabilistic models and lists their individual problems.",
        "In section 3, we propose a new probabilistic model based on a Single random Variable with Multiple Values (SVMV).",
        "Our model is very simple, but solves some problems of the previous models.",
        "In section 4, we verify our model's superiority over the others through experiments in which we categorize \"Wall Street Journal\" articles."
      ]
    },
    {
      "heading": "2 A Brief Survey of Probabilistic Text Categorization",
      "text": [
        "In this section, we will briefly review three major probabilistic models for text categorization.",
        "Originally, these models have been exploited for information retrieval, but the adaptation to text categorization is straightforward.",
        "In a model of probabilistic text categorization, P(cld) = \"the probability that a document d is categorized into a category c\" (1) is calculated.",
        "Usually, a set of categories is defined beforehand.",
        "For every document di, probability P(cldi) is calculated and all the documents are ranked in decreasing order according to their probabilities.",
        "The larger P(cldi) a document di has, the more probably it will be categorized into category c. This is called the Probabilistic Ranking Principle (PRP) (Robertson, 1977).",
        "Several strategies can be used to assign categories to a document based on PRP (Lewis, 1992).",
        "There are several ways to calculate P(cld).",
        "Three representatives are (Robertson and Sparck Jones, 1976), (Kwok, 1990), and (Fuhr, 1989).",
        "2.1 Probabilistic Relevance Weighting (PRW) Robertson and Sparck Jones (1976) make use of the well-known logistic (or log-odds) transformation of the",
        "where E means \"not c\", that is \"a document is not categorized into c.\" Since this is a monotonic transformation of P(c!",
        "d), PRP is still satisfied after transformation.",
        "Using Bayes' theorem, Eq.",
        "(2) becomes",
        "Here, P(c) is the prior probability that a document is categorized into c. This is estimated from given training data, i.e., the number of documents assigned to the category c. P(dic) is calculated as follows.",
        "If we assume that a document consists of a set of terms (usually nouns are used for the first approximation) and each term appears independently in a document, P(d1c) is decomposed to",
        "where \"c – d\" is a set of terms that do not appear in d but appear in the training cases assigned to c. \"ti\" represents the name of a term and \"Ti 1, 0\" represents whether or not the corresponding term \"ti\" appears in a document.",
        "Therefore, P(Ti = 1, 01c) is the probability that a document does or does not contain the term ti, given that the document is categorized into c. This probability is estimated from the training data; the number of documents that are categorized into c and have the term ti.",
        "Substituting Eq.",
        "(4) into Eq.",
        "(3) yields",
        "We refer to Robertson and Sparck Jones' formulation as Probabilistic Relevance Weighting (PRW).",
        "While PRW is the first attempt to formalize well-known relevance weighting (Sparck Jones, 1972; Salton and McGill, 1983) by probability theory, there are several drawbacks in PRW.",
        "[Problem 1] no within-document term frequencies PRW does not make use of within-document term frequencies.",
        "P(T = 1, 01c) in Eq.",
        "(5) takes into account only the existence/absence of the term t in a document.",
        "In general, frequently appearing terms in a document play an important role in information retrieval (Salton and McGill, 1983).",
        "Salton and Yang experimentally verified the importance of within-document term frequencies in their vector model (Salton and Yang, 1973).",
        "[Problem 2] no term weighting for target documents In the PRW formulation, there is no factor of term weighting for target documents (i.e., P(•Id)).",
        "According to Eq.",
        "(5), even if a term exists in a target document, only the importance of the term in a category (i.e., P(T = 11c)) is considered for overall probability.",
        "Term weighting for target documents would also be necessary for sophisticated information retrieval (Fuhr, 1989; Kwok, 1990).",
        "[Problem 3] affected by having insufficient training cases In practical situations, the estimation of P(T = 1, 01c) is not always straightforward.",
        "Let us consider the following case.",
        "In the training data, we are given R documents that are assigned to c. Among them, r documents have the term t. In this example, the straightforward estimate of P(T = 11c) is \"r 1 R.\" If \"r = 0\" (i.e., none of the documents in c has t) and the target document d contains the term t, g(cid) becomes – oo, which means that d is never categorized into c. Robertson and Sparck Jones mentioned other special cases like the above example (Robertson and Sparck Jones, 1976).",
        "A well-known remedy for this problem is to use \"(r + 0.5)/(R + 1)\" as the estimate of P(T = 11c) (Robertson and Sparck Jones, 1976).",
        "While various smoothing methods (Church and Gale, 1991; Jelinek, 1990) are also applicable to these situations and would be expected to work better, we used the simple \"add one\" remedy in the following experiments."
      ]
    },
    {
      "heading": "2.2 Component Theory (CT)",
      "text": [
        "To solve problems 1 and 2 of PRW, Kwok (1990) stresses the assumption that a document consists of terms.",
        "This theory is called the Component Theory (CT).",
        "To introduce within-document term frequencies (i.e., to solve problem 1), CT assumes that a document is completely decomposed into its constituting terms.",
        "Therefore, rather than counting the number of documents, as in PRW, CT counts the number of terms in a document for probability estimation.",
        "This leads to within-document term frequencies.",
        "Moreover, to incorporate term weighting for target documents (i.e., to solve problem 2), CT defines g(c1d) as the geometric mean probabilities over components of the target document d;",
        "For precise derivation, refer to (Kwok, 1990).",
        "Here, note that P(T = tld) and P(T = tic) represent the frequency of a term t within a target document d and that within a category c respectively.",
        "Therefore, CT is not subject to problems 1 and 2.",
        "However, problem 3 still affects CT.",
        "Furthermore, Fuhr (1989) pointed out that transformation, as in Eq.",
        "(6), is not monotonic of P(cid).",
        "It follows then, that CT does not satisfy the probabilistic ranking principle (PRP) any more."
      ]
    },
    {
      "heading": "2.3 Retrieval with Probabilistic Indexing",
      "text": [
        "(RPI) Fuhr (1989) solves problem 2 by assuming that a document is probabilistically indexed by its term vectors.",
        "This model is called Retrieval with Probabilistic Indexing (RPI).",
        "In RPI, a document d has a binary vector x = ,Tn) where each component corresponds to a term.",
        "T1 = 1 means that the document d contains the term ti.",
        "X is defined as the set of all possible in-dexings, where 1X1 = r. Conditioning P(cid) for each possible indexing gives",
        "By assuming conditional independence between c and d given z1, and using Bayes' theorem, Eq.",
        "(8) becomes,",
        "Assuming that each term appears independently in a target document d and in a document assigned to c, Eq.",
        "(9) is rewritten as As far as other problems are concerned, RPI still problematic.",
        "In particular, because of problem 3, P(cid) would become an illegitimate value.",
        "In our experiments, as well as in Lewis' experiments (1992), P(cid) ranges from 0 to more than 1010."
      ]
    },
    {
      "heading": "3 A Probabilistic Model Based on a Single Random Variable with Multiple Values (SVMV)",
      "text": [
        "In this section, we propose a new probabilistic model for text categorization, and compare it to the previous three models from several viewpoints.",
        "Our model is very simple, but yet solves problems 1, 2, and 3 in PRW.",
        "Document representation of our model is basically the same as CT, that is a document is a set of its constituting terms.",
        "The major difference between our model and others is the way of document characterization through probabilities.",
        "While almost all previous models assume that an event space for a document is whether the document is indexed or not by a term2, our model characterizes a document as random sampling of a term from the term set that represents the document.",
        "For example, an event \"T = ti\" means that a randomly selected term from a document is ti.",
        "If we want to emphasis indexing process like other models, it is possible to interpret \"T ti\" as a randomly selected element from a document being indexed by the term ti.",
        "Formally, our model can be seen as modifying Fuhr's derivation of P(cid) by replacing an index vector with a single random variable whose value is one of possible terms.",
        "Conditioning P(cid) for each possible event gives",
        "If we assume conditional independence between c and (10) d, given T = ti, that is P(cld,T = ti) = P(clT = ti), we obtain Here, all the probabilities are estimated from the training data using the same method described in Section 2.1.",
        "Since Eq.",
        "(10) includes the factor P(T = 1,01d) as well as P(T = 1, 01c), RPI takes into account term weighting for target documents.",
        "While this in principle solves problem 2, if we use a simple estimation method counting the number of documents which have a term, P(T = 1, 01d) reduces to 1 or 0 (i.e, binary, not weighted).",
        "For example, when a target document d has a term t, P(t = 11d) = 1 and when not, P(T = 11d) = 0.",
        "In the following experiments we used this binary estimation method, but non binary estimates could be used as in (Fuhr, 1989).",
        "'More precisely, P(cld, x) P(clx) which assumes that if we know x, information for c is independent of that for d. This assumption sounds valid because x is a kind of representation of d. t, All the probabilities in Eq.",
        "(13) can be estimated from given training data based on the following definitions.",
        "• P(T = t21c) is the probability that a randomly selected term in a document is ti, given that the document is assigned to c. We used cas the estimator.",
        "NCi is the frequency of the term ti in the category c, and NC is the total frequency of terms in c. 21n section 2 explaining previous models, we simplified \"a document is indexed by a term\" as \"a document contains a term\" for ease of explanation.",
        "• P(T = 111d) is the probability that a randomly selected term in a target document d is ti.",
        "We used MI as the estimator.",
        "ND; is the frequency of the term ti in the document d, and ND is the total frequency of terms in d. • P(T =ti) is the prior probability that a randomly selected term in a randomly selected document is ti.",
        "We used 1*- as the estimator.",
        "AT,: is the frequency of the term ti in the given training documents, and N is the total frequency of terms in the training documents.",
        "• P(c) is the prior probability that a randomly selected document is categorized into c. We used -",
        "D a as the estimator.",
        "D, is the frequency of documents that is categorized to c in the given training documents, and D is the frequency of documents in the training documents.",
        "Here, let us recall the three problems of PRW.",
        "Since SVMV's primitive probabilities are based on within-document term frequencies, SVMV does not have problem 1.",
        "Furthermore, SVMV does not have problem 2 either because Eq.",
        "(13) includes a factor P(T = tld), which accomplishes term weighting for a target document d. For problem 3, let us reconsider the previous example; R documents in the training data are categorized into a category c, none of the R documents has term ti, but a target document d does.",
        "If the straightforward estimate of P(T• = 11c) = 0 or P(T = t1Jc) = 0 is adopted, the document d would never be categorized into c in the previous models (PRW, CT, and RPI).",
        "In SVMV, the probability P(cld) is much less affected by such estimates.",
        "This is because P(cld) in Eq.",
        "(13) takes the sum of each term's weight.",
        "In this example, the weight for ti is estimated to be 0 as in the other models, but this little affect the total value of P(cjd).",
        "A similar argument applies to all other problems in (Robertson and Sparck Jones, 1976) that are caused by having insufficient training cases.",
        "SVMV is formally proven not to suffer from the serious effects (like never being assigned to a category or always being assigned to a category) by having insufficient training cases.",
        "In other words, SVMV can directly use the straightforward estimates.",
        "In addition, we experimentally verified that the value of P(djc) in SVMV is always a legitimate value (i.e., 0 to 1) unlike in RPI.",
        "Table 1 summarizes the characteristics of the four probabilistic models.",
        "As illustrated in the table, SVMV has better characteristics for text categorization compared to the previous models.",
        "In the next section, we will experimentally verify SVMV's superiority."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": [
        "This section describes experiments conducted to evaluate the performance of our model (SVMV) compared to the other three (PRW, CT, and RPI)."
      ]
    },
    {
      "heading": "4.1 Data and Preprocessing",
      "text": [
        "A collection of Wall Street Journal (WSJ) full-text news stories (Liberman, 1991)3 was used in the experiments.",
        "We extracted all 12,380 articles from 1989/7/25 to 1989/11/2.",
        "The WSJ articles from 1989 are indexed with 78 categories (topics).",
        "Articles having no category were excluded.",
        "8,907 articles remained; each having 1.94 categories on the average.",
        "The largest category is"
      ]
    },
    {
      "heading": "\"TENDER OFFERS, MERGERS, ACQUISITIONS (TNM)\"",
      "text": [
        "which encompassed 2,475 articles; the smallest one is \"RUBBER (RUB)\", assigned to only 2 articles.",
        "On the average, one category is assigned to 443 articles.",
        "All 8,907 articles were tagged by the Xerox Part-of-Speech Tagger (Cutting et al., 1992)4.",
        "From the tagged articles, we extracted the root words of nouns using the \"ispell\" program'.",
        "As a result, each article has a set of root words representing it, and each element in the set (i.e. root word of a noun) corresponds to a term.",
        "We did not reduce the number of terms by using stop words list or feature selection method, etc.",
        "The number of terms amounts to 32,975.",
        "Before the experiments, we divided 8,907 articles into two sets; one for training (i.e., for probability estimation), and the other for testing.",
        "The division was made according to chronology.",
        "All articles that appeared from 1989/7/25 to 1989/9/29 went into a training set of 5,820 documents, and all articles from 1989/10/2 to 1989/11/2 went into a test set of 3,087 documents."
      ]
    },
    {
      "heading": "4.2 Category Assignment Strategies",
      "text": [
        "In the experiments, the probabilities, P(c), P(T; = 11c), P(T = ti lc), and so forth, were estimated from the 5,820 training documents, as described in the previous sections.",
        "Using these estimates, we calculated the posterior probability (P(cjd)) for each document (d) of the 3,087 test documents and each of the 78 categories 3We used \"ACL/DCI (September 1991)\" CD-ROM which is distributed from the Linguistic Data Consortium (LDC).",
        "For more details, please contact Mark Liberman (mylaunagi.cis.upenn.edu).",
        "'The xerox part-of-speech tagger version 1.0 is available via anonymous FTP from the host parcf tp.",
        "xerox .com in the directory pub/tagger.",
        "(c).",
        "The four probabilistic models are compared in this calculation.",
        "There are several strategies for assigning categories to a document based on the probability P(c1d).",
        "The simplest one is the k-per-doc strategy (Field, 1975) that assigns the top k categories to each document.",
        "A more sophisticated one is the probability threshold strategy, in which all the categories above a user-defined threshold are assigned to a document.",
        "Lewis proposed the proportional assignment strategy based on the probabilistic ranking principle (Lewis, 1992).",
        "Each category is assigned to its top scoring documents in proportion to the number of times the category was assigned in the training data.",
        "For example, a category assigned to 2% of the training documents would be assigned to the top scoring 0.2% of the test documents if the proportionality constant was 0.1, or to 10% of the test documents if the proportionality constant was 5.0."
      ]
    },
    {
      "heading": "4.3 Results and Discussions",
      "text": [
        "By using a category assignment strategy, several categories are assigned to each test document.",
        "The best known measures for evaluating text categorization models are recall and precision, calculated by the following equations (Lewis, 1992): the number of categories that are correctly assigned to documents R – the number of categories that should be' assigned to documents the number of categories that are correctly assigned to documents Precision – the number of categories that are' assigned to documents Note that recall and precision have somewhat mutually exclusive characteristics.",
        "To raise the recall value, one can simply assign many categories to each document.",
        "However, this leads to a degradation in precision; i.e., almost all the assigned categories are false.",
        "A breakeven point might be used to summarize the balance between recall and precision, the point at which they are equal.",
        "For each strategy, we calculated breakeven points by using the four probabilistic models.",
        "Table 2 shows the best breakeven points identified for the three strategies along with the used models.",
        "From Table 2, we find that SVMV with proportional assignment gives the best result (0.63).",
        "The superiority of proportional assignment over the other strategies has already been reported by Lewis (1992).",
        "Our experiment verified Lewis' assumption.",
        "In addition, for any of the three strategies, SVMV gives the highest breakeven point among the four probabilistic models.",
        "Figure 1 shows the recall/precision trade off for the four probabilistic models with proportional assignment strategy.",
        "As a reference, the recall/precision curve of a well-known vector model (Salton and Yang, 1973) (\"TF4DF\")6 is also presented.",
        "Table 3 lists the breakeven point for each model.",
        "All the breakeven points were obtained when proportionality constant was about 1.0.",
        "From Figure 1 and Table 3, we can see that:",
        "• as far as this dataset is concerned, SVMV with proportional assignment strategy gives the best result among the four probabilistic models, • the models that consider within-document term frequencies (SVMV, CT) are better than those that do not (PRW, RPI),",
        "61n the model we used, each element of document vector is the \"term frequency\" multiplied by the \"inverted document frequency.\" Similarity between every pair of vectors is measured by cosine.",
        "Note that this is the simplest version of TF•IDF model, and there has been many improvements which we did not consider in the experiments.",
        "• the models that consider term weighting for target documents (SVMV, CT) are better than those that do not (PRW, (RPI)), and • the models that are less affected by having insufficient training cases (SVMV) are better than those that are (CT, RPI, PRW)."
      ]
    },
    {
      "heading": "5 Conclusion",
      "text": [
        "We have proposed a new probabilistic model for text categorization.",
        "Compared to previous models, our model has the following advantages; 1) it considers within document term frequencies, 2) considers term weighting for target documents, and 3) is less affected by having insufficient training cases.",
        "We have also provided empirical results verifying our model's superiority over the others in the task of categorizing news articles from the \"Wall Street Journal.\" There are several directions along which this work could be extended.",
        "• We have to compare our probabilistic model to other non probabilistic models like decision tree/rule based models, one of which has recently been reported to be promising (Apte et al., 1994).",
        "• While we used simple document representation in which a document is defined as a set of nouns, there could be considered several improvements, such as using phrasal information (Lewis, 1992), clustering terms (Sparck Jones, 1973), reducing the number of features by using local dictionary (Apte et al., 1994), etc.",
        "• We are incorporating our probabilistic model into cluster-based text categorization that offers an efficient and effective search strategy."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "The authors are grateful to Hiroshi Motoda for beneficial discussions, and would like to thank the anonymous reviewers for their useful comments."
      ]
    }
  ]
}
