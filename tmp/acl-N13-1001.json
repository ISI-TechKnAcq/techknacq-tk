{
  "info": {
    "authors": [
      "Nadir Durrani",
      "Alexander Fraser",
      "Helmut Schmid"
    ],
    "book": "NAACL",
    "id": "acl-N13-1001",
    "title": "Model With Minimal Translation Units, But Decode With Phrases",
    "url": "https://aclweb.org/anthology/N13-1001",
    "year": 2013
  },
  "references": [
    "acl-D08-1089",
    "acl-J03-1002",
    "acl-J04-4002",
    "acl-J06-4004",
    "acl-J93-2003",
    "acl-J97-3002",
    "acl-N03-1017",
    "acl-N07-2035",
    "acl-N10-1140",
    "acl-N10-2003",
    "acl-P02-1040",
    "acl-P05-1069",
    "acl-P07-2045",
    "acl-P11-1086",
    "acl-P11-1105",
    "acl-W04-3250",
    "acl-W99-0604"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "N-gram-based models coexist with their phrase-based counterparts as an alternative SMT framework.",
        "Both techniques have pros and cons.",
        "While the N-gram-based framework provides a better model that captures both source and target contexts and avoids spurious phrasal segmentation, the ability to memorize and produce larger translation units gives an edge to the phrase-based systems during decoding, in terms of better search performance and superior selection of translation units.",
        "In this paper we combine N-gram-based modeling with phrase-based decoding, and obtain the benefits of both approaches.",
        "Our experiments show that using this combination not only improves the search accuracy of the N-gram model but that it also improves the BLEU scores.",
        "Our system outperforms state-of-the-art phrase-based systems (Moses and Phrasal) and N-gram-based systems by a significant margin on German, French and Spanish to English translation tasks."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Statistical Machine Translation advanced from word-based models (Brown et al., 1993) towards more sophisticated models that take contextual information into account.",
        "Phrase-based (Och and Ney, 2004; Koehn et al., 2003) and N-gram-based (Marin?o et al., 2006) models are two instances of such frameworks.",
        "While the two models have some common properties, they are substantially different.",
        "?Much of the work presented here was carried out while the first author was at the University of Stuttgart.",
        "Phrase-based systems employ a simple and effective machinery by learning larger chunks of translation called phrases1.",
        "Memorizing larger units enables the phrase-based model to learn local dependencies such as short reorderings, idioms, insertions and deletions, etc.",
        "The model however, has the following drawbacks: i) it makes independence assumptions over phrases ignoring the contextual information outside of phrases ii) it has issues handling long-distance reordering iii) it has the spurious phrasal segmentation problem which allows multiple derivations of a bilingual sentence pair having different model scores for each segmentation.",
        "Modeling with minimal translation units helps address some of these issues.",
        "The N-gram-based SMT framework is based on tuples.",
        "Tuples are minimal translation units composed of source and target cepts2.",
        "N-gram-based models are Markov models over sequences of tuples (Marin?o et al., 2006; Crego and Marin?o, 2006) or operations encapsulating tuples (Durrani et al., 2011).",
        "This mechanism has several useful properties.",
        "Firstly, no phrasal independence assumption is made.",
        "The model has access to both source and target context outside of phrases.",
        "Secondly the model learns a unique derivation of a bilingual sentence given its alignment, thus avoiding the spurious segmentation problem.",
        "Using minimal translation units, however, results",
        "poor translation selection, ii) inaccurate future-cost estimates and iii) incorrect early pruning of hypotheses that would produce better model scores if allowed to continue.",
        "In order to deal with these problems, search is carried out only on a graph of pre-calculated orderings, and ad-hoc reordering limits are imposed to constrain the search space (Crego et al., 2005; Crego and Marin?o, 2006), or a higher beam size is used in decoding (Durrani et al., 2011).",
        "The ability to memorize and produce larger translation chunks during decoding, on the other hand, gives a distinct advantage to the phrase-based system during search.",
        "Phrase-based systems i) have access to uncommon translations, ii) do not require higher beam sizes, iii) have more accurate future-cost estimates because of the availability of phrase-internal language model context before search is started.",
        "To illustrate this consider the German-English phrase-pair ?scho?",
        "ein Tor ?",
        "scored a goal?, composed from the tuples (cept-pairs) ?scho?",
        "?",
        "scored?, ?ein ?",
        "a?",
        "and ?Tor ?",
        "goal?.",
        "It is likely that the N-gram system does not have the tuple ?scho?",
        "?",
        "scored?",
        "in its n-best translation options because ?scored?",
        "is an uncommon translation for ?scho??",
        "outside the sports domain.",
        "Even if ?scho?",
        "?",
        "scored?",
        "is hypothesized, it will be ranked quite low in the stack until ?ein?",
        "and ?Tor?",
        "are generated in the next steps.",
        "A higher beam is required to prevent it from getting pruned.",
        "Phrase-based systems, on the other hand, are likely to have access to the phrasal unit ?scho?",
        "ein Tor ?",
        "scored a goal?",
        "and can generate it in a single step.",
        "Moreover, a more accurate future-cost estimate can be computed because of the available context internal to the phrase.",
        "In this work, we extend the N-gram model, based on operation sequences (Durrani et al., 2011), to use phrases during decoding.",
        "The main idea is to study whether a combination of modeling with minimal translation units and using phrasal information during decoding helps to solve the above-mentioned problems.",
        "The remainder of this paper is organized as follows.",
        "The next two sections review phrase-based and N-gram-based SMT.",
        "Section 2 provides a comparison of phrase-based and N-gram-based SMT.",
        "Section 3 summarizes the operation sequence model (OSM), the main baseline for this work.",
        "Section"
      ]
    },
    {
      "heading": "4 analyzes the search problem when decoding with",
      "text": [
        "minimal units.",
        "Section 5 discusses how information available in phrases can be used to improve search performance.",
        "Section 6 presents the results of this work.",
        "We conducted experiments on the German-to-English and French-to-English translation tasks and found that using phrases in decoding improves both search accuracy and BLEU scores.",
        "Finally we compare our system with two state-of-the-art phrase-based systems (Moses and Phrasal) and two state-of-the-art N-gram-based systems (Ncode and OSM) on standard translation tasks."
      ]
    },
    {
      "heading": "2 Previous Work",
      "text": [
        "Phrase-based and N-gram-based SMT are alternative frameworks for string-to-string translation.",
        "Phrase-based SMT segments a bilingual sentence pair into phrases that are continuous sequences of words (Och and Ney, 2004; Koehn et al., 2003) or discontinuous sequences of words (Galley and Manning, 2010).",
        "These phrases are then reordered through a lexicalized reordering model that takes into account the orientation of a phrase with respect to its previous phrase (Tillmann and Zhang, 2005) or block of phrases (Galley and Manning, 2008).",
        "There are several drawbacks of the phrase-based model.",
        "Firstly it makes an independence assumption over phrases, according to which phrases are translated independently of each other, thus ignoring the contextual information outside of the phrasal boundary.",
        "This problem is corrected by the monolingual language model that takes context into account.",
        "But often the language model cannot compensate for the dispreference of the translation model for non-local dependencies.",
        "The second problem is that the model is unaware of the actual phrasal segmentation of a sentence during training.",
        "It therefore learns all possible ways of segmenting a bilingual sentence.",
        "Different segmentations of a bilingual sentence re",
        "sult in different probability scores for the translation and reordering models, causing spurious ambiguity in the model.",
        "See Figure 1.",
        "In the first segmentation, the model learns the lexical and reordering probabilities of the phrases ?sie wu?rden ?",
        "they would?",
        "and ?gegen ihre kampagne abstimmen ?",
        "vote against your campaign?.",
        "In the second segmentation, the model learns the lexical and reordering probabilities of the phrases ?sie ?",
        "they?",
        "?wu?rden ?",
        "would?, ?abstimmen ?",
        "vote?, ?gegen ihre kampagne ?",
        "against your campaign?.",
        "Both segmentations result in different translation and reordering scores.",
        "This kind of ambiguity in the model subsequently results in the presence of many different equivalent segmentations in the search space.",
        "Also note that the two segmentations contain different information.",
        "From the first segmentation the model learns the dependency between the verb ?abstimmen ?",
        "vote?",
        "and the phrase ?gegen ihre kampagne ?",
        "against your campaign?.",
        "The second segmentation allows the model to capture the reordering of the complex verb predicate ?wu?rden ?",
        "would?",
        "and ?abstimmen ?",
        "vote?",
        "by learning that the verb ?abstimmen ?",
        "vote?",
        "is discontinuous with respect to the auxiliary.",
        "This information cannot be captured in the first segmentation because of the phrasal independence assumption and stiff phrasal boundaries.",
        "The model loses one of the dependencies depending upon which segmentation it chooses during decoding.",
        "N-gram-based SMT is an instance of a joint model that generates source and target strings together in bilingual translation units called tuples.",
        "Tuples are essentially phrases but they are atomic units that cannot be decomposed any further.",
        "This condition of atomicity results in a unique segmentation of the bilingual sentence pair given its alignments.",
        "The model does not make any phrasal independence assumption and generates a tuple by looking at a context of n ?",
        "1 previous tuples (or operations).",
        "This allows the N-gram model to model all the dependencies through a single derivation.",
        "The main drawback of N-gram-based SMT is its poor search mechanism which is inherent from using minimal translation units during search.",
        "Decoding with tuples has problems with a high number of search errors caused by lower translation coverage, inaccurate future-cost estimation and pruning of correct hypotheses (see Section 4.2 for details).",
        "Crego and Marin?o (2006) proposed a way to couple reordering and search through POS-based rewrite rules.",
        "These rules are learned during training when units with crossing alignments are unfolded through source linearization to form minimal tuples.",
        "For example, in Figure 1, the N-gram-based MT will linearize the word sequence ?gegen ihre kampagne abstimmen?",
        "to ?abstimmen gegen ihre kampagne?, so that it is in the same order as the English words.",
        "It also learns a POS-rule ?IN PRP NN VB ?",
        "VB IN PRP NN?.",
        "The POS-based rewrite rules serve to precompute the orderings that are hypothesized during decoding.",
        "Coupling reordering and search allows the N-gram model to arrange hypotheses in 2m stacks (for an m word source sentence), each containing hypotheses that cover exactly the same foreign words.",
        "This removes the need for future-cost estimation3.",
        "Secondly, memorizing POS-based rules enables phrase-based like reordering, however without lexical selection.",
        "There are three drawbacks of this approach.",
        "Firstly, lexical generation and reordering are decoupled.",
        "Search is only performed on a small number of reorderings, pre-calculated using the source side and completely ignoring the target-side.",
        "And lastly, the POS-based rules face data sparsity problems especially in the case of long distance reorderings.",
        "Durrani et al. (2011) recently addressed these problems by proposing an operation sequence N-gram model which strongly couples translation and reordering, hypothesizes all possible reorderings and does not require POS-based rules.",
        "Representing bilingual sentences as a sequence of operations enables them to memorize phrases and lexical reordering triggers like PBSMT.",
        "However, using minimal units during decoding and searching over all possible reorderings means that hypotheses can no longer be arranged in 2m stacks.",
        "The problem of inaccurate future-cost estimates resurfaces resulting in more search errors.",
        "A higher beam size of 500 is therefore used to produce translation units in comparison to phrase-based systems.",
        "This, however, still does not eliminate all search errors.",
        "This paper shows that using phrases instead of cepts in de-3Using m stacks with future-cost estimation is a more efficient solution but is not used ?due to the complexity of accurately computing these estimations in the N-gram architecture?",
        "(Crego et al., 2011).",
        "coding improves the search accuracy and translation quality.",
        "It also shows that using some phrasal information incept-based decoding captures some of these improvements."
      ]
    },
    {
      "heading": "3 Operation Sequence Model",
      "text": [
        "The N-gram model with integrated reordering models a sequence of operations obtained through the transformation of a bilingual sentence pair.",
        "An operation can either be to i) generate a sequence of source and target words, ii) to insert a gap as a place-holder for skipped words, iii) or to jump forward and backward in a sentence to translate words discontinuously.",
        "The translate operation Generate(X,Y) encapsulates the translation tuple (X,Y).",
        "It generates source and target translations simultaneously4.",
        "This is similar to N-gram-based SMT except that the tuples in the N-gram-based model are generated monotonically, whereas in this case lexical generation and reordering information is strongly coupled in an operation sequence.",
        "Consider the phrase pair: The model memorizes it through the sequence: Generate(Wie, What is)?",
        "Gap?",
        "Generate (Sie, your)?",
        "Jump Back (1)?Generate (heissen, name) Let O = o1, .",
        ".",
        ".",
        ", oj?1 be a sequence of operations as hypothesized by the translator to generate the bilingual sentence pair ?F,E?",
        "with an alignment function A.",
        "The translation model is defined as:",
        "where n indicates the amount of context used.",
        "The translation model is implemented as an N-gram model of operations using SRILM-Toolkit (Stol-cke, 2002) with Kneser-Ney smoothing.",
        "A 9-gram model is used.",
        "Several count-based features such as gap and open gap penalties and distance-based features such as gap-width and reordering distance are added to the model, along with the lexical weighting and length penalty features in a standard log-linear framework (Durrani et al., 2011)."
      ]
    },
    {
      "heading": "4 Search",
      "text": []
    },
    {
      "heading": "4.1 Overview of Decoding Framework",
      "text": [
        "The decoding framework used in the operation sequence model is based on Pharaoh (Koehn, 2004a).",
        "The decoder uses beam search to build up the translation from left to right.",
        "The hypotheses are arranged in m stacks such that stack i maintains hypotheses that have already translated i many foreign words.",
        "The ultimate goal is to find the best scoring hypothesis, that has translated all the words in the foreign sentence.",
        "The overall process can be roughly divided into the following steps: i) extraction of translation units ii) future-cost estimation, iii) hypothesis extension iv) recombination and pruning.",
        "During the hypothesis extension each extracted phrase is translated into a sequence of operations.",
        "The reordering operations (gaps and jumps) are generated by looking at the position of the translator, the last foreign word generated etc.",
        "(Refer to Algorithm 1 in Durrani et al. (2011)).",
        "The probability of an operation depends on the n ?",
        "1 previous operations.",
        "The model backs-off to the smaller n-grams of operations if the full history is unknown.",
        "We use Kneser-Ney smoothing to handle back-off5."
      ]
    },
    {
      "heading": "4.2 Drawbacks of Cept-based Decoding",
      "text": [
        "One of the main drawbacks of the operation sequence model is that it has a more difficult search problem than the phrase-based model.",
        "The operation model, although based on minimal translation units, can learn larger translation chunks by memorizing a sequence of operations.",
        "However, using cepts during decoding has the following drawbacks: i) the cept-based decoder does not have access to all the translation units that a phrase-based decoder uses as part of a larger phrase.",
        "ii) it requires a higher beam size to prevent early pruning of better hypotheses that lead toward higher model scores when allowed to continue and iii) it uses worse future-cost estimates than the phrase-based decoder.",
        "Recall the example from the last section.",
        "For the cept-based decoder to generate the same phrasal translation, it requires three separate tuple translations ?Wie ?",
        "what is?, ?Sie ?",
        "your?",
        "and ?hei?en ?",
        "name?.",
        "Here we are faced with three challenges.",
        "Translation Coverage: The first problem is that the N-gram model does not have the same coverage of translation options.",
        "The English cepts ?what is?, ?your?",
        "and ?name?",
        "are not good candidate translations for the German cepts ?Wie?, ?Sie?",
        "and ?hei?en?, respectively.",
        "When extracting tuple translations for these cepts from the Europarl data for our system, the tuple ?Wie ?",
        "what is?",
        "is ranked 124th, ?hei?en ?",
        "name?",
        "is ranked 56th, and ?Sie ?",
        "your?",
        "is ranked 9th in the list of n-best translation candidates.",
        "Typically only the 20 best translation options are used, to reduce the decoding time, and such phrasal units with less frequent cept translations are never hypothesized in the N-gram-based systems.",
        "The phrase-based system on the other hand can extract the phrase ?Wie hei?en Sie ?",
        "what is your name?",
        "even if it is observed only once during training.",
        "A similar problem is also reported in Costa-jussa` et al. (2007).",
        "When trying to reproduce the sentences in the n-best translation output of the phrase-based system, the N-gram-based system was only able to produce 37.5% of the sentences in the Spanish-to-English and 37.2% in the English-to-Spanish translation tasks.",
        "In comparison the phrase-based system was able to reproduce 57.5% and 48.6% of the sentences in the n-best translation output of the Spanish-to-English and English-to-Spanish N-gram-based systems.",
        "Larger Beam Size: A related problem is that a higher beam size is required incept-based decoding to prevent uncommon translations from getting pruned.",
        "The phrase-based system can generate the phrase-pair ?Wie hei?en Sie ?",
        "what is your name?",
        "in a single step placing it directly into the stack three words to the right.",
        "The cept-based decoder generates this phrase in three stacks with the tuple translations ?Wie ?",
        "What is?, ?Sie ?",
        "your?",
        "and ?hei?en ?",
        "name?.",
        "A very large stack size is required during decoding to prevent the pruning of ?Wie ?",
        "What is?",
        "which is ranked quite low in the stack until the tuple ?Sie ?",
        "your?",
        "is hypothesized in the next stack.",
        "Costa-jussa` et al. (2007) reports a significant drop in the performance of N-gram-based SMT when a beam size of 10 is used instead of 50 in their experiments.",
        "For the (cept-based) operation sequence model, Durrani et al.",
        "(2011) required a stack size of 500.",
        "In comparison, the translation quality achieved by phrase-based SMT remains the same when varying the beam size between 5 and 50.",
        "Future-Cost Estimation: A third problem is caused by inaccurate future-cost estimation.",
        "Using phrases helps phrase-based SMT to better estimate the future language model cost because of the larger context available, and allows the decoder to capture local (phrase-internal) reorderings in the future cost.",
        "In comparison the future cost for tuples is mostly un-igram probabilities.",
        "The future-cost estimate for the phrase pair ?Wie hei?en Sie ?",
        "What is your name?",
        "is estimated by calculating the cost of each feature.",
        "The language model cost, for example, is estimated in the phrase-based system as follows: plm = p(What)?",
        "p(is|What)?",
        "p(your|What is) ?",
        "p(name|What is your) The cost of the direct phrase translation probability, one of the features used in the phrase translation model, is estimated as: ptm = p(What is your name|Wie hei?en Sie) Phrase-based SMT is aware during the preprocessing step that the words ?Wie hei?en Sie?",
        "may be translated as a phrase.",
        "This is helpful for estimating a more accurate future cost because the phrase-internal context is already available.",
        "The same is not true for the operation sequence model, to which only minimal units are available.",
        "The operation model does not have the information that ?Wie hei?en Sie?",
        "may be translated as a phrase during decoding.",
        "The future-cost estimate available to the operation model for the span covering ?Wie hei?en Sie?",
        "will have un-igram probabilities for both the translation and language model:",
        "Thus the future-cost estimate in the operation model is much worse than that of the phrase-based model.",
        "The poor future-cost estimation leads to search errors, causing a drop in the translation quality.",
        "A more accurate future-cost estimate for the translation model cost would be:",
        "where C is the context, i.e., the n?1 previously generated operations.",
        "The future-cost estimates computed in this manner are much more accurate because they not only consider context, but also take the reordering operations into account."
      ]
    },
    {
      "heading": "5 N-gram Model with Phrase-based",
      "text": []
    },
    {
      "heading": "Decoding",
      "text": [
        "In the last section we discussed the disadvantages of using cepts during search in a left-to-right decoding framework.",
        "We now define a method to empirically study the mentioned drawbacks and whether using information available in phrase-pairs during decoding can help improve search accuracy and translation quality."
      ]
    },
    {
      "heading": "5.1 Training",
      "text": [
        "We extended the training steps in Durrani et al. (2011) to extract a phrase lexicon from the parallel data.",
        "We extract all phrase pairs of length 6 and below, that are consistent (Och et al., 1999) with the word alignments.",
        "Only continuous phrases as used in a traditional phrase-based system are extracted thus allowing only inside-out (Wu, 1997) type of alignments.",
        "The future cost of each feature component used in the log-linear model is calculated.",
        "The operation sequence required to hypothesize each phrase is generated and its future cost is calculated.",
        "The future costs of other features such as language models, lexicalized probability features, etc.",
        "are also estimated.",
        "The estimates of the count-based reordering penalties (gap penalty and open gap penalty) and the distance-based features (gap-width and reordering distance) could not be estimated previously with cepts but are available when using phrases."
      ]
    },
    {
      "heading": "5.2 Decoding",
      "text": [
        "We extended the decoder developed by Durrani et al. (2011) and tried three ideas.",
        "In our primary experiments we enabled the decoder to use phrases instead of cepts.",
        "This allows the decoder to i) use phrase-internal context when computing the future-cost estimates, ii) hypothesize translation options not available to the cept-based decoder iii) cover multiple source words in a single step subsequently improving translation coverage and search.",
        "Note that using phrases instead of cepts during decoding, does not reintroduce the spurious phrasal segmentation problem as is present in the phrase-based system, because the model is built on minimal units which avoids segmentation ambiguity.",
        "Different compositions of the same phrasal unit lead to exactly the same model score.",
        "We therefore do not create any alternative compositions of the same phrasal unit during decoding.",
        "This option is not available in phrase-based decoding, because an alternative composition may lead towards a better model score.",
        "In our secondary set of experiments, we used cept-based decoding but modified the decoder to use information available from the phrases extracted for the test sentences.",
        "Firstly, we used future-cost estimates from the extracted phrases (see system cept.500.fc in Table1).",
        "This however, leads to inconsistency in the cases where the future cost is estimated from some phrasal unit that cannot be generated through the available cept translations.",
        "For example, say the best cost to cover the sequence ?Wie hei?en Sie?",
        "is given by the phrase ?What is your name?.",
        "The 20-best translation options incept-based system, however, do not have tuples ?Wie ?",
        "What?",
        "and ?hei?en ?",
        "name?.",
        "To remove this discrepancy, we add all such tuples that are used in the extracted phrases, to the list of extracted cepts (system cept.500.fc.t).",
        "We also studied how much gain we obtain by only adding tuples from phrases and using cept-based future-cost estimates (system cept.500.t)."
      ]
    },
    {
      "heading": "5.3 Evaluation Method",
      "text": [
        "To evaluate our modifications we apply a simple strategy.",
        "We hold the model constant and change the search to use the baseline decoder, which uses minimal translation units, or the modified decoders that use phrasal information during decoding.",
        "The model parameters are optimized by running MERT (minimum error rate training) for the baseline decoder on the dev set.",
        "After we have the optimized weights, we run the baseline decoder and our modifications on the test.",
        "Note that because all the decoding runs use the same feature vector, the model",
        "stays constant, only search changes.",
        "This allows us to compare different decoding runs, obtained using the same parameters, but different search strategies, in terms of model scores.",
        "We compute a search accuracy and translation quality for each run.",
        "Search accuracy is computed by comparing translation hypotheses from the different decoding runs.",
        "We form a collection of the best scoring hypotheses by traversing through all the runs and selecting the sentences with highest model score.",
        "For each input sentence we select a single best scoring hypothesis.",
        "The best scoring hypothesis can be contributed from several runs.",
        "In this case all these runs will be given a credit for that particular sentence when computing the search accuracy.",
        "The search accuracy of a decoding run is defined as the percentage of hypotheses that were contributed from this run, when forming a list of best scoring hypotheses.",
        "For example, for a test set of 1000 sentences, the accuracy of a decoding run would be 30% if it was able to produce the best scoring hypothesis for 300 sentences in the test set.",
        "Translation quality is measured through BLEU (Papineni et al., 2002)."
      ]
    },
    {
      "heading": "6 Experimental Setup",
      "text": [
        "We initially experimented with two language pairs: German-to-English (G-E) and French-to-English (F-E).",
        "We trained our system and the baseline systems on most of the data6 made available for the translation task of the Fourth Workshop on Statistical Machine Translation.7 We used 1M bilingual sentences, for the estimation of the translation model and 2M sentences from the monolingual corpus (news commentary) which also contains the English part of the bilingual corpus.",
        "Word alignments are obtained by running GIZA++ (Och and Ney, 2003) with the grow-diag-final-and (Koehn et al., 2005) symmetrization heuristic.",
        "We follow the training steps described in Durrani et al. (2011), consisting of i) post-processing the alignments to remove discontinuous and unaligned target cepts, ii) conversion of bilingual alignments into operation sequences, iii) estimation of the n-gram language models."
      ]
    },
    {
      "heading": "6.1 Search Accuracy Results",
      "text": [
        "We divided our evaluation into two halves.",
        "In the first half we carried out experiments to measure search accuracy and translation quality of our decoders against the baseline cept-based OSM (cept.500) that uses minimal translation units with a stack size of 500.",
        "We used the version of the cept-based OSM system that does not allow discontinuous8 source cepts.",
        "To increase the speed of the system we used a hard reordering limit of 159, in the baseline decoder and our modifications, disallowing jumps that are beyond 15 words from the first open gap.",
        "For each extracted cept or phrase 10-best translation options are extracted.",
        "Using phrases in search reduces the decoding speed.",
        "In order to make a fair comparison, both the phrase-based and the baseline cept-based decoders should be allowed to run for the same amount of time.",
        "We therefore reduced the stack size in the phrase-based decoder so that it runs in the same amount of time as the cept-based decoder.",
        "We found that using a stack size of 20010 for the phrase-based decoder was comparable in speed to using a stack-size of 500 in the cept-based decoding.",
        "We first tuned the baseline on dev11 to obtain an optimized weight vector.",
        "We then ran the baseline and our decoders as discussed in Section 5.2 on the dev-test.",
        "Then we repeated this experiment by tuning the weights with our phrase-based decoder (using a stack size of 100) and ran all the decoders again using the new weights.",
        "Table 1 shows the average search accuracies and BLEU scores of the two experiments.",
        "Using phrases during decoding in the GE experiments resulted in a statistically significant12 0.69 BLEU points gain comparing our best system phrase.200 with the baseline system cept.500.",
        "We mark a result as sig",
        "the Baseline and Our Decoders with different Stack Sizes (fc = Future Cost Estimated from Phrases, t = Cept Translation Options enriched from Phrases) nificant if the improvement shown by our decoder over the baseline decoder (cept.500) is significant at the p ?",
        "0.05 level, in both the runs.",
        "All the outputs that show statistically significant improvements over the baseline decoder (cept.500) in Table 1 are marked with an asterisk.",
        "The search accuracy of our best system (phrase.200), in GE experiments is roughly 80%, which means that 80% of the times the phrase-based decoder (using stack size 200) was able to produce the same model score or a better model score than the cept-based decoders (using a stack size of 500).",
        "Our F-E experiments also showed improvements in BLEU and model scores.",
        "The search accuracy of our best system phrase.200 is roughly 93% as compared with 55% in the baseline decoder (cept.500) giving a BLEU point gain of +0.30 over the baseline.",
        "Our modifications to the cept-based decoder also showed improvements.",
        "We found that extending the cept translation table (cept.500.t) using phrases helps both in GE and F-E experiments by extending the list of n-best translation options by 18% and 18.30% respectively.",
        "Using future costs estimated from phrases (cept.500.fc) improved both search accuracy and BLEU scores in GE experiments, but does not lead to any improvements in the F-E experiments, as both BLEU and model scores drop slightly.",
        "We looked at a few examples where the model score dropped and found that in these cases, the best scoring hypotheses are ranked very low earlier in the decoding and make their way to the top gradually in subsequent steps.",
        "A slight difference in the future-cost estimate prunes these hypotheses in one or the other decoder.",
        "We found future cost to be more critical in GE than F-E experiments.",
        "This can be explained by the fact that more reordering is required in GE and it is necessary to account for the reordering operations and jump-based features (gap-based penalties, reordering distance and gap-width) in the future-cost estimation.",
        "F-E on the other hand is largely monotonic except for a few short distance reorderings such as flipping noun and adjective."
      ]
    },
    {
      "heading": "6.2 Comparison with other Baseline Systems",
      "text": [
        "In the second half of our evaluation we compared our best system phrase.200 with the baseline system cept.500, and other state-of-the-art phrase-based and N-gram-based systems on German-to-English, French-to-English, and Spanish-to-English tasks13.",
        "We used the official evaluation data (news-test sets) from the Statistical Machine Translation Workshops 2009-2011 for all three language pairs (German, Spanish and French).",
        "The feature weights for all the systems are tuned using the dev set news-dev2009a.",
        "We separately tune the baseline system (cept.500) and the phrase-based system (phrase.200) and do not hold the lambda vector constant like before.",
        "Baseline Systems: We also compared our system with i) Moses (Koehn et al., 2007), ii) Phrasal14 (Cer et al., 2010), and iii) Ncode (Crego et al., 2011).",
        "We used the default stack sizes of 100 for Moses15, 200 for Phrasal, 25 for Ncode (with 2m stacks).",
        "A 5-gram English language model is used.",
        "Both phrase-based systems use 20-best phrases for translation, Ncode uses 25-best tuple translations.",
        "The training and test data for Ncode was tagged using TreeTagger (Schmid, 1994).",
        "All the baseline systems used lexicalized reordering model.",
        "A hard reordering limit16 of 6 words is used as a default in 13We did not include the results of Spanish in the previous section due to space limitations but these are similar to those of the French-to-English translation task.",
        "14Phrasal provides two extensions to Moses: i) hierarchical reordering model (Galley and Manning, 2008) and ii) discontinuous phrases (Galley and Manning, 2010).",
        "15Using stacks sizes from 200?1000 did not improve results.",
        "16We tried to increase the distortion limit in the baseline sys",
        "both the baseline phrase-based systems.",
        "Amongst the other defaults we retained the hard source gap penalty of 15 and a target gap penalty of 7 in Phrasal.",
        "We provide Moses and Ncode with the same post-edited alignments17 from which we removed target-side discontinuities.",
        "We feed the original alignments to Phrasal because of its ability to learn discontinuous source and target phrases.",
        "All the systems use MERT for the optimization of the weight vector.",
        "based decoder against the baselines.",
        "Our system shows an improvement over all the baseline systems for the GE pair, in 11 out of 12 cases in the F-E pair and in 8 out of 12 cases in the SE language pair.",
        "We mark a baseline with ?*?",
        "to indicate that our decoder shows an improvement over this baseline result which is significant at the p ?",
        "0.05 level."
      ]
    },
    {
      "heading": "7 Conclusion and Future Work",
      "text": [
        "We proposed a combination of using a model based on minimal units and decoding with phrases.",
        "Modeling with minimal units enables us to learn local and non-local dependencies in a unified manner and avoid spurious segmentation ambiguities.",
        "However, using minimal units also in the search presents a significant challenge because of the poor translation coverage, inaccurate future-cost estimates and tems to 15 (in GE experiments) as used in our systems but the results dropped significantly in case of Moses and slightly for Phrasal so we used the default limits for both decoders.",
        "17Using post-processed alignments gave slightly better results than the original alignments for these baseline systems.",
        "Details are omitted due to space limitation.",
        "the pruning of the correct hypotheses.",
        "Phrase-based SMT on the other hand overcomes these drawbacks by using larger translation chunks during search.",
        "However, the drawback of the phrase-based model is the phrasal independence assumption, spurious ambiguity in segmentation and a weak mechanism to handle non-local reorderings.",
        "We showed that combining a model based on minimal units with phrase-based decoding can improve both search accuracy and translation quality.",
        "We also showed that the phrasal information can be indirectly used incept-based decoding with improved results.",
        "We tested our system against the state-of-the-art phrase-based and N-gram-based systems, for German-to-English, French-to-English, and Spanish-to-English for three standard test sets.",
        "Our system showed statistically significant improvements over all the baseline systems in most of the cases.",
        "We have shown the benefits of using phrase-based search with a model based on minimal units.",
        "In future work, we would like to study whether a phrase-based system like Moses or Phrasal can profit from an OSM-style or N-gram-style feature.",
        "Feng et al. (2010) previously showed that adding a linearized source-side language model in a phrase-based system helped.",
        "It would also be interesting to study whether the insight of using minimal units for modeling and phrase-based search would hold for hierarchical SMT.",
        "Vaswani et al.",
        "(2011) recently showed that a Markov model over the derivation history of minimal rules can obtain the same translation quality as using grammars formed with composed rules."
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We would like to thank the anonymous reviewers for their helpful feedback and suggestions.",
        "Nadir Durrani and Alexander Fraser were funded by",
        "2013) under grant agreement n ?",
        "287658.",
        "Helmut Schmid was supported by Deutsche Forschungsgemeinschaft grant SFB 732.",
        "This work was supported in part by the IST Programme of the European Community, under the PASCAL2 Network of Excellence, IST-2007-216886.",
        "This publication only reflects the authors?",
        "views."
      ]
    }
  ]
}
