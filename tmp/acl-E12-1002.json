{
  "info": {
    "authors": [
      "Spyros Martzoukos",
      "Christof Monz"
    ],
    "book": "EACL",
    "id": "acl-E12-1002",
    "title": "Power-Law Distributions for Paraphrases Extracted from Bilingual Corpora",
    "url": "https://aclweb.org/anthology/E12-1002",
    "year": 2012
  },
  "references": [
    "acl-C04-1051",
    "acl-C10-1069",
    "acl-D08-1021",
    "acl-D11-1108",
    "acl-J10-3003",
    "acl-N06-1003",
    "acl-N10-1017",
    "acl-P05-1074",
    "acl-P08-1089",
    "acl-P10-2001",
    "acl-P11-2096",
    "acl-W07-0718",
    "acl-W08-2006"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We describe a novel method that extracts paraphrases from a bitext, for both the source and target languages.",
        "In order to reduce the search space, we decompose the phrase-table into sub-phrase-tables and construct separate clusters for source and target phrases.",
        "We convert the clusters into graphs, add smoothing/syntacticinformation-carrier vertices, and compute the similarity between phrases with a random walk-based measure, the commute time.",
        "The resulting phrase-paraphrase probabilities are built upon the conversion of the commute times into artificial co-occurrence counts with a novel technique.",
        "The co-occurrence count distribution belongs to the power-law family."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Paraphrase extraction has emerged as an important problem in NLP.",
        "Currently, there exists an abundance of methods for extracting paraphrases from monolingual, comparable and bilingual corpora (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010); we focus on the latter and specifically on the phrase-table that is extracted from a bitext during the training stage of Statistical Machine Translation (SMT).",
        "Bannard and Callison-Burch (2005) introduced the pivoting approach, which relies on a 2-step transition from a phrase, via its translations, to a paraphrase candidate.",
        "By incorporating the syntactic structure of phrases (Callison-Burch, 2005), the quality of the paraphrases extracted with pivoting can be improved.",
        "Kok and Brockett (2010) (henceforth KB) used a random walk framework to determine the similarity between phrases, which was shown to outperform pivoting with syntactic information, when multiple phrase-tables are used.",
        "In SMT, extracted paraphrases with associated pivot-based (Callison-Burch et al. 2006; Onishi et al. 2010) and cluster-based (Kuhn et al., 2010) probabilities have been found to improve the quality of translation.",
        "Pivoting has also been employed in the extraction of syntactic paraphrases, which are a mixture of phrases and non-terminals (Zhao et al. 2008; Ganitkevitch et al. 2011).",
        "We develop a method for extracting paraphrases from a bitext for both the source and target languages.",
        "Emphasis is placed on the quality of the phrase-paraphrase probabilities as well as on providing a stepping stone for extracting syntactic paraphrases with equally reliable probabilities.",
        "In line with previous work, our method depends on the connectivity of the phrase-table, but the resulting construction treats each side separately, which can potentially be benefited from additional monolingual data.",
        "The initial problem in harvesting paraphrases from a phrase-table is the identification of the search space.",
        "Previous work has relied on breadth first search from the query phrase with a depth of 2 (pivoting) and 6 (KB).",
        "The former can be too restrictive and the latter can lead to excessive noise contamination when taking shallow syntactic information features into account.",
        "Instead, we choose to cluster the phrase-table into separate source and target clusters and in order to make this task computationally feasible, we decompose the phrase-table into sub-phrase-tables.",
        "We propose a novel heuristic algorithm for the decomposition of the phrase-table (Section 2.1), and use a well-established co-clustering algorithm for clustering each sub-phrase-table (Section 2.2).",
        "The underlying connectivity of the source and target clusters gives rise to a natural graph representation for each cluster (Section 3.1).",
        "The vertices of the graphs consist of phrases and features with a dual smoothing/syntacticinformation-carrier role.",
        "The latter allow (a) redistribution of the mass for phrases with no appropriate paraphrases and (b) the extraction of syntactic paraphrases.",
        "The proximity among vertices of a graph is measured by means of a randomwalk distance measure, the commute time (Aldous and Fill, 2001).",
        "This measure is known to perform well in identifying similar words on the graph of WordNet (Rao et al. 2008) and a related measure, the hitting time is known to perform well in harvesting paraphrases on a graph constructed from multiple phrase-tables (KB).",
        "Generally in NLP, power-law distributions are typically encountered in the collection of counts during the training stage.",
        "The distances of Section 3.1 are converted into artificial co-occurrence counts with a novel technique (Section 3.2).",
        "Although they need not be integers, the main challenge is the type of the underlying distributions; it should ideally emulate the resulting count distributions from the phrase extraction stage of a monolingual parallel corpus (Dolan et al. 2004).",
        "These counts give rise to the desired probability distributions by means of relative frequencies."
      ]
    },
    {
      "heading": "2 Sub-phrase-tables & Clustering",
      "text": []
    },
    {
      "heading": "2.1 Extracting Connected Components",
      "text": [
        "For the decomposition of the phrase-table into sub-phrase-tables it is convenient to view the phrase-table as an undirected, unweighted graph P with the vertex set being the source and target phrases and the edge set being the phrase-table entries.",
        "For the rest of this section, we do not distinguish between source and target phrases, i.e. both types are treated equally as vertices of P .",
        "When referring to the size of a graph, we mean the number of vertices it contains.",
        "A trivial initial decomposition of P is achieved by identifying all its connected components (components for brevity), i.e. the mutually disjoint connected subgraphs, {P0, P1, ..., Pn}.",
        "It turns out (see Section 4.1) that the largest component, say P0, is of significant size.",
        "We call P0 giant and it needs to be further decomposed.",
        "This is done by identifying all vertices such that, upon removal, the component becomes disconnected.",
        "Such vertices are called articulation points or cut-vertices.",
        "Cut-vertices of high connectivity degree are removed from the giant component (see Section 4.1).",
        "For the remaining vertices of the giant component, new components are identified and we proceed iteratively, while keeping track of the cut-vertices that are removed at each iteration, until the size of the largest component is less than a certain threshold ?",
        "(see Section 4.1).",
        "Note that at each iteration, when removing cut-vertices from a giant component, the resulting collection of components may include graphs consisting of a single vertex.",
        "We refer to such vertices as residues.",
        "They are excluded from the resulting collection and are considered for separate treatment, as explained later in this section.",
        "The cut-vertices need to be inserted appropriately back to the components: Starting from the last iteration step, the respective cut-vertices are added to all the components of P0 which they used to ?glue?",
        "together; this process is performed iteratively, until there are no more cut-vertices to add.",
        "By ?addition?",
        "of a cut-vertex to a component, we mean the re-establishment of edges between the former and other vertices of the latter.",
        "The result is a collection of components whose total number of unique vertices is less than the number of vertices of the initial giant component P0.",
        "These remaining vertices are the residues.",
        "We then construct the graph R which consists of the residues together with all their translations (even those that are included in components of the above collection) and then identify its components {R0, ..., Rm}.",
        "It turns out, that the largest component, say R0, is giant and we repeat the decomposition process that was performed on P0.",
        "This results in a new collection of components as well as new residues: The components need to be pruned (see Section 4.1) and the residues give rise to a new graph R?",
        "which is constructed in the same way asR.",
        "We proceed iteratively until the number of residues stops changing.",
        "For each remaining residue u, we identify its translations, and for each translation v we identify the largest component of which v is a member and add u to that component.",
        "The final result is a collection C = D ?",
        "F , where D is the collection of components emerging from the entire iterative decomposition of P0",
        "and R, and F = {P1, ..., Pn}.",
        "Figure 1 shows the decomposition of a connected graph G0; for simplicity we assume that only one cut-vertex is removed at each iteration and ties are resolved arbitrarily.",
        "In Figure 2 the residue graph is constructed and its two components are identified.",
        "The iterative insertion of the cut vertices is also depicted.",
        "The resulting two components together with those from R form the collection D for G0.",
        "The addition of cut-vertices into multiple components, as well as the construction method of the residue-based graph R, can yield the occurrences of a vertex in multiple components in D. We exploit this property in two ways: (a) In order to mitigate the risk of excessive decomposition (which implies greater risk of good paraphrases being in different components), as well as to reduce the size of D, a conservative merging algorithm of components is employed.",
        "Suppose that the elements of D are ranked according to size in ascending order as",
        "?, for i = 1, ..., k, and some threshold ?",
        "(see Section 4.1).",
        "Each component Di with i ?",
        "{1, ..., k} is examined as follows: For each vertex of Di the number of its occurrences inD is inspected; this is done in order to identify an appropriate vertex b to act as a bridge between Di and other components of which b is a member.",
        "Note that translations of a vertex b with smaller number of occurrences in D are less likely to capture their full spectrum of paraphrases.",
        "We thus choose a vertex b from Di with the smallest number of occurrences in D , resolving ties arbitrarily, and proceed with merging Di with the largest component, say Dj with j ?",
        "{1, ..., |D |?",
        "1}, of which b is also a member.",
        "The resulting merged component Dj?",
        "contains all vertices and edges of Di and Dj and new edges, which are formed according to the rule: if u is a vertex of Di and v is a vertex of Dj and (u, v) is a phrase-table entry, then (u, v) is an edge in Dj?",
        ".",
        "As long as no connected component has identified Di as the component with which it should be merged, then Di is deleted from the collection D.",
        "(b) We define an idf inspired measure for each phrase pair (x, x?)",
        "of the same type (source or target) as",
        "where c(x, x?)",
        "is the number of components in which the phrases x and x?",
        "co-occur, and equivalently for c(?).",
        "The purpose of this measure is for pruning paraphrase candidates and its use is explained in Section 3.1.",
        "Note that idf(x, x?)",
        "?",
        "[0, 1].",
        "The merging process and the idf measure are irrelevant for phrases belonging to the components of F , since the vertex set of each component of F is mutually disjoint with the vertex set of any other component in C.",
        "si and tj : The cut-vertex of the ith iteration is denoted by ci, and r collects the residues after each iteration.",
        "The task is completed in Figure 2.",
        "G s0s1 t 0t2 s0s1 t 0t2s2 t 1 =c3 s3 t3 =c4=c4s2 t 1t2 s2 t2t 1s3s0 t3 s0s2 t2t 1",
        "(no further decomposition is required).",
        "Bottom: Adding cut-vertices back to their components."
      ]
    },
    {
      "heading": "2.2 Clustering Connected Components",
      "text": [
        "The aim of this subsection is to generate separate clusters for the source and target phrases of each sub-phrase-table (component) C ?",
        "C. For this purpose the Information-Theoretic Co-Clustering (ITC) algorithm (Dhillon et al. 2003) is employed, which is a general principled clustering algorithm that generates hard clusters (i.e. ev",
        "ery element belongs to exactly one cluster) of two interdependent quantities and is known to perform well on high-dimensional and sparse data.",
        "In our case, the interdependent quantities are the source and target phrases and the sparse data is the phrase-table.",
        "ITC is a search algorithm similar to K-means, in the sense that a cost function, is minimized at each iteration step and the number of clusters for both quantities are meta-parameters.",
        "The number of clusters is set to the most conservative initialization for both source and target phrases, namely to as many clusters as there are phrases.",
        "At each iteration, new clusters are constructed based on the identification of the argmin of the cost function for each phrase, which gradually reduces the number of clusters.",
        "We observe that conservative choices for the meta-parameters often result in good paraphrases being in different clusters.",
        "To overcome this problem, the hard clusters are converted into soft (i.e. an element may belong to several clusters): One step before the stopping criterion is met, we modify the algorithm so that instead of assigning a phrase to the cluster with the smallest cost we select the bottom-X clusters ranked by cost.",
        "Additionally, only a certain number of phrases is chosen for soft clustering.",
        "Both selections are done conservatively with criteria based on the properties of the cost functions.",
        "The formation of clusters leads to a natural refinement of the idf measure defined in eqn.",
        "(1): The quantity c(x, x?)",
        "is redefined as the number of components in which the phrases x and x?",
        "co-occur in at least one cluster."
      ]
    },
    {
      "heading": "3 Monolingual Graphs & Counts",
      "text": [
        "We proceed with converting the clusters into directed, weighted graphs and then extract paraphrases for both the source and target side.",
        "For brevity we explain the process restricted to the source clusters of a sub-phrase-table, but the same method applies for the target side and for all sub-phrase-tables in the collection C."
      ]
    },
    {
      "heading": "3.1 Monolingual graphs",
      "text": [
        "Each source cluster is converted into a graph G as follows: The vertex set consists of the phrases of the cluster and an edge between s and s?",
        "exists, if (a) s and s?",
        "have at least one translation from the same target cluster, and (b) idf(s, s?)",
        "is greater than some threshold ?",
        "(see Section 4.1).",
        "If two phrases that satisfy condition (b) and have translations in more than one common target cluster, a distinct such edge is established.",
        "All edges are bidirectional with distinct weights for both directions.",
        "Figure 3 depicts an example of such a construction; a link between a phrase si and a target cluster implies the existence of at least one translation for si in that cluster.",
        "We are not interested in the target phrases and they are thus not shown.",
        "For simplicity we assume that condition (b) is always satisfied and the extracted graph contains the maximum possible edges.",
        "Observe that phrases s3 and s4 have two edges connecting them, (due to target clusters Tc and Td) and that the target cluster Ta is irrelevant to the construction of the graph, since s1 is the only phrase with translations in it.",
        "This conversion of a source cluster into a graph G",
        "phrases s1,..., s8 and the associated target clusters Ta,..., Tf .",
        "Bottom: The extracted graph from the source cluster.",
        "All edges are bi-directional.",
        "results in the formation of subgraphs in G, where each subgraph is generated by a target cluster.",
        "In general, if condition (b) is not always satisfied, then G need not be connected and each connected component is treated as a distinct graph.",
        "Analogous to KB, we introduce feature vertices to G: For each phrase vertex s, its part-of-speech (POS) tag sequence and stem sequence are identified and inserted into G as new vertices with bidirectional weighted edges connected to s. If phrase vertices s and s?",
        "have the same POS tag sequence, then they are connected to the same POS tag feature vertex.",
        "Similarly for stem feature vertices.",
        "See Figure 4 for an example.",
        "Note that we do not allow edges between POS tag and stem fea",
        "graph (has) ??",
        "(owns) ??",
        "(i have) ??",
        "(i had).",
        "Phrase, POS tag feature and stem feature vertices are drawn in circles, dotted rectangles and solid rectangles respectively.",
        "All edges are bidirectional.",
        "ture vertices.",
        "The purpose of the feature vertices, unlike KB, is primarily for smoothing and secondarily for identifying paraphrases with the same syntactic information and this will become clear in the description of the computation of weights.",
        "The set of all phrase vertices that are adjacent to s is written as ?",
        "(s), and referred to as the neighborhood of s. Let n(s, t) denote the co-occurrence count of a phrase-table entry (s, t) (Koehn, 2009).",
        "We define the strength of s in the subgraph generated by cluster T as",
        "which is simply a partial occurrence count for s. We proceed with computing weights for all edges of G: Phrase?",
        "?phrase weights: Inspired by the notion of preferential attachment (Yule, 1925), which is known to produce power-law weight distributions for evolving weighted networks (Barrat et al. 2004), we set the weight of a directed edge from s to s?",
        "to be proportional to the strengths of s?",
        "in all subgraphs in which both s and s?",
        "are members.",
        "Thus, in the random walk framework, s is more likely to visit a stronger (more reliable) neighbor.",
        "If Ts,s?",
        "= {T |s and s?",
        "coexist in subgraph generated by T}, then the weight w(s ?",
        "s?)",
        "of the directed edge from s to s?",
        "is given by",
        "if s?",
        "?",
        "?",
        "(s) and 0 otherwise.",
        "Phrase?",
        "?feature weights: As mentioned above, feature vertices have the dual role of carrying syntactic information and smoothing.",
        "From eqn.",
        "(3) it can be deduced that, if for a phrase s, the amount of its outgoing weights is close to the amount of its incoming weights, then this is an indication that at least a significant part of its neighborhood is reliable; the larger the strengths, the more certain the indication.",
        "Otherwise, either s or a significant part of its neighborhood is unreliable.",
        "The amount of weight from s to its feature vertices should depend on this observation and we thus let",
        "where prevents net(s) from becoming 0 (see Section 4.1).",
        "The net weight of a phrase vertex s is distributed over its feature vertices as",
        "where the first summand is the average weight from s to its neighboring phrase vertices and",
        "sequences, we distribute the weight of eqn.",
        "(5) relatively to the co-occurrences of s with the respective POS tag feature vertices.",
        "The quantity < w(s ?",
        "s?)",
        "> accounts for the basic smoothing and is augmented by a value net(s) that measures the reliability of s's neighborhood; the more unreliable the neighborhood, the larger the net weight and thus larger the overall weights to the feature vertices.",
        "The choice for the opposite direction is trivial:",
        "where X = POS,STEM.",
        "Note the effect of eqns.",
        "(4)?",
        "(6) in the case where the neighborhood of s has unreliable strengths: In a random walk the feature vertices of s will be preferred and the resulting similarities between s and other phrase vertices will be small, as desired.",
        "Nonetheless, if the syntactic information is the same with any other phrase vertex inG, then the paraphrases will be captured.",
        "The transition probability from any vertex u to any other vertex v in G, i.e., the probability of",
        "hopping from u to v in one step, is given by",
        "where we sum over all vertices adjacent to u inG.",
        "We can thus compute the similarity between any two vertices u and v in G by their commute time, i.e., the expected number of steps in a round trip, in a random walk from u to v and then back to u, which is denoted by ?",
        "(u, v) (see Section 4.1 for the method of computation of ?).",
        "Since ?",
        "(u, v) is a distance measure, the smaller its value, the more similar u and v are."
      ]
    },
    {
      "heading": "3.2 Counts",
      "text": [
        "We convert the distance ?",
        "(u, v) of a vertex pair u, v in a graph G into a co-occurrence count nG(u, v) with a novel technique: In order to assess the quality of the pair u, v with respect to G we compare ?",
        "(u, v) with ?",
        "(u, x) and ?",
        "(v, x) for all other vertices x in G. We thus consider the average distance of u with the other vertices of G other than v, and similarly for v. This quantity is denoted by ?",
        "(u; v) and ?",
        "(v;u) respectively, and by definition it is given by",
        "where pG(x|i) ?",
        "p(x|G, i) is a yet unknown probability distribution with respect to G. The quantity (?",
        "(u; v)+?",
        "(v;u))/2 can then be viewed as the average distance of the pair u, v to the rest of the graph G. The co-occurrence count of u and v in G is thus defined by",
        "In order to calculate the probabilities pG(?|?)",
        "we employ the following heuristic: Starting with a",
        "for all pairs of vertices u, v in G until convergence.",
        "Experimentally, we find that convergence is always achieved.",
        "After the execution of this iterative process we divide each count by the smallest count in order to achieve a lower bound of 1.",
        "A pair u, v may appear in multiple graphs in the same sub-phrase-tableC.",
        "The total co-occurrence count of u and v in C and the associated conditional probabilities are thus given by",
        "A pair u, v may appear in multiple sub-phrase-tables and for the calculation of the final count n(u, v) we need to average over the associated counts from all sub-phrase-tables.",
        "Moreover, we have to take into account the type of the vertices: For the simplest case where both u and v represent phrase vertices, their expected count is, by definition, given by",
        "?",
        ")p(C|s, s?).",
        "(15) On the other hand, if at least one of u or v is a feature vertex, then we have to consider the phrase vertex that generates this feature: Suppose that u is the phrase vertex s=?acquire?",
        "and v the POS tag vertex f=?NN?",
        "and they co-occur in two sub-phrase-tables C and C ?",
        "with positive counts nC(s, f) and nC?",
        "(s, f) respectively; the feature vertex f is generated by the phrase vertices ?ownership?",
        "in C and by ?possession?",
        "in C ?.",
        "In that case, an interpolation of the counts nC(s, f) and nC?",
        "(s, f) as in eqn.",
        "(15) would be incorrect and a direct sum nC(s, f) + nC?",
        "(s, f) would provide the true count.",
        "As a result we have",
        "where the first summation is over all phrase vertices s?",
        "such that f(s?)",
        "= f .",
        "With a similar argument we can write",
        "For the interpolants, from standard probability we",
        "where the probabilities p(C|u) can be computed by considering the likelihood function",
        "and by maximizing the average log-likelihood",
        "N log `(u), where N is the total number of vertices with which u co-occurs with positive counts in all sub-phrase-tables.",
        "Finally, the desired probability distributions are given by the relative frequencies",
        "for all pairs of vertices u, v."
      ]
    },
    {
      "heading": "4 Experiments",
      "text": []
    },
    {
      "heading": "4.1 Setup",
      "text": [
        "The data for building the phrase-table P is drawn from DE-EN bitexts crawled from www.project-syndicate.org, which is a standard resource provider for the WMT campaigns (News Commentary bitexts, see, e.g. (Callison-Burch et al. 2007) ).",
        "The filtered bitext consists of 125K sentences; word alignment was performed running GIZA++ in both directions and generating the symmetric alignments using the ?grow-diag-final-and?",
        "heuristics.",
        "The resulting P has 7.7M entries, 30% of which are ?1-1?, i.e. entries (s, t) that satisfy p(s|t) = p(t|s) = 1.",
        "These entries are irrelevant for paraphrase harvesting for both the baseline and our method, and are thus excluded from the process.",
        "The initial giant component P0 contains 1.7M vertices (Figure 5), of which 30% become residues and are used to construct R. At each iteration of the decomposition of a giant component, we remove the top 0.5% ?",
        "size cut-vertices ranked by degree of connectivity, where size is the number of vertices of the giant component and set ?",
        "= 2500 as the stopping criterion.",
        "The latter choice is appropriate for the subsequent step of co-clustering the components, for both time complexity and performance of the ITC algorithm.",
        "extracted from the decomposition of P0.",
        "In the components emerging from the decomposition of R0, we observe an excessive number of cut-vertices.",
        "Note that vertices that consist these components can be of two types: i) former residues, i.e., residues that emerged from the decomposition of P0, and ii) other vertices of P0.",
        "Cut-vertices can be of either type.",
        "For each component, we remove cut-vertices that are not translations of the former residues of that component.",
        "Following this pruning strategy, the degeneracy of excessive cut-vertices does not reappear in the subsequent iterations of decomposing components generated by new residues, but the emergence of two giant components was observed: One consisting mostly of source type vertices and one of target type vertices.",
        "Without going into further details, the algorithm can extend to multiple giant components straightforwardly.",
        "For the merging process of the collection D we set ?",
        "= 5000, to avoid the emergence of a giant component.",
        "The sizes of the resulting sub-phrase-tables are shown in Figure 6.",
        "For the ITC algorithm we use the smoothing technique discussed in (Dhillon and Guan, 2003) with ?",
        "= 106.",
        "For the monolingual graphs, we set ?",
        "= 0.65 and discard graphs with more than 20 phrase vertices, as they contain mostly noise.",
        "Thus, the sizes of the graphs allow us to use analytical methods to compute the commute times: For a graph G, we form the transition matrix P , whose entries",
        "tables according to their size (number of source and target phrases).",
        "tal matrix (Grinstead and Snell, 2006; Boley et al. 2011) Z = (I?P +1piT )?1, where I is the identity matrix, 1 denotes the vector of all ones and pi is the vector of stationary probabilities (Aldous and Fill, 2001) which is such that piTP = piT and piT1 = 1 and can be computed as in (Hunter, 2000).",
        "The commute time between any vertices u and v in G is then given by (Grinstead and Snell,",
        "For the parameter of eqn.",
        "(4), an appropriate choice is = |?",
        "(s) |+ 1; for reliable neighbor-hoods, this quantity is insignificant.",
        "POS tags and lemmata are generated with TreeTagger1.",
        "that can be extracted from a cluster; it includes two source phrase vertices a, b, of different syntactic information.",
        "Suppose that both a and b are highly reliable with strengths n(a;T ) = n(b;T ) = 40, for some target cluster T .",
        "The resulting conditional probabilities adequately represent the proximity of the involved vertices.",
        "On the other hand, the range of the co-occurrence counts is not compatible with that of the strengths.",
        "This is because i) there are no phrase vertices with small strength in the graph, and ii) eqn.",
        "(9) is essentially a comparison between a pair of vertices and the rest of the graph.",
        "To overcome this problem inflation vertices ia and ib of strength 1 with accompanying feature vertices are introduced to",
        "the graph.",
        "Figure 8 depicts the new graph, where the lengths of the edges represent the magnitude of commute times.",
        "Observe that the quality of the probabilities is preserved but the counts are inflated, as required.",
        "In general, if a source phrase vertex s has at least one translation t such that n(s, t) ?",
        "3, then a triplet (is, f(is), g(is)) is added to the graph as in",
        "with all other phrase and inflation vertices in the graph and weights are computed as in Section 3.1.",
        "The pipeline remains the same up to eqn.",
        "(13), where all counts that include inflation vertices are",
        "panying distinct POS sequence vertices f(?)",
        "and stem sequence vertices g(?).",
        "Bottom: The resulting co-occurrence counts and conditional probabilities for a."
      ]
    },
    {
      "heading": "4.2 Results",
      "text": [
        "Our method generates conditional probabilities for any pair chosen from {phrase, POS sequence, stem sequence}, but for this evaluation we restrict ourselves to phrase pairs.",
        "For a phrase s, the quality of a paraphrase s?",
        "is assessed by",
        "where f1(s?)",
        "and f2(s?)",
        "denote the POS tag sequence and stem sequence of s?",
        "respectively.",
        "All three summands of eqn.",
        "(21) are computed from eqn.",
        "(19).",
        "The baseline is given by pivoting (Bannard and Callison-Burch, 2005),",
        "where p(t|s) and p(s?|t) are the phrase-based relative frequencies of the translation model.",
        "We select 150 phrases (an equal number for unigrams, bigrams and trigrams), for which we expect to see paraphrases, and keep the top-10 paraphrases for each phrase, ranked by the above measures.",
        "We follow (Kok and Brockett, 2010; Metzler et al. 2011) in the evaluation of the extracted paraphrases: Each phrase-paraphrase pair is manually annotated with the following options: 0) Different meaning; 1) (i) Same meaning, but potential replacement of the phrase with the paraphrase in a sentence ruins the grammatical structure of the sentence.",
        "(ii) Tokens of the paraphrase are morphological inflections of the phrase's tokens.",
        "2) Samemeaning.",
        "Although useful for SMT purposes, ?super/substrings of?",
        "are annotated with 0 to achieve an objective evaluation.",
        "Both methods are evaluated in terms of the Mean Expected Precision (MEP) at k; the Expected Precision for each selected phrase s at rank k is computed by Es[p@k] = 1k",
        "where pi is the proportion of positive annotations for item i.",
        "The desired metric is thus given by",
        "s Es[p@k].",
        "The contribution to pi can be restricted to perfect paraphrases only, which leads to a strict strategy for harvesting paraphrases.",
        "Table 1 summarizes the results of our evaluation and we deduce that our method can lead to improvements over the baseline.",
        "An important accomplishment of our method is that the distribution of counts n(u, v), (as given",
        "der lenient and strict evaluation criteria.",
        "by eqns.",
        "(15)?",
        "(17)) for all vertices u and v, belongs to the power-law family (Figure 9).",
        "This is evidence that the monolingual graphs can simulate the phrase extraction process of a monolingual parallel corpus.",
        "Intuitively, we may think of the German side of the DE?EN parallel corpus as the ?English?",
        "approximation to a ?EN?",
        "?EN parallel corpus, and the monolingual graphs as the word alignment process.",
        "vertices according to their counts"
      ]
    },
    {
      "heading": "5 Conclusions & Future Work",
      "text": [
        "We have described a new method that harvests paraphrases from a bitext, generates artificial co-occurrence counts for any pair chosen from {phrase, POS sequence, stem sequence}, and potentially identifies patterns for the syntactic information of the phrases.",
        "The quality of the paraphrases?",
        "ranked lists outperforms that of a standard baseline.",
        "The quality of the resulting conditional probabilities is promising and will be evaluated implicitly via an application to SMT.",
        "This research was funded by the European Commission through the CoSyne project FP7ICT- 4-248531."
      ]
    }
  ]
}
