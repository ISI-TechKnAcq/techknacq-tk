{
  "info": {
    "authors": [
      "Vincent Ng"
    ],
    "book": "HLT-NAACL",
    "id": "acl-N09-1065",
    "title": "Graph-Cut-Based Anaphoricity Determination for Coreference Resolution",
    "url": "https://aclweb.org/anthology/N09-1065",
    "year": 2009
  },
  "references": [
    "acl-C02-1139",
    "acl-C08-1121",
    "acl-C96-1021",
    "acl-E06-1007",
    "acl-H05-1004",
    "acl-J01-4004",
    "acl-J94-4002",
    "acl-J96-1002",
    "acl-M95-1005",
    "acl-N06-1025",
    "acl-N07-1010",
    "acl-N07-1030",
    "acl-P02-1014",
    "acl-P03-2012",
    "acl-P04-1018",
    "acl-P04-1020",
    "acl-P04-1035",
    "acl-P07-1067",
    "acl-P08-1002",
    "acl-P08-2012",
    "acl-P99-1048",
    "acl-W04-0707"
  ],
  "sections": [
    {
      "text": [
        "Recent work has shown that explicitly identifying and filtering non-anaphoric mentions prior to coreference resolution can improve the performance of a coreference system.",
        "We present a novel approach to this task of anaphoricity determination based on graph cuts, and demonstrate its superiority to competing approaches by comparing their effectiveness in improving a learning-based coref-erence system on the ACE data sets."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Coreference resolution is the problem of identifying which noun phrases (NPs, or mentions) refer to the same real-world entity in a text or dialogue.",
        "According to Webber (1979), coreference resolution can be decomposed into two complementary tasks: \"(1) identifying what a text potentially makes available for anaphoric reference and (2) constraining the candidate set of a given anaphoric expression down to one possible choice.\"",
        "The first task is nowadays typically formulated as an anaphoricity determination task, which aims to classify whether a given mention is anaphoric or not.",
        "Knowledge of anaphoricity could improve the precision of a coreference system, since non-anaphoric mentions do not have an antecedent and therefore do not need to be resolved.",
        "Previous work on anaphoricity determination can be broadly divided into two categories (see Poe-sio et al.",
        "(2004) for an overview).",
        "Research in the first category aims to identify specific types of non-anaphoric phrases, with some identifying pleonastic it (using heuristics [e.g., Paice and Husk (1987),",
        "Lappin and Leass (1994), Kennedy and Boguraev (1996)], supervised approaches [e.g., Evans (2001), Muller (2006), Versley et al.",
        "(2008)], and distributional methods [e.g., Bergsma et al.",
        "(2008)]), and others identifying non-anaphoric definite descriptions (using rule-based techniques [e.g., Vieira and Poesio (2000)] and unsupervised techniques [e.g., Bean and Riloff (1999)]).",
        "On the other hand, research in the second category focuses on (1) determining the anaphoricity of all types of mentions, and (2) using the resulting anaphoricity information to improve coreference resolution.",
        "For instance, Ng and Cardie (2002a) train an anaphoricity classifier to determine whether a mention is anaphoric, and let an independently-trained coreference system resolve only those mentions that are classified as anaphoric.",
        "Somewhat surprisingly, they report that using anaphoricity information adversely affects the performance of their coreference system, as a result of an overly conservative anaphoricity classifier that misclassifies many anaphoric mentions as non-anaphoric.",
        "One solution to this problem is to use anaphoricity information as soft constraints rather than as hard constraints for coreference resolution.",
        "For instance, when searching for the best partition of a set of mentions, Luo (2007) combines the probabilities returned by an anaphoricity model and a coreference model to score a coreference partition, such that a partition is penalized whenever an anaphoric mention is resolved.",
        "Another, arguably more popular, solution is to \"improve\" the output of the anaphoricity classifier by exploiting the dependency between anaphoricity determination and coreference resolution.",
        "For instance, noting that Ng and Cardie's anaphoricity classifier is too conservative, Ng (2004) first parameterizes their classifier such that its conservativeness can be varied, and then tunes this parameter so that the performance of the coreference system is maximized.",
        "As another example, Denis and Baldridge (2007) and Finkel and Manning (2008) perform joint inference for anaphoricity determination and coreference resolution, by using Integer Linear Programming (ILP) to enforce the consistency between the output ofthe anaphoricity classifier and that of the coreference classifier.",
        "While this ILP approach and Ng's (2004) approach to improving the output of an anaphoricity classifier both result in increased coreference performance, they have complementary strengths and weaknesses.",
        "Specifically, Ng's approach can directly optimize the desired coreference evaluation metric, but by treating the coreference system as a black box during the optimization process, it does not exploit the potentially useful pairwise probabilities provided by the coreference classifier.",
        "On the other hand, the ILP approach does exploit such pairwise probabilities, but optimizes an objective function that does not necessarily have any correlation with the desired evaluation metric.",
        "Our goals in this paper are two-fold.",
        "First, motivated in part by previous work, we propose a graph-cut-based approach to anaphoricity determination that combines the strengths of Ng's approach and the ILP approach, by exploiting pairwise corefer-ence probabilities when coordinating anaphoricity and coreference decisions, and at the same time allowing direct optimization of the desired corefer-ence evaluation metric.",
        "Second, we compare our cut-based approach with the five aforementioned approaches to anaphoricity determination (namely, Ng and Cardie (2002a), Ng (2004), Luo (2007), Denis and Baldridge (2007), and Finkel and Manning (2008)) in terms of their effectiveness in improving a learning-based coreference system.",
        "To our knowledge, there has been no attempt to perform a comparative evaluation of existing approaches to anaphoricity determination.",
        "It is worth noting, in particular, that Luo (2007), Denis and Baldridge their approaches on true mentions extracted from the answer keys.",
        "Since true mentions are composed of all the NPs involved in coreference relations but only a subset of the singleton NPs (i.e., NPs that are not coreferent with any other NPs) in a text, evaluating the utility of anaphoricity determination on true mentions to some extent defeats the purpose of performing anaphoricity determination, which precisely aims to identify non-anaphoric mentions.",
        "Hence, we hope that our evaluation on mentions extracted using an NP chunker can reveal their comparative strengths and weaknesses.",
        "We perform our evaluation on three ACE coref-erence data sets using two commonly-used scoring programs.",
        "Experimental results show that (1) employing our cut-based approach to anaphoric-ity determination yields a coreference system that achieves the best performance for all six data-set/scoring-program combinations, and (2) among the five existing approaches, none performs consistently better than the others.",
        "The rest of the paper is organized as follows.",
        "Section 2 describes our learning-based coreference system.",
        "In Section 3, we give an overview of the five baseline approaches to anaphoricity determination.",
        "Section 4 provides the details of our graph-cut-based approach.",
        "Finally, we present evaluation results in Section 5 and conclude in Section 6."
      ]
    },
    {
      "heading": "2. Baseline Coreference Resolution System",
      "text": [
        "Our baseline coreference system implements the standard machine learning approach to coreference resolution (see Ng and Cardie (2002b), Ponzetto and Strube (2006), Yang and Su (2007), for instance), which consists of probabilistic classification and clustering, as described below.",
        "We use maximum entropy (MaxEnt) classification (Berger et al., 1996) in conjunction with the 33 features described inNg (2007) to acquire a model, Pc, for determining the probability that two mentions, mi and nij, are coreferent.",
        "Hence,",
        "Pc(mi,mj) = P(coreferent | mi,mj-).",
        "In the rest of the paper, we will refer to Pc (mi, mj ) as the pairwise coreference probability between mi and mj.",
        "To generate training instances, we employ Soon et al.",
        "'s (2001) procedure, relying on the training texts to create (1) a positive instance for each anaphoric mention, mj, and its closest antecedent, mi, and (2) a negative instance for mj paired with each of the intervening mentions, mi+1, mi+2,..., mj-1.",
        "When training the feature-weight parameters of the MaxEnt model, we use 100 iterations of the improved iterative scaling (IIS) algorithm (Della Pietra et al., 1997) together with a Gaussian prior to prevent overfitting.",
        "After training, the coreference model is used to select an antecedent for each mention in a test text.",
        "Following Soon et al.",
        "(2001), we select as the antecedent of each mention, mj, the closest preceding mention that is classified as coreferent with mj, where mention pairs with pairwise probabilities ofat least 0.5 are considered coreferent.",
        "If no such mention exists, no antecedent will be selected for mj.",
        "In essence, we use a closest-first clustering algorithm to impose a partitioning on the mentions."
      ]
    },
    {
      "heading": "3. Baseline Approaches to Anaphoricity Determination",
      "text": [
        "As mentioned previously, we will use five existing approaches to anaphoricity determination as baselines in our evaluation.",
        "Common to all five approaches is the acquisition of an anaphoricity model, Pa, for determining the probability that a mention, mj, is anaphoric.",
        "Hence,",
        "To train Pa, we again employ MaxEnt modeling, and create one training instance from each mention in a training text.",
        "Hence, each instance represents a single mention and consists of 37 features that are potentially useful for distinguishing anaphoric and non-anaphoric mentions (see Ng and Cardie (2002a) for a detailed description of these features).",
        "The classification of a training instance – one of anaphoric or not anaphoric – is derived directly from the coreference chains in the associated training text.",
        "Like the coreference model, the anaphoricity model is trained by running 100 iterations of IIS with a Guassian prior.",
        "The resulting model is then applied to a test text to determine the probability that a mention is anaphoric.",
        "In the rest of this section, we provide an overview of the five baseline approaches to anaphoricity determination.",
        "We will characterize each approach along two dimensions: (1) whether it attempts to improve Pa, and if so, how; and (2) whether the resulting anaphoricity information is used as hard constraints or soft constraints by the coreference system.",
        "Ng and Cardie (N&C) do not attempt to improve Pa, simply using the anaphoricity information it provides as hard constraints for coreference resolution.",
        "Specifically, the coreference system resolves only those mentions that are determined as anaphoric by Pa , where a mention is classified as anaphoric if the classification threshold is at least 0.5.",
        "Pa may not be \"sufficiently\" accurate, however, as N&C report a significant drop in the performance of their coreference system after incorporating anaphoricity information, owing in part to their overly conservative anaphoricity model that misclassifies many anaphoric mentions as non-anaphoric.",
        "To address this problem, Ng (2004) attempts to improve Pa by introducing a threshold parameter c to adjust the conservativeness of Pa as follows.",
        "Given a specific c (0 < c < 1), a mention mj is classified as anaphoric by Pa if and only if Pa (mj ) > c. It should be easy to see that decreasing c yields progressively less conservative anaphoricity models (i.e., more mentions will be classified as anaphoric).",
        "The parameter c is tuned using held-out development data to optimize the performance of the coreference system that employs anaphoricity information (as hard constraints).",
        "In essence, Ng's approach to improving Pa treats the coreference system as a black box, merely selecting the value for c that yields the best score according to the desired coreference evaluation metric on the held-out data.",
        "In particular, unlike some of the anaphoricity determination approaches discussed later on, this approach does not attempt to coordinate the anaphoricity decisions and the pairwise coreference decisions.",
        "Nevertheless, as mentioned before, a unique strength of this approach lies in its ability to optimize directly the desired coreference",
        "Among the five anaphoricity determination approaches, Luo's (2007) is the only one where anaphoricity information is exploited as soft constraints by the coreference model, Pc .",
        "Specifically, Luo's algorithm attempts to find the most probable coreference partition of a given set of mentions.",
        "To do so, it scores a partition using the probabilities provided by Pa and Pc .",
        "Let us illustrate how this can be done via the following example.",
        "Given a document with four mentions, m1;..., m4, and a partition of the mentions, {[m1,m3,m4], [m2]}, automatically produced by some coreference system, Luo's algorithm scores the partition by considering the mentions in the document in a left-to-right manner.",
        "As the first mention in the document, m1 is not anaphoric, and the probability that it is non-anaphoric is 1 – Pa (m1).",
        "Then, the algorithm processes m2, which according to the partition is non-anaphoric, and the probability of its being non-anaphoric is 1 – Pa(m2).",
        "Next, it processes m3, which is coreferent with m1 with probability Pc (m1;m3).",
        "Finally, it processes m4, which is coreferent with m1 and m3.",
        "The probability that m4 is coreferent with the cluster consisting of m1 and m3 is defined to be max(PC(m1 , m4),PC(m3,m4)), according to Luo's algorithm.",
        "The score of this partition is the product of these four probabilities, two provided by Pa and two by Pc .",
        "As can be seen, a partition is penalized whenever a mention that is unlikely to be anaphoric (according to Pa) is being resolved to some antecedent according to the partition.",
        "Nevertheless, it is computationally infeasible to score all possible partitions given a set of mentions, as the number of partitions is exponential in the number of mentions.",
        "To cope with this computational complexity, Luo employs the algorithm proposed in Luo et al.",
        "(2004) to heuristically search for the most probable partition by performing a beam search through a Bell tree.",
        "In essence, only the most promising nodes in the tree are expanded at each step of the search process, where the \"promise\" of a node is defined in terms of the probabilities provided by Pa and Pc, as described above.",
        "Details of this process can be found in Luo et al.",
        "(2004).",
        "As mentioned before, Denis and Baldridge (D&B) aim to improve the outputs of Pa and Pc by employing Integer Linear Programming (ILP) to perform joint inference for anaphoricity determination and coreference resolution.",
        "The ILP approach is motivated by the observation that the outputs of these two models have to satisfy certain constraints.",
        "For instance, if Pc determines that a mention, mj, is not coreferent with any other mentions in the associated text, then Pa should determine that mj is non-anaphoric.",
        "In practice, however, since Pa and Pc are trained independently of each other, this and other constraints cannot be enforced.",
        "ILP provides a framework for jointly determining anaphoricity and coreference decisions for a given set of mentions based on the probabilities provided by Pa and Pc, such that the resulting joint decisions satisfy the desired constraints while respecting as much as possible the probabilistic decisions made by the independently-trained Pa and Pc .",
        "Specifically, an ILP program is composed of an objective function to be optimized subject to a set of linear constraints, and is created for each test text D as follows.",
        "Let M be the set of mentions in D, and P be the set of mention pairs formed from M (i.e., P = {(mi,mj-) | mi,mj- e M, i < j}).",
        "Each ILP program has a set of indicator variables.",
        "In our case, we have one binary-valued variable for each anaphoric-ity decision and coreference decision to be made by an ILP solver.",
        "Following D&B's notation, we use yj to denote the anaphoricity decision for mention mj, and to denote the coreference decision involving mentions mi and mj.",
        "In addition, each variable is associated with an assignment cost.",
        "Specifically, let c^i j = – log(PC(mi; mj)) be the cost of setting £(ij) to 1, and = – log(1 – Pc (mi, mj)) be the complementary cost of setting to 0.",
        "We can similarly define the cost associated with each yj, letting cA = – log(PA(mj)) be the cost of setting yj to 1, and cA = – log(1 – PA(mj)) be the complementary cost of setting yj to 0.",
        "Given these costs, we aim to optimize the following objective function:",
        "subject to a set of manually-specified linear constraints.",
        "D&B specify four types of constraints: (1) each indicator variable can take on a value of O or 1; (2) if mi and mj are coreferent (x(i;j>=1), then mj is anaphoric (yj=1); (3) if mj is anaphoric (yj=1), then it must be coreferent with some preceding mention mi; and (4) if mj is non-anaphoric, then it cannot be coreferent with any mention.",
        "Note that we are minimizing the objective function, since each assignment cost is expressed as a negative logarithm value.",
        "We use Ipsolve, an ILP solver, to solve this program.",
        "It is easy to see that enforcing consistency using ILP amounts to employing anaphoricity information as hard constraints for the coreference system.",
        "Since transitivity is not guaranteed by the above constraints, we follow D&B and use the aggressive-merge clustering algorithm to put any two mentions that are posited as coreferent into the same cluster.",
        "Finkel and Manning (F&M) present one simple extension to D&B s ILP approach: augmenting the set of linear constraints with the transitivity constraint.",
        "This ensures that if x^i;j>=1 and x j;k>=1, then £(i;k>=1.",
        "As a result, the coreference decisions do not need to be coordinated by a separate clustering mechanism."
      ]
    },
    {
      "heading": "4. Cut-Based Anaphoricity Determination",
      "text": [
        "As mentioned in the introduction, our graph-cut-based approach to anaphoricity determination is motivated by Ng's (2OO4) and the ILP approach, aiming to combine the strengths of the two approaches.",
        "Specifically, like Ng (2OO4), our approach allows direct optimization of the desired coreference evaluation metric; and like the ILP approach, our approach coordinates anaphoricity decisions and coreference decisions by exploiting the pairwise probabilities provided by a coreference model.",
        "In this section, we will introduce our cut-based approach, starting by reviewing concepts related to minimum cuts.",
        "Assume that we want to partition a set of n objects, {x1, x2,..., xn}, into two sets, Y1 and Y2.",
        "We have two types of scores concerning the x's and the Y 's:",
        "membership scores and similarity scores.",
        "The membership score, memYi(xj), is anon-negative quantity that approximates the \"affinity\" of Xj to Yi.",
        "On the other hand, the similarity score, sim(xj , xk), is a non-negative quantity that provides an estimate of the similarity between Xj and xk.",
        "Informally, our goal is to maximize each object s net happiness, which is computed by subtracting its membership score of the class it is not assigned to from its membership score of the class it is assigned to.",
        "However, at the same time, we want to avoid assigning similar objects to different classes.",
        "More formally, we seek to minimize the partition cost:",
        "There exists an efficient algorithm for solving this seemingly intractable problem when it is recast as a graph problem.",
        "So, let us construct a graph, G, based on the available scores as follows.",
        "First, we create two nodes, s and t (called the source and the sink, respectively), to represent the two classes.",
        "Then, we create one \"object\" node for each of the n objects.",
        "For each object, Xj, we add two directed edges, one from s to Xj (with weight memYl (xj)) and the other from xj to t (with weight memY2 (xj)).",
        "Moreover, for each pair of object nodes, xj and xk, we add two directed edges (one from xj to xk and another from xk to xj), both of which have weight sim(xj, xk).",
        "A cut in G is defined as a partition of the nodes into two sets, S and T, such that s e S, t e T; and the cost of the cut, cost(S, T), is the sum of the weights of the edges going from S to T. A minimum cut is a cut that has the lowest cost among all the cuts of G. It can be proved that finding a minimum cut of G is equivalent to minimizing the partition cost defined as above.",
        "The main advantage of recasting the above minimization problem as a graph-cut problem is that there exist polynomial-time maxflow algorithms for finding a minimum cut.",
        "Next, we show how to construct the graph to which the mincut-finding algorithm will be applied.",
        "The ultimate goal is to use the mincut finder to partition a given set of mentions into two subsets, so that our coreference system will attempt to resolve only those mentions that are in the subset corresponding to ANAPHORIC.",
        "In other words, the resulting anaphoricity information will be used to identify and filter non-anaphoric mentions prior to coreference resolution.",
        "The graph construction process, which takes as input a set of mentions in a test text, is composed ofthree steps, as described below.",
        "Step 1: Mimicking Ng and Cardie (2002a) To construct the desired graph, G, we first create the source, s, and the sink, t, that represent the classes anaphoric and not anaphoric, respectively.",
        "Then, for each mention mn in the input text, we create one node, n, and two edges, sn and nt, connecting n to s and t. Next, we compute wsnand the weights associated with sn and nt.",
        "A natural choice would be to use PA(mn) as the weight of sn and (1 – wsn) as the weight of nt.",
        "(We will assume throughout that wnt is always equal to 1 – If we apply the mincut finder to the current G, it should be easy to see that (1) any node n where wsn > 0.5 will be assigned to s, (2) any node n where wsn < 0.5 will be assigned to t, and (3) any remaining node will be assigned to one of them.",
        "(Without loss of generality, we assume that such nodes are assigned to s.) Hence, the set of mentions determined as anaphoric by the mincut finder is identical to the set of mentions classified as anaphoric by Pa , thus yielding a coreference system that is functionally equivalent to N&C's.",
        "This also implies that G shares the same potential weakness as Pa : being overly conservative in determining a mention as anaphoric.",
        "one way to \"improve\" G is to make it functionally equivalent to Ng's (2004) approach.",
        "Specifically, our goal in Step 2 is to modify the edge weights in G (without adding new edges or nodes) such that the mincut finder classifies a node n as anaphoric if and only if PA(mn) > c for some c e [0,1].",
        "Now, recall from Step 1 that the mincut finder classifies a node n as anaphoric if and only if wsn > 0.5.",
        "Hence, to achieve the aforementioned goal, we just need to ensure the property that wsn > 0.5 if and only if Pa (mn) > c. Consequently, we compute wsn using a sigmoid function:",
        "where a is a constant that controls the \"steepness\" of the sigmoid.",
        "It should be easy to verify that the sigmoid satisfies the aforementioned property.",
        "As noted before, wnt = 1 – wsn for each node n. Inspired by Ng (2004), the value of the parameter c will be tuned based on held-out development data to maximize coreference performance.",
        "Step 3: Incorporating coreference probabilities Like Ng's (2004) approach, the current G suffers from the weakness of not exploiting the pairwise probabilities provided by Pc .",
        "Fortunately, these probabilities can be naturally incorporated into G as similarity scores.",
        "To see why these pairwise probabilities are potentially useful, consider two mentions, mi and mj, in a text D that are coreferent and are both anaphoric.",
        "Assume that the graph G constructed from D has these edge weights: wsi = 0.8, wsj = 0.3, and wij = wji = 0.8.",
        "Without the similarity scores, the mincut finder will correctly determine mi as anaphoric but incorrectly classify mj as non-anaphoric.",
        "on the other hand, if the similarity scores are taken into account, the mincut finder will correctly determine both mentions as anaphoric.",
        "The above discussion suggests that it is desirable to incorporate edges between two nodes, i and j, when mi and mj are likely to be coreferent (i.e., Pc(mi; mj) > c2 for some constant c2).",
        "In our implementation, we tune this new parameter, c2, jointly with c (see Step 2) on development data to maximize coreference performance.",
        "While it is possible to imagine scenarios where incorporating pairwise probabilities is not beneficial, we believe that these probabilities represent a source of information that could be profitably exploited via learning appropriate values for c and c2."
      ]
    },
    {
      "heading": "5. Evaluation",
      "text": [
        "For evaluation, we use the ACE Phase II coreference corpus, which is composed ofthree sections: Broadcast News (BNEWS), Newspaper (NPAPER), and",
        "Newswire (NWIRE).",
        "Each section is in turn composed of a training set and a test set.",
        "For each section, we train an anaphoricity model, Pa, and a coreference model, P , on the training set, and evaluate P (when used in combination with different approaches to anaphoricity determination) on the test set.",
        "As noted before, the mentions used are extracted automatically using an in-house NP chunker.",
        "Results are reported in terms of recall (R), precision (P), and F-measure (F), obtained using two corefer-ence scoring programs: the MUC scorer (Vilain et al., 1995) and the CEAF scorer (Luo, 2005).",
        "\"No Anaphoricity\" baseline.",
        "Our first baseline is the learning-based coreference system described in Section 2, which does not employ any anaphoric-ity determination algorithm.",
        "Results using the MUC scorer and the CEAF scorer are shown in row 1 of Tables 1 and 2, respectively.",
        "As we can see, MUC F-score ranges from 55.0 to 61.7 and CEAF F-score ranges from 55.3 to 61.2.",
        "Duplicated Ng and Cardie (2002a) baseline.",
        "Next, we evaluate our second baseline, which is N&C's coreference system.",
        "As seen from row 2 of Tables 1 and 2, MUC F-score ranges from 50.5 to 60.0 and CEAF F-score ranges from 54.5 to 59.4.",
        "In comparison to the first baseline, we see drops in F-score in all cases as a result of considerable precipitation in recall, which can in turn be attributed to the misclassification of many anaphoric mentions by the anaphoricity model.",
        "More specifically, MUC F-score decreases by 1.7-5.5%, whereas CEAF F-score decreases by 0.5-1.8%.",
        "These trends are consistent with those reported in N&C's paper.",
        "Duplicated Ng (2004) baseline.",
        "Our third baseline is Ng's (2004) coreference system.",
        "Recall that this resolver requires the tuning of the conservativeness parameter, c, on held-out data.",
        "To ensure a fair comparison between different resolvers, we do not",
        "plicitly address this issue, simply letting the coreference clustering algorithm discover that first mentions are non-anaphoric.",
        "rely on additional data for parameter tuning.",
        "Rather, we reserve | of the available training data for tuning c, for which we tested values from 0 to 1 in steps of 0.01, and use the remaining | of the data for training Pa and Pc .",
        "Results are shown in row 3 of Tables 1 and 2, where MUC F-score ranges from 57.0 to 61.9 and CEAF F-score ranges from 55.5 to 60.6.",
        "In comparison to the first baseline, we obtain mixed results: MUC F-score increases by 2.0% and 0.2% for",
        "BNEWS and NPAPER, respectively, but drops by 0.1% forNWIRE; CEAF F-score increases by 0.2% and 1.1% for BNEWS and NPAPER, respectively, but drops by 0.6% for NWIRE.",
        "Duplicated Luo (2007) baseline.",
        "Results of our fourth baseline, in which the anaphoricity and pairwise coreference probabilities are combined to score a partition using Luo's system, are shown in row 4 of Tables 1 and 2.",
        "Here, we see that MUC F-score ranges from 55.8 to 62.1 and CEAF F-score ranges from 56.3 to 61.5.",
        "In comparison to the first baseline, performance improves, though insignificantly,in all cases: MUC F-score increases by 0.2 – 0.8%, whereas CEAF F-score increases by 0.3 – 1.0%.",
        "Duplicated Denis and Baldridge (2007) baseline.",
        "Our fifth baseline performs joint inference for anaphoricity determination and coreference resolution using D&B's ILP approach.",
        "Results are shown in row 5 of Tables 1 and 2, where MUC F-score ranges from 56.2 to 63.8 and CEAF F-score ranges from 56.9 to 61.5.",
        "In comparison to the first baseline, MUC F-score always increases, with improvements ranging from 1.2% to 2.1%.",
        "CEAF results are mixed: F-score increases significantly for BNEWS, drops insignificantly for NPAPER, and rises insignificantly for NWIRE.",
        "The difference in performance trends between the two scorers can be attributed to the fact that the MUC scorer typically under-penalizes errors due to over-merging, which occurs as a result of D&B's using the aggressive-merge clustering algorithm.",
        "In addition, we can see that D&B's approach performs at least as good as Luo's approach in all but one case (NPAPER/CEAF).",
        "Duplicated Finkel and Manning (2008) baseline.",
        "our sixth baseline is F&M's coreference system,",
        "Table 1: MUC scores for the three ACE data sets.",
        "F-scores that represent statistically significant gains and drops with respect to the \"No Anaphoricity\" baseline are marked with an asterisk (*) and a dagger (t), respectively.",
        "Table 2: CEAF scores for the three ACE data sets.",
        "F-scores that represent statistically significant gains and drops with respect to the \"No Anaphoricity\" baseline are marked with an asterisk (*) and a dagger (t), respectively.",
        "which is essentially D&B's approach augmented with transitivity constraints.",
        "Results are shown in row 6 of Tables 1 and 2, where MUC F-score ranges from 55.8 to 63.8 and CEAF F-score ranges from 56.7 to 61.3.",
        "In comparison to the D&B baseline, we see that F-score never improves, regardless ofwhich scoring program is used.",
        "In fact, recall slightly deteriorates, and this can be attributed to F&M's observation that transitivity constraints tend to produce smaller clusters.",
        "overall, these results suggest that enforcing transitivity for coreference resolution is not useful for improving coreference performance.",
        "Our graph-cut-based approach.",
        "Finally, we evaluate the coreference system using the anaphoric-ity information provided by our cut-based approach.",
        "As before, we reserve | of the training data for jointly tuning the two parameters, c and c2, and use the remaining | for training Pa and Pc-For tuning, we tested values from 0 to 1 in steps of 0.1 for both c and c2.",
        "Results are shown in row 7 of Tables 1 and 2.",
        "As we can see, MUC F-score ranges from 59.4 to 63.9 and CEAF F-score ranges from 59.4 to 63.8, representing a significant improvement over the first baseline in all six cases: MUC F-score rises by 2.0 – 4.4% and CEAF F-score rises by 2.6 – 4.1%.",
        "Such an improvement can be attributed to a large gain in precision and a smaller drop in recall.",
        "This implies that our mincut algorithm has successfully identified many non-anaphoric mentions, but in comparison to N&C's approach, it misclassifies a smaller number of anaphoric mentions.",
        "Moreover, our approach achieves the best F-score for each data-set/scoring-program combination, and significantly outperforms the best baseline (D&B) in all but two cases, NPAPER/MUC and NWIRE/MUC."
      ]
    },
    {
      "heading": "6. Conclusions",
      "text": [
        "We have presented a graph-cut-based approach to anaphoricity determination that (1) directly optimizes the desired coreference evaluation metric through parameterization and (2) exploits the probabilities provided by the coreference model when coordinating anaphoricity and coreference decisions.",
        "Another major contribution of our work is the empirical comparison of our approach against five existing approaches to anaphoricity determination in terms of their effectiveness in improving a coref-erence system using automatically extracted mentions.",
        "our approach demonstrates effectiveness and robustness by achieving the best result on all three ACE data sets according to both the MUC scorer and the CEAF scorer.",
        "We believe that our cut-based approach provides a flexible mechanism for coordinating anaphoricity and coreference decisions.",
        "Broadcast News",
        "Newspaper",
        "Newswire",
        "Approach to Anaphoricity Determination",
        "R",
        "P",
        "F",
        "R",
        "P",
        "F",
        "R",
        "P",
        "F",
        "1",
        "No Anaphoricity",
        "57.7",
        "52.6",
        "55.0",
        "60.8",
        "62.6",
        "61.7",
        "59.1",
        "58.1",
        "58.6",
        "2",
        "Duplicated Ng and Cardie (2002a)",
        "40.3",
        "67.7",
        "50.5f",
        "52.1",
        "70.6",
        "60.0",
        "43.0",
        "69.3",
        "53.lt",
        "3",
        "Duplicated Ng (2004)",
        "51.9",
        "63.2",
        "57.0",
        "60.0",
        "63.8",
        "61.9",
        "59.3",
        "57.7",
        "58.5",
        "4",
        "Duplicated Luo (2007)",
        "55.4",
        "56.1",
        "55.8",
        "60.6",
        "63.7",
        "62.1",
        "58.4",
        "59.2",
        "58.8",
        "5",
        "Duplicated Denis and Baldndge (2007)",
        "57.3",
        "55.1",
        "56.2*",
        "63.8",
        "63.7",
        "63.8*",
        "60.4",
        "59.3",
        "59.8*",
        "6",
        "Duplicated Finkel and Manning (2008)",
        "56.4",
        "55.3",
        "55.8",
        "63.8",
        "63.7",
        "63.8*",
        "59.7",
        "59.2",
        "59.5",
        "7",
        "Graph Minimum Cut",
        "53.1",
        "67.5",
        "59.4*",
        "57.9",
        "71.2",
        "63.9*",
        "54.1",
        "69.0",
        "60.6*",
        "Broadcast News",
        "Newspaper",
        "Newswire",
        "Approach to Anaphoricity Determination",
        "R",
        "P",
        "F",
        "R",
        "P",
        "F",
        "R",
        "P",
        "F",
        "1",
        "No Anaphoricity",
        "63.2",
        "49.2",
        "55.3",
        "64.5",
        "54.3",
        "59.0",
        "67.3",
        "56.1",
        "61.2",
        "2",
        "Duplicated Ng and Cardie (2002a)",
        "55.9",
        "53.3",
        "54.5",
        "60.7",
        "56.3",
        "58.5",
        "60.6",
        "58.2",
        "59.4",
        "3",
        "Duplicated Ng (2004)",
        "62.5",
        "49.9",
        "55.5",
        "63.5",
        "57.0",
        "60.1",
        "65.6",
        "56.3",
        "60.6",
        "4",
        "Duplicated Luo (2007)",
        "62.7",
        "51.1",
        "56.3",
        "64.6",
        "55.4",
        "59.6",
        "67.0",
        "56.8",
        "61.5",
        "5",
        "Duplicated Denis and Baldridge (2007)",
        "63.8",
        "51.4",
        "56.9*",
        "62.6",
        "53.6",
        "57.8",
        "67.0",
        "56.8",
        "61.5",
        "6",
        "Duplicated Finkel and Manning (2008)",
        "63.2",
        "51.3",
        "56.7*",
        "62.6",
        "53.6",
        "57.8",
        "66.7",
        "56.7",
        "61.3",
        "7",
        "Graph Minimum Cut",
        "61.4",
        "57.6",
        "59.4*",
        "64.1",
        "59.4",
        "61.7*",
        "65.7",
        "61.9",
        "63.8*"
      ]
    },
    {
      "heading": "Acknowledgments",
      "text": [
        "We thank the three anonymous reviewers for their invaluable comments, Kazi Saidul Hasan for his help on using Ipsolve, and NSF for its gracious support of this work under Grant IIS-0812261.",
        "The description of the minimum cut framework in Section 4.1 was inspired by Pang and Lee (2004)."
      ]
    }
  ]
}
