{
  "info": {
    "authors": [
      "Petra Galuščáková",
      "Martin Popel",
      "Ondřej Bojar"
    ],
    "book": "Workshop on Statistical Machine Translation",
    "id": "acl-W13-2216",
    "title": "PhraseFix: Statistical Post-Editing of TectoMT",
    "url": "https://aclweb.org/anthology/W13-2216",
    "year": 2013
  },
  "references": [
    "acl-D09-1125",
    "acl-J03-1002",
    "acl-N07-1029",
    "acl-N07-1064",
    "acl-P02-1040",
    "acl-P03-1021",
    "acl-P07-2045",
    "acl-P09-2037",
    "acl-P13-3025",
    "acl-W04-3250",
    "acl-W07-0704",
    "acl-W07-0733",
    "acl-W08-0328",
    "acl-W09-0405",
    "acl-W09-0419",
    "acl-W10-1730",
    "acl-W11-2101",
    "acl-W12-3102",
    "acl-W12-4205",
    "acl-W13-2208"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "We present two English-to-Czech systems that took part in the WMT 2013 shared task: TECTOMT and PHRASEFIX.",
        "The former is a deep-syntactic transfer-based system, the latter is a more-or-less standard statistical post-editing (SPE) applied on top of TECTOMT.",
        "In a brief survey, we put SPE in context with other system combination techniques and evaluate SPE vs. another simple system combination technique: using synthetic parallel data from TECTOMT to train a statistical MT system (SMT).",
        "We confirm that PHRASEFIX (SPE) improves the output of TECTOMT, and we use this to analyze errors in TECTOMT.",
        "However, we also show that extending data for SMT is more effective."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "This paper describes two submissions to the WMT 2013 shared task:1 TECTOMT ?",
        "a deep-syntactic tree-to-tree system and PHRASEFIX ?",
        "statistical post-editing of TECTOMT using Moses (Koehn et al., 2007).",
        "We also report on experiments with another hybrid method where TECTOMT is used to produce additional (so-called synthetic) parallel training data for Moses.",
        "This method was used in CU-BOJAR and CU-DEPFIX submissions, see Bojar et al. (2013)."
      ]
    },
    {
      "heading": "2 Overview of Related Work",
      "text": [
        "The number of approaches to system combination is enormous.",
        "We very briefly survey those that form the basis of our work reported in this paper."
      ]
    },
    {
      "heading": "2.1 Statistical Post-Editing",
      "text": [
        "for improving outputs of a rule-based MT system.",
        "In principle, SPE could be applied to any type of first-stage system including a statistical one (Oflazer and El-Kahlout, 2007; B?chara et al., 2011), but most benefit could be expected from post-editing rule-based MT because of the complementary nature of weaknesses and advantages of rule-based and statistical approaches.",
        "SPE is usually done with an off-the-shelf SMT system (e.g. Moses) which is trained on output of the first-stage system aligned with reference translations of the original source text.",
        "The goal of SPE is to produce translations that are better than both the first-stage system alone and the second-stage SMT trained on the original training data.",
        "Most SPE approaches use the reference translations from the original training parallel corpus to train the second-stage system.",
        "In contrast, Simard et al. (2007) use human-post-edited first-stage system outputs instead.",
        "Intuitively, the latter approach achieves better results because the human-post-edited translations are closer to the first-stage output than the original reference translations.",
        "Therefore, SPE learns to perform the changes which are needed the most.",
        "However, creating human-post-edited translations is laborious and must be done again for each new (version of the) first-stage system in order to preserve its full advantage over using the original references.2 Rosa et al. (2013) have applied SPE on English?Czech SMT outputs.",
        "They have used the approach introduced by B?chara et al. (2011), but no improvement was achieved.",
        "However, their rule-based post-editing were found helpful.",
        "Our SPE setting (called PHRASEFIX) uses TECTOMT as the first-stage system and Moses as the second-stage system.",
        "Ideally, TECTOMT pre-2If more reference translations are available, it would be beneficial to choose such references for training SPE which are most similar to the first-stage outputs.",
        "However, in our experiments only one reference is available.",
        "serves well-formed syntactic sentence structures, and the SPE (Moses) fixes low fluency wordings."
      ]
    },
    {
      "heading": "2.2 MT Output Combination",
      "text": [
        "An SPE system is trained to improve the output of a single first-stage system.",
        "Sometimes, more (first-stage) systems are available, and we would like to combine them.",
        "In MT output selection, for each sentence one system's translation is selected as the final output.",
        "In MT output combination, the final translation of each sentence is a combination of phrases from several systems.",
        "In both approaches, the systems are treated as black boxes, so only their outputs are needed.",
        "In the simplest setting, all systems are supposed to be equally good/reliable, and the final output is selected by voting, based on the number of shared n-grams or language model scores.",
        "The number and the identity of the systems to be combined therefore do not need to be known in advance.",
        "More sophisticated methods learn parameters/weights specific for the individual systems.",
        "These methods are based e.g. on confusion networks (Rosti et al., 2007; Matusov et al., 2008) and joint optimization of word alignment, word order and lexical choice (He and Toutanova, 2009)."
      ]
    },
    {
      "heading": "2.3 Synthetic Data Combination",
      "text": [
        "Another way to combine several first-stage systems is to employ a standard SMT toolkit, e.g. Moses.",
        "The core of the idea is to use the n first-stage systems to prepare synthetic parallel data and include them in the training data for the SMT.",
        "Corpus Combination (CComb) The easiest method is to use these n newly created parallel corpora as additional training data, i.e. train Moses on a concatenation of the original parallel sentences (with human-translated references) and the new parallel sentences (with machine-translated pseudo-references).",
        "Phrase Table Combination (PTComb) Another method is to extract n phrase tables in addition to the original phrase table and exploit the Moses option of multiple phrase tables (Koehn and Schroeder, 2007).",
        "This means that given the usual five features (forward/backward phrase/lexical log probability and phrase penalty), we need to tune 5 ?",
        "(n+1) features.",
        "Because such MERT (Och, 2003) tuning may be unstable for higher n, several methods were proposed where the n+1 phrase tables are merged into a single one (Eisele et al., 2008; Chen et al., 2009).",
        "Another issue of phrase table combination is that the same output can be achieved with phrases from several phrase tables, leading to spurious ambiguity and thus less diversity in n-best lists of a given size (see Chen et al. (2009) for one possible solution).",
        "CComb does not suffer from the spurious ambiguity issue, but it does not allow to tune special features for the individual first-stage systems.",
        "In our experiments, we use both CComb and PTComb approaches.",
        "In PTComb, we use TECTOMT as the only first-stage system and Moses as the second-stage system.",
        "We use the two phrase tables separately (the merging is not needed; 5 ?",
        "2 is still a reasonable number of features in MERT).",
        "In CComb, we concatenate English?Czech parallel corpus with English?",
        "?synthetic Czech?",
        "corpus translated from English using TECTOMT.",
        "A single phrase table is created from the concatenated corpus."
      ]
    },
    {
      "heading": "3 TECTOMT",
      "text": [
        "TECTOMT is a linguistically-motivated tree-to-tree deep-syntactic translation system with transfer based on Maximum Entropy context-sensitive translation models (Marec?ek et al., 2010) and Hidden Tree Markov Models (?abokrtsk?",
        "and Popel, 2009).",
        "It employs some rule-based components, but the most important tasks in the analysis-transfer-synthesis pipeline are based on statistics and machine learning.",
        "There are three main reasons why it is a suitable candidate for SPE and other hybrid methods.",
        "?",
        "TECTOMT has quite different distribution and characteristics of errors compared to standard SMT (Bojar et al., 2011).",
        "?",
        "TECTOMT is not tuned for BLEU using MERT (its development is rather driven by human inspection of the errors although different setups are regularly evaluated with BLEU as an additional guidance).",
        "?",
        "TECTOMT uses deep-syntactic dependency language models in the transfer phase, but it does not use standard n-gram language models on the surface forms because the current synthesis phase supports only 1-best output.",
        "The version of TECTOMT submitted to WMT 2013 is almost identical to the WMT 2012 version.",
        "Only a few rule-based components (e.g. detection of surface tense of English verbs) were refined."
      ]
    },
    {
      "heading": "4 Common Experimental Setup",
      "text": [
        "All our systems (including TECTOMT) were trained on the CzEng (Bojar et al., 2012) parallel corpus (development and evaluation subsets were omitted), see Table 1 for statistics.",
        "We translated the English side of CzEng with TECTOMT to obtain ?synthetic Czech?.",
        "This way we obtained a new parallel corpus, denoted tmt(CzEng), with English?",
        "synthetic Czech sentences.",
        "Analogically, we translated the WMT 2013 test set (newstest2013) with TECTOMT and obtained tmt(newstest2013).",
        "Our baseline SMT system (Moses) trained on CzEng corpus only was then also used for WMT 2013 test set translation, and we obtained smt(newstest2013).",
        "For all MERT tuning, newstest2011 was used."
      ]
    },
    {
      "heading": "4.1 Alignment",
      "text": [
        "All our parallel data were aligned with GIZA++ (Och and Ney, 2003) and symmetrized with the ?grow-diag-final-and?",
        "heuristics.",
        "This applies also to the synthetic corpora tmt(CzEng), tmt(newstest2013),3 and smt(newstest2013).",
        "For the SPE experiments, we decided to base alignment on (genuine and synthetic Czech) lemmas, which could be acquired directly from the TECTOMT output.",
        "For the rest of the experiments, we approximated lemmas with just the first four lowercase characters of each (English and Czech) token."
      ]
    },
    {
      "heading": "4.2 Language Models",
      "text": [
        "In all our experiments, we used three language models on truecased forms: News Crawl as provided by WMT organizers,4 the Czech side of CzEng and the Articles section of the Czech Web",
        "Best results are in bold.",
        "Corpus (Spoustov?",
        "and Spousta, 2012).",
        "We used SRILM (Stolcke, 2002) with modified Kneser-Ney smoothing.",
        "We trained 5-grams on CzEng; on the other two corpora, we trained 7 grams and pruned them if the (training set) perplexity increased by less than 10?14 relative.",
        "The domain of the pruned corpora is similar to the test set domain, therefore we trained 7-grams on these corpora.",
        "Adding CzEng corpus can then increase the results only very slightly ?",
        "training 5-grams on CzEng is therefore sufficient and more efficient.",
        "Each of the three LMs got its weight assigned by MERT.",
        "Across the experiments, Czech Web Corpus usually gained the largest portion of weights (40?17% of the total weight assigned to language models), WMT News Crawl was the second (32?15%), and CzEng was the least useful (15?7%), perhaps due to its wide domain mixture."
      ]
    },
    {
      "heading": "5 SPE Experiments",
      "text": [
        "We trained a base SPE system as described in Section 2.1 and dubbed it PHRASEFIX.",
        "First two rows of Table 2 show that the first-stage TECTOMT system (serving here as the baseline) was significantly improved in terms of BLEU (Papineni et al., 2002) by PHRASEFIX (p < 0.001 according to the paired bootstrap test (Koehn, 2004)), but the difference in TER (Snover et al., 2006) is not significant.",
        "The preliminary results of WMT 2013 manual evaluation show only a minor improvement: TECTOMT=0.476 vs. PHRASEFIX=0.484 (higher means better, for details on the ranking see Callison-Burch et al. (2012)).",
        "ent tokenization and normalization.",
        "It seems that statmt.org disables the --international-tokenization switch, so e.g. the correct Czech quotes (?word?)",
        "are not tokenized, hence the neighboring tokens are never counted as matching the reference (which is tokenized as \" word \").",
        "Despite of the improvement, PHRASEFIX's phrase table (synthetic Czech ?",
        "genuine Czech) still contains many wrong phrase pairs that worsen the TECTOMT output instead of improving it.",
        "They naturally arise in cases where the genuine Czech is a too loose translation (or when the English-Czech sentence pair is simply misaligned in CzEng), and the word alignment between genuine and synthetic Czech struggles.",
        "Apart from removing such garbage phrase pairs, it would also be beneficial to have some control over the SPE.",
        "For instance, we would like to generally prefer the original output of TECTOMT except for clear errors, so only reliable phrase pairs should be used.",
        "We examine several strategies: Phrase table filtering.",
        "We filter out all phrase pairs with forward probability ?",
        "0.7 and all singleton phrase pairs.",
        "These thresholds were set based on our early experiments.",
        "Similar filtering was used by Dugast et al. (2009).",
        "Marking of reliable phrases.",
        "This strategy is similar to the previous one, but the low-frequency phrase pairs are not filtered-out.",
        "Instead, a special feature marking these pairs is added.",
        "The subsequent MERT of the SPE system selects the best weight for this indicator feature.",
        "The frequency and probability thresholds for marking a phrase pair are the same as in the previous case.",
        "Marking of identities A special feature indicating the equality of the source and target phrase in a phrase pair is added.",
        "In general, if the output of TECTOMT matched the reference, then such output was probably good and does not need any post-editing.",
        "These phrase pairs should be perhaps slightly preferred by the SPE.",
        "As apparent from Table 2, marking either reliable phrases or identities is useful in our SPE setting in terms of BLEU score.",
        "In terms of TER measure, marking the identities slightly improves PHRASEFIX.",
        "However, none of the improvements is statistically significant."
      ]
    },
    {
      "heading": "6 Data Combination Experiments",
      "text": [
        "We now describe experiments with phrase table and corpus combination.",
        "In the training step, the source-language monolingual corpus that serves as the basis of the synthetic parallel data can be: ?",
        "the source side of the original parallel training corpus (resulting in tmt(CzEng)), ?",
        "a huge source-language monolingual corpus for which no human translations are available (we have not finished this experiment yet), ?",
        "the source side of the test set (resulting in tmt(newstest2013) if translated by TECTOMT or smt(newstest2013) if translated by baseline configuration of Moses trained on CzEng), or ?",
        "a combination of the above.",
        "There is a trade-off in the choice: the source side of the test set is obviously most useful for the given input, but it restricts the applicability (all systems must be installed or available online in the testing time) and speed (we must wait for the slowest system and the combination).",
        "So far, in PTComb we tried adding the full synthetic CzEng (?CzEng + tmt(CzEng)?",
        "), adding the test set (?CzEng + tmt(newstest2013)?",
        "and ?CzEng + smt(newstest2013)?",
        "), and adding both (?CzEng + tmt(CzEng) + tmt(newstest2013)?).",
        "In CComb, we concatenated CzEng and full synthetic CzEng (?CzEng + tmt(CzEng)?).",
        "There are two flavors of PTComb: either the two phrase tables are used both at once as alternative decoding paths (?Alternative?",
        "), where each source span is equipped with translation options from any of the tables, or the synthetic Czech phrase table is used only as a back-off method if a source phrase is not available in the primary table (?Back-off?).",
        "The back-off model was applied to source phrases of up to 5 tokens.",
        "Table 3 summarizes our results with phrase table and corpus combination.",
        "We see that adding synthetic data unrelated to the test set does bring only a small benefit in terms of BLEU in the case of CComb, and we see a small improvement in TER in two cases.",
        "Adding the (synthetic) translation of the test set helps.",
        "However, adding translated source side of the test set is helpful only if it is translated by the TECTOMT system.",
        "If our baseline system is used for this translation, the results even slightly drop.",
        "Somewhat related experiments for pivot languages by Galu?c??kov?",
        "and Bojar (2012) showed a significant gain when the outputs of a rule-based system were added to the training data of Moses.",
        "In their case however, the genuine parallel corpus was much smaller than the synthetic data.",
        "The benefit of unrelated synthetic data seems to vanish with larger parallel data available.",
        "of sentences judged better than the other system) evaluation of SPE vs. PTComb."
      ]
    },
    {
      "heading": "7 Discussion",
      "text": []
    },
    {
      "heading": "7.1 Comparison of SPE and PTComb",
      "text": [
        "Assuming that our first-stage system, TECTOMT, guarantees the grammaticality of the output (sadly often not quite true), we see SPE and PTComb as two complementary methods that bring in the goods of SMT but risk breaking the grammaticality.",
        "Intuitively, SPE feels less risky, because one would hope that the post-edits affect short sequences of words and not e.g. the clause structure.",
        "With PTComb, one relies purely on the phrase-based model and its well-known limitations with respect to grammatical constraints.",
        "Table 4 compares the two approaches empirically.",
        "For SPE, we use the default PHRASEFIX; for PTComb, we use the option ?CzEng + tmt(newstest2013)?.",
        "The BLEU scores are repeated.",
        "We ran a small manual evaluation where three annotators judged which of the two outputs was better.",
        "The identity of the systems was hidden, but the annotators had access to both the source and the reference translation.",
        "Overall, we collected 333 judgments over 120 source sentences.",
        "Of the 333 judgments, 17 marked the two systems as equally correct, and 44 marked the systems as incomparably wrong.",
        "Across the remaining 275 non-tying comparisons, PTComb won ?",
        "152 vs. 123.",
        "We attribute the better performance of PTComb to the fact that, unlike SPE, it has direct access to the source text.",
        "Also, the risk of flawed sentence structure in PTComb is probably not too bad, but this can very much depend on the language pair.",
        "English?Czech translation does not need much reordering in general.",
        "Based on the analysis of the better marked results of the PTComb system, the biggest problem is the wrong selection of the word and word form, especially for verbs.",
        "PTComb also outperforms SPE in processing of frequent phrases and subordinate clauses.",
        "This problem could be solved by enhancing fluency in SPE or by incorporating more training data.",
        "Another possibility would be to modify TECTOMT system to produce more than one-best translation as the correct word or word form may be preserved in sequel translations."
      ]
    },
    {
      "heading": "7.2 Error Analysis of TECTOMT",
      "text": [
        "While SPE seems to perform worse, it has a unique advantage: it can be used as a feedback for improving the first stage system.",
        "We can either inspect the filtered SPE phrase table or differences in translated sentences.",
        "After submitting our WMT 2013 systems, this comparison allowed us to spot a systematic error in TECTOMT tagging of latin-origin words: source pancreas TECTOMT slinivek [plural] PHRASEFIX slinivky [singular] br?i?n?",
        "The part-of-speech tagger used in TECTOMT incorrectly detects pancreas as plural, and the wrong morphological number is used in the synthesis.",
        "PHRASEFIX correctly learns that the plural form slinivek should be changed to singular slinivky, which has also a higher language model score.",
        "Moreover, PHRASEFIX also learns that the trans",
        "lation of pancreas should be two words (br?i?n?",
        "means abdominal).",
        "TECTOMT currently uses a simplifying assumption of 1-to-1 correspondence between content words, so it is not able to produce the correct translation in this case.",
        "Another example shows where PHRASEFIX recovered from a lexical gap in TECTOMT: source people who are strong-willed TECTOMT lid?",
        ", kter??",
        "jsou siln?",
        "willed PHRASEFIX lid?",
        ", kter??",
        "maj?",
        "silnou vu?li TECTOMT's primary translation model considers strong-willed an OOV word, so a back-off dictionary specialized for hyphen compounds is used.",
        "However, this dictionary is not able to translate willed.",
        "PHRASEFIX corrects this and also the verb jsou = are (the correct Czech translation is maj?",
        "silnou vu?li = have a strong will).",
        "Finally, PHRASEFIX can also break things: source You won't be happy here TECTOMT Nebudete ?t?astn?",
        "tady PHRASEFIX Vy tady ?t?astn?",
        "[you here happy] Here, PHRASEFIX damaged the translation by omitting the negative verb nebudete = you won't."
      ]
    },
    {
      "heading": "8 Conclusion",
      "text": [
        "Statistical post-editing (SPE) and phrase table combination (PTComb) can be seen as two complementary approaches to exploiting the mutual benefits of our deep-transfer system TECTOMT and SMT.",
        "We have shown that SPE improves the results of TECTOMT.",
        "Several variations of SPE have been examined, and we have further improved SPE results by marking identical and reliable phrases using a special feature.",
        "However, SMT still outperforms SPE according to BLEU and TER measures.",
        "Finally, employing PTComb, we have improved the baseline SMT system by utilizing additional data translated by the TECTOMT system.",
        "A small manual evaluation suggests that PTComb is on average better than SPE, though in about one third of sentences SPE was judged better.",
        "In our future experiments, we plan to improve SPE by applying techniques suited for monolingual alignment, e.g. feature-based aligner considering word similarity (Rosa et al., 2012) or extending the parallel data with vocabulary identities to promote alignment of the same word form (Dugast et al., 2009).",
        "Marking and filtering methods for SPE also deserve a deeper study.",
        "As for PTComb, we plan to combine several sources of synthetic data (including a huge source-language monolingual corpus)."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "This research is supported by the grants GAUK 9209/2013, FP7-ICT-2011-7-288487 (MosesCore) of the European Union and SVV project number 267 314.",
        "We thank the two anonymous reviewers for their comments."
      ]
    }
  ]
}
