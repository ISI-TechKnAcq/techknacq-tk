{
  "info": {
    "authors": [
      "Dominic Balasuriya",
      "Nicky Ringland",
      "Joel Nothman",
      "Tara Murphy",
      "James R. Curran"
    ],
    "book": "Proceedings of the 2009 Workshop on the People’s Web Meets NLP: Collaboratively Constructed Semantic Resources (People's Web)",
    "id": "acl-W09-3302",
    "title": "Named Entity Recognition in Wikipedia",
    "url": "https://aclweb.org/anthology/W09-3302",
    "year": 2009
  },
  "references": [],
  "sections": [
    {
      "text": [
        "Dominic Balasuriya Nicky Ringland Joel Nothman Tara Murphy James R. Curran",
        "Named entity recognition (NER) is used in many domains beyond the newswire text that comprises current gold-standard corpora.",
        "Recent work has used Wikipedia's link structure to automatically generate near gold-standard annotations.",
        "Until now, these resources have only been evaluated on newswire corpora or themselves.",
        "We present the first NER evaluation on a Wikipedia gold standard (wg) corpus.",
        "Our analysis of cross-corpus performance on wg shows that Wikipedia text may be a harder NER domain than newswire.",
        "We find that an automatic annotation of Wikipedia has high agreement with wg and, when used as training data, outperforms newswire models by up to 7.7%."
      ]
    },
    {
      "heading": "1. Introduction",
      "text": [
        "Named Entity Recognition (NER) is the task of identifying and classifying people, organisations and other named entities (NE) within text.",
        "NER is central to many NLP systems, especially information extraction and question answering.",
        "Machine learning approaches now dominate NER, learning patterns associated with individual entity classes from annotated training data.",
        "This training data, including English newswire from the MUC-6, MUC-7 (Chinchor, 1998), and CoNLL-03 (Tjong Kim Sang and De Meulder, 2003) competitive evaluation tasks, and the bbn Pronoun Coreference and Entity Type Corpus (Weischedel and Brunstein, 2005), is critical to the success of these approaches.",
        "This data dependence has impeded the adaptation or porting of existing NER systems to new domains, such as scientific or biomedical text, e.g. Nobata et al.",
        "(2000).",
        "Similar domain sensitivity is exhibited by most tasks across NLP, e.g.",
        "parsing (Gildea, 2001), and the adaptation penalty is still apparent even when the same set of named entity classes is used in text from similar domains (Ciaramita and Altun, 2005).",
        "Wikipedia is an important corpus for information extraction, e.g. Bunescu and Pa§ca (2006) and Wu et al.",
        "(2008) because of its size, currency, rich semi-structured content, and its closer resemblance to web text than newswire.",
        "Recently, Wikipedia's markup has been exploited to automatically derive NE annotated text for training statistical models (Richman and Schone, 2008; Mika et al., 2008; Nothman et al., 2008).",
        "However, without a gold standard, existing evaluations of these models were forced to compare against mismatched newswire corpora or the noisy Wikipedia-derived annotations themselves.",
        "Further, it was not possible to directly ascertain the accuracy of these automatic extraction methods.",
        "We have manually annotated 39,007 tokens of Wikipedia with coarse-grained named entity tags (wg).",
        "We present the first evaluation of Wiki-pedia-trained models on Wikipedia: the c&c NER tagger (Curran and Clark, 2003b) trained on (a) automatically annotated Wikipedia text (WP2) extracted by Nothman et al.",
        "(2009); and (b) traditional newswire NER corpora (MUC, CoNLL and BBN).",
        "The WP2 model, though trained on noisy annotations, outperforms newswire models on wg by 7.7%.",
        "However, every model, including WP2, performs far worse on wg than on the newswire.",
        "We examined the quality of wg, and found that our annotation strategy produced a high-quality, consistent corpus.",
        "Our analysis suggests that it is the form and distribution of NEs in Wikipedia that make it a difficult target domain.",
        "Finally, we compared wg with the annotations extracted by Nothman et al.",
        "(2009), and found agreement comparable to our inter-annotator agreement, demonstrating that NE corpora can be derived very accurately from Wikipedia."
      ]
    },
    {
      "heading": "2. Background",
      "text": [
        "Traditional evaluations of ner have considered the performance of a tagger on test data from the same source as its training data.",
        "Although the majority of annotated corpora available consist of newswire text, recent practical applications cover a far wider range of genres, including Wikipedia, blogs, RSS feeds, and other data sources.",
        "Cia-ramita and Altun (2005) showed that even when moving a short distance, e.g. annotating WSJ text with the same scheme as CoNLL's Reuters, the performance was 26% worse than on the original text.",
        "Similar differences are reported by Nothman et al.",
        "(2009) who compared muc, CoNLL and bbn annotations reduced to a common tag-set.",
        "They found poor cross-corpus performance to be due to tokenisation and annotation scheme mismatch, missing frequent lexical items, and naming conventions.",
        "They then compared automatically-annotated Wikipedia text as training data and found it also differs in otherwise inconsequential ways from the newswire corpora, in particular lacking abbreviations necessary to tag news text.",
        "Wikipedia, a collaboratively-written online encyclopedia, is readily exploited in nlp, because it is large, semi-structured and multilingual.",
        "Its articles often correspond to NEs, so it has been used for NE recognition (Kazama and Torisawa, 2007) and disambiguation (Bunescu and Pa§ca, 2006; Cucerzan, 2007).",
        "Wikipedia links often span NEs, which may be exploited to automatically create annotated NER training data by determining the entity class of the linked article and then labelling the link text with it.",
        "Richman and Schone (2008) use article classification knowledge from English Wikipedia to produce NE-annotated corpora in other languages (evaluated against NE gold standards for French, Spanish, and Ukrainian).",
        "Mika et al.",
        "(2008) explored the use of tags from a CoNLL-trained tagger to seed the labelling of entities and evaluate the performance of a Wikipedia-trained model by hand.",
        "We make use of an approach described by Nothman et al.",
        "(2009) which is engineered to perform well on bbn data with a reduced tag-set (loc, Mise, org, per).",
        "They derive an annotated corpus with the following steps:"
      ]
    },
    {
      "heading": "1.. Classify Wikipedia articles into entity classes",
      "text": []
    },
    {
      "heading": "2.. Split the articles into tokenised sentences",
      "text": []
    },
    {
      "heading": "3.. Label expanded links according to target NEs",
      "text": []
    },
    {
      "heading": "4.. Select sentences for inclusion in a corpus",
      "text": [
        "To prepare the text, they use mwlib (Pedi-aPress, 2007) to parse Wikipedia's native markup retaining only paragraph text with links, apply Punkt (Kiss and Strunk, 2006) estimated on Wikipedia text to perform sentence boundary detection, and tokenise the resulting text using regular expressions.",
        "Nothman et al.",
        "(2009) infer additional NEs not provided by existing links, and apply rules to adjust link boundaries and classifications to closer match BBN annotations.",
        "Meaningful automatic evaluation of ner is difficult and a number of metrics have been proposed (Nadeau and Sekine, 2007).",
        "Ambiguity leads to entities correctly delimited but misclassified, or boundaries mismatched despite correct classification.",
        "Although the MUC-7 evaluation (Chinchor, 1998) defined a metric which was less sensitive to often-meaningless boundary errors, we consider only exact entity matches as correct, following the standard CoNLL evaluation (Tjong Kim Sang, 2002).",
        "We report precision, recall and F-score for each entity type."
      ]
    },
    {
      "heading": "3. Creating the Wikipedia gold standard",
      "text": [
        "We created a corpus by manually annotating the text of 149 articles from the May 22, 2008 dump of English Wikipedia.",
        "The articles were selected at random from all articles describing named entities, with a roughly equal proportion of article topics from each of the four CoNLL-03 classes (loc, Misc, org, per).",
        "We adopted Nothman et al.",
        "'s (2008) preprocessing described above to produce tokenised sentences for annotation.",
        "Only body text was extracted from the chosen articles for inclusion in the corpus.",
        "Four articles were found not to have any usable text, consisting solely of tables, lists, templates and section headings, which we remove.",
        "Their exclusion leaves a corpus of 145 articles.",
        "Annotation was initially carried out using a finegrained tag-set which was expanded by the an-",
        "[company Aero Gare] was a kitplane manufacturer founded by [person Gary LeGare] in [city Mojave] , [state California] to marketed the [plane Sea Hawker] amphibious aircraft.",
        "(a) Fine-grained annotation",
        "[org Aero Gare] was a kitplane manufacturer founded by [per Gary LeGare] in [loc Mojave] , [loc California] to marketed the [misc Sea Hawker] amphibious aircraft.",
        "(b) Coarse-grained annotation notators as annotation progressed, and eventually contained 96 tags.",
        "We created a mapping from these fine-grained tags to the four coarse-grained tags used in the CoNLL-03 data: per, loc, misc and org.",
        "This enables evaluation with existing ner models.",
        "We believe this two-phase approach allowed annotators to defer difficult mapping decisions, (e.g. should an airport be classified as a loc, org, or Misc?)",
        "which can then be made after discussion.",
        "The mapping could also be modified to suit a particular evaluation task.",
        "Figure 1 shows an example of the use of fine and coarse-grained tags to annotate a sentence.",
        "Tags such as person correspond directly to coarsegrained tags, while most map to a more general tag, such as state and city mapping to loc.",
        "plane is an example of a fine-grained tag that cannot be mapped to loc, org, or per.",
        "These tags may be mapped to Misc; some are not considered entities under the CoNLL scheme and are left unlabelled in the coarse-grained annotation.",
        "Three independent annotators were involved in the annotation process.",
        "Annotator 1 annotated all 145 articles using the fine-grained tags.",
        "Annotators 2 and 3 then re-annotated 19 of these articles (316 sentences or 8030 tokens), amounting to 21% of the corpus.",
        "Annotator 2 used the fine-grained tags described above, while Annotator 3 used the four coarse-grained CoNLL tags.",
        "To measure variation, all three annotations of this common portion were mapped down to the CoNLL tag-set and inter-annotator agreement was calculated.",
        "We found that 202 tokens were disagreed upon by at least one annotator (2.5% of all tokens annotated), and these discrepancies were then discussed by the three annotators.",
        "The inter-annotator agreement will be analysed in more detail in Section 5.",
        "Sentences containing grammatical and typographical errors were not corrected, so that the corpus would be as close as possible to the source text.",
        "Web text often contains errors, such as to",
        "marketed the Sea Hawker from the example in Figure 1, so any NER system must deal with these errors.",
        "Sentences with poor tokenisation or sentence boundary detection were identified and corrected manually, since these errors are introduced by our processing and annotation, and do not exist in the source text.",
        "The final corpus was created by correcting annotation mistakes, with annotators 2 and 3 each correcting 50% of the corpus.",
        "The fine-grained tags were mapped to the four CoNLL tags before the final corrections were made.",
        "The final WG corpus consists of the body text of 145 Wikipedia articles tagged with the four CoNLL-03 tags."
      ]
    },
    {
      "heading": "4. ner on the Wikipedia gold-standard",
      "text": [
        "Nothman et al.",
        "(2009) have previously shown that that an ner system trained on automatically annotated Wikipedia corpora performs reasonably well on non-Wikipedia text.",
        "Having created our WG corpus of gold-standard annotations, we are able to evaluate the performance of these models on Wikipedia text.",
        "We compare the C&C NE maximum-entropy tagger (Curran and Clark, 2003b) trained on gold-standard newswire corpora (MUC-7, BBN and CoNLL-03) with the same tagger trained on automatically annotated Wikipedia text, WP2.",
        "WG is too small to train a reasonable ner model on gold-standard Wikipedia annotations.",
        "Part-of-speech tags are added to all corpora using the c&c pos tagger (Curran and Clark, 2003a) before training and testing.",
        "We evaluate each model on traditional newswire evaluation corpora as well as WG.",
        "Table 1 gives the size of each corpus.",
        "Train",
        "Test",
        "P",
        "R",
        "F",
        "WP2",
        "wg",
        "66.5",
        "61A",
        "66.9",
        "bbn",
        "wg",
        "59.2",
        "59.1",
        "59.2",
        "CoNLL",
        "wg",
        "54.3",
        "57.2",
        "55.7",
        "WP2 *",
        "wg *",
        "75.1",
        "67.7",
        "71.2",
        "bbn *",
        "wg *",
        "57.2",
        "64.1",
        "60.4",
        "CoNLL *",
        "wg *",
        "53.1",
        "62.7",
        "57.5",
        "MUC *",
        "wg *",
        "52.3",
        "57.2",
        "54.6",
        "WP2",
        "bbn",
        "73.4",
        "74.6",
        "74.0",
        "WP2",
        "CoNLL",
        "73.6",
        "64.9",
        "69.0",
        "WP2 *",
        "MUC *",
        "86.2",
        "68.9",
        "76.6",
        "bbn",
        "bbn",
        "85.7",
        "87.3",
        "86.5",
        "CoNLL",
        "CoNLL",
        "85.3",
        "86.5",
        "85.9",
        "MUC",
        "MUC",
        "81.0",
        "83.6",
        "82.3",
        "The results are shown in Table 2.",
        "The WP2 tagger performed substantially better on WG than taggers trained on newswire text, with a 7 – 11% increase in F-score compared to BBN and CoNLL-03, and a 16% increase compared to MUC-7, when miscellaneous NEs in the corpus are not considered in the evaluation.",
        "The Wikipedia trained model thus outperforms newswire models on our new WG corpus even though the training annotations were automatically extracted.",
        "The WP2 tagger performed worse on WG than on gold-standard news corpora (BBN and CoNLL), with a 2 – 7% reduction in F-score.",
        "Further, the performance of WP2 on WG is 11 – 20% F-score lower than same-source evaluation results, e.g. BBN on BBN, CoNLL on CoNLL.",
        "Therefore, despite WP2 showing an advantage in tagging WG due to their common source domain, we find that WG's annotations are harder to predict than the newswire test data commonly used for evaluation.",
        "One possible explanation is that our WG corpus has been inconsistently annotated.",
        "When NEs of miscellaneous type are not considered in the evaluation (asterisks in Table 2), the performance of all taggers on WG improves, with WP2 demonstrating a 4% increase.",
        "This result suggests another partial explanation: that Misc NEs in Wikipedia are more difficult to annotate correctly, due to their poor definition and broad coverage.",
        "A third explanation is that the automatic conversion process proposed by Nothman et al.",
        "(2008) produces much lower quality training data than manual annotation.",
        "We explore these three possibilities below.",
        "^oth taggers are available from http://svn.ask.",
        "it.usyd.edu.au/trac/candc."
      ]
    },
    {
      "heading": "5. Quality of the Wikipedia gold standard",
      "text": [
        "The low performance observed on WG may be due to the poor quality of its annotation.",
        "We ensure that this is not the case by measuring inter-annotator agreement.",
        "The WG annotation process produced three independent annotations of a subset of WG.",
        "These annotations were compared using Cohen's k (Fleiss and Cohen, 1973) between pairs of annotators, and Fleiss' k (Fleiss, 1971), which generalises Cohen's k to more than two concurrent annotations.",
        "Table 3 shows the three types of k values calculated.",
        "Token is calculated on a per token basis, comparing the agreement of annotators on each token in the corpus; NE only, is calculated on the agreement between entities alone, excluding agreement in cases where all annotators agreed that a token was not a NE; Exact refers to the agreement between annotators where all annotators have agreed on the boundaries of a NE, but disagree on the type of NE.",
        "Annotator 1 originally annotated the entire corpus, and Annotators 2 and 3 then corrected exactly half of the corpus each after a discussion between the three annotators to resolve ambiguities.",
        "Landis and Koch (1977) determine that a n value greater than 0.81 indicates almost perfect agreement.",
        "By this standard, our three annotators were in strong agreement prior to discussion, with our Fleiss' n values all greater than 0.81.",
        "Inconsistencies in the corpus due to annotation mistakes by Annotator 1 were corrected by Annotators 2 and 3.",
        "Inter-annotator agreement for cases where the annotators agreed on NE boundaries was higher than agreement on each token, which suggests that many discrepancies resulted from NE bound-",
        "WG",
        "WP2",
        "BBN",
        "CoNLL-03",
        "MUC-7",
        "Test",
        "Train",
        "Train",
        "Test",
        "Train",
        "Test",
        "Train",
        "Test",
        "Tokens",
        "39007",
        "3 500032",
        "901849",
        "129654",
        "203 621",
        "46 435",
        "83 601",
        "60 436",
        "Sentences",
        "1696",
        "146543",
        "37 843",
        "5 462",
        "14987",
        "3 453",
        "3 485",
        "2419",
        "Articles",
        "145",
        " – ",
        "1775",
        "238",
        "946",
        "231",
        "102",
        "99",
        "NES",
        "3 558",
        "288545",
        "49 999",
        "7 307",
        "23 498",
        "5 648",
        "4315",
        "3 540",
        "Token",
        "Exact",
        "ne only",
        "Al and A2",
        "0.95",
        "0.99",
        "0.88",
        "Al and A3",
        "0.91",
        "0.95",
        "0.81",
        "A2 and A3",
        "0.91",
        "0.96",
        "0.79",
        "Fleiss' Kappa",
        "0.92",
        "0.97",
        "0.83",
        "Table 4: NE class distribution, tag entropy and NE density statistics for gold-standard corpora and WG.",
        "ary ambiguities, or disagreement as to whether a phrase constituted a NE at all.",
        "Higher inter-annotator agreement between Annotators 1 and 2 leads us to believe that the two-phase annotation strategy, where an initially fine-grained tag-set is reduced, results in more consistent annotation.",
        "Our analysis demonstrates that WG is annotated in a consistent and accurate manner and the small number of errors cannot alone explain the reduced performance figures."
      ]
    },
    {
      "heading": "6. Comparing gold-standard corpora",
      "text": [
        "Table 4 compares the distribution of different classes of NEs across different corpora on the four CoNLL categories.",
        "WG has a higher proportion of PER and MISC NEs and a lower proportion of ORG NEs than the BBN corpus.",
        "This is also found in the MUC corpus, although comparisons to MUC are affected by its lack of a MISC category.",
        "The CoNLL-03 corpus is most similar to WG in terms of the distribution of the NE classes, although CoNLL-03 has a smaller proportion of MISC NEs than WG.",
        "An analysis of the lengths of NEs in CoNLL shows, however, that they are very different to those in WG (see Table 8), perhaps explaining the difference in performance observed.",
        "Tag entropy H(C) was calculated for each corpus with respect to the 5 possible classes (4 NE classes, and the o tag, indicating non-entities).",
        "H(C) is a measure of the amount of information required to represent the classification of each token in the corpus.",
        "Two calculations are made, including and excluding the frequent O tag.",
        "Our results (Table 4) suggest that WG's tags are least predictable, with a tag entropy of 2.0 bits (without the o class) compared to 1.7 and 1.9 bits for BBN and CoNLL respectively.",
        "While the CoNLL-03 and MUC evaluation corpora are marked up with only very coarse tags, the BBN corpus uses 29 coarse tags, many with specific subtypes, including NEs, descriptors of NEs and non-NEs, intended as answer types for question answering (Brunstein, 2002).",
        "Non-NE types include MONEY and TIME, which are also tagged in the MUC corpus, and others such as ANIMAL.",
        "When evaluating the performance of the taggers, each of bbn's 150 fine-grained tags was mapped to one of four coarse-grained classes or none, using a mapping described in Nothman (2008).",
        "However, since the WG corpus was initially annotated using 96 distinct classes, we map these tags to the corresponding fine-grained BBN NE classes.",
        "In some cases, the tags map exactly (e.g. COUNTRY mapped to LOCATION:COUNTRY); in other cases, classes have to be merged or not mapped at all, where the BBN and WG annotations differ in granularity.",
        "Where possible, we map to fine-grained BBN categories.",
        "We create mappings to a total of 36 BBN entity types, and apply them across the WG corpus.",
        "Table 5 shows the distribution of the most common tags, calculated as a percentage of all counts of the 36 selected tags across each corpus.",
        "Tags for which there is at least a twofold difference in proportion between BBN and WG are marked in bold.",
        "The comparison is dominated by the presence of a disproportionate number of ORG:CORPORATIONS in the BBN corpus compared to wg.",
        "It also mentions many more governmental organisations.",
        "Prominent cases of tags found in higher proportions in wg are works of art, organisations of type OTHER (e.g. bands, sports teams, clubs), events and attractions.",
        "LOC",
        "Mise",
        "ORG",
        "PER",
        "H(C): Witho",
        "Without O",
        "Total nes",
        "% ne tokens",
        "WG",
        "28.5",
        "20.0",
        "25.2",
        "26.3",
        "0.98",
        "2.0",
        "3 558",
        "17.1",
        "BBN",
        "22.4",
        "9.8",
        "46.4",
        "21.3",
        "0.61",
        "1.7",
        "49 999",
        "9.6",
        "MUC",
        "33.3",
        " – ",
        "40.7",
        "26.1",
        "0.52",
        "1.5",
        "4315",
        "8.1",
        "CoNLL",
        "30.4",
        "14.6",
        "26.9",
        "28.1",
        "0.98",
        "1.9",
        "23 498",
        "17.1",
        "Mapped BBN tag",
        "WG",
        "BBN",
        "PERSON",
        "25.9",
        "19.3",
        "ORGAN IZATION:OTH ER",
        "13.0",
        "2.8",
        "ORGANIZATION:CORPORATION",
        "9.2",
        "43.1",
        "GPE:CITY",
        "8.0",
        "6.7",
        "WORK_OF_ART:SONG",
        "4.7",
        "0.1",
        "NORP",
        "4.3",
        "3.1",
        "WORK_OF_ART:OTHER",
        "4.1",
        "1.3",
        "GPE:COUNTRY",
        "3.5",
        "5.1",
        "ORGANIZATION: EDUCATIONAL",
        "3.0",
        "0.9",
        "GPE:STATE_PROVINCE",
        "2.8",
        "2.8",
        "ORGANIZATION:POLITICAL",
        "2.6",
        "0.6",
        "EVENTOTHER",
        "2.5",
        "0.4",
        "ORGAN IZATION:GOVERNMENT",
        "2.0",
        "7.5",
        "WORK_OF_ART:BOOK",
        "1.6",
        "0.4",
        "EVENT:WAR",
        "1.6",
        "0.1",
        "FACOTHER",
        "1.4",
        "0.2",
        "LOCATION :REG ION",
        "1.3",
        "0.8",
        "FAC ATTRACTION",
        "1.2",
        "0.0",
        "This comparison demonstrates that there are observable differences in NE types between the news and Wikipedia domains.",
        "These differences are reflected in the distribution of both coarse and finegrained types of NEs.",
        "The more complex entity distribution in Wikipedia is a likely cause for reduced NER performance on wg.",
        "Nobata et al.",
        "(2000) use gain ratio as an information-theoretic measure of corpus difficulty:",
        "where J(C; F) = H(C) - H{C\\F) is the information gain of the NE tag distribution (C) with respect to a feature set F.",
        "This gain ratio normalises the information gain over the tag entropy, which Nobata et al.",
        "(2000) suggest allows us to compare gain ratios between corpora.",
        "It also makes the impact of including the 'o' tag negligible for our calculations.",
        "We apply this approach to measure the relative difficulty of tagging NEs in the wg corpus.",
        "Table 7 shows that wg tags seem generally harder to predict than those in newswire, on the basis of words, pos tags or orthographic word-types (like those used in the Curran and Clark (2003b) tagger as proposed by Collins (2002)).",
        "In particular, pos tags are less indicative than in BBN and CoNLL, suggesting a wider variety of grammatical functions in ne names in Wikipedia - this might be expected with more band names, and song and movie titles.",
        "Alternatively, it may be an indication that the pos tagging is less reliable on Wikipedia using newswire-trained models.",
        "The previous word's orthographic form also provides less information, which may relate to titles like Mr. and Mrs., strong indicators of per entities, which are frequent in bbn and to a lesser extent CoNLL, but are almost absent in Wikipedia.",
        "The number of tokens in NEs is substantially different between wg and other gold-standard corpora.",
        "When compared with wg, other gold-standard corpora have a larger proportion of single-word NEs (between 7 and 13% more), as shown in Table 8.",
        "The distribution of ne lengths in bbn is most similar to wg, but it still differs significantly in the proportion of single-word NEs.",
        "Additionally, wg has a larger number of long multi-word NEs than the other gold-standard corpora.",
        "Longer entities are more difficult to classify, since boundary resolution is more error prone and they typically contain lowercase words with a wider range of syntactic roles.",
        "This adds to the difficulty of correctly identifying NEs in wg.",
        "The difference in entity lengths is most pronounced Misc NEs (Table 6), with Wikipedia having a substantially smaller number of single-word Misc NEs.",
        "The presence of a large number of long miscellaneous NEs, including song, film and book titles, and other works of art are a feature that characterises the nature of Wikipedia text in contrast to newswire text.",
        "Typically, longer misc NEs in newswire text are laws and norps, which also appear in Wikipedia text.",
        "1",
        "2",
        "3",
        "4",
        "5",
        "6",
        "7+",
        "#NES",
        "wg",
        "53.0",
        "77.0",
        "88.9",
        "94.8",
        "96.6",
        "98.2",
        "100",
        "712",
        "BBN (train)",
        "75.0",
        "91.0",
        "95.4",
        "97.2",
        "98.2",
        "98.7",
        "100",
        "4913",
        "CoNLL (train)",
        "75.0",
        "93.8",
        "98.1",
        "99.5",
        "99.9",
        "99.9",
        "100",
        "3 437",
        "Feature group",
        "wg",
        "BBN",
        "CoNLL",
        "Current token",
        "0.88",
        "0.89",
        "0.93",
        "Current pos",
        "0.43",
        "0.57",
        "0.48",
        "Current word-type",
        "0.42",
        "0.49",
        "0.48",
        "Previous token",
        "0.46",
        "0.43",
        "0.47",
        "Previous pos",
        "0.12",
        "0.19",
        "0.14",
        "Previous word-type",
        "0.07",
        "0.14",
        "0.12",
        "1",
        "2",
        "3",
        "4",
        "5",
        "6",
        "7+",
        "wg",
        "49.9",
        "81.7",
        "93.1",
        "97.4",
        "98.6",
        "99.4",
        "100",
        "BBN (train)",
        "57.4",
        "83.3",
        "92.9",
        "97.4",
        "99.1",
        "99.6",
        "100",
        "CoNLL (train)",
        "63.1",
        "94.5",
        "98.4",
        "99.4",
        "99.8",
        "99.9",
        "100",
        "muc (train)",
        "62.0",
        "89.1",
        "96.1",
        "99.1",
        "99.7",
        "99.8",
        "100"
      ]
    },
    {
      "heading": "7. Evaluation of automatic annotation",
      "text": [
        "We compared the gold-standard annotations in our wg corpus to those sentences that were automatically annotated by Nothman et al.",
        "(2009).",
        "Their automatic annotation process does not retain all Wikipedia sentences.",
        "Rather, it selects sentences where, on the basis of capitalisation heuristics, it seems all named entities in the sentence have been tagged by the automatic process.",
        "We adopt this confidence criterion to produce automatically-annotated subsets of the wg corpus.",
        "Two variants of their automatic annotation procedure were used: wp2 uses a few rules to infer tags for non-linked NEs in Wikipedia; wp4 has looser criteria for inferring additional links, and its over-generation typically reduced its performance as training data (Nothman et al., 2009).",
        "A large proportion of sentences in our wg corpus cannot be automatically tagged with confidence.",
        "Sentence selection leaves 571 sentences (33.7%) after the wp2 process and 698 (41.2%) after the wp4 process (see Table 10).",
        "The use of the more permissive wp4 process may lead to the labelling of more NEs, but many may be spurious.",
        "We use three approaches to compare automatic and manual annotations of wg text: (a) treat each corpus as test data and evaluate ner performance on each; (b) treat wp2 and wp4-style subsets as ner predictions on the wg corpus to calculate an F-score; and (c) treat the automatic annotations like human annotators and calculate k values.",
        "We first evaluate the wp2 model on each corpus and find that performance is higher on automatically-annotated subsets of wg (Table 11).",
        "This is unsurprising given the common automatic annotation process and the effects of the selection criterion.",
        "However, Nothman (2008) provides an",
        "Table 11: ner performance of the wp2-trained model on auto-annotated subsets of wg.",
        "Table 12: Comparing wp2-style wg and wp4-style wg on wg.",
        "The automatically annotated data was treated as predicted annotations on wg.",
        "F-score for the wp2 model when evaluated on 10 folds of automatically-annotated (wp2-style) test data.",
        "This F-score is 8 – 10% higher than wp2's performance on the wp2-style subset of wg, suggesting that wg's text is somewhat more difficult to annotate than typical portions of wp2-style text.",
        "We compare the annotations of wg text more directly by treating the automatic annotations as if they are the output from a tagger run on the 698 and 571 sentences that were confidently chosen.",
        "A reasonable agreement between the gold standard and automatic annotation is observed (Table 12), with F-scores of 87.2% and 89.0% achieved by wp2 and wp4.",
        "Table 12 also shows inter-annotator agreement calculated between the automatically annotated subsets and the gold-standard annotations in wg, using Cohen's k in the same way as for human annotators.",
        "The agreement was very high: equal or better than the agreement between human annotators prior to discussion and correction."
      ]
    },
    {
      "heading": "8. Conclusion",
      "text": [
        "We have presented the first evaluation of named entity recognition (ner) on a gold-standard evaluation of Wikipedia, a resource of increasing importance in Computational Linguistics.",
        "We annotated a corpus of Wikipedia articles (wg) with gold-standard NE tags.",
        "Using this new resource as test data we have evaluated models trained on three gold-standard news wire corpora for ner, and compared them to a model trained on Wikipedia-derived ner annotations (Nothman et al., 2009).",
        "We found that this WP2 model outperformed models trained on muc, CoNLL, and bbn data by more than 7.7% F-score.",
        "1",
        "2",
        "3",
        "4",
        "5",
        "6",
        "7+",
        "#NES",
        "wg",
        "49.2",
        "82.9",
        "94.2",
        "98.0",
        "99.2",
        "99.8",
        "100",
        "2 846",
        "bbn (train)",
        "55.4",
        "82.4",
        "92.6",
        "97.4",
        "99.2",
        "99.7",
        "100",
        "45 086",
        "CoNLL (train)",
        "61.1",
        "94.7",
        "98.4",
        "99.4",
        "99.8",
        "99.9",
        "100",
        "20 061",
        "muc (train)",
        "62.0",
        "89.1",
        "96.1",
        "99.1",
        "99.7",
        "99.8",
        "100",
        "4315",
        "# Sents",
        "# with nes",
        "#nes",
        "wg",
        "1 696",
        "1341",
        "3 558",
        "wg WP2-style",
        "571",
        "298",
        "569",
        "wg WP4-style",
        "698",
        "425",
        "831",
        "train",
        "test",
        "P",
        "R",
        "F",
        "WP2",
        "wg manual",
        "66.5",
        "67.4",
        "66.9",
        "WP2",
        "wg WP2-style",
        "76.0",
        "72.9",
        "74.4",
        "WP2",
        "wg WP4-style",
        "75.5",
        "71.4",
        "73.4",
        "WP2",
        "wp2 ten folds",
        " – ",
        " – ",
        "83.6",
        "WP2 *",
        "wg manual *",
        "75.1",
        "67.7",
        "71.2",
        "WP2 *",
        "wg WP2-style *",
        "81.5",
        "74.4",
        "77.8",
        "WP2 *",
        "wg WP4-style *",
        "81.9",
        "74.6",
        "78.1",
        "WP2 *",
        "wp2 ten folds *",
        " – ",
        " – ",
        "86.1",
        "ne k",
        "P",
        "R",
        "F",
        "WP2-style WP4-style",
        "0.94 0.93",
        "0.84 0.83",
        "89.0 86.8",
        "89.0 87.6",
        "89.0 87.2",
        "However, we found that all four models performed significantly worse on the wg corpus than they did on news text, suggesting that Wikipedia as a textual domain is more difficult for ner.",
        "We initially suspected that annotation quality was responsible, but found that we had very high inter-annotator agreement even before further discussion and correction of the corpus.",
        "This also validates our approach of creating many fine-grained categories and then reducing them down to the four CoNLL types.",
        "To further examine the difficulty of tagging wg, we compared the distribution of fine-grained entity types in wg and BBN, finding a more even distribution over a larger range of types in wg.",
        "We found that the standard ner features such as current and previous pos tags and words had lower predictive power on wg.",
        "We also compared the distribution of NEs lengths and showed that wg entities are longer on average (for instance song and book titles).",
        "This all suggests that Wikipedia is genuinely more difficult to automatically annotate with named entities than newswire.",
        "Finally, we compared the common sentences between Nothman et al.",
        "'s (2009) automatic NE annotation of Wikipedia and wg, directly measuring the quality of automatically deriving NE annotations from Wikipedia.",
        "We found that WP2 agreed with our final wg corpus to a high degree, demonstrating that Wikipedia is a viable source of automatically annotated NE annotated data, reducing our dependence on expensive manual annotation for training NER systems."
      ]
    },
    {
      "heading": "Acknowledgements",
      "text": [
        "We would like to thank the anonymous reviewers for their helpful feedback.",
        "This work was supported by the Australian Research Council under Discovery Project DP0665973.",
        "Dominic Bal-asuriya was supported by a University of Sydney Outstanding Achievement Scholarship.",
        "Nicky Ringland was supported by a Capital Markets CRC High Achievers Scholarship.",
        "Joel Nothman was supported by a Capital Markets CRC PhD Scholarship and a University of Sydney Vice-Chancellor's Research Scholarship."
      ]
    }
  ]
}
