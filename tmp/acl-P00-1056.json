{
  "info": {
    "authors": [
      "Franz Josef Och",
      "Hermann Ney"
    ],
    "book": "Annual Meeting of the Association for Computational Linguistics",
    "id": "acl-P00-1056",
    "title": "Improved Statistical Alignment Models",
    "url": "https://aclweb.org/anthology/P00-1056",
    "year": 2000
  },
  "references": [
    "acl-C00-2163",
    "acl-C96-2141",
    "acl-H93-1039",
    "acl-J93-2003"
  ],
  "sections": [
    {
      "heading": "Abstract",
      "text": [
        "In this paper, we present and compare various single-word based alignment models for statistical machine translation.",
        "We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications.",
        "We present different methods to combine alignments.",
        "As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.",
        "We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "In statistical machine translation we set up a statistical translation model Pr(fl lcl) which describes the relationship between a source language (SL) string fl and a target language (TL) string cf. .",
        "In (statistical) alignment models Pr(fl , ai lcl), a `hidden' alignment ai is introduced which describes a mapping from source word fj to a target word cap .",
        "We discuss here the IBM translation models IBM-1 to IBM-5 (Brown et al., 1993b) and the Hidden-Markov alignment model (Vogel et al., 1996; Och and Ney, 2000).",
        "The different alignment models we present provide different decompositions of Pr(fl ,ajlcl).",
        "An alignment ai for which holds ai = arg max Pr(fl , ai lci) al for a specific model is called Viterbi alignment of this model.",
        "So far, no well established evaluation criterion exists in the literature for these alignment models.",
        "For various reasons (non-unique reference translation, over-fitting and statistically deficient models) it seems hard to use training/test perplexity as in language modeling.",
        "Using translation quality is problematic, as translation quality is not well defined and as there are additional influences such as language model or decoder properties.",
        "We propose in this paper to measure the quality of an alignment model using the quality of the Viterbi alignment compared to a manually produced alignment.",
        "This allows an automatic evaluation, once a reference alignment has been produced.",
        "In addition, it results in a very precise and reliable evaluation criterion that is well suited to assess various design decisions in modeling and training of statistical alignment models."
      ]
    },
    {
      "heading": "2 Models",
      "text": []
    },
    {
      "heading": "In this paper we use the models IBM-1",
      "text": [
        "to IBM-5 from (Brown et al., 1993b) and the Hidden-Markov alignment model (HMM) from (Vogel et al., 1996; Och and Ney, 2000).",
        "All these models provide different decompositions of the probability Pr(fl , ai jel).",
        "The alignment �al may contain alignments aj 0 with the `empty' word co to account for French words that are not aligned to any English word.",
        "All models include lexicon parameters p(f I e) and additional parameters describing the probability of an alignment.",
        "We now sketch the structure of the six mod",
        "els: • In IBM-1 all alignments have the same probability.",
        "• IBM-2 uses a zero-order alignment model p(aj l j, I, J) where different alignment positions are independent from each other.",
        "• The HMM uses a first-order model",
        "p(aj l aj_1) where the alignment position aj depends on the previous alignment position aj_1.",
        "• In IBM-3 we have an (inverted) zero",
        "order alignment model p(j I aj , I, J) with an additional fertility model p(0Ie) which describes the number of words 0 aligned to an English word e.",
        "• In IBM-4 we have an (inverted) first-order alignment model p(j l j') and a fertility model p(ole).",
        "• The models IBM-3 and IBM-4 are defi",
        "cient as they waste probability mass on non-strings.",
        "IBM-5 is a reformulation of IBM-4 with a suitably refined alignment model in order to avoid deficiency.",
        "So the main differences of these models lie in the alignment model (which may be zero-order or first-order), in the existence of an explicit fertility model and whether the model is deficient or not.",
        "For HMM, IBM-4 and IBM-5 it is straightforward to extend the alignment parameters to include a dependence on the word classes of the words around the alignment position.",
        "In the HMM alignment model we allow for a dependence from the class E = C(eaj_1).",
        "Correspondingly, we can include similar dependencies on French and English word classes in IBM-4 and IBM-5 (Brown et al., 1993b).",
        "The classification of the words into a given number of classes (here: 50) is performed automatically by another statistical learning procedure (Kneser and Ney,"
      ]
    },
    {
      "heading": "3 Trainingl",
      "text": [
        "The training of all alignment models is done by the EM-algorithm using a parallel training corpus (f(s), e(s)), s = 1, ... , S .",
        "In the E-step the counts for one sentence pair (f, e) are calculated.",
        "For the lexicon parameters the",
        "Since there is no efficient way in the fertility models IBM-3 to 5 to avoid the explicit summation over all alignments in the EM-algorithm, the counts are collected only over a subset of promising alignments.",
        "For IBM3, IBM-4 and IBM-5 we perform the count collection only over a small number of good alignments.",
        "In order to keep the training fast we can take into account only a small fraction",
        "of all alignments.",
        "We will compare three different possibilities of using subsets of different size: • The simplest possibility is to perform",
        "Viterbi training using only the best alignment that can be found.",
        "As the calculation of the Viterbi alignment itself is very time-consuming it is computed only approximately using the method described in (Brown et al., 1993b).",
        "• In (Al-Onaizan et al., 1999) it was sug",
        "gested to use also the neighboring alignments (i.e. alignments differing by one 'Our implementation of the IBM translation models is based on GIZA which is part of the publicly available toolkit for statistical machine translation EGYPT (Al-Onaizan et al., 1999).",
        "1991).",
        "move/swap) from the best alignment reachable.",
        "• In (Brown et al., 1993b) an even larger set of alignments was used including also the `pegged' alignments.",
        "The different models are trained in succession on the same data, where the final parameter values of a simpler model serve as starting point for a more complex model.",
        "In section 8 we will show that by using the HMM instead of IBM-2 while bootstrapping to IBM-4/IBM5 the alignment quality can be significantly improved."
      ]
    },
    {
      "heading": "4 Smoothing",
      "text": [
        "To overcome the problem of over-fitting on the training data and to cope better with rare words we apply smoothing on alignment and fertility probabilities.",
        "For the alignment probabilities of the HMM (and correspondingly for IBM-4 and IBM-5) we perform an interpolation with a constant distribution: pl(aj jaj-1, I) = a 1 + (1 – a) • p(aj jaj-1, I) For the fertility probabilities we assume that there is a dependence on the number of letters g(e) of e and estimate also a distribution p(01g) using the EM-algorithm.",
        "Figure 1 shows the relation between the number of letters g of a (German) word and the average fertility (O(g) _ EO0 • p(ojg)).",
        "We can see that longer words have a higher fertility.",
        "The fertility distribution used in training is then computed as follows: p'(OIe) _ (e) P(Ole) e) + +� (e)p(OI g(e)) + Here n(e) denotes the frequency of e in the training corpus.",
        "This ensures that for frequent words, i.e. n(e) » , , the specific distribution p(oje) dominates and for rare words, i.e. n(e) K , , the general distribution p(Ojg(e)) dominates.",
        "The interpolation parameters a and � are optimized with respect to alignment quality on a validation corpus.",
        "the length (in letters) of a German word (on VERBMOBIL task, see later).",
        "5 Is deficiency a problem?",
        "When using the EM-algorithm on IBM-3 and IBM-4, we observed that during the EM-iterations more and more words are aligned to the empty word.",
        "This results in a bad alignment quality as too many words are aligned to the empty word.",
        "This does not occur when using the other models.",
        "We believe that the reason of this lies in the fact that IBM-3 and IBM-4 are deficient.",
        "The use of the EM-algorithm guarantees that the likelihood an alignment model assigns to the training corpus is steadily increasing.",
        "This is true for deficient and for non-deficient models likewise.",
        "However, for deficient models the likelihood can be increased simply by reducing the amount of deficiency.",
        "In IBM-3 and IBM-4 as defined in (Brown et al., 1993b) the distortion model for real words is deficient, but the distortion model for the empty word is non-deficient, so the EM-algorithm can increase likelihood by simply aligning more and more words to the empty word.",
        "2 Therefore, we changed IBM-3 and IBM-4 slightly to obtain also a deficient distortion model for the empty word.",
        "The distortion",
        "as IBM-3 and IBM-4 were not trained directly.",
        "with S(ure) (filled dots) and P(ossible) connections."
      ]
    },
    {
      "heading": "6 Evaluation methodology",
      "text": [
        "In the following, we present an annotation scheme for single-word based alignments and a corresponding evaluation criterion.",
        "For a different approach to assess alignment quality see (Ahrenberg et al., 2000).",
        "It is well known that manually performing a word alignment is a complicated and ambiguous task (Melamed, 1998).",
        "Therefore, we developed an annotation scheme that makes it possible to annotate explicitly the ambiguous alignments.",
        "We allowed human experts who performed the annotation to specify two different kinds of alignments: an S (sure) alignment which is used for alignments that are unambiguous and a P (possible) alignment which is used for alignments that might or might not exist.",
        "The P relation is used especially to align words within idiomatic expressions, free translations, and missing function words (S C P).",
        "The thus obtained reference alignment may contain many-to-one and one-to-many relationships.",
        "Figure 2 shows an example of a manually aligned sentence with S and P relations.",
        "The quality of an alignment A = { (j, aj) I aj > 0} is then computed by appropriately redefined precision and recall measures: recall = IAISISI, precision = IAJAIPI and the following error rate:",
        "Thereby, a recall error can only occur if a S(ure) alignment is not found and a precision error can only occur if a found alignment is not even P(ossible).",
        "The set of sentence pairs for which the manual alignment is produced is randomly selected from the training corpus.",
        "As the alignment is learned unsupervised, these sentence pairs may also be used in training.",
        "Normally, the annotation is performed by two annotators, producing sets Sl, Pl, S2, P2.",
        "To increase the quality of the reference alignment the annotators are presented the mutual errors and are asked to improve their alignment if possible.",
        "From these alignments we finally generate a reference alignment which contains only those S(ure) connections where both annotators agree and it contains all the P(ossible) connections from both annotators.",
        "This can be done by forming the intersection of the sure alignments",
        "force that, if we compare the sure alignments of every single annotator with the combined reference alignment we obtain an AER of zero percent.",
        "7 Generalized alignments The baseline alignment model does not permit a source word to be aligned with two or more target words.",
        "Therefore, lexical correspondences like `Zahnarzttermin' for dentist's appointment cause problems because a single source word must be mapped on two or more target words.",
        "To solve this problem, we perform a training in both translation directions (source to target, target to source).",
        "Thus we obtain two alignment vectors ai and bi for each sentence pair.",
        "In the following, Al = {(aj, j)Iaj > 0} and A2 = f (i, bi) lbi > 01 denote the sets of links in the two Viterbi alignments.",
        "We increase the quality of the alignments with respect to precision, recall or AER by combin- �W 0.1 ing Al and A2 into one alignment matrix A using the following combination methods: 0.08",
        "• Intersection: A = Al n A2 • Union: A = Al U A2 • Refined: In a first step the intersection",
        "A = Al n A2 is determined.",
        "The elements within A are justified by both Viterbi alignments and are therefore very reliable.",
        "We now extend the alignment A",
        "iteratively by adding links (i, j) occurring only in Al or in A2 if neither fi nor ei have an alignment in A or if the following conditions hold: – the link (i, j) has a horizontal neigh",
        "already in A, and – the set A U {(i, j)} does not contain alignments with both horizontal and vertical neighbors.",
        "Obviously, the intersection leads to an alignment that has only one-to-one alignments with higher precision and a lower recall.",
        "The union leads to a higher recall and a lower precision of the combined alignment.",
        "We typically observe that the refined combination is able to produce an alignment with better recall and precision."
      ]
    },
    {
      "heading": "8 Experiments",
      "text": [
        "We present results on the VERBMOBIL and the HANSARDS task (Table 1).",
        "For both tasks we manually aligned a randomly chosen subset of the training corpus (Table 2).",
        "From this corpus the first 100 sentences were used as validation corpus to optimize the smoothing parameters and the remaining sentences were used as test corpus.",
        "In the following graphs, we display the AER for every iteration of the EM-algorithm.",
        "training of IBM-3/4/5 (VERBMOBIL task.)",
        "Unless noted otherwise, we used for training of IBM-3/4 our modified version described in section 5.",
        "The number of alignments in training Figure 3 compares the results obtained by using different numbers of alignments in the training of the sophisticated alignment models on the VERBMOBIL task.",
        "In order to reduce training time we restricted the number of pegged alignments by using only those alignments where Pr(f, ale) is not too much smaller than the probability of the Viterbi alignment.",
        "If we use only the Viterbi alignment, the results are significantly worse than additionally using the neighborhood of the Viterbi alignment.",
        "By doing `pegging', we obtain an additional small improvement.",
        "Table 3 shows the computing time for performing one iteration of the EM-algorithm.",
        "Using a larger set of alignments significantly increases the training time for the models IBM-4 and IBM-5.",
        "As `pegging' yields only a moderate improvement, all following results are obtained using the neighborhood of the Viterbi alignment.",
        "model parameters are used to directly estimate the IBM-4 model parameters.",
        "In the later iterations, IBM-4 is able to reduce the advantage of using HMM.",
        "But in the end we still obtain better results when using in bootstrapping HMM (AER: 5.8 %) instead of IBM-2 (AER: 7.4 %).",
        "In the HANSARDS(50k) task (see Figure 5), the error rates are higher especially because of the high vocabulary size.",
        "The use of HMM in training yields an even stronger reduction in AER.",
        "Interestingly, already the AER of the final iteration of HMM (18.0%) yields better results than the best EM-iteration when using IBM-2 in bootstrapping (20.0%).",
        "We conclude that it is important to start the training of the sophisticated alignment models with good initial parameters.",
        "The use of IBM-3 after HMM makes results worse, but finally IBM-4 produces best results (15.0%).",
        "Astonishingly, IBM-5 produces worse results than IBM-4.",
        "This is maybe because IBM-5 has a lot more training parameters and the distortion model uses only a dependence on the French word class.",
        "For the following experiments we use training scheme"
      ]
    },
    {
      "heading": "Effect of Smoothing",
      "text": [
        "Figure 6 shows the effect of using our mod",
        "HMM in bootstrapping IBM-3/4/5 (VERBMOBIL task).",
        "using the standard version of IBM-4 yields a higher AER which is mainly due to a worse recall.",
        "Without smoothing, we also observe early over-fitting: AER increases after the second iteration of HMM.",
        "Analyzing the alignments shows that the smoothing of fertility probabilities also significantly reduces the problem that rare words often form `garbage collectors' in that they tend to align to a lot of words (see (Brown et al., 1993a)).",
        "task)."
      ]
    },
    {
      "heading": "Using a larger training corpus",
      "text": [
        "Table 4 shows the effect of using different amounts of training data.",
        "As expected, more training data helps to improve alignment quality for all models.",
        "However, for IBM-1 the relative improvement is very small compared to the relative improvement using HMM and IBM-4."
      ]
    },
    {
      "heading": "Generalized Alignments",
      "text": [
        "Table 5 shows precision, recall and AER of the last iteration of IBMA for both translation directions.",
        "Especially for the language pair German-English (VERBMOBIL task) we observe that by using German as source language the AER is much higher than by using English as source language.",
        "This is because the baseline alignment representation as a vector ai does in that case forbid that the often occurring German word compounds align to more than only one English word.",
        "The effect of merging alignments by forming the intersection, the union or the refined combination of the Viterbi alignments (see section 7) of both translation directions is shown in Table 6.",
        "By using the refined combination we can increase precision and recall on all tasks.",
        "The lowest AER on the VERBMOBIL task is obtained using the refined combination method.",
        "The lowest AER on the HANSARDS task is obtained using intersection.",
        "By forming a union or intersection of the alignments we can obtain recall or precision values (but not both) over 96 %."
      ]
    },
    {
      "heading": "9 Conclusion",
      "text": [
        "We have discussed various extensions to statistical alignment models.",
        "An evaluation criterion, i.e. the alignment error rate, was suggested and results on different tasks were presented.",
        "We have shown that sophisticated alignment models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2.",
        "We have described various heuristics that improve precision, recall or both by combining Viterbi alignments of both translation directions.",
        "Further improvements in producing better alignments are expected from making use of cognates, and from statistical alignment models that are based on word groups rather than single words."
      ]
    },
    {
      "heading": "Acknowledgment",
      "text": [
        "This work has been partially supported as part of the Verbmobil project (contract number 01 IV 701 T4) by the German Federal Ministry of Education, Science, Research and Technology and as part of the EuTrans project by the by the European Community (ESPRIT project number 30268)."
      ]
    }
  ]
}
