{
  "info": {
    "authors": [
      "Guangyou Zhou",
      "Jun Zhao",
      "Daojian Zeng"
    ],
    "book": "COLING",
    "id": "acl-C14-1126",
    "title": "Sentiment Classification with Graph Co-Regularization",
    "url": "https://aclweb.org/anthology/C14-1126",
    "year": 2014
  },
  "references": [
    "acl-N13-1008",
    "acl-P02-1053",
    "acl-P06-2079",
    "acl-P07-1056",
    "acl-P09-1027",
    "acl-P09-1028",
    "acl-P11-1013",
    "acl-P11-1014",
    "acl-P11-1033",
    "acl-P12-1060",
    "acl-P13-1084",
    "acl-W02-1011",
    "acl-W06-3808"
  ],
  "sections": [
    {
      "text": [
        "Sentiment Classification with Graph Co-Regularization Guangyou Zhou, Jun Zhao, and Daojian Zeng National Laboratory of Pattern Recognition Institute of Automation, Chinese Academy of Sciences 95 Zhongguancun East Road, Beijing 100190, China {gyzhou,jzhao,djzeng}@nlpr.ia.ac.cn",
        "Abstract",
        "Sentiment classification aims to automatically predict sentiment polarity (e.g., positive or neg-ative) of user-generated sentiment data (e.g., reviews, blogs).",
        "To obtain sentiment classification with high accuracy, supervised techniques require a large amount of manually labeled data.",
        "The labeling work can be time-consuming and expensive, which makes unsupervised (or semi-supervised) sentiment analysis essential for this application.",
        "In this paper, we propose a novel algorithm, called graph co-regularized non-negative matrix tri-factorization (GNMTF), from the geometric perspective.",
        "GNMTF assumes that if two words (or documents) are sufficiently close to each other, they tend to share the same sentiment polarity.",
        "To achieve this, we encode the geometric information by constructing the nearest neighbor graphs, in conjunction with a nonnegative matrix tri-factorization framework.",
        "We derive an efficient algorithm for learning the factorization, analyze its complexity, and provide proof of convergence.",
        "Our empirical study on two open data sets validates that GNMTF can consistently improve the sentiment classification accuracy in comparison to the state-of-the-art methods."
      ]
    },
    {
      "heading": "1 Introduction",
      "text": [
        "Recently, sentiment classification has gained a wide interest in natural language processing (NLP) community.",
        "Methods for automatically classifying sentiments expressed in products and movie reviews can roughly be divided into supervised and unsupervised (or semi-supervised) sentiment analysis.",
        "Supervised techniques have been proved promising and widely used in sentiment classification (Pang et al., 2002; Pang and Lee, 2008; Liu, 2012).",
        "However, the performance of these methods relies on manually labeled training data.",
        "In some cases, the labeling work may be time-consuming and expensive.",
        "This motivates the problem of learning robust sentiment classification via unsupervised (or semi-supervised) paradigm.",
        "A traditional way to perform unsupervised sentiment analysis is the lexicon-based method (Turney, 2002; Taboada et al., 2011).",
        "Lexicon-based methods employ a sentiment lexicon to determine overall sentiment orientation of a document.",
        "However, it is difficult to define a universally optimal sentiment lexicon to cover all words from different domains (Lu et al., 2011a).",
        "Besides, most semi-automated lexicon-based methods yield unsatisfactory lexicons, with either high coverage and low precision or vice versa (Ng et al., 2006).",
        "Thus it is challenging for lexicon-based methods to accurately identify the overall sentiment polarity of users generated sentiment data.",
        "Recently, Li et al. (2009) proposed a constrained non-negative matrix tri-factorization (CNMTF) approach to sentiment classification, with a domain-independent sentiment lexicon as prior knowledge.",
        "Experimental results show that CNMTF achieves state-of-the-art performance.",
        "From the geometric perspective, the data points (words or documents) may be sampled from a distribution supported by a low-dimensional manifold embedded in a high-dimensional space (Cai et al., 2011).",
        "This geometric structure, meaning that two words (or documents) sufficiently close to each other tend to share the same sentiment polarity, should be preserved during the matrix factorization.",
        "Research studies ",
        "Licence details: http:// creativecommons.org/licenses/by/4.0/ 1331 have shown that learning performance can be significantly enhanced in many real applications (e.g., text mining, computer vision, etc.)",
        "if the geometric structure is exploited (Roweis and Saul, 2000; Tenen-baum et al., 2000).",
        "However, CNMTF fails to exploit the geometric structure, it is not clear whether this geometric information is useful for sentiment classification, which remains an under-explored area.",
        "This paper is thus designed to fill the gap.",
        "In this paper, we propose a novel algorithm, called graph co-regularized non-negative matrix tri-factorization (GNMTF).",
        "We construct two affinity graphs to encode the geometric information underlying the word space and the document space, respectively.",
        "Intuitively, if two words or documents are sufficiently close to each other, they tend to share the same sentiment polarity.",
        "Taking these two graphs as co-regularization for the non-negative matrix tri-factorization, leading to the better sentiment polarity prediction which respects to the geometric structures of the word space and document space.",
        "We also derive an efficient algorithm for learning the tri-factorization, analyze its complexity, and provide proof of convergence.",
        "Empirical study on two open data sets shows encouraging results of the proposed method in comparison to state-of-the-art methods.",
        "The remainder of this paper is organized as follows.",
        "Section 2 introduces the basic concept of matrix tri-factorization.",
        "Section 3 describes our graph co-regularized non-negative matrix tri-factorization (GN- MTF) for sentiment classification.",
        "Section 4 presents the experimental results.",
        "Section 5 introduces the related work.",
        "In section 6, we conclude the paper and discuss future research directions.",
        "2 Preliminaries 2.1 Non-negative Matrix Tri-factorization Li et al. (2009) proposed a matrix factorization based framework for unsupervised (or semi-supervised) sentiment analysis.",
        "The proposed framework is built on the orthogonal non-negative matrix tri-factorization (NMTF) (Ding et al., 2006).",
        "In these models, a term-document matrixX = [x 1 , ?",
        "?",
        "?",
        ",x n ] ?",
        "R m?n is approximated by three factor matrices that specify cluster labels for words and documents by solving the following optimization problem: min U,H,V?0 O = ?",
        "?",
        "X?UHV T ?",
        "?",
        "2 F + ?",
        "1 ?",
        "?",
        "U T U?",
        "I ?",
        "?",
        "2 F + ?",
        "2 ?",
        "?",
        "V T V ?",
        "I ?",
        "?",
        "2 F (1) where ?",
        "1 and ?",
        "2 are the shrinkage regularization parameters, U = [u 1 , ?",
        "?",
        "?",
        ",u k ] ?",
        "R m?k + is the word-sentiment matrix, V = [v 1 , ?",
        "?",
        "?",
        ",v n ] ?",
        "R n?k + is the document-sentiment matrix, and k is the number of sentiment classes for documents.",
        "Our task is polarity sentiment classification (positive or negative), i.e., k = 2.",
        "For example,V i1 = 1 (orU i1 = 1) represents that the sentiment polarity of document i (or word i) is positive, andV i2 = 1 (orU i2 = 1) represents that the sentiment polarity of document i (or word i) is negative.",
        "V i?",
        "= 0 (orU i?",
        "= 0) represents unknown, i.e., the document i (or word i) is neither positive or negative.",
        "H ?",
        "R k?k + provides a condensed view of X; ?",
        "?",
        "?",
        "F is the Frobenius norm and I is a k ?",
        "k identity matrix with all entries equal to 1.",
        "Based on the shrinkage methodology, we can approximately satisfy the orthogonality constraints forU andV by preventing the second and third terms from getting too large.",
        "2.2 Constrained NMTF Lexical knowledge in the form of the polarity of words in the lexicon can be introduced in matrix tri-factorization.",
        "By partially specifying word polarity viaU, the lexicon influences the sentiment prediction V over documents.",
        "Following the literature (Li et al., 2009), let U 0 represent lexical prior knowledge about sentiment words in the lexicon, e.g., if word i is positive (U 0 ) i1 = 1 while if it is negative (U 0 ) i2 = 1, and if it does not exist in the lexicon (U 0 ) i?",
        "= 0.",
        "Li et al. (2009) also investigated that we had a few documents manually labeled for the purpose of capturing some domain-specific connotations.",
        "LetV 0 denote the manually labeled documents, if the document expresses positive sentiment (V 0 ) ii = 1, and (V 0 ) i2 = 1 for negative sentiment.",
        "Therefore, the semi-supervised learning with lexical knowledge can be written as: min U,H,V?0 O + ?Tr [ (U?U 0 ) T C u (U?U 0 ) ] + ?Tr [ (V ?V 0 ) T C v (V ?V 0 ) ] (2) 1332 where Tr(?)",
        "denotes the trace of a matrix, ?",
        "> 0 and ?",
        "> 0 are the parameters which control the contribution of lexical prior knowledge and manually labeled documents.",
        "C u ?",
        "{0, 1} m?m is a diagonal matrix whose entry C u ii = 1 if the category of the i-th word is known and C u ii = 0 otherwise.",
        "C v ?",
        "{0, 1} n?n is a diagonal matrix whose entry C v ii = 1 if the category of the i-th document is labeled and C v ii = 0 otherwise.",
        "3 Graph Co-regularized Non-negative Matrix Tri-factorization In this section, we introduce our proposed graph co-regularized non-negative matrix tri-factorization (GNMTF) algorithm which avoids this limitation by incorporating the geometrically based co-regularization.",
        "3.1 Model Formulation Based on the manifold assumption (Belkin and Niyogi, 2001), if two documents x i and x j are sufficiently close to each other in the intrinsic geometric of the documents distribution, then their sentiment polarity v i and v j should be close.",
        "In order to model the geometric structure, we construct a document-document graphG v .",
        "In the graph, nodes represent documents in the corpus and edges represent the affinity between the documents.",
        "The affinity matrixW v ?",
        "R n?n of the graph G v is defined as W v ij = { cos(x i ,x j ) if x i ?",
        "N p (x j ) or x j ?",
        "N p (x i ) 0 otherwise (3) where N p (x i ) represents the p-nearest neighbors of document x i .",
        "Many matrices, e.g., 0-1 weighting, textual similarity and heat kernel weighting (Belkin and Niyogi, 2001), can be used to obtain nearest neighbors of a document, and further define the affinity matrix.",
        "Since W v ij in our paper is only for measuring the closeness, we only use the simple textual similarity and do not treat the different weighting schemes separately due to the limited space.",
        "For further information, please refer to (Cai et al., 2011).",
        "Preserving the geometric structure in the document space is reduced to minimizing the following loss function: R v = 1 2 n ?",
        "i,j=1 ?",
        "?",
        "v i ?",
        "v j ?",
        "?",
        "2 2 W v ij = n ?",
        "i=1 v T i v i D v ii ?",
        "n ?",
        "i,j=1 v T i v j W v ij = Tr(V T D v V)?",
        "Tr(V T W v V) = Tr(V T L v V) (4) whereD v ?",
        "R n?n is a diagonal matrix whose entries are column (or row, sinceD v is symmetric) sums ofW v ,D v ii = ?",
        "n j=1 W v ij , and L v = D v ?W v is the Laplacian matrix (Chung, 1997) of the constructed graph G v .",
        "Similarly to document-document geometric structure, if two words w i = [x i1 , ?",
        "?",
        "?",
        ",x in ] and w j = [x j1 , ?",
        "?",
        "?",
        ",x jn ] are sufficiently close to each other in the intrinsic geometric of the words distribution, then their sentiment polarity u i and u j should be close.",
        "In order to model the geometric structure in the word space, we construct a word-word graph G u .",
        "In the graph, nodes represent distinct words and edges represent the affinity between words.",
        "The affinity matrixW u ?",
        "R m?m of the graph G u is defined as W u ij = { cos(w i ,w j ) ifw i ?",
        "N p (w j ) orw j ?",
        "N p (w i ) 0 otherwise (5) where N p (w j ) represents the p-nearest neighbor of word w j .",
        "Here, we represent a term w j as a document vector [x j1 , ?",
        "?",
        "?",
        ",x jn ].",
        "To measure the closeness of two words, a common way is to calculate the similarity of their vector representations.",
        "Although there are several ways (e.g., co-occurrence infor-mation, semantic similarity computed by WordNet, Wikipedia, or search engine have been empirically studied in NLP literature (Hu et al., 2009)) to define the affinity matrixW u , we do not treat the different ways separately and leave this investigation for future work.",
        "Preserving the geometric structure in the word space is reduced to minimizing the following loss function: R u = 1 2 m ?",
        "i,j=1 ?",
        "?",
        "u i ?",
        "u j ?",
        "?",
        "2 2 W u ij = Tr(U T L u U) (6) 1333 where L u = D u ?W u is the Laplacian matrix of the constructed graph G u , and D u ?",
        "R m?m is a diagonal matrix whose entries areD u ii = ?",
        "m j=1 W u ij .",
        "Finally, we treat unsupervised (or semi-supervised) sentiment classification as a clustering problem, employing lexical prior knowledge and partial manually labeled data to guide the learning process.",
        "More-over, we introduce the geometric structures from both document and word sides as co-regularization.",
        "Therefore, our proposed unsupervised (or semi-supervised) sentiment classification framework can be mathematically formulated as solving the following optimization problem: min U,H,V?0 L = ?",
        "?",
        "X?UHV T ?",
        "?",
        "2 F + ?",
        "1 ?",
        "?",
        "U T U?",
        "I ?",
        "?",
        "2 F + ?",
        "2 ?",
        "?",
        "V T V ?",
        "I ?",
        "?",
        "2 F + ?Tr [ (U?U 0 ) T C u (U?U 0 ) ] + ?Tr(U T L u U) + ?Tr [ (V ?V 0 ) T C v (V ?V 0 ) ] + ?Tr(V T L v V) (7) where ?",
        "> 0 and ?",
        "> 0 are parameters which control the contributions of document space and word space geometric information, respectively.",
        "With the optimization results, the sentiment polarity of a new document x i can be easily inferred by f(x i ) = argmax j?",
        "{p, n} V ij .",
        "3.2 Learning Algorithm We present the solution to the GNMTF optimization problem in equation (7) as the following theorem.",
        "The theoretical aspects of the optimization are presented in the next subsection.",
        "Theorem 3.1.",
        "Updating U, H and V using equations (8)?",
        "(10) will monotonically decrease the objective function in equation (7) until convergence.",
        "U?",
        "U ?",
        "[ XVH T + ?",
        "1 U+ ?C u U 0 + ?W u U ] [ UHV T VH T + ?",
        "1 UU T U+ ?C u U+ ?D u U ] (8) H?",
        "H ?",
        "[ U T XV ] [ U T UHV T V ] (9) V?",
        "V ?",
        "[ X T UH+ ?",
        "2 V + ?C v V 0 + ?W v V ] [ VH T U T UH+ ?",
        "2 VV T V + ?C v V + ?D v V ] (10) where operator ?",
        "is element-wise product and [?]",
        "[?]",
        "is element-wise division.",
        "Based on Theorem 3.1, we note that the multiplicative update rules given by equations (8)?",
        "(10) are obtained by extending the updates of standard NMTF (Ding et al., 2006).",
        "A number of techniques can be used here to optimize the objective function in equation (7), such as alternating least squares (Kim and Park, 2008), the active set method (Kim and Park, 2008), and the projected gradients approach (Lin, 2007).",
        "Nonetheless, the multiplicative updates derived in this paper has reasonably fast convergence behavior as shown empirically in the experiments.",
        "3.3 Theoretical Analysis In this subsection, we give the theoretical analysis of the optimization, convergence and computational complexity.",
        "Without loss of generality, we only show the optimization ofU and formulate the Lagrange function with constraints as follows: L(U) = ?",
        "?",
        "X?UHV T ?",
        "?",
        "2 F + ?",
        "1 ?",
        "?",
        "U T U?",
        "I ?",
        "?",
        "2 F + ?Tr [ (U?U 0 ) T C u (U?U 0 ) ] + Tr(?U T ) (11) where ?",
        "is the Lagrange multiplier for the nonnegative constraintU ?",
        "0.",
        "The partial derivative of L(U) w.r.t.",
        "U is ?",
        "U L(U) = ?2XVH T + 2UHV T VH T + 2?",
        "1 UU T U?",
        "2?",
        "1 U + 2?C u U?",
        "2?C u U 0 + 2?D u U?",
        "2?W u U+?",
        "1334 Using the Karush-Kuhn-Tucker (KKT) (Boyd and Vandenberghe, 2004) condition ?",
        "?U = 0, we can obtain ?",
        "U L(U) ?U = [ UHV T VH T + ?",
        "1 UU T U+ ?C u U+ ?D u U ] ?U ?",
        "[ XVH T + ?",
        "1 U+ ?C u U 0 + ?W u U ] ?U = 0 This leads to the update rule in equation (8).",
        "Following the similar derivations as shown above, we can obtain the updating rules for all the other variables H and V in GNMTF optimization, as shown in equations (9) and (10).",
        "3.3.1 Convergence Analysis In this subsection, we prove the convergence of multiplicative updates given by equations (8)?(10).",
        "We first introduce the definition of auxiliary function as follows.",
        "Definition 3.1.",
        "F(Y,Y ? )",
        "is an auxiliary function for L(Y) if L(Y) ?",
        "F(Y,Y ? )",
        "and equality holds if and only if L(Y) = F(Y,Y).",
        "Lemma 3.1.",
        "(Lee and Seung, 2001) If F is an auxiliary function for L, L is non-increasing under the updateY (t+1) = argmin Y F(Y,Y (t) ) Proof.",
        "By Definition 3.1, L(Y (t+1) ) ?",
        "F(Y (t+1) ,Y (t) ) ?",
        "F(Y (t) ,Y (t) ) = L(Y (t) ) Theorem 3.2.",
        "Let function F(U ij ,U (t) ij ) = L(U (t) ij ) + L ?",
        "(U (t) ij )(U ij ?U (t) ij ) + [ UHV T VH T + ?",
        "1 UU T U+ ?C u U+ ?D u U ] ij U ij ( U ij ?U (t) ij ) (12) be a proper auxiliary function for L(U ij ), where L ?",
        "(U ij ) = [?",
        "U L(U)] ij is the first-order derivatives of L(U ij ) with respect toU ij .",
        "Theorem 3.2 can be proved similarly to (Ding et al., 2006).",
        "Due to limited space, we omit the details of the validation.",
        "Based on Lemmas 3.1 and Theorem 3.2, the update rule for U can be obtained by minimizing F(U (t+1) ij ,U (t) ij ).",
        "When setting ?",
        "U (t+1) ij F(U (t+1) ij ,U (t) ij ), we can obtain U (t+1) ij = U (t) ij [ XVH T + ?",
        "1 U+ ?C u U 0 + ?W u U ] ij [ UHV T VH T + ?",
        "1 UU T U+ ?C u U+ ?D u U ] ij By Lemma 3.1 and Theorem 3.2, we have L(U (0) ) = F(U (0) ,U (0) ) ?",
        "F(U (1) ,U (0) ) ?",
        "F(U (1) ,U (1) ) = L(U (1) ) ?",
        "?",
        "?",
        "?",
        "?",
        "L(U (Iter) ), where Iter denotes the number of iteration number.",
        "Therefore, U is monotonically decreasing.",
        "Since the objective function L is lower bounded by 0, the correctness and convergence of Theorem 3.1 is validated.",
        "3.3.2 Time Complexity Analysis In this subsection, we discuss the time computational complexity of the proposed algorithm GNMTF.",
        "Besides expressing the complexity of the algorithm using big O notation, we also count the number of arithmetic operations to provide more details about running time.",
        "We show the results in Table 1, where m ?",
        "k and n ?",
        "k. Based on the updating rules summarized in Theorem 3.1, it it not hard to count the arithmetic operators of each iteration in GNMTF.",
        "It is important to note thatC u is a diagonal matrix, the nonzero elements on each row of C u is 1.",
        "Thus, we only need zero addition and mk multiplications to compute C u U. Simi-larly, forC u U 0 ,C v V,C v V 0 ,D u U andD v V, we also only need zero addition and mk multiplications for each of them.",
        "Besides, we also note thatW u is a sparse matrix, if we use a p-nearest neighbor graph, the average nonzero elements on each row of W u is p. Thus, we only need mpk additions and mpk multiplications to compute W u U.",
        "Similarly, for W v V, we need the same operation counts as W u U.",
        "Suppose the multiplicative updates stop after Iter iterations, the time cost of multiplicative updates then becomes O(Iter ?",
        "mnk).",
        "Therefore, the overall running time of GNMTF is similar to the standard NMTF and CNMTF.",
        "1335 addition multiplication division overall GNMTF:U 2k 3 + (2m+ n)k 2 +m(n+ p)k 2k 3 + (2m+ n)k 2 +m(n+ p+ 7)k mk O(mnk) GNMTF:H 2k 3 + (m+ n+ 2)k 2 +mnk 2k 3 + (m+ n+ 1)k 2 +mnk k 2 O(mnk) GNMTF:V 2k 3 + (2n+m)k 2 + n(m+ p)k 2k 3 + (2n+m)k 2 + n(m+ p+ 7)k nk O(mnk) Table 1: Computational operation counts for each iteration in GNMTF.",
        "4 Experiments 4.1 Data Sets Sentiment classification has been extensively studied in the literature.",
        "Among these, a large majority proposed experiments performed on the benchmarks made of Movies Reviews (Pang et al., 2002) and Amazon products (Blitzer et al., 2007).",
        "Movies data This data set has been widely used for sentiment analysis in the literature (Pang et al., 2002), which consists of 1000 positive and 1000 negative reviews drawn from the IMDB archive of rec.arts.movies.reviews.newsgroups.",
        "Amazon data This data set is heterogeneous, heavily unbalanced and large-scale, a smaller version has been released.",
        "The reduced data set contains 4 product types: Kitchen, Books, DVDs, and Electronics (Blitzer et al., 2007).",
        "There are 4000 positive and 4000 negative reviews.",
        "1 For these two data sets, we select 8000 words with highest document-frequency to generate the vocabulary.",
        "Stopwords 2 are removed and a normalized term-frequency representation is used.",
        "In order to construct the lexical prior knowledge matrixU 0 , we use the sentiment lexicon generated by (Hu and Liu, 2004).",
        "It contains 2,006 positive words (e.g., ?beautiful?)",
        "and 4,783 negative words (e.g., ?upset?).",
        "4.2 Unsupervised Sentiment Classification Our first experiment is to explore the benefits of incorporating the geometric information in the unsupervised paradigm (that is C v = 0).",
        "Therefore, the third part in equation (7) will be ignored.",
        "For this unsupervised paradigm of GNMTF, we empirically set ?",
        "= ?",
        "= ?",
        "= 1, ?",
        "1 = ?",
        "2 = 1, Iter = 100 and run GNMTF 10 repeated times to remove any randomness caused by the random initialization.",
        "Due to limited space, we do not present the impacts of the parameters on the learning model.",
        "Now we compare our proposed GNMTF with the following four categories of methods: (1) Lexicon-Based Methods (LBM in short): Taboada et al. (2011) proposed to incorporate intensification and negation to refine the sentiment score for each document.",
        "This is the state-of-the-art lexicon-based method for unsupervised sentiment classification.",
        "(2) Document Clustering Methods: We choose the most representative cluster methods, K-means, NMTF, Information-Theoretic Co-clustering (ITCC) (Dhillon et al., 2003), and Euclidean Co-clustering method (ECC) (Cho et al., 2004).",
        "We set the number of clusters as two in these methods.",
        "Note that all these methods do not make use of the sentiment lexicon.",
        "(3) Constrained NMTF (CNMTF in short): Li et al. (2009) incorporated the sentiment lexicon into NMTF as a domain-independent prior constraint.",
        "(4) Graph co-regularized Non-negative Matrix Tri-factorization (GNMTF in short): It is a new algorithm proposed in this paper.",
        "We use cosine similarity for constructing the p-nearest neighbor graph for its simplicity.",
        "The number of nearest neighbor p is set to 10 empirically both on document and word spaces.",
        "4.2.1 Sentiment Classification Results The experimental results are reported in Table 2.",
        "We perform a significant test, i.e., a t-test with a default significant level of 0.05.",
        "From Table 2, we can see that (1) Both CNMTF and GNMTF consider the lexical prior knowledge from off-the-shelf sentiment lexicon and achieve better performance than NMTF.",
        "This suggests the importance of the lexical prior knowledge in learning the sentiment classification (row 1 The data set can be freely downloaded from http://www.cs.jhu.edu/ mdredze/datasets/sentiment/.",
        "2 http://truereader.com/manuals/onix/stopwords1.html 1336 # Methods Movies Amazon 1 LBM 0.632 0.580 2 K-means 0.543 (-8.9%) 0.535 (-4.5%) 3 NMTF 0.561 (-7.1%) 0.547 (-3.3%) 4 ECC 0.678 (+4.6%) 0.642 (+6.2%) 5 ITCC 0.714 (+8.2%) 0.655 (+7.5%) 6 CNMTF 0.695 (+6.3%) 0.658 (+7.8%) 7 GNMTF 0.736 (+10.4%) 0.705 (+12.5%) Table 2: Sentiment classification accuracy of unsupervised paradigm on the data sets.",
        "Improvements of K-means, NMTF, ITCC, ECC, CNMTF and GNMTF over baseline LBM are shown in parentheses.",
        "0 20 40 60 80 1000.2 0.25 0.3 0.35 0.4 0.45 0.5 (a) Movies data Objec tive fu nction value 0 20 40 60 80 1000.46 0.48 0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 (b) Amazon data Objec tive fu nction value Figure 1: Convergence curves of GNMTF on both data sets.",
        "3 vs. row 6 and row 7); (2) Regardless of the data sets, our GNMTF significantly outperforms state-of-the-art CNMTF and achieves the best performance.",
        "This shows the superiority of geometric information and graph co-regularization framework (row 4 vs. row 5, the improvements are statistically significant at p < 0.05).",
        "4.2.2 Convergence Behavior In subsection 3.3.1, we have shown that the multiplicative updates given by equations (8)?",
        "(10) are convergent.",
        "Here, we empirically show the convergence behavior of GNMTF.",
        "Figure 1 shows the convergence curves of GNMTF on Movies and Amazon data sets.",
        "From the figure, y-axis is the value of objective function and x-axis denotes the iteration number.",
        "We can see that the multiplicative updates for GNMTF converge very fast, usually within 50 iterations.",
        "4.3 Semi-supervised Sentiment Classification In this subsection, we describe our proposed GNMTF with a few labeled documents.",
        "For this semi-supervised paradigm of GNMTF, we empirically set Iter = 100, ?",
        "1 = ?",
        "2 = 2, ?",
        "= ?",
        "= ?",
        "= ?",
        "= 1 and p = 10 on document and word spaces and also run 10 repeated times to remove any randomness caused by the random initialization.",
        "Due to limited space, we do not give an in-depth parameter analysis.",
        "For CNMTF, we set ?",
        "= ?",
        "= 1 for fair comparison.",
        "We also compare our proposed GNMTF with some representative semi-supervised approaches described in (Li et al., 2009): (1) Semi-supervised learning with local and global consistency (Consistency Method in short) (Zhou et al., 2004); (2) Semi-supervised learning using gaussian fields and harmonic functions (GFHF in short) (Zhu et al., 2003).",
        "Besides, we also compare the results of our proposed GNMTF with the representative supervised classification method: support vector machine (SVM), which has been widely used in sentiment classification (Pang et al., 2002).",
        "The results are presented in Figure 2.",
        "From the figure, we can see that GNMTF outperforms other methods over the entire range of number of labeled documents on both data sets.",
        "By this observation, we can conclude that taking the geometric information can still improve the sentiment classification accuracy in semi-supervised paradigm.",
        "1337 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.45 0.5 0.55 0.6 0.65 0.7 0.75 0.8 (a) Movies data Senti ment class ificati on ac curac y SVMConsistency MethodGFHFCNMTFGNMTF 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.50.55 0.6 0.65 0.7 0.75 0.8 0.85 (b) Amazon data Senti ment class ificati on ac curac y SVMConsistency MethodGFHFCNMTFGNMTF Figure 2: Sentiment classification accuracy vs. different percentage of labeled documents, where x-axis denotes the number of documents labeled as a fraction of the original labeled documents.",
        "5 Related Work Sentiment classification has gained widely interest in NLP community, we point the readers to recent books (Pang and Lee, 2008; Liu, 2012) for an in-depth survey of literature on sentiment analysis.",
        "Methods for automatically classifying sentiments expressed in products and movie reviews can roughly be divided into supervised and unsupervised (or semi-supervised) sentiment analysis.",
        "Supervised techniques have been proved promising and widely used in sentiment classification (Pang et al., 2002; Pang and Lee, 2008; Liu, 2012).",
        "However, the performance of these methods relies on manually labeled training data.",
        "In some cases, the labeling work may be time-consuming and expensive.",
        "This motivates the problem of learning robust sentiment classification via unsupervised (or semi-supervised) paradigm.",
        "The most representative way to perform semi-supervised paradigm is to employ partial labeled data to guide the sentiment classification (Goldberg and Zhu, 2006; Sindhwani and Melville, 2008; Wan, 2009; Li et al., 2011).",
        "However, we do not have any labeled data at hand in many situations, which makes the unsupervised paradigm possible.",
        "The most representative way to perform unsupervised paradigm is to use a sentiment lexicon to guide the sentiment classification (Turney, 2002; Taboada et al., 2011) or learn sentiment orientation via a matrix factorization clustering framework (Li et al., 2009; ?",
        "; Hu et al., 2013).",
        "In contrast, we perform sentiment classification with the different model formulation and learning algorithm, which considers both word-level and document-level sentiment-related contextual information (e.g., the neighboring words or documents tend to share the same sentiment polarity) into a unified framework.",
        "The proposed framework makes use of the valuable geometric information to compensate the problem of lack of labeled data for sentiment classification.",
        "In addition, some researchers also explored the matrix factorization techniques for other NLP tasks, such as relation extraction (Peng and Park, 2013) and question answering (Zhou et al., 2013) Besides, many studies address some other aspects of sentiment analysis, such as cross-domain sentiment classification (Blitzer et al., 2007; Pan et al., 2010; Hu et al., 2011; Bollegala et al., 2011; Glorot et al., 2011), cross-lingual sentiment classification (Wan, 2009; Lu et al., 2011b; Meng et al., 2012) and imbalanced sentiment classification (Li et al., 2011), which are out of scope of this paper.",
        "6 Conclusion and Future Work In this paper, we propose a novel algorithm, called graph co-regularized non-negative matrix tri-factorization (GNMTF), from a geometric perspective.",
        "GNMTF assumes that if two words (or docu-ments) are sufficiently close to each other, they tend to share the same sentiment polarity.",
        "To achieve this, we encode the geometric information by constructing the nearest neighbor graphs, in conjunction with a non-negative matrix tri-factorization framework.",
        "We derive an efficient algorithm for learning the factorization, analyze its complexity, and provide proof of convergence.",
        "Our empirical study on two open data sets validates that GNMTF can consistently improve the sentiment classification accuracy in comparison to state-of-the-art methods.",
        "1338 There are some ways in which this research could be continued.",
        "First, some other ways should be considered to construct the graphs (e.g., hyperlinks between documents, synonyms or co-occurrences between words).",
        "Second, we will try to extend the proposed framework for other aspects of sentiment analysis, such as cross-domain or cross-lingual settings.",
        "Acknowledgments This work was supported by the National Natural Science Foundation of China (No.",
        "61303180 and No.",
        "61272332), the Beijing Natural Science Foundation (No.",
        "4144087), CCF Opening Project of Chinese Information Processing, and also Sponsored by CCF-Tencent Open Research Fund.",
        "We thank the anonymous reviewers for their insightful comments.",
        "References"
      ]
    }
  ]
}
