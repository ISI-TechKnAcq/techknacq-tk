{
  "info": {
    "authors": [
      "David Martin Ward Powers"
    ],
    "book": "EACL",
    "id": "acl-E12-1035",
    "title": "The Problem with Kappa",
    "url": "https://aclweb.org/anthology/E12-1035",
    "year": 2012
  },
  "references": [
    "acl-J04-1005",
    "acl-J07-3002",
    "acl-J96-2004",
    "acl-W12-0706",
    "acl-W98-1226"
  ],
  "sections": [
    {
      "text": [
        "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 345?355, Avignon, France, April 23 - 27 2012. c?2012 Association for Computational Linguistics The Problem with Kappa David M W Powers Centre for Knowledge & Interaction Technology, CSEM Flinders University David.Powers@flinders.edu.au Abstract It is becoming clear that traditional evaluation measures used in Computational Linguistics (including Error Rates, Accuracy, Recall, Precision and F-measure) are of limited value for unbiased evaluation of systems, and are not meaningful for comparison of algorithms unless both the dataset and algorithm parameters are strictly controlled for skew (Prevalence and Bias).",
        "The use of techniques originally designed for other purposes, in particular Receiver Operating Characteristics Area Under Curve, plus variants of Kappa, have been proposed to fill the void.",
        "This paper aims to clear up some of the confusion relating to evaluation, by demonstrating that the usefulness of each evaluation method is highly dependent on the assumptions made about the distributions of the dataset and the underlying populations.",
        "The behaviour of a number of evaluation measures is compared under common assumptions.",
        "Deploying a system in a context which has the opposite skew from its validation set can be expected to approximately negate Fleiss Kappa and halve Cohen Kappa but leave Powers Kappa unchanged.",
        "For most performance evaluation purposes, the latter is thus most appropriate, whilst for comparison of behaviour, Matthews Correlation is recommended.",
        "Introduction Research in Computational Linguistics usually requires some form of quantitative evaluation.",
        "A number of traditional measures borrowed from Information Retrieval (Manning & Sch√ºtze, 1999) are in common use but there has been considerable critical evaluation of these measures themselves over the last decade or so (Entwisle & Powers, 1998, Flach, 2003, Ben-David.",
        "2008).",
        "Receiver Operating Analysis (ROC) has been advocated as an alternative by many, and in particular has been used by F?rnkranz and Flach (2005), Ben-David (2008) and Powers (2008) to better understand both learning algorithms relationship and the between the various measures, and the inherent biases that make many of them suspect.",
        "One of the key advantages of ROC is that it provides a clear indication of chance level performance as well as a less well known indication of the relative cost weighting of positive and negative cases for each possible system or parameterization represented.",
        "ROC Area Under the Curve (Fig.",
        "1) has been also used as a performance measure but averages over the false positive rate (Fallout) and is thus a function of cost that is dependent on the classifier rather than the application.",
        "For this reason it has come into considerable criticism and a number of variants and alternatives have been proposed (e.g. AUK, Kaymak et.",
        "Al, 2010 and H-measure, Hand, 2009).",
        "An AUC curve that is at least as good as a second curve at all points, is said to dominate it and indicates that the first classifier is equal or better than the second for all plotted values of the parameters, and all cost ratios.",
        "However AUC being greater for one classifier than another does not have such a property ?",
        "indeed deconvexities within or",
        "intersections of ROC curves are both prima facie evidence that fusion of the parameterized classifiers will be useful (cf. Provost and Facett, 2001; Flach and Wu, 2005).",
        "AUK stands for Area under Kappa, and represents a step in the advocacy of Kappa (Ben-David, 2008ab) as an alternative to the traditional measures and ROC AUC.",
        "Powers (2003,2007) has also proposed a Kappa-like measure (Informedness) and analysed it in terms of ROC, and there are many more, Warrens (2010) analyzing the relationships between some of the others.",
        "Systems like RapidMiner (2011) and Weka (Witten and Frank, 2005) provide almost all of the measures we have considered, and many more besides.",
        "This encourages the use of multiple measures, and indeed it is now becoming routine to display tables of multiple results for each system, and this is in particular true for the frameworks of some of the challenges and competitions brought to the communities (e.g. 2nd i2b2 Challenge in NLP for Clinical Data, 2011; 2nd Pascal Challenge on HTC, 2011)).",
        "This use of multiple statistics is no doubt in response to the criticism levelled at the evaluation mechanisms used in earlier generations of competitions and the above mentioned critiques, but the proliferation of alternate measures in some ways merely compounds the problem.",
        "Researchers have the temptation of choosing those that favour their system as they face the dilemma of what to do about competing (and often disagreeing) evaluation measures that they do not completely understand.",
        "These systems and competitions also exhibit another issue, the tendency to macro-averages over multiple classes, even of measures that are not denominated in class (e.g. that are proportions of predicted labels rather than real classes, as with Precision).",
        "This paper is directed at better understanding some of these new and old measures as well as providing recommendations as to which measures are appropriate in which circumstances.",
        "What's in a Kappa?",
        "In this paper we focus on the Kappa family of measures, as well as some closely related statistics named for other letters of the Greek alphabet, and some measures that we will show behave as Kappa measures although they were not originally defined as such.",
        "These include Informedness, Gini Coefficient and single point ROC AUC, which are in fact all equivalent to DeltaP?",
        "in the dichotomous case, which we deal with first, and to the other Kappas when the marginal prevalences (or biases) match.",
        "1.1 Two classes and non-negative Kappa.",
        "Kappa was originally proposed (Cohen, 1960) to compare human ratings in a binary, or dichotomous, classification task.",
        "Cohen (1960) recognized that Rand Accuracy did not take chance into account and therefore proposed to subtract off the chance level of Accuracy and then renormalize to the form of a probability: K(Acc) = [Acc ?",
        "E(Acc)] / [1 ?",
        "E(Acc)] (1) This leaves the question of how to estimate the expected Accuracy, E(Acc).",
        "Cohen (1960) made the assumption that raters would have different distributions that could be estimated as the products of the corresponding marginal coefficients of the contingency table: +ve Class ?ve Class +ve Prediction A=TP B=FP PP ?ve Prediction C=FN D=TN PN Notation RP RN N Table 1.",
        "Statistical and IR Contingency Notation In order to discuss this further it is important to discuss our notational conventions, and it is noted that in statistics, the letters AD (upper case or lower case) are conventionally used to label the cells, and their sums may be used to label the marginal cells.",
        "However in the literature on ROC analysis, which we follow here, it is usual to talk about true and false positives (that is positive predictions that are correct or incorrect), and conversely true and false negatives.",
        "Often upper case is used to indicate counts in the contingency table, which sum to the number of instances, N. In this case lower case letters are used to indicate probabilities, which means that the corresponding upper case values in the contingency table are all divided by N, and n=1.",
        "Statistics relative to (the total numbers of items in) the real classes are called Rates and have the number (or proportion) of Real Positives (RP) or Real Negatives (RN) in the denominator.",
        "In this notation, we have Recall = TPR = TP/RP.",
        "Conversely statistics relative to the (number of) predictions are called Accuracies, so relative to the predictions that label instances positively, Predicted Positives (PP), we have Precision = TPA = TP/PP.",
        "The accuracy of all our predictions, positive or negative, is given by Rand Accuracy = (TF+TN)/N = tf+tn, and this is what is meant in general by the unadorned term Accuracy, or the abbreviation Acc.",
        "Rand Accuracy is the weighted average of Precision and Inverse Precision (probability that negative predictions are correctly labeled), where the weighting is made according to the number of predictions made for the corresponding labels.",
        "Rand Accuracy is also the weighted average of Recall and Inverse Recall (probability that negative instances are correctly predicted), where the weighting is made according to the number of instances in the corresponding classes.",
        "The marginal probabilities rp and pp are also known as Prevalence (the class prevalence of positive instances) and Bias (the label bias to positive predictions), and the corresponding probabilities of negative classes and labels are the Inverse Prevalence and Inverse Bias respectively.",
        "In the ROC literature, the ratios of negative to positive classes is often referred to as the class ratio or skew.",
        "We can similarly also refer to a label ratio, prediction ratio or prediction skew.",
        "Note that optimal performance can only be achieved if class skew = label skew.",
        "The Expected True Positives and Expected True Negatives for Cohen Kappa, as well as Chi-squared significance, are estimated as the product of Bias and Prevalence, and the product of Inverse Bias and Inverse Prevalence, resp., where traditional uses of Kappa for agreement of human raters, the contingency table represents one rater as providing the classification to be predicted by the other rater.",
        "Cohen assumes that their distribution of ratings are independent, as reflected both by the margins and the contingencies: ETP = RP*PP; ETN = RN*NN.",
        "This gives us E(Acc) = (ETP+ETN)/N=etp+etn.",
        "By contrast the two rater two class form of Fleiss (1981) Kappa, also known as Scott Pi, assumes that both raters are labeling independently using the same distribution, and that the margins reflect this potential variation.",
        "The expected number of positives is thus effectively estimated as the average of the two raters?",
        "counts, so that EP = (RP+PP)/2, and EN = (RN+PN)/2, ETP = EP2 and ETN = EN2.",
        "1.2 Inverting Kappa The definition of Kappa in Eqn (1) can be seen to be applicable to arbitrary definitions of Expected Accuracy, and in order to discover how other measures relate to the family of Kappa measures it is useful to invert Kappa to discover the implicit definition of Expected Accuracy that allows a measure to be interpreted as a form of Kappa.",
        "We simply make E(Acc) the subject by multiplying out Eqn (1) to a common denominator and associating factors of E(Acc):",
        "prevalence and bias of each class/label).",
        "Our focus in this paper is the behaviour of the various Kappa measures as we move from strongly matched to strongly mismatched biases.",
        "Cohen (1968) also introduced a weighted variant of Kappa.",
        "We have also discussed cost weighting in the context of ROC, and Hand (2009) seeks to improve on ROC AUC by introducing a beta distribution as an estimated cost profile, but we will not discuss them further here as we are more interested in the effectiveness of the classifer overall rather than matching a particular cost profile, and are skeptical about any generic cost distribution.",
        "In particular the beta distribution gives priority to central tendency rather than boundary conditions, but boundary conditions are frequently encountered in optimization.",
        "Similarly Kaymak et als (2010) proposal to replace AUC by AUK corresponds to a Cohen Kappa reweighting of ROC that eliminates many of its useful properties, without any expectation that the measure, as an integration across a surrogate cost distribution, has any validity for system selection.",
        "Introducing alternative weights is also allowed in the definition of F-Measure, although in practice this is almost invariably employed as the equally weighted harmonic mean of Recall and Precision.",
        "Introducing additional weight or distribution parameters, just multiplies the confusion as to which measure to believe.",
        "Powers (2003) derived a further multiclass Kappa-like measure from first principles, dubbing it Informedness, based on an analogy of Bookmaker associating costs/payoffs based on the odds.",
        "This is then proven to measure the proportion of time (or probability) a decision is informed versus random, based on the same assumptions re expectation as Cohen Kappa, and we will thus call it Powers Kappa, and derive an formulation of the corresponding expectation.",
        "Powers (2007) further identifies that the dichotomous form of Powers Kappa is equivalent to the Gini cooefficient as a deskewed version of the weighted Relative Accuracy proposed by Flach (2003) based on his analysis and deskewing of common evaluation measures in the ROC paradigm.",
        "Powers (2007) also identifies that Dichotomous Informedness is equivalent to an empirically derived psychological measure called DeltaP?",
        "(Perruchet et al2004).",
        "DeltaP?",
        "(and its dual DeltaP) were derived based on analysis of human word association data ?",
        "the combination of this empirical observation with the place of DeltaP?",
        "as the dichotomous case of",
        "Powers?",
        "?Informedness?",
        "suggests that human association is in some sense optimal.",
        "Powers (2007) also introduces a dual of Informedness that he names Markedness, and shows that the geometric mean of Informedness and Markedness is Matthews Correlation, the nominal analog of Pearson Correlation.",
        "Powers?",
        "Informedness is in fact a variant of Kappa with some similarities to Cohen Kappa, but also some advantages over both Cohen and Fleiss Kappa due to its asymmetric relation with Recall, in the dichotomous form of Powers (2007), Informedness = Recall + InverseRecall ?",
        "1 = (Recall ?",
        "Bias) / (1 ?",
        "Prevalence).",
        "If we think of Kappa as assessing the relationship between two raters, Powers?",
        "statistic is not evenhanded and the Informedness and Markedness duals measure the two directions of prediction, normalizing Recall and Precision.",
        "In fact, the relationship with Correlation allows these to be interpreted as regression coefficients for the prediction function and its inverse.",
        "1.4 Kappa vs Correlation It is often asked why we don't just use Correlation to measure.",
        "In fact, Castellan (1996) uses Tetrachoric Correlation, another generalization of Pearson Correlation that assumes that the two class variables are given by underlying normal distributions.",
        "Uebersax (1987), Hutchison (1993) and Bonnet and Price (2005) each compare Kappa and Correlation and conclude that there does not seem to be any situation where Kappa would be preferable to Correlation.",
        "However all the Kappa and Correlation variants considered were symmetric, and it is thus interesting to consider the separate regression coefficients underlying it that represent the Powers Kappa duals of Informedness and Markedness, which have the advantage of separating out the influences of Prevalence and Bias (which then allows macro-averaging, which is not admissable for any symmetric form of Correlation or Kappa, as we will discuss shortly).",
        "Powers (2007) regards Matthews Correlation as an appropriate measure for symmetric situations (like rater agreement) and generalizes the relationships between Correlation and Significance to the Markedness and Informedness Measures.",
        "The differences between Informedness and Markedness, which relate to mismatches in Prevalence and Bias, mean that the pair of numbers provides further information about the nature of the relationship between the two classifications or raters, whilst the ability to take the geometric mean (of macro-averaged) Informedness and Markedness means that a single Correlation can be provided when appropriate.",
        "Our aim now is therefore to characterize Informedness (and hence as its dual Markedness) as a Kappa measure in relation to the families of Kappa measures represented by Cohen and Fleiss Kappa in the dichotomous case.",
        "Note that Warrens (2011) shows that a linearly weighted versions of Cohen's (1968) Kappa is in fact a weighted average of dichotomous Kappas.",
        "Similarly Powers (2003) shows that his Kappa (Informedness) has this property.",
        "Thus it is appropriate to consider the dichotomous case, and from this we can generalize as required.",
        "1.5 Kappa vs Determinant Warrens (2010c) discusses another commonly used measure, the Odds Ratio ad/bc (in Epidemiology rather than Computer Science or Computational Linguistics).",
        "Closely related to this is the Determinant of the Contingency Matrix dtp = ad-bc = etp-etn (in the Chi-Sqr, Cohen and Powers sense based on independent marginal probabilities).",
        "Both show whether the odds favour positives over negatives more for the first rater (real) than the second (predicted) ?",
        "for the ratio it is if it is greater than one, for the difference it is if it is greater than 0.",
        "Note that taking logs of all coefficients would maintain the same relationship and that the difference of the logs corresponds to the log of the ratio, mapping into the information domain.",
        "Warrens (2010c) further shows (in cost-weighted form) that Cohen Kappa is given by the following (in the notation of this paper, but preferring the notations Prevalence and Inverse Prevalence to rp and rn for clarity): KC = dtp/[(Prev*IBias+Bias*IPrev)/2].",
        "(5) Based on the previous characterization of Fleiss Kappa, we can further characterize it by KF = dtp/[(Prev+Bias)*(IBias+IPrev)/4].",
        "(6) Powers (2007) also showed corresponding formulations for Bookmaker Informedness (B, or Powers Kappa = KP), Markedness and Matthews Correlation: B = dtp/[(Prev*IPrev)].",
        "(7) M = dtp/[(Bias*IBias)].",
        "(8) C = dtp/[?(Prev*IPrev*Bias*IBias)].",
        "(9) These elegant dichotomous forms are straightforward, with the independence assumptions on Bias and Prevalence clear in",
        "Informed 1:1/1:1 4:1/4:1 4:1/1:4 Acc 50% 68% 32% 0% F 50% 80% 32% Acc 100% 100% 100% 100% F 100% 100% 100% Acc 57.5% 72.8% 42.2% 15% F 57.5% 83% 46.97% Acc 42.5% 57.8% 27.2% -15% F 42.5% 72% 27.2% Table 2.",
        "Accuracy and F-Measure for different mixes of prevalence and bias skew (odds ratio shown) as well as different proportions of correct (informed) answers versus guessing ?",
        "negative proportions imply that the informed decisions are deliberately made incorrectly (oracle tells me what to do and I do the opposite).",
        "From Table 2 we note that the first set of statistics notes the chance level varies from the 50% expected for Bias=Prevalence=50%.",
        "This is in fact the E(Acc) used in calculating Cohen Kappa.",
        "Where Prevalences and Biases are equal and balanced, all common statistics agree ?",
        "Recall = Precision = Accuracy = F, and they are interpretable with respect to this 50% chance level.",
        "All the Kappas will also agree, as the different averages of the identical prevalences and biases all come down to 50% as well.",
        "So subtracting 50% from 57.5% and normalizing (dividing) by the average effective prevalence of 50%, we return 15% informed decisions in all cases (as seen in detail in Table 3).",
        "However, F-measure gives an inflated estimate when it focus on the more prevalent positive class, with corresponding bias in the chance component.",
        "Worse still is the strength of the Acc and F scores under conditions of matched bias and prevalence when the deviation from chance is -15% - that is making the wrong decision 15% of the time and guessing the rest of the time.",
        "In academic terms, if we bump these rates up to ?25% F-factor gives a High Distinction for guessing 75% of the time and putting the right answer for the other 25%, a Distinction for 100% guessing, and a Credit for guessing 75% of the time and putting a wrong answer for the other 25%!",
        "In fact, the Powers Kappa corresponds to the methodology of multiple choice marking, where for questions with k+1 choices, a right answer gets 1 mark, and a wrong answer gets -1/k so that guessing achieves an expected mark of 0.",
        "Cohen Kappa achieves a very similar result for unbiased guessing strategies.",
        "We now turn to macro-averaging across multiple classifiers or raters.",
        "The Area Under the Curve measures are all of this form, whether we are talking about ROC, Kappa, Recall-Precision curves or whatever.",
        "The controversy over these averages, and macro-averaging in general, relates to one of two issues: 1.",
        "The averages are not in general over the appropriate units or denominators of the individual statistics; or 2.",
        "The averages are over a classifier determined cost function rather than an externally or standardly defined cost function.",
        "AUK and H-Measure seek to address these issues as discussed earlier.",
        "In fact they both boil down to averaging with an inappropriate distribution of weights.",
        "Commonly macro-averaging averages across classes as average statistics derived for each class weighted by the cardinality of the class (viz. prevalence).",
        "In our review above, we cited four examples, but we will refer only to WEKA (Witten et al. 2005) here as a commonly used system and associated text book that employs and advocates macro-averaging.",
        "WEKA averages over tpr, fpr, Recall (yes redundantly), Precision, F-Factor and ROC AUC.",
        "Only the average over tpr=Recall is actually meaningful, because only it has the number of members of the class, or its prevalence, as its denominator.",
        "Precision needs to be macro-averaged over the number of predictions for each class, in which case it is equivalent to micro-averaging.",
        "Other micro-averaged statistics are also shown, including Kappa (with the expectation determined from ZeroR ?",
        "predicting the majority class, leading to a Cohen-like Kappa).",
        "AUC will be pointwise for classifiers that don't provide any probabilistic information associated with label prediction, and thus don't allow varying a threshold for additional points on the ROC or other threshold curves.",
        "In the case where multiple threshold points are available, ROC AUC cannot be interpreted as having any relevance to any particular classifier, but is an average over a range of classifiers.",
        "Even then it is not so meaningful as AUCH, which should be used as classifiers on the convex hull are usually available.",
        "The AUCH measure will then dominate any individual classifiers, as if the convex hull is not the same as the single classifier it must include points that are above the classifier curve and thus its enclosed area totally includes the area that is enclosed by the individual classifier.",
        "Macroaveraging of the curve based on each class in turn as the Positive Class, and weighted",
        "by the size of the positive class, is not meaningful as effectively shown by Powers (2003) for the special case of the single point curve given its equivalence to Powers Kappa.",
        "In fact Markedness does admit averaging over classes, whilst Informedness requires averaging over predicted labels, as does Precision.",
        "The other Kappa and Correlations are more complex (note the demoninators in Eqns 5-9) and how they might be meaningfully macro-averaged is an open question.",
        "However, microaveraging can always be done quickly and easily by simply summing all the contingency tables (the true contingency tables are tables of counts, not probabilities, as shown in Table 1).",
        "Macroaveraging should never be done except for the special cases of Recall and Markedness when it is equivalent to micro-average, which is only slightly more expensive/complicated to do.",
        "Comparison of Kappas We now turn to explore the different definitions of Kappas, using the same approach employed with Accuracy and F-Factor in Table 1: We will consider 0%, 100%, 15% and -15% informed decisions, with random decisions modelled on the basis of independent Bias and Prevalence.",
        "This clearly biases against the Fleiss family of Kappas, which is entirely appropriate.",
        "As pointed out by Entwisle & Powers (1998) the practice of deliberately skewing bias to achieve better statistics is to be deprecated ?",
        "they used the real-life example of a CL researcher choosing to say water was always a noun because it was a noun more often than not.",
        "With Cohen or Powers?",
        "measures, any actual power of the system to determine PoS, however weak, would be reflected in an improvement in the scores versus any random choice, whatever the distribution.",
        "Recall that choosing one answer all the time corresponds to the extreme points of the chance line in the ROC curve.",
        "Studies like Fitzgibbon et al2007) and Leibbrandt and Powers (2012) show divergences amongst the conventional and debiased measures, but it is tricky to prove which is better.",
        "Kappa in the Limit It is however straightforward to derive limits for the various Kappas and Expectations under extreme and central conditions of bias and prevalence, including both match and mismatch.",
        "The 36 theoretical results match the mixture model results in Table 3, however, due to space constraints, formal treatment will be limited to two of the more complex cases that both relate to Fleiss Kappa with its mismatch to the marginal independence assumptions we prefer.",
        "These will provide informedness of probability B plus a remaining proportion 1-B of random responses exhibiting extreme bias versus both neutral and contrary prevalence.",
        "Note that we consider only |B|<1 as all Kappas give Acc=1 and thus K=1 for B=1, and only Powers Kappa is designed to work for B<1, giving K= 1 for B= -1.",
        "Recall that the general calculation of Expected Accuracy is E(Acc) = etp+etn (11) For Fleiss Kappa we must calculate the expected values of the correct contingencies as discussed previously with expected probabilities ep = (rp+pp)/2 & en = (rn+pn)/2 (12) etp = ep2 & etn = en2 (13) We first consider cases where prevalence is extreme and the chance component exhibits inverse bias.",
        "We thus consider limits as rp?0, rn?1, pp?1-B, pn?B.",
        "This gives us (assuming |B|<1) EF(Acc) = (1/4+B2/4+B/2)2+(1/4+B2/4-B/2)2 = (1+B2)/2 (14) KF(Acc) = (1-B)2/[B2-2] (15) We second consider cases where the prevalence is balanced and chance extreme, with rp?0.5, rn?0.5, pp?1-B, pn?B, giving EF(Acc) = 1/2 + (B-1/2)2/2 = 5/8 + B(B-1)/2 (16) KF(Acc)=[(B-1/2)-(B-1/2)2/2]/[1/2-(B-1/2)2/2] (17) =[B-5/8+B(B-1)/2]/[1-(5/8+B(B-1)/2) Conclusions The asymmetric Powers Informedness gives the clearest measure of the predictive value of a system, while the Matthews Correlation (as geometric mean with the Powers Markedness dual) is appropriate for comparing equally valid classifications or ratings into an agreed number of classes.",
        "Concordance measures should be used if number of classes is not agreed or specified.",
        "For mismatch cases (15) Fleiss is always negative for |B|<1) and thus fails to adequately reward good performance under these marginal conditions.",
        "For the chance case (17), the first form we provide shows that the deviation from matching Prevalence is a driver in a Kappa-like function.",
        "Cohen on the other hand (Table 3) tends to apply multiply the weight given to error in even mild prevalence-bias mismatch conditions.",
        "None of the symmetric Kappas designed for raters are suitable for classifiers.",
        "1:1 1:1 4:1 4:1 4:1 1:4 1:1 1:1 4:1 4:1 4:1 1:4 1:1 1:1 4:1 4:1 4:1 1:4 Informedness 0% 0% 0% 0% 0% 0% 0% 0% 0% Prevalence 50% 80% 80% 50% 80% 80% 50% 20% 20% Iprevalence 50% 20% 20% 50% 20% 20% 50% 80% 80% Bias 50% 80% 20% 50% 80% 20% 50% 20% 80% Ibias 50% 20% 80% 50% 20% 80% 50% 80% 20% SkewR 100% 25% 25% 100% 25% 25% 100% 400% 400% SkewP 100% 25% 400% 100% 25% 400% 100% 400% 25% OddsRatio 100% 100% 6% 100% 100% 6% 100% 100% 1600% ePowers 50% 68% 32% 50% 68% 32% 50% 68% 32% eCohen 50% 68% 32% 50% 68% 32% 50% 68% 32% eFleiss 50% 68% 50% 50% 68% 50% 50% 68% 50% kPowers 0% 0% 0% 0% 0% 0% 0% 0% 0% kCohen 0% 0% 0% 0% 0% 0% 0% 0% 0% kFleiss 0% 0% -36% 0% 0% -36% 0% 0% -36% Informedness 100% 100% 100% 100% 100% 100% 100% 100% 100% Prevalence 50% 80% 80% 50% 80% 80% 50% 20% 20% Iprevalence 50% 20% 20% 50% 20% 20% 50% 80% 80% Bias 50% 80% 80% 50% 80% 80% 50% 20% 20% Ibias 50% 20% 20% 50% 20% 20% 50% 80% 80% SkewR 100% 25% 25% 100% 25% 25% 100% 400% 400% SkewP 100% 25% 25% 100% 25% 25% 100% 400% 400% OddsRatio 100% 100% 100% 100% 100% 100% 100% 100% 100% ePowers 50% 68% 68% 50% 68% 68% 50% 68% 68% aCohen 50% 68% 68% 50% 68% 68% 50% 68% 68% aFleiss 50% 68% 68% 50% 68% 68% 50% 68% 68% kPowers 100% 100% 100% 100% 100% 100% 100% 100% 100% kCohen 100% 100% 100% 100% 100% 100% 100% 100% 100% kFleiss 100% 100% 100% 100% 100% 100% 100% 100% 100% Informedness 15% 15% 15% 99% 99% 99% 99% 99% 99% Prevalence 50% 80% 80% 50% 80% 80% 50% 20% 20% Iprevalence 50% 20% 20% 50% 20% 20% 50% 80% 80% Bias 50% 80% 29% 50% 80% 79% 50% 20% 79% Ibias 50% 20% 71% 50% 20% 21% 50% 80% 21% SkewR 100% 25% 25% 100% 25% 25% 100% 400% 400% SkewP 100% 25% 245% 100% 25% 26% 100% 400% 26% OddsRatio 100% 100% 6% 100% 100% 6% 100% 100% 1600% ePowers 50% 68% 32% 50% 68% 32% 50% 68% 32% eCohen 50% 68% 37% 50% 68% 68% 50% 68% 32% eFleiss 50% 68% 50% 50% 68% 68% 50% 68% 50% kPowers 15% 15% 15% 99% 99% 99% 1% 1% 1% kCohen 15% 15% 8% 99% 99% 98% 1% 1% 0% kFleiss 15% 15% -17% 99% 99% 98% 1% 1% -35% Informedness -15% -15% -15% -99% -99% -99% -99% -99% -99% Prevalence 50% 80% 20% 50% 80% 80% 50% 20% 20% Iprevalence 50% 20% 80% 50% 20% 20% 50% 80% 80% Bias 50% 71% 80% 50% 21% 20% 50% 21% 80% Ibias 50% 29% 20% 50% 79% 80% 50% 79% 20% SkewR 100% 25% 400% 100% 25% 25% 100% 400% 400% SkewP 100% 41% 25% 100% 385% 400% 100% 385% 25% OddsRatio 100% 65% 1038% 100% 25% 25% 100% 104% 1542% ePowers 50% 63% 37% 50% 50% 50% 50% 68% 32% eCohen 50% 63% 32% 50% 32% 32% 50% 68% 32% eFleiss 50% 63% 50% 50% 50% 50% 50% 68% 50% kPowers -15% -15% -15% -99% -99% -99% -1% -1% -1% kCohen -15% -13% -7% -99% -47% -47% -1% -1% 0% kFleiss -15% -14% -46% -99% -99% -99% -1% -1% -37% Table 3.",
        "Empirical Results for Accuracy and Kappa for Fleiss/Scott, Cohen and Powers.",
        "Shaded cells indicate misleading results, which occur for both Cohen and Fleiss Kappas."
      ]
    }
  ]
}
