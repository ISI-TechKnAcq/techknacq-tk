Before Submodular: 
56
acl-P08-2040 - {'url': 'https://aclweb.org/anthology/P08-2040', 'id': 'acl-P08-2040', 'abstract': ['Boxing Chen, Min Zhang, Aiti Aw and Haizhou Li', 'Institute for Infocomm Research 21 Heng Mui Keng Terrace, 119613, Singapore {bxchen, mzhang, aaiti, hli}@i2r.a-star.edu.sg', 'Word and n-gram posterior probabilities estimated on N-best hypotheses have been used to improve the performance of statistical machine translation (SMT) in a rescoring framework.', 'In this paper, we extend the idea to estimate the posterior probabilities on N-best hypotheses for translation phrase-pairs, target language n-grams, and source word re-orderings.', 'The SMT system is self-enhanced with the posterior knowledge learned from N-best hypotheses in a re-decoding framework.', 'Experiments on NIST Chinese-to-English task show performance improvements for all the strategies.', 'Moreover, the combination of the three strategies achieves further improvements and outperforms the baseline by 0.67 BLEU score on NIST-2003 set, and 0.64 on NIST-2005 set, respectively.'], 'year': 2008, 'title': 'Exploiting N-best Hypotheses for SMT Self-Enhancement', 'book': 'Annual Meeting of the Association for Computational Linguistics', 'score': 0.5261784134372253, 'type': 'unknown', 'authors': ['Boxing Chen', 'Min Zhang', 'Aiti Aw', 'Haizhou Li']}
acl-P00-1056 - {'url': 'https://aclweb.org/anthology/P00-1056', 'id': 'acl-P00-1056', 'abstract': ['In this paper, we present and compare various single-word based alignment models for statistical machine translation.', 'We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications.', 'We present different methods to combine alignments.', 'As evaluation criterion we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.', 'We show that models with a first-order dependence and a fertility model lead to significantly better results than the simple models IBM-1 or IBM-2, which are not able to go beyond zero-order dependencies.'], 'year': 2000, 'title': 'Improved Statistical Alignment Models', 'book': 'Annual Meeting of the Association for Computational Linguistics', 'score': 0.767028756136451, 'type': 'unknown', 'authors': ['Franz Josef Och', 'Hermann Ney']}
acl-P14-2095 - {'url': 'https://aclweb.org/anthology/P14-2095', 'id': 'acl-P14-2095', 'abstract': ['Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 579?585, Baltimore, Maryland, USA, June 23-25 2014. c?2014 Association for Computational Linguistics Cross-lingual Model Transfer Using Feature Representation Projection Mikhail Kozhevnikov MMCI, University of Saarland Saarbr?ucken, Germany mkozhevn@mmci.uni-saarland.de Ivan Titov ILLC, University of Amsterdam Amsterdam, Netherlands titov@uva.nl', 'Abstract', 'We propose a novel approach to cross-lingual model transfer based on feature representation projection.', 'First, a compact feature representation relevant for the task in question is constructed for either language independently and then the mapping between the two representations is determined using parallel data.', 'The target instance can then be mapped into the source-side feature representation using the derived mapping and handled directly by the source-side model.', 'This approach displays competitive performance on model transfer for semantic role labeling when compared to direct model transfer and annotation projection and suggests interesting directions for further research.'], 'year': 2014, 'title': 'Cross-lingual Model Transfer Using Feature Representation Projection', 'book': 'ACL', 'score': 0.44787132327976686, 'type': 'unknown', 'authors': ['Mikhail Kozhevnikov', 'Ivan Titov']}
acl-W09-2413 - {'url': 'https://aclweb.org/anthology/W09-2413', 'id': 'acl-W09-2413', 'abstract': ['Els Lefever1,2 and Veronique Hoste', 'LT3, Language and Translation Technology Team, University College Ghent', 'Groot-Brittannielaan 45, 9000 Gent, Belgium Department of Applied Mathematics and Computer Science, Ghent University Krijgslaan 281 (S9), 9000 Gent, Belgium', '(Els.Lefever, Veronique.Hoste}@hogent.be', 'We propose a multilingual unsupervised Word Sense Disambiguation (WSD) task for a sample of English nouns.', 'Instead of providing manually sense-tagged examples for each sense of a polysemous noun, our sense inventory is built up on the basis of the Europarl parallel corpus.', 'The multilingual setup involves the translations of a given English polyse-mous noun in five supported languages, viz. Dutch, French, German, Spanish and Italian.', 'The task targets the following goals: (a) the manual creation of a multilingual sense inventory for a lexical sample of English nouns and (b) the evaluation of systems on their ability to disambiguate new occurrences of the selected polysemous nouns.', 'For the creation of the hand-tagged gold standard, all translations of a given polysemous English noun are retrieved in the five languages and clustered by meaning.', 'Systems can participate in 5 bilingual evaluation subtasks (English - Dutch, English - German, etc.)'], 'year': 2009, 'title': 'SemEval-2010 Task 3: Cross-lingual Word Sense Disambiguation', 'book': 'Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009)', 'score': 0.4206817456456919, 'type': 'unknown', 'authors': ['Els Lefever', 'Véronique Hoste']}
acl-P08-2038 - {'url': 'https://aclweb.org/anthology/P08-2038', 'id': 'acl-P08-2038', 'abstract': ['In this paper, we propose a linguistically annotated reordering model for BTG-based statistical machine translation.', 'The model incorporates linguistic knowledge to predict orders for both syntactic and non-syntactic phrases.', 'The linguistic knowledge is automatically learned from source-side parse trees through an annotation algorithm.', 'We empirically demonstrate that the proposed model leads to a significant improvement of 1.55% in the BLEU score over the baseline reordering model on the NIST MT-05 Chinese-to-English translation task.'], 'year': 2008, 'title': 'A Linguistically Annotated Reordering Model for BTG-based Statistical Machine Translation', 'book': 'Annual Meeting of the Association for Computational Linguistics', 'score': 0.6368380145476898, 'type': 'unknown', 'authors': ['Deyi Xiong', 'Min Zhang', 'Aiti Aw', 'Haizhou Li']}
acl-J10-3009 - {'url': 'https://aclweb.org/anthology/J10-3009', 'id': 'acl-J10-3009', 'abstract': ['Linguistic knowledge plays an important role in phrase movement in statistical machine translation.', 'To efficiently incorporate linguistic knowledge into phrase reordering, we propose a new approach: Linguistically Annotated Reordering (LAR).', 'In LAR, we build hard hierarchical skeletons and inject soft linguistic knowledge from source parse trees to nodes ofhard skeletons during translation.', 'The experimental results on large-scale training data show that LAR is comparable to boundary word-based reordering (BWR) (Xiong, Liu, and Lin 2006), which is a very competitive lexicalized reordering approach.', 'When combined with BWR, LAR provides complementary information for phrase reordering, which collectively improves the BLEU score significantly.', 'To further understand the contribution oflinguistic knowledge in LAR to phrase reordering, we introduce a syntax-based analysis method to automatically detect constituent movement in both reference and system translations, and summarize syntactic reordering patterns that are captured by reordering models.', 'With the proposed analysis method, we conduct a comparative analysis that not only provides the insight into how linguistic knowledge affects phrase movement but also reveals new challenges in phrase reordering.'], 'year': 2010, 'title': 'Linguistically Annotated Reordering: Evaluation and Analysis', 'book': 'Computational Linguistics', 'score': 0.5665052485545983, 'type': 'unknown', 'authors': ['Deyi Xiong', 'Min Zhang', 'Aiti Aw', 'Haizhou Li']}
acl-C94-2175 - {'url': 'https://aclweb.org/anthology/C94-2175', 'id': 'acl-C94-2175', 'abstract': ['This paper describes a unified framework for bilingual text matching by combining existing handwritten bilingual dictionaries and statistical techniques.', 'The process of bilingual text matching consists of two major steps: sentence alignment and structural matching of bilingual sentences.', 'Statistical techniques are applied to estimate word correspondences not included in bilingual dictionaries.', 'Estimated word correspondences are useful for improving both sentence alignment and structural matching.'], 'year': 1994, 'title': 'Bilingual Text, Matching Using Bilingual Dictionary and Statistics', 'book': 'International Conference on Computational Linguistics', 'score': 0.5927511131437411, 'type': 'unknown', 'authors': ['Takehito Utsuro', 'Hiroshi Ikeda', 'Masaya Yamane', 'Yuji Matsumoto', 'Makoto Nagao']}
acl-W04-1121 - {'url': 'https://aclweb.org/anthology/W04-1121', 'id': 'acl-W04-1121', 'abstract': ['Large amounts of bilingual resource on the Internet provide us with the probability of building a large scale of bilingual corpus.', 'The irregular characteristics of the real texts, especially without the strictly aligned paragraph boundaries, bring a challenge to alignment technology.', 'The traditional alignment methods have some difficulties in competency for doing this.', 'This paper describes a new method for aligning real bilingual texts using sentence pair location information.', 'The model was motivated by the observation that the location of a sentence pair with certain length is distributed in the whole text similarly.', 'It uses (1:1) sentence beads instead of high frequency words as the candidate anchors.', 'The method was developed and evaluated through many different test data.', 'The results show that it can achieve good aligned performance and be robust and language independent.', 'It can resolve the alignment problem on real bilingual text.'], 'year': 2004, 'title': 'Aligning Bilingual Corpora Using Sentences Location Information', 'book': 'Workshop on Automatic Alignment and Extraction of Bilingual Domain Ontology for Medical Domain Web Search', 'score': 0.5317585015077735, 'type': 'unknown', 'authors': ['Weigang Li', 'Ting Liu', 'Zhen Wang', 'Sheng Li']}
acl-J13-3001 - {'url': 'https://aclweb.org/anthology/J13-3001', 'id': 'acl-J13-3001', 'abstract': ['Paraphrases are sentences or phrases that convey the same meaning using different wording.', 'Although the logical definition of paraphrases requires strict semantic equivalence, linguistics accepts a broader, approximate, equivalence?thereby allowing far more examples of ?quasi-paraphrase.?', 'But approximate equivalence is hard to define.', 'Thus, the phenomenon of paraphrases, as understood in linguistics, is difficult to characterize.', 'In this article, we list a set of 25 operations that generate quasi-paraphrases.', 'We then empirically validate the scope and accuracy of this list by manually analyzing random samples of two publicly available paraphrase corpora.', 'We provide the distribution of naturally occurring quasi-paraphrases in English text.'], 'year': 2013, 'title': 'Squibs: What is a Paraphrase?', 'book': 'Computational Linguistics', 'score': 0.6501073523435825, 'type': 'unknown', 'authors': ['Rahul Bhagat', 'Eduard Hovy']}
acl-C10-4001 - {'url': 'https://aclweb.org/anthology/C10-4001', 'id': 'acl-C10-4001', 'abstract': ['Shiqi Zhao Haifeng Wang', 'Baidu, Inc. Baidu, Inc.'], 'year': 2010, 'title': 'Paraphrases and Applications', 'book': 'COLING – Tutorial Notes', 'score': 0.5877299308358976, 'type': 'unknown', 'authors': ['Shiqi Zhao', 'Haifeng Wang']}
acl-W11-2103 - {'url': 'https://aclweb.org/anthology/W11-2103', 'id': 'acl-W11-2103', 'abstract': ['Chris Callison-Burch Philipp Koehn', 'Center for Language and Speech Processing School of Informatics', 'Johns Hopkins University University of Edinburgh', 'Informatics Institute University of Amsterdam', 'Center for Language and Speech Processing Johns Hopkins University', 'This paper presents the results of the WMT11 shared tasks, which included a translation task, a system combination task, and a task for machine translation evaluation metrics.', 'We conducted a large-scale manual evaluation of 148 machine translation systems and 41 system combination entries.', 'We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 21 evaluation metrics.', 'This year featured a Haitian Creole to English task translating SMS messages sent to an emergency response service in the aftermath of the Haitian earthquake.', "We also conducted a pilot 'tunable metrics' task to test whether optimizing a fixed system to different metrics would result in perceptibly different translation quality."], 'year': 2011, 'title': 'Findings of the 2011 Workshop on Statistical Machine Translation', 'book': 'Proceedings of the Sixth Workshop on Statistical Machine Translation', 'score': 0.7275698227756415, 'type': 'unknown', 'authors': ['Chris Callison-Burch', 'Philipp Koehn', 'Christof Monz', 'Omar F. Zaidan']}
acl-W10-1703 - {'url': 'https://aclweb.org/anthology/W10-1703', 'id': 'acl-W10-1703', 'abstract': ['Chris Callison-Burch Philipp Koehn Christof Monz', 'Johns Hopkins University University of Edinburgh University of Amsterdam ccb@cs.jhu.edu pkoehn@inf.ed.ac.uk c.monz@uva.nl', 'Kay Peterson and Mark Przybocki Omar F. Zaidan', 'This paper presents the results of the WMT10 and MetricsMATR10 shared tasks, which included a translation task, a system combination task, and an evaluation task.', 'We conducted a large-scale manual evaluation of 104 machine translation systems and 41 system combination entries.', 'We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 26 metrics.', "This year we also investigated increasing the number of human judgments by hiring non-expert annotators through Amazon's Mechanical Turk."], 'year': 2010, 'title': 'Findings of the 2010 Joint Workshop on Statistical Machine Translation and Metrics for Machine Translation', 'book': 'Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR', 'score': 0.6678447745451398, 'type': 'unknown', 'authors': ['Chris Callison-Burch', 'Philipp Koehn', 'Christof Monz', 'Kay Peterson', 'Mark A. Przybocki', 'Omar F. Zaidan']}
acl-P12-2007 - {'url': 'https://aclweb.org/anthology/P12-2007', 'id': 'acl-P12-2007', 'abstract': ["This paper presents an extension of Chiang's hierarchical phrase-based (HPB) model, called Head-Driven HPB (HD-HPB), which incorporates head information in translation rules to better capture syntax-driven information, as well as improved reordering between any two neighboring non-terminals at any stage of a derivation to explore a larger reordering search space.", "Experiments on Chinese-English translation on four NIST MT test sets show that the HD-HPB model significantly outperforms Chiang's model with average gains of 1.91 points absolute in BLEU."], 'year': 2012, 'title': 'Head-Driven Hierarchical Phrase-based Translation', 'book': 'ACL', 'score': 0.7662520728002502, 'type': 'unknown', 'authors': ['Junhui Li', 'Zhaopeng Tu', 'Guodong Zhou', 'Josef van Genabith']}
acl-P09-1063 - {'url': 'https://aclweb.org/anthology/P09-1063', 'id': 'acl-P09-1063', 'abstract': ['Yang Liu and Yajuan Lii and Qun Liu', 'Key Laboratory of Intelligent Information Processing Institute of Computing Technology Chinese Academy of Sciences P.O.', 'Box 2704, Beijing 100190, China (yliu,lvyajuan,liuqun}@ict.ac.cn', 'Current tree-to-tree models suffer from parsing errors as they usually use only 1-best parses for rule extraction and decoding.', 'We instead propose a forest-based tree-to-tree model that uses packed forests.', 'The model is based on a probabilistic synchronous tree substitution grammar (STSG), which can be learned from aligned forest pairs automatically.', 'The decoder finds ways of decomposing trees in the source forest into elementary trees using the source projection of STSG while building target forest in parallel.', 'Comparable to the state-of-the-art phrase-based system Moses, using packed forests in tree-to-tree translation results in a significant absolute improvement of 3.6 BLEU points over using 1-best trees.'], 'year': 2009, 'title': 'Improving Tree-to-Tree Translation with Packed Forests', 'book': 'ACL-IJCNLP', 'score': 0.7290738296043581, 'type': 'unknown', 'authors': ['Yang Liu', 'Yajuan Lü', 'Qun Liu']}
acl-C04-1032 - {'url': 'https://aclweb.org/anthology/C04-1032', 'id': 'acl-C04-1032', 'abstract': ['In this paper, we address the word alignment problem for statistical machine translation.', 'We aim at creating a symmetric word alignment allowing for reliable one-to-many and many-to-one word relationships.', 'We perform the iterative alignment training in the source-to-target and the target-to-source direction with the well-known IBM and HMM alignment models.', 'Using these models, we robustly estimate the local costs of aligning a source word and a target word in each sentence pair.', 'Then, we use efficient graph algorithms to determine the symmetric alignment with minimal total costs (i. e. maximal alignment probability).', 'We evaluate the automatic alignments created in this way on the German–English Verb-mobil task and the French–English Canadian Hansards task.', 'We show statistically significant improvements of the alignment quality compared to the best results reported so far.', 'On the Verbmobil task, we achieve an improvement of more than 1% absolute over the baseline error rate of 4.7%.'], 'year': 2004, 'title': 'Symmetric Word Alignments for Statistical Machine Translation', 'book': 'International Conference on Computational Linguistics', 'score': 0.720495037846413, 'type': 'unknown', 'authors': ['Evgeny Matusov', 'Richard Zens', 'Hermann Ney']}
acl-C04-1045 - {'url': 'https://aclweb.org/anthology/C04-1045', 'id': 'acl-C04-1045', 'abstract': ['In this paper, we present an approach to include morpho-syntactic dependencies into the training of the statistical alignment models.', 'Existing statistical translation systems usually treat different derivations of the same base form as they were independent of each other.', 'We propose a method which explicitly takes into account such interdependencies during the EM training of the statistical alignment models.', 'The evaluation is done by comparing the obtained Viterbi alignments with a manually annotated reference alignment.', 'The improvements of the alignment quality compared to the, to our knowledge, best system are reported on the German-English Verbmobil corpus.'], 'year': 2004, 'title': 'Improving Word Alignment Quality Using Morpho-Syntactic Information', 'book': 'International Conference on Computational Linguistics', 'score': 0.6975033529905635, 'type': 'unknown', 'authors': ['Hermann Ney', 'Maja Popović']}
acl-W12-3147 - {'url': 'https://aclweb.org/anthology/W12-3147', 'id': 'acl-W12-3147', 'abstract': ['This paper describes the development of French?English and English?French statistical machine translation systems for the 2012 WMT shared task evaluation.', 'We developed phrase-based systems based on the Moses decoder, trained on the provided data only.', 'Additionally, new features this year included improved language and translation model adaptation using the cross-entropy score for the corpus selection.'], 'year': 2012, 'title': 'LIUM’s SMT Machine Translation Systems for WMT 2012', 'book': 'Proceedings of the Seventh Workshop on Statistical Machine Translation', 'score': 0.62039405477476, 'type': 'unknown', 'authors': ['Christophe Servan', 'Patrik Lambert', 'Anthony Rousseau', 'Holger Schwenk', 'Loïc Barrault']}
acl-I08-1042 - {'url': 'https://aclweb.org/anthology/I08-1042', 'id': 'acl-I08-1042', 'abstract': ['Jesus Gimenez and Lluis Märquez', 'Combining different metrics into a single measure of quality seems the most direct and natural way to improve over the quality of individual metrics.', 'Recently, several approaches have been suggested (Kulesza and Shieber, 2004; Liu and Gildea, 2007; Albrecht and Hwa, 2007a).', 'Although based on different assumptions, these approaches share the common characteristic of being parametric.', 'Their models involve a number of parameters whose weight must be adjusted.', 'As an alternative, in this work, we study the behaviour of non-parametric schemes, in which metrics are combined without having to adjust their relative importance.', 'Besides, rather than limiting to the lexical dimension, we work on a wide set of metrics operating at different linguistic levels (e.g., lexical, syntactic and semantic).', 'Experimental results show that non-parametric methods are a valid means of putting different quality dimensions together, thus tracing a possible path towards heterogeneous automatic MT evaluation.'], 'year': 2008, 'title': 'Heterogeneous Automatic MT Evaluation Through Non-Parametric Metric Combinations', 'book': 'Proceedings of the Third International Joint Conference on Natural Language Processing', 'score': 0.7490549476435829, 'type': 'unknown', 'authors': ['Jesús Giménez', 'Lluís Màrquez']}
acl-W12-3107 - {'url': 'https://aclweb.org/anthology/W12-3107', 'id': 'acl-W12-3107', 'abstract': ["This paper describes Stanford University's submission to the Shared Evaluation Task of WMT 2012.", 'Our proposed metric (SPEDE) computes probabilistic edit distance as predictions of translation quality.', 'We learn weighted edit distance in a probabilistic finite state machine (pFSM) model, where state transitions correspond to edit operations.', 'While standard edit distance models cannot capture long-distance word swapping or cross alignments, we rectify these shortcomings using a novel pushdown automaton extension of the pFSM model.', 'Our models are trained in a regression framework, and can easily incorporate a rich set of linguistic features.', 'Evaluated on two different prediction tasks across a diverse set of datasets, our methods achieve state-of-the-art correlation with human judgments.'], 'year': 2012, 'title': 'SPEDE: Probabilistic Edit Distance Metrics for MT Evaluation', 'book': 'Proceedings of the Seventh Workshop on Statistical Machine Translation', 'score': 0.7397981177652936, 'type': 'unknown', 'authors': ['Mengqiu Wang', 'Christopher Manning']}
acl-W11-2158 - {'url': 'https://aclweb.org/anthology/W11-2158', 'id': 'acl-W11-2158', 'abstract': ["LIUM's SMT Machine Translation Systems for WMT 2011", 'Holger Schwenk, Patrik Lambert, Loïc Barrault, Christophe Servan, Haithem Afli, Sadaf Abdul-Rauf and Kashif Shah', 'LIUM, University of Le Mans 72085 Le Mans cedex 9, FRANCE FirstName.LastName@lium.univ-lemans.fr', 'Abstract 2 Resources Used', 'This paper describes the development of French-English and English-French statistical machine translation systems for the 2011 WMT shared task evaluation.', 'Our main systems were standard phrase-based statistical systems based on the Moses decoder, trained on the provided data only, but we also performed initial experiments with hierarchical systems.', 'Additional, new features this year include improved translation model adaptation using monolingual data, a continuous space language model and the treatment of unknown words.'], 'year': 2011, 'title': 'LIUM’s SMT Machine Translation Systems for WMT 2011', 'book': 'Proceedings of the Sixth Workshop on Statistical Machine Translation', 'score': 0.6155744999697585, 'type': 'unknown', 'authors': ['Holger Schwenk', 'Patrik Lambert', 'Loïc Barrault', 'Christophe Servan', 'Sadaf Abdul-Rauf', 'Haithem Afli', 'Kashif Shah']}
acl-W10-1716 - {'url': 'https://aclweb.org/anthology/W10-1716', 'id': 'acl-W10-1716', 'abstract': ['Patrik Lambert, Sadaf Abdul-Rauf and Holger Schwenk', 'LIUM, University of Le Mans 72085 Le Mans cedex 9, FRANCE FirstName.LastName@lium.univ-lemans.fr', 'This paper describes the development of French-English and English-French machine translation systems for the 2010 WMT shared task evaluation.', 'These systems were standard phrase-based statistical systems based on the Moses decoder, trained on the provided data only.', 'Most of our efforts were devoted to the choice and extraction of bilingual data used for training.', 'We filtered out some bilingual corpora and pruned the phrase table.', 'We also investigated the impact of adding two types of additional bilingual texts, extracted automatically from the available monolingual data.', 'We first collected bilingual data by performing automatic translations of monolingual texts.', 'The second type of bilingual text was harvested from comparable corpora with Information Retrieval techniques.'], 'year': 2010, 'title': 'LIUM SMT Machine Translation System for WMT 2010', 'book': 'Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and MetricsMATR', 'score': 0.6131327846717648, 'type': 'unknown', 'authors': ['Patrik Lambert', 'Sadaf Abdul-Rauf', 'Holger Schwenk']}
acl-H93-1001 - {'url': 'https://aclweb.org/anthology/H93-1001', 'id': 'acl-H93-1001', 'abstract': ['For five years, 1988-1992, the Defense Advanced Projects Agency sponsored a series of meetings called the DARPA Speech and Natural Language Workshops.', 'These workshops provided a forum where researchers in speech and natural language, particularly as relating to the DARPA programs in spoken and written language understanding, could exchange information about recent research and technical progress.', 'Participants included researchers funded under the DARPA programs, other researchers who voluntarily participated in these programs or in related evaluations, government researchers and consumers of these research results, and invited attendees from inside and outside the US.', 'Proceedings of these workshops were published by Morgan Kaufmann.'], 'year': 1993, 'title': 'Overview of the ARPA Human Language Technology Workshop', 'book': 'Human Language Technology Conference', 'score': 0.6896844984773962, 'type': 'unknown', 'authors': ['Madeleine Bates']}
acl-W00-1429 - {'url': 'https://aclweb.org/anthology/W00-1429', 'id': 'acl-W00-1429', 'abstract': ['We describe the knowledge acquisition (KA) techniques used to build the STOP system, especially sorting and think-aloud protocols.', 'That is, we describe the ways in which we interacted with domain experts to determine appropriate user categories, schemas, detailed content rules, and so forth for STOP.', 'Informal evaluations of these techniques suggest that they had some benefit, but perhaps were most successful as a source of insight and hypotheses, and should ideally have been supplemented by other techniques when deciding on the specific rules and knowledge incorporated into STOP.'], 'year': 2000, 'title': 'Knowledge Acquisition for Natural Language Generation', 'book': 'International Conference on Natural Language Generation', 'score': 0.5257659543297751, 'type': 'unknown', 'authors': ['Ehud Reiter', 'Roma Robertson', 'Liesl Osman']}
acl-P01-1057 - {'url': 'https://aclweb.org/anthology/P01-1057', 'id': 'acl-P01-1057', 'abstract': ['The STOP system, which generates personalised smoking-cessation letters, was evaluated by a randomised controlled clinical trial.', 'We believe this is the largest and perhaps most rigorous task effectiveness evaluation ever performed on an NLG system.', 'The detailed results of the clinical trial have been presented elsewhere, in the medical literature.', 'In this paper we discuss the clinical trial itself: its structure and cost, what we did and did not learn from it (especially considering that the trial showed that STOP was not effective), and how it compares to other NLG evaluation techniques.'], 'year': 2001, 'title': 'Using a Randomised Controlled Clinical Trial to Evaluate an NLG System', 'book': 'Annual Meeting of the Association for Computational Linguistics', 'score': 0.519462531022038, 'type': 'unknown', 'authors': ['Ehud Reiter', 'Roma Robertson', 'A. Scott Lennox', 'Liesl Osman']}
acl-C08-2019 - {'url': 'https://aclweb.org/anthology/C08-2019', 'id': 'acl-C08-2019', 'abstract': ['We report on work in progress on using very simple statistics in an unsupervised fashion to re-rank search engine results when review-oriented queries are issued; the goal is to bring opinionated or subjective results to the top of the results list.', 'We find that our proposed technique performs comparably to methods that rely on sophisticated pre-encoded linguistic knowledge, and that both substantially improve the initial results produced by the Yahoo!', 'search engine.'], 'year': 2008, 'title': 'Using Very Simple Statistics for Review Search: An Exploration', 'book': 'COLING – Posters', 'score': 0.1716554896314715, 'type': 'unknown', 'authors': ['Bo Pang', 'Lillian Lee']}
acl-P12-2018 - {'url': 'https://aclweb.org/anthology/P12-2018', 'id': 'acl-P12-2018', 'abstract': ['Variants of Naive Bayes (NB) and Support Vector Machines (SVM) are often used as baseline methods for text classification, but their performance varies greatly depending on the model variant, features used and task/ dataset.', 'We show that: (i) the inclusion of word bigram features gives consistent gains on sentiment analysis tasks; (ii) for short snippet sentiment tasks, NB actually does better than SVMs (while for longer documents the opposite result holds); (iii) a simple but novel SVM variant using NB log-count ratios as feature values consistently performs well across tasks and datasets.', 'Based on these observations, we identify simple NB and SVM variants which outperform most published results on sentiment analysis datasets, sometimes providing a new state-of-the-art performance level.'], 'year': 2012, 'title': 'Baselines and Bigrams: Simple, Good Sentiment and Topic Classification', 'book': 'ACL', 'score': 0.1673894466436222, 'type': 'unknown', 'authors': ['Sida Wang', 'Christopher Manning']}
acl-W12-3212 - {'url': 'https://aclweb.org/anthology/W12-3212', 'id': 'acl-W12-3212', 'abstract': ['We describe how paperXML, a logical document structure markup for scholarly articles, is generated on the basis of OCR tool outputs.', 'PaperXML has been initially developed for the ACL Anthology Searchbench.', 'The main purpose was to robustly provide uniform access to sentences in ACL Anthology papers from the past 46 years, ranging from scanned, typewriter-written conference and workshop proceedings papers, up to recent high-quality typeset, born-digital journal articles, with varying layouts.', 'PaperXML markup includes information on page and paragraph breaks, section headings, footnotes, tables, captions, boldface and italics character styles as well as bibliographic and publication meta-data.', 'The role of paperXML in the ACL Contributed Task Rediscovering 50 Years of Discoveries is to serve as fall-back source (1) for older, scanned papers (mostly published before the year 2000), for which born-digital PDF sources are not available, (2) for born-digital PDF papers on which the PDFExtract method failed, (3) for document parts where PDFExtract does not output useful markup such as currently for tables.', "We sketch transformation of paperXML into the ACL Contributed Task's TEI P5 XML."], 'year': 2012, 'title': 'Combining OCR Outputs for Logical Document Structure Markup. Technical Background to the ACL 2012 Contributed Task', 'book': 'Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries', 'score': 0.6676956381545492, 'type': 'unknown', 'authors': ['Ulrich Schäfer', 'Benjamin Weitz']}
acl-W06-1613 - {'url': 'https://aclweb.org/anthology/W06-1613', 'id': 'acl-W06-1613', 'abstract': ['Citation function is defined as the author’s reason for citing a given paper (e.g. acknowledgement of the use of the cited method).', 'The automatic recognition of the rhetorical function of citations in scientific text has many applications, from improvement of impact factor calculations to text summarisation and more informative citation indexers.', 'We show that our annotation scheme for citation function is reliable, and present a supervised machine learning framework to automatically classify citation function, using both shallow and linguistically-inspired features.', 'We find, amongst other things, a strong relationship between citation function and sentiment classification.'], 'year': 2006, 'title': 'Automatic Classification of Citation Function', 'book': 'Conference on Empirical Methods in Natural Language Processing', 'score': 0.6095497865399048, 'type': 'unknown', 'authors': ['Simone Teufel', 'Advaith Siddharthan', 'Dan Tidhar']}
acl-H93-1047 - {'url': 'https://aclweb.org/anthology/H93-1047', 'id': 'acl-H93-1047', 'abstract': ['In this paper we describe a new technique for parsing free text: a transformational grammar/ is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled.', 'The algorithm works by beginning in a very naive state of knowledge about phrase structure.', 'By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error.', 'After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.'], 'year': 1993, 'title': 'Automatic Grammar Induction and Parsing Free Text: A Transformation-Based Approach', 'book': 'Human Language Technology Conference', 'score': 0.5671820190796741, 'type': 'unknown', 'authors': ['Eric Brill']}
acl-C00-1074 - {'url': 'https://aclweb.org/anthology/C00-1074', 'id': 'acl-C00-1074', 'abstract': ['A hybrid system for tagging part of speech is described that consists of a neuro tagger and a rule-based corrector.', 'The neuro tagger is an initial-state annotator that uses different lengths of contexts based on longest context priority.', 'its inputs are weighted by information gains that are obtained by information maximization.', 'The rule-based corrector is constructed by a set of transformation rules to make up for the shortcomings of the neuro tagger.', 'Computer experiments show that almost 20% of the errors made by the neuro tagger are corrected by these transformation rules, so that the hybrid system can reach an accuracy of 95.5% counting only flue ambiguous words and 99.1% counting all words when a small Thai corpus with 22,311 ambiguous words is used for training.', 'This accuracy is far higher than that using an 11MM and is also higher than that using a rule-based model.'], 'year': 2000, 'title': 'Hybrid Neuro and Rule-Based Part of Speech Taggers', 'book': 'International Conference on Computational Linguistics', 'score': 0.5018102050885659, 'type': 'unknown', 'authors': ['Qing Ma', 'Masaki Murata', 'Kiyotaka Uchimoto', 'Hitoshi Isahara']}
acl-P14-2010 - {'url': 'https://aclweb.org/anthology/P14-2010', 'id': 'acl-P14-2010', 'abstract': ['aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa References'], 'year': 2014, 'title': 'Sprinkling Topics for Weakly Supervised Text Classification', 'book': 'ACL', 'score': 0.9969348779647177, 'type': 'unknown', 'authors': ['Swapnil Hingmire', 'Sutanu Chakraborti']}
acl-P14-2015 - {'url': 'https://aclweb.org/anthology/P14-2015', 'id': 'acl-P14-2015', 'abstract': ['aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa aa aa aa aa aaaa aaa aaa aaa aaa aaa aaas aaaa aaa aaa aaa aaaa aaaa aaaa aaaaa aaaaa a aaaa a a a aaa a a a aaaa aa aaaa aaa aaaa aaaa References'], 'year': 2014, 'title': "Entities' Sentiment Relevance", 'book': 'ACL', 'score': 0.992911134267781, 'type': 'unknown', 'authors': ['Zvi Ben-Ami', 'Ronen Feldman', 'Binyamin Rosenfeld']}
acl-C10-3013 - {'url': 'https://aclweb.org/anthology/C10-3013', 'id': 'acl-C10-3013', 'abstract': ['Dorothée Beermann Pavel Mihaylov', 'Norwegian University of Science and Technology Ontotext', 'The system presented is a web application designed to aid linguistic research with data collection and online publishing.', 'It is a service mainly for linguists and language experts working with language description of less-documented and less-resourced languages.', 'When the central concern is in-depth linguistic analysis, maintaining and administering software can be a burden.', 'Cloud computing offers an alternative.', 'At present mainly used for archiving, we extend linguistic web applications to allow creation, search and storage of interlinear annotated texts.', 'By combining a conceptually appealing online glosser with an SQL database and a wiki, we make the online publication of linguistic data an easy task also for non-computationally oriented researchers.'], 'year': 2010, 'title': 'Cloud Computing for Linguists', 'book': 'COLING – Demos', 'score': 0.5370970028311163, 'type': 'unknown', 'authors': ['Dorothee Beermann', 'Pavel Mihaylov']}
acl-P02-1022 - {'url': 'https://aclweb.org/anthology/P02-1022', 'id': 'acl-P02-1022', 'abstract': ['In this paper we present GATE, a framework and graphical development environment which enables users to develop and deploy language engineering components and resources in a robust fashion.', 'The GATE architecture has enabled us not only to develop a number of successful applications for various language processing tasks (such as Information Extraction), but also to build and annotate corpora and carry out evaluations on the applications generated.', 'The framework can be used to develop applications and resources in multiple languages, based on its thorough Unicode support.'], 'year': 2002, 'title': 'GATE: A Framework and Graphical Development Environment for Robust NLP Tools and Applications', 'book': 'Annual Meeting of the Association for Computational Linguistics', 'score': 0.5053581577856099, 'type': 'unknown', 'authors': ['Hamish Cunningham', 'Diana Maynard', 'Kalina Bontcheva', 'Valentin Tablan']}
acl-W02-0112 - {'url': 'https://aclweb.org/anthology/W02-0112', 'id': 'acl-W02-0112', 'abstract': ['The paper gives a review of teaching Computational Linguistics (CL) at the University of Tartu.', 'The current curriculum foresees the possibility of studying CL as an independent 4-year subject in the Faculty of Philosophy on the bachelor stage.', 'In connection with the higher education reform in Estonia, new curricula will be introduced from the next study year where the 3-year bachelor stage will be followed by a 2 year master’s stage.', 'It will then be possible to study CL proceeding from two paths: in the Faculty of Philosophy, and additionally also in the Faculty of Mathematics and Computer Science.', 'This way two types of specialists will be trained who will hopefully be able to complement each other in teamwork.'], 'year': 2002, 'title': 'Teaching Computational Linguistics at the University of Tartu: Experience, Perspectives and Challenges', 'book': 'Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics', 'score': 0.6840342841769208, 'type': 'unknown', 'authors': ['Mare Koit', 'Tiit Roosmaa', 'Haldur Õim']}
acl-W02-0106 - {'url': 'https://aclweb.org/anthology/W02-0106', 'id': 'acl-W02-0106', 'abstract': ['The Language Technologies Institute (LTI) of the School of Computer Science at Carnegie Mellon University is one of the largest programs of its kind.', 'We present here the initial design and subsequent evolution of our MS and PhD programs in Language Technologies.', 'The motivations for the design and evolution are also presented.'], 'year': 2002, 'title': 'Design and Evolution of a Language Technologies Curriculum', 'book': 'Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics', 'score': 0.6798698036644222, 'type': 'unknown', 'authors': ['Robert E. Frederking', 'Eric H. Nyberg', 'Teruko Mitamura', 'Jaime G. Carbonnell']}
acl-A97-1056 - {'url': 'https://aclweb.org/anthology/A97-1056', 'id': 'acl-A97-1056', 'abstract': ['Statistical models of word–sense disambiguation are often based on a small number of contextual features or on a model that is assumed to characterize the interactions among a set of features.', 'Model selection is presented as an alternative to these approaches, where a sequential search of possible models is conducted in order to find the model that best characterizes the interactions among features.', 'This paper expands existing model selection methodology and presents the first comparative study of model selection search strategies and evaluation criteria when applied to the problem of building probabilistic classifiers for word–sense disambiguation.'], 'year': 1997, 'title': 'Sequential Model Selection for Word Sense Disambiguation', 'book': 'Applied Natural Language Processing Conference', 'score': 0.3537152848989457, 'type': 'unknown', 'authors': ['Ted Pedersen', 'Rebecca F. Bruce', 'Janyce Wiebe']}
acl-J14-3004 - {'url': 'https://aclweb.org/anthology/J14-3004', 'id': 'acl-J14-3004', 'abstract': ['Feature-Frequency?Adaptive On-line Training for Fast and Accurate Natural Language Processing Xu Sun?', 'Peking University Wenjie Li??', 'Hong Kong Polytechnic University Houfeng Wang?', 'Peking University Qin Lu?', 'Hong Kong Polytechnic University Training speed and accuracy are two major concerns of large-scale natural language processing systems.', 'Typically, we need to make a tradeoff between speed and accuracy.', 'It is trivial to improve the training speed via sacrificing accuracy or to improve the accuracy via sacrificing speed.', 'Nevertheless, it is nontrivial to improve the training speed and the accuracy at the same time, which is the target of this work.', 'To reach this target, we present a new training method, feature-frequency?adaptive on-line training, for fast and accurate training of natural language processing systems.', 'It is based on the core idea that higher frequency features should have a learning rate that decays faster.'], 'year': 2014, 'title': 'Feature-Frequency–Adaptive On-line Training for Fast and Accurate Natural Language Processing', 'book': 'CL', 'score': 0.6796828956026063, 'type': 'unknown', 'authors': ['Xu Sun', 'Wenjie Li', 'Houfeng Wang', 'Qin Lu']}
acl-P09-1054 - {'url': 'https://aclweb.org/anthology/P09-1054', 'id': 'acl-P09-1054', 'abstract': ['Stochastic Gradient Descent Training for Ll-regularized Log-linear Models with Cumulative Penalty', 'Yoshimasa Tsuruoka"^ Jun\'ichi Tsujii^* Sophia Ananiadou"^', 't School of Computer Science, University of Manchester, UK', 'Stochastic gradient descent (SGD) uses approximate gradients estimated from subsets of the training data and updates the parameters in an online fashion.', 'This learning framework is attractive because it often requires much less training time in practice than batch training algorithms.', 'However, L1-regularization, which is becoming popular in natural language processing because of its ability to produce compact models, cannot be efficiently applied in SGD training, due to the large dimensions of feature vectors and the fluctuations of approximate gradients.', 'We present a simple method to solve these problems by penalizing the weights according to cumulative values for L1 penalty.', 'We evaluate the effectiveness of our method in three applications: text chunking, named entity recognition, and part-of-speech tagging.', 'Experimental results demonstrate that our method can produce compact and accurate models much more quickly than a state-of-the-art quasiNewton method for L1-regularized loglinear models.'], 'year': 2009, 'title': 'Stochastic Gradient Descent Training for L1-regularized Log-linear Models with Cumulative Penalty', 'book': 'ACL-IJCNLP', 'score': 0.5636729215407645, 'type': 'unknown', 'authors': ['Yoshimasa Tsuruoka', "Jun'ichi Tsujii", 'Sophia Ananiadou']}
acl-W96-0210 - {'url': 'https://aclweb.org/anthology/W96-0210', 'id': 'acl-W96-0210', 'abstract': ['This paper describes measures for evaluating the three determinants of how well a probabilistic classifier performs on a given test set.', 'These determinants are the appropriateness, for the test set, of the results of (1) feature selection, (2) formulation of the parametric form of the model, and (3) parameter estimation.', 'These are part of any model formulation procedure, even if not broken out as separate steps, so the tradeoffs explored in this paper are relevant to a wide variety of methods.', 'The measures are demonstrated in a large experiment, in which they are used to analyze the results of roughly 300 classifiers that perform word-sense disambiguation.'], 'year': 1996, 'title': 'The Measure of a Model', 'book': 'Conference on Empirical Methods in Natural Language Processing', 'score': 0.3205780000583197, 'type': 'unknown', 'authors': ['Rebecca F. Bruce', 'Janyce Wiebe', 'Ted Pedersen']}
acl-J99-2002 - {'url': 'https://aclweb.org/anthology/J99-2002', 'id': 'acl-J99-2002', 'abstract': ['at Asheville In this paper, we describe a framework for developing probabilistic classifiers in natural language processing.', 'Our focus is on formulating models that capture the most important interdependencies among features, to avoid overfitting the data while also characterizing the data well.', 'The class of probability models and the associated inference techniques described here were developed in mathematical statistics, and are widely used in artificial intelligence and applied statistics.', 'Our goal is to make this model selection framework accessible to researchers in NLP, and provide pointers to available software and important references.', 'In addition, we describe how the quality of the three determinants of classifier performance (the features, the form of the model, and the parameter estimates) can be separately evaluated.', 'We also demonstrate the classification performance of these models in a large-scale experiment involving the disambiguation of 34 words taken from the HECTOR word sense corpus (Hanks 1996).', 'In 10-fold cross-validations, the model search procedure performs significantly better than naive Bayes on 6 of the words without being significantly worse on any of them.'], 'year': 1999, 'title': 'Decomposable Modeling in Natural Language Processing', 'book': 'Computational Linguistics', 'score': 0.26524946213472506, 'type': 'unknown', 'authors': ['Rebecca F. Bruce', 'Janyce Wiebe']}
acl-P09-2060 - {'url': 'https://aclweb.org/anthology/P09-2060', 'id': 'acl-P09-2060', 'abstract': ["We investigate the use of Fisher's exact significance test for pruning the translation table of a hierarchical phrase-based statistical machine translation system.", "In addition to the significance values computed by Fisher's exact test, we introduce compositional properties to classify phrase pairs of same significance values.", 'We also examine the impact of using significance values as a feature in translation models.', 'Experimental results show that 1% to 2% BLEU improvements can be achieved along with substantial model size reduction in an Iraqi/English two-way translation task.'], 'year': 2009, 'title': 'Toward Smaller, Faster, and Better Hierarchical Phrase-based SMT', 'book': 'ACL-IJCNLP: Short Papers', 'score': 0.37934743680956795, 'type': 'unknown', 'authors': ['Mei Yang', 'Jing Zheng']}
acl-C10-1056 - {'url': 'https://aclweb.org/anthology/C10-1056', 'id': 'acl-C10-1056', 'abstract': ['Fei Huang and Bing Xiang', 'IBM T. J. Watson Research Center', 'This paper proposes a new approach to phrase rescoring for statistical machine translation (SMT).', 'A set of novel features capturing the translingual equivalence between a source and a target phrase pair are introduced.', 'These features are combined with linear regression model and neural network to predict the quality score of the phrase translation pair.', "These phrase scores are used to dis-criminatively rescore the baseline MT system's phrase library: boost good phrase translations while prune bad ones.", 'This approach not only significantly improves machine translation quality, but also reduces the model size by a considerable margin.'], 'year': 2010, 'title': 'Feature-Rich Discriminative Phrase Rescoring for SMT', 'book': 'COLING', 'score': 0.3184529170048533, 'type': 'unknown', 'authors': ['Fei Huang', 'Bing Xiang']}
acl-D12-1088 - {'url': 'https://aclweb.org/anthology/D12-1088', 'id': 'acl-D12-1088', 'abstract': ['Phrase-based machine translation models have shown to yield better translations than Word-based models, since phrase pairs encode the contextual information that is needed for a more accurate translation.', 'However, many phrase pairs do not encode any relevant context, which means that the translation event encoded in that phrase pair is led by smaller translation events that are independent from each other, and can be found on smaller phrase pairs, with little or no loss in translation accuracy.', 'In this work, we propose a relative entropy model for translation models, that measures how likely a phrase pair encodes a translation event that is derivable using smaller translation events with similar probabilities.', 'This model is then applied to phrase table pruning.', 'Tests show that considerable amounts of phrase pairs can be excluded, without much impact on the translation quality.', 'In fact, we show that better translations can be obtained using our pruned models, due to the compression of the search space during decoding.'], 'year': 2012, 'title': 'Entropy-based Pruning for Phrase-based Machine Translation', 'book': 'EMNLP', 'score': 0.3114890936388646, 'type': 'unknown', 'authors': ['Wang Ling', 'João Graça', 'Isabel Trancoso', 'Alan Black']}
acl-I05-1051 - {'url': 'https://aclweb.org/anthology/I05-1051', 'id': 'acl-I05-1051', 'abstract': ["Hendra Setiawan', Haizhou Li,Min Zhang, and Beng Chin Ooi", 'Institute for Infocomm Research, 21 Heng Mui Keng Terrace, Singapore 119613 {stuhs, hli, mzhang}@i2r.a-star.edu.sg School of Computing, National University of Singapore, Singapore 117543 {hendrase, ooibc}@comp.nus.edu.sg', 'Abstract.', 'The merit of phrase-based statistical machine translation is often reduced by the complexity to construct it.', 'In this paper, we address some issues in phrase-based statistical machine translation, namely: the size of the phrase translation table, the use of underlying translation model probability and the length of the phrase unit.', 'We present Level-Of-Detail (LOD) approach, an agglomerative approach for learning phrase-level alignment.', 'Our experiments show that LOD approach significantly improves the performance of the word-based approach.', 'LOD demonstrates a clear advantage that the phrase translation table grows only sub-linearly over the maximum phrase length, while having a performance comparable to those of other phrase-based approaches.'], 'year': 2005, 'title': 'Phrase-Based Statistical Machine Translation: A Level of Detail Approach', 'book': 'Second International Joint Conference on Natural Language Processing: Full Papers', 'score': 0.3088836790750615, 'type': 'unknown', 'authors': ['Hendra Setiawan', 'Haizhou Li', 'Min Zhang', 'Beng Chin Ooi']}
acl-W02-0715 - {'url': 'https://aclweb.org/anthology/W02-0715', 'id': 'acl-W02-0715', 'abstract': ['by-one.', 'The actual plotted broken line is averaged over 10 random trials.', 'Figure 9 shows the relationship between iteration and the system’s TOEIC score.', 'In this figure, the horizontal axis represents the iteration, and the vertical axis TOEIC score.', 'The broken line and the solid line are plotted using the same denotation as that in Figure 8.', 'In Figure 8, the solid line always lies on a lower position than the broken line.', 'In Figure 9, from iteration 1 to around iteration 200, the broken line does not deviate from the actual system’s TOEIC score, which is 548.', 'Considering these results, the test set optimized for TDMT is shown to be applicable for evaluating ATR-MATRIX.'], 'year': 2002, 'title': 'Quality-Sensitive Test Set Selection for a Speech Translation System', 'book': 'Workshop on Speech-To-Speech Translation: Algorithms and Systems', 'score': 0.28448395218006456, 'type': 'unknown', 'authors': ['Fumiaki Sugaya', 'Keiji Yasuda', 'Toshiyuki Takezawa', 'Seiichi Yamamoto']}
acl-A92-1028 - {'url': 'https://aclweb.org/anthology/A92-1028', 'id': 'acl-A92-1028', 'abstract': ['A method of anaphoral resolution of zero pronouns in Japanese language texts using the verbal semantic attributes is suggested.', 'This method focuses attention on the semantic attributes of verbs and examines the context from the relationship between the semantic attributes of verbs governing zero pronouns and the semantic attributes of verbs governing their referents.', 'The semantic attributes of verbs are created using 2 different viewpoints: dynamic characteristics of verbs and the relationship of verbs to cases.', 'By using this method, it is shown that, in the case of translating newspaper articles, the major portion (93%) of anaphoral resolution of zero pronouns necessary for machine translation can be achieved by using only linguistic knowledge.', 'Factors to be given special attention when incorporating this method into a machine translation system are examined, together with suggested conditions for the detection of zero pronouns and methods for their conversion.', 'This study considers four factors that are important when implementing this method in a Japanese to English machine translation system: the difference in conception between Japanese and English expressions, the difference in case frame patterns between Japanese and English, restrictions by voice and restriction by translation structure.', 'Implementation of the proposed method with due consideration of these points leads to a viable method for anaphoral resolution of zero pronouns in a practical machine translation system.'], 'year': 1992, 'title': 'Zero Pronoun Resolution in a Machine Translation System by Using Japanese to English Verbal Semantic Attributes', 'book': 'Applied Natural Language Processing Conference', 'score': 0.7238661008796474, 'type': 'unknown', 'authors': ['Hiromi Nakaiwa', 'Satoru Ikehara']}
acl-C96-2146 - {'url': 'https://aclweb.org/anthology/C96-2146', 'id': 'acl-C96-2146', 'abstract': ['This paper describes a method for analyzing Japanese double-subject construction having an adjective predicate based on the valency structure.', 'A simple sentence usually has only one subjective case in most languages.', 'However, many Japanese adjectives (and some verbs) can dominate two surface subjective cases within a simple sentence.', 'Such sentence structure is called the double-subject construction.', 'This paper classifies the Japanese double-subject construction into four types and describes problems arising when analyzing these types using ordinary Japanese construction approaches.', 'This paper proposes a method for analyzing a Japanese double-subject construction having an adjective predicate in order to overcome the problems described.', 'By applying this method to Japanese sentence analysis in Japanese-to-English machine translation systems, translation accuracy can be improved because this method can analyze correctly the double-subject construction.'], 'year': 1996, 'title': 'Analyzing Japanese Double-Subject Construction Having an Adjective Predicate', 'book': 'International Conference on Computational Linguistics', 'score': 0.6614859634739717, 'type': 'unknown', 'authors': ['Masahiro Oku']}
acl-C04-1014 - {'url': 'https://aclweb.org/anthology/C04-1014', 'id': 'acl-C04-1014', 'abstract': ['Ngram models are simple in language modeling and have been successfully used in speech recognition and other tasks.', 'However, they can only capture the short distance context dependency within an n-words window where currently the largest practical n for a natural language is three while much of the context dependency in a natural language occurs beyond a three words window.', 'In order to incorporate this kind of long distance context dependency in the ngram model of our Mandarin speech recognition system, this paper proposes a novel MI-Ngram modeling approach.', 'This new MI-Ngram model consists of two components: a normal ngram model and a novel MI model.', 'The ngram model captures the short distance context dependency within an n-words window while the MI model captures the context dependency between the word pairs over a long distance by using the concept of mutual information.', 'That is, the MI-Ngram model incorporates the word occurrences beyond the scope of the normal ngram model.', 'It is found that MI-Ngram modeling has much better performance than the normal word ngram modeling.', 'Experimentation shows that about 20% of errors can be corrected by using a MI-Trigram model compared with the pure word trigram model.'], 'year': 2004, 'title': 'Modeling of Long Distance Context Dependency', 'book': 'International Conference on Computational Linguistics', 'score': 0.6569717581749753, 'type': 'unknown', 'authors': ['Guodong Zhou']}
acl-D09-1078 - {'url': 'https://aclweb.org/anthology/D09-1078', 'id': 'acl-D09-1078', 'abstract': ['Robert C. Moore Chris Quirk', 'The recent availability of large corpora for training N-gram language models has shown the utility of models of higher order than just trigrams.', 'In this paper, we investigate methods to control the increase in model size resulting from applying standard methods at higher orders.', 'We introduce significance-based N-gram selection, which not only reduces model size, but also improves perplexity for several smoothing methods, including Katz backoff and absolute discounting.', 'We also show that, when combined with a new smoothing method and a novel variant of weighted-difference pruning, our selection method performs better in the trade-off between model size and perplexity than the best pruning method we found for modified Kneser-Ney smoothing.'], 'year': 2009, 'title': 'Less is More: Significance-Based N-gram Selection for Smaller, Better Language Models', 'book': 'EMNLP', 'score': 0.6362706169150789, 'type': 'unknown', 'authors': ['Robert C. Moore', 'Chris Quirk']}
acl-P09-2008 - {'url': 'https://aclweb.org/anthology/P09-2008', 'id': 'acl-P09-2008', 'abstract': ["Han-Cheol Cho} Do-Gil Lee,§ Jung-Tae Lee,§ Pontus Stenetorp} Jun'ichi TsujiiWid Hae-Chang Rim§", 't Graduate School of Information Science and Technology, The University of Tokyo, Tokyo, Japan §Dept.', 'of Computer & Radio Communications Engineering, Korea University, Seoul, Korea {hccho,pontus,tsujii}@is.s.u-tokyo.ac.jp, {dglee,jtlee,rim}@nlp.korea.ac.kr', 'Most NLP applications work under the assumption that a user input is error-free; thus, word segmentation (WS) for written languages that use word boundary markers (WBMs), such as spaces, has been regarded as a trivial issue.', 'However, noisy real-world texts, such as blogs, e-mails, and SMS, may contain spacing errors that require correction before further processing may take place.', 'For the Korean language, many researchers have adopted a traditional WS approach, which eliminates all spaces in the user input and reinserts proper word boundaries.', 'Unfortunately, such an approach often exacerbates the word spacing quality for user input, which has few or no spacing errors; such is the case, because a perfect WS model does not exist.', 'In this paper, we propose a novel WS method that takes into consideration the initial word spacing information of the user input.', 'Our method generates a better output than the original user input, even if the user input has few spacing errors.', 'Moreover, the proposed method significantly outperforms a state-of-the-art Korean WS model when the user input initially contains less than 10% spacing errors, and performs comparably for cases containing more spacing errors.'], 'year': 2009, 'title': 'A Novel Word Segmentation Approach for Written Languages with Word Boundary Markers', 'book': 'ACL-IJCNLP: Short Papers', 'score': 0.2753588349223253, 'type': 'unknown', 'authors': ['Han-Cheol Cho', 'Do-Gil Lee', 'Jung-Tae Lee', 'Pontus Stenetorp', "Jun'ichi Tsujii", 'Hae-Chang Rim']}
acl-P07-2020 - {'url': 'https://aclweb.org/anthology/P07-2020', 'id': 'acl-P07-2020', 'abstract': ['Ensemble Document Clustering Using Weighted Hypergraph Generated by NMF', 'Hiroyuki Shinnou, Minoru Sasaki', 'In this paper, we propose a new ensemble document clustering method.', 'The novelty of our method is the use of Non-negative Matrix Factorization (NMF) in the generation phase and a weighted hypergraph in the integration phase.', 'In our experiment, we compared our method with some clustering methods.', 'Our method achieved the best results.'], 'year': 2007, 'title': 'Ensemble document clustering using weighted hypergraph generated by NMF', 'book': '45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions', 'score': 0.23119178840532434, 'type': 'unknown', 'authors': ['Hiroyuki Shinnou', 'Minoru Sasaki']}
acl-P09-1018 - {'url': 'https://aclweb.org/anthology/P09-1018', 'id': 'acl-P09-1018', 'abstract': ['Hua Wu and Haifeng Wang', 'Toshiba (China) Research and Development Center 5/F., Tower W2, Oriental Plaza, Beijing, 100738, China {wuhua, wanghaifeng}@rdc.toshiba.com.cn', 'This paper revisits the pivot language approach for machine translation.', 'First, we investigate three different methods for pivot translation.', 'Then we employ a hybrid method combining RBMT and SMT systems to fill up the data gap for pivot translation, where the source-pivot and pivot-target corpora are independent.', 'Experimental results on spoken language translation show that this hybrid method significantly improves the translation quality, which outperforms the method using a source-target corpus of the same size.', 'In addition, we propose a system combination approach to select better translations from those produced by various pivot translation methods.', 'This method regards system combination as a translation evaluation problem and formalizes it with a regression learning model.', 'Experimental results indicate that our method achieves consistent and significant improvement over individual translation outputs.'], 'year': 2009, 'title': 'Revisiting Pivot Language Approach for Machine Translation', 'book': 'ACL-IJCNLP', 'score': 0.5256681349989019, 'type': 'unknown', 'authors': ['Hua Wu', 'Haifeng Wang']}
acl-D14-1174 - {'url': 'https://aclweb.org/anthology/D14-1174', 'id': 'acl-D14-1174', 'abstract': ['Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1665?1675, October 25-29, 2014, Doha, Qatar.', 'Abstract To overcome the scarceness of bilingual corpora for some language pairs in machine translation, pivot-based SMT uses pivot language as a "bridge" to generate source-target translation from source-pivot and pivot-target translation.', 'One of the key issues is to estimate the probabilities for the generated phrase pairs.', 'In this paper, we present a novel approach to calculate the translation probability by pivoting the co-occurrence count of source-pivot and pivot-target phrase pairs.', 'Experimental results on Europarl data and web data show that our method leads to significant improvements over the baseline systems.'], 'year': 2014, 'title': 'Improving Pivot-Based Statistical Machine Translation by Pivoting the Co-occurrence Count of Phrase Pairs', 'book': 'EMNLP', 'score': 0.5239626365947524, 'type': 'unknown', 'authors': ['Xiaoning Zhu', 'Zhongjun He', 'Hua Wu', 'Conghui Zhu', 'Haifeng Wang', 'Tiejun Zhao']}
acl-W07-0724 - {'url': 'https://aclweb.org/anthology/W07-0724', 'id': 'acl-W07-0724', 'abstract': ["NRC's PORTAGE system for WMT 2007", 'Nicola Ueffing, Michel Simard, Samuel Larkin Howard Johnson', 'Interactive Language Technologies Group Interactive Information Group', 'National Research Council Canada', 'Gatineau, Quebec, Canada firstname.lastname@nrc.gc.ca', 'National Research Council Canada Ottawa, Ontario, Canada Howard.Johnson@nrc.gc.ca', 'We present the PORTAGE statistical machine translation system which participated in the shared task of the ACL 2007 Second Workshop on Statistical Machine Translation.', 'The focus of this description is on improvements which were incorporated into the system over the last year.', 'These include adapted language models, phrase table pruning, an IBMl-based decoder feature, and rescor-ing with posterior probabilities.'], 'year': 2007, 'title': "NRC's PORTAGE System for WMT 2007", 'book': 'Workshop on Statistical Machine Translation', 'score': 0.5167615615075047, 'type': 'unknown', 'authors': ['Nicola Ueffing', 'Michel Simard', 'Samuel Larkin', 'Howard Johnson']}
acl-D07-1030 - {'url': 'https://aclweb.org/anthology/D07-1030', 'id': 'acl-D07-1030', 'abstract': ['Xiaoguang Hu, Haifeng Wang, Hua Wu', 'Toshiba (China) Research and Development Center 5/F., Tower W2, Oriental Plaza No.1, East Chang An Ave., Dong Cheng District Beijing, 100738, China', '{huxiaoguang, wanghaifeng, wuhua}@rdc.toshiba.com.cn', 'This paper proposes a method using the existing Rule-based Machine Translation (RBMT) system as a black box to produce synthetic bilingual corpus, which will be used as training data for the Statistical Machine Translation (SMT) system.', 'We use the existing RBMT system to translate the monolingual corpus into synthetic bilingual corpus.', 'With the synthetic bilingual corpus, we can build an SMT system even if there is no real bilingual corpus.', 'In our experiments using BLEU as a metric, the system achieves a relative improvement of 11.7% over the best RBMT system that is used to produce the synthetic bilingual corpora.', 'We also interpolate the model trained on a real bilingual corpus and the models trained on the synthetic bilingual corpora.', 'The interpolated model achieves an absolute improvement of 0.0245 BLEU score (13.1% relative) as compared with the individual model trained on the real bilingual corpus.'], 'year': 2007, 'title': 'Using RBMT Systems to Produce Bilingual Corpus for SMT', 'book': '2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL)', 'score': 0.5141486557633063, 'type': 'unknown', 'authors': ['Xiaoguang Hu', 'Haifeng Wang', 'Hua Wu']}
Lambda = 0 -> No penalty
bound: 0.0
find max: 8.766674955461314
len: 1 55
bound: 8.766674955461314
find max: 15.545544081040203
len: 2 54
bound: 15.545544081040203
find max: 21.243251295732
len: 3 53
bound: 21.243251295732
find max: 26.155416091752745
len: 4 52
bound: 26.155416091752745
find max: 30.13596022712404
len: 5 51
bound: 30.13596022712404
find max: 33.75908932163567
len: 6 50
bound: 33.75908932163567
find max: 37.22093135047125
len: 7 49
bound: 37.22093135047125
find max: 40.06324742706998
len: 8 48
bound: 40.06324742706998
find max: 42.49313571433506
len: 9 47
bound: 42.49313571433506
find max: 44.64686685690346
len: 10 46
10
acl-W10-1703
acl-W10-1716
acl-P09-1018
acl-P08-2038
acl-D07-1030
acl-C04-1032
acl-W02-0106
acl-P02-1022
acl-C10-1056
acl-W02-0715
Lambda = 0.1
bound: 0.0
find max: 8.766674955461314
len: 1 55
bound: 8.766674955461314
find max: 15.448211228361746
len: 2 54
bound: 15.448211228361746
find max: 20.979029073527403
len: 3 53
bound: 20.979029073527403
find max: 25.675851573101752
len: 4 52
bound: 25.675851573101752
find max: 29.548662486296777
len: 5 51
bound: 29.548662486296777
find max: 32.95133410671496
len: 6 50
bound: 32.95133410671496
find max: 36.18815125389403
len: 7 49
bound: 36.18815125389403
find max: 38.85276492207583
len: 8 48
bound: 38.85276492207583
find max: 41.062010524606286
len: 9 47
bound: 41.062010524606286
find max: 43.01444540807584
len: 10 46
10
acl-W10-1703
acl-W10-1716
acl-P09-1018
acl-P08-2038
acl-D07-1030
acl-W02-0106
acl-C04-1032
acl-P02-1022
acl-C10-1056
acl-D09-1078
Lambda = 0.3
bound: 0.0
find max: 8.766674955461314
len: 1 55
bound: 8.766674955461314
find max: 15.253545523004831
len: 2 54
bound: 15.253545523004831
find max: 20.45058462911821
len: 3 53
bound: 20.45058462911821
find max: 24.71672253579977
len: 4 52
bound: 24.71672253579977
find max: 28.374067004642246
len: 5 51
bound: 28.374067004642246
find max: 31.65839780822564
len: 6 50
bound: 31.65839780822564
find max: 34.30108788200133
len: 7 49
bound: 34.30108788200133
find max: 36.610296733349244
len: 8 48
bound: 36.610296733349244
find max: 38.66245426529192
len: 9 47
bound: 38.66245426529192
find max: 40.374764610219046
len: 10 46
10
acl-W10-1703
acl-W10-1716
acl-P09-1018
acl-P08-2038
acl-D07-1030
acl-W02-0106
acl-A97-1056
acl-P02-1022
acl-D14-1174
acl-J14-3004
Lambda = 0.6
bound: 0.0
find max: 8.766674955461314
len: 1 55
bound: 8.766674955461314
find max: 14.96154696496946
len: 2 54
bound: 14.96154696496946
find max: 19.675679516324358
len: 3 53
bound: 19.675679516324358
find max: 23.412986423181188
len: 4 52
bound: 23.412986423181188
find max: 26.947423988572275
len: 5 51
bound: 26.947423988572275
find max: 29.718993360491663
len: 6 50
bound: 29.718993360491663
find max: 31.931418591454484
len: 7 49
bound: 31.931418591454484
find max: 33.72449606396059
len: 8 48
bound: 33.72449606396059
find max: 35.15234140197067
len: 9 47
bound: 35.15234140197067
find max: 36.34812059986596
len: 10 46
10
acl-W10-1703
acl-W10-1716
acl-P08-2038
acl-D07-1030
acl-W02-0106
acl-P09-1018
acl-A97-1056
acl-C00-1074
acl-J14-3004
acl-C04-1045
Lambda = 1.0
bound: 0.0
find max: 8.766674955461314
len: 1 55
bound: 8.766674955461314
find max: 14.572215554255628
len: 2 54
bound: 14.572215554255628
find max: 18.69848430332808
len: 3 53
bound: 18.69848430332808
find max: 22.105648847615612
len: 4 52
bound: 22.105648847615612
find max: 25.510309276102088
len: 5 51
bound: 25.510309276102088
find max: 27.762343850812776
len: 6 50
bound: 27.762343850812776
find max: 29.553197261431578
len: 7 49
bound: 29.553197261431578
find max: 31.075679603268732
len: 8 48
bound: 31.075679603268732
find max: 32.07708681052135
len: 9 47
bound: 32.07708681052135
find max: 32.77229491658016
len: 10 46
10
acl-W10-1703
acl-W10-1716
acl-P08-2038
acl-D07-1030
acl-W02-0106
acl-J14-3004
acl-A97-1056
acl-D14-1174
acl-H93-1047
acl-C08-2019
Lambda = 2.0
bound: 0.0
find max: 8.766674955461314
len: 1 55
bound: 8.766674955461314
find max: 13.789414821900042
len: 2 54
bound: 13.789414821900042
find max: 17.113229040843937
len: 3 53
bound: 17.113229040843937
find max: 19.715234270375973
len: 4 52
bound: 19.715234270375973
find max: 21.799526143515898
len: 5 51
bound: 21.799526143515898
find max: 23.874925737908402
len: 6 50
bound: 23.874925737908402
find max: 25.174995753322595
len: 7 49
bound: 25.174995753322595
find max: 25.796355757231975
len: 8 48
bound: 25.796355757231975
find max: 26.275309003403812
len: 9 47
bound: 26.275309003403812
find max: 26.621772525986678
len: 10 46
10
acl-W10-1703
acl-W11-2158
acl-J14-3004
acl-A97-1056
acl-P01-1057
acl-I05-1051
acl-C94-2175
acl-I08-1042
acl-P09-2060
acl-C04-1014
